[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "",
    "text": "Syllabus",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#land-acknowledgement",
    "href": "index.html#land-acknowledgement",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Land Acknowledgement",
    "text": "Land Acknowledgement\nAlthough our students come from many locations around the world, we wish to recognize the land on which the University of Toronto was built. This land has historically been and still is the home of the Huron-Wendat, the Seneca, and the Mississaugas of the Credit River.\nThere is a First Nations House for Indigenous Student Services on campus. Please refer to their web page for more resources and information about honouring our land and their services for students.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#diversity-and-inclusion-statement",
    "href": "index.html#diversity-and-inclusion-statement",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Diversity and inclusion statement",
    "text": "Diversity and inclusion statement\nAs students, you all have something unique and special to offer to science. It is our intent that students from all backgrounds and perspectives be well served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that students bring to this class be recognized as a resource, strength, and benefit.\nDiversity can refer to multiple ways that we identify ourselves, including but not limited to race, national origin, language, cultural heritage, physical ability, neurodiversity, age, sexual orientation, gender identity, religion, and socio-economic class. Each of these varied, and often intersecting, identities, along with many others not mentioned here, shape the perspectives we bring to this class, to this department, and to the greater EEB community. We will work to promote diversity, equity, and inclusion not only because diversity fuels excellence and innovation, but because we want to pursue justice.\nWe expect that everybody in this class will respect each other, and demonstrate diligence in understanding how other people’s perspectives, behaviors, and worldviews may be different from their own. Racist, sexist, colonialist, homophobic, transphobic, and other abusive and discriminatory behavior and language will not be tolerated in this class and will result in disciplinary action, such as removal from class session or revocation of group working privileges. Please consult the University of Toronto Code of Student Conduct for details on unacceptable conduct and possible sanctions.\nPlease let us know if something said or done in this class, by either a member of the teaching team or other students, is particularly troubling or causes discomfort or offense. While our intention may not be to cause discomfort or offense, the impact of what happens throughout the course is not to be ignored and is something that we consider to be very important and deserving of attention. If and when this occurs, there are several ways to alleviate some of the discomfort or hurt you may experience:\n\nDiscuss the situation privately with a member of the teaching team. We are always open to listening to students’ experiences, and want to work with students to find acceptable ways to process and address the issue.\nNotify us of the issue through another source such as a trusted faculty member or a peer. If for any reason you do not feel comfortable discussing the issue directly with us, we encourage you to seek out another, more comfortable avenue to address the issue.\nContact the Anti-Racism and Cultural Diversity Office to report an incident and receive complaint resolution support, which may include consultations and referrals.\n\nWe acknowledge our imperfections while we also fully commit to the work, inside and outside of our classrooms, of building and sustaining a community that increasingly embraces these core values. Your suggestions and feedback are encouraged and appreciated. Please let us know ways to improve the effectiveness of the course for you personally or for other students or student groups.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#wellness-statement",
    "href": "index.html#wellness-statement",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Wellness statement",
    "text": "Wellness statement\nWe on the teaching team value your health and wellness. In order to succeed in this class, in university, and beyond, you must balance your work with rest, exercise, and attention to your mental and physical health. Working until exhaustion is NOT a badge of honor. If you are finding it difficult to balance your health and well-being with your work in this class, please do not hesitate to let us know. We are happy to help connect you with resources and services on campus and also to make accommodations to our course plan as needed. Our inboxes are always open, and we are also available for virtual chats by appointment. You have our support, and we believe in you.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Course Overview",
    "text": "Course Overview\nThis course covers statistical and data analysis, reproducible quantitative methods, and scientific computing in R to answer questions in ecology and evolutionary biology. Statistical and data analysis, modeling, and computing are essential skills for all biologists. This course is designed to meet a growing demand for reproducible, openly accessible, analytically thorough, and well-documented science. Students will learn to analyze and visualize data, develop mathematical models, and document their research using the R programming language. No programming experience is required.\nPrerequisites: BIO220H1 and one of EEB225H1, STA288H1, or STA220H1",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#time",
    "href": "index.html#time",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Time",
    "text": "Time\nTuesdays 2:10 - 4:00 PM and Thursdays 12:10 - 2:00 PM EST.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#class-locations",
    "href": "index.html#class-locations",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Class Locations",
    "text": "Class Locations\nAll classes will be on the St. George campus. On Tuesdays, we will be in 371 Bloor, Room 36. On Thursdays, we will be in Sidney Smith Hall, Room 561.\n\n\n\nOffice hours (in EST)\n\n\n\n\n\n\nMete\nTu 12-1pm\nESC3044\n\n\nZoë\nWed 2-3pm\nESC3044\n\n\nJessie\nTu 4-5pm\n371 Bloor, Room 36\n\n\nGavia\nTh 2-3pm\nESC3044",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#contact-protocol",
    "href": "index.html#contact-protocol",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Contact protocol",
    "text": "Contact protocol\nIf you have assignment-related questions, please email Gavia AND Jessie. If you have course-related or lecture-related questions, please email Mete AND Zoë. Include “EEB313” in the subject line. If you do not receive a reply within 48 hours (excluding weekends), please send us a reminder.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#absence-policy",
    "href": "index.html#absence-policy",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Absence policy",
    "text": "Absence policy\nIf you are feeling unwell, please do not come to class. Instead, take the time to recover fully. Please let us know if you are feeling sick - you will not be penalized for missing a lecture, and we will do our best to ensure that you are up-to-date with class materials when you return.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#accessibility-needs",
    "href": "index.html#accessibility-needs",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Accessibility needs",
    "text": "Accessibility needs\nIf you require accommodations for a disability, or have any accessibility concerns about the course or course materials, please notify your course instructors (Mete and Zoë), or contact Accessibility Services, as soon as possible regarding accommodations.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#course-learning-outcomes",
    "href": "index.html#course-learning-outcomes",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Course learning outcomes",
    "text": "Course learning outcomes\n\nDevelop proficiency in the programming language R.\nUse R to apply appropriate statistical tools to analyze and interpret data.\nDevelop familiarity with mathematical models used in EEB.\nDevelop familiarity with the command line and Git.\nLearn and use techniques and best practices for reproducible, high-quality science.\nLearn how to work as part of a research team to produce a scientific product.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#lecture-schedule",
    "href": "index.html#lecture-schedule",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Lecture schedule",
    "text": "Lecture schedule\n\n\n\nWeek\nDate\nTopic\nInstructor\n\n\n\n\n1\nSep 3\nIntro to course\nEveryone\n\n\n1\nSep 5\nBase R: assignment, vectors, functions, strings, loops, etc.\nZoë\n\n\n2\nSep 10\nIntro to command line and GitHub\nMete\n\n\n2\nSep 12\nData wrangling!\nZoë\n\n\n3\nSep 17\nData visualization in ggplot\nZoë\n\n\n4\nSep 19\nExploratory data analysis\nZoë\n\n\n4\nSep 24\nReview using Farrell & Davies (2019) dataset\n\n\n\n4\nSep 26\nProject work\n\n\n\n5\nOct 01\nIntroduction to statistical inference I\nMete\n\n\n5\nOct 03\nIntroduction to statistical inference II\nMete\n\n\n6\nOct 08\nLinear models I\nMete\n\n\n6\nOct 10\nLinear models II\nMete\n\n\n7\nOct 15\nModel selection\nJessie\n\n\n7\nOct 17\nMultivariate statistics\nGavia\n\n\n8\nOct 22\nMathematical models in ecology and evolution I\nMete\n\n\n8\nOct 24\nMathematical models in ecology and evolution II\nMete\n\n\n9\nOct 29\nReading break\n-\n\n\n9\nOct 31\nReading break\n-\n\n\n10\nNov 05\nProject work\n\n\n\n10\nNov 07\nProject work\n\n\n\n11\nNov 12\nProject work\n\n\n\n11\nNov 14\nProject work\n\n\n\n12\nNov 19\nProject work\n\n\n\n12\nNov 21\nProject work\n\n\n\n13\nNov 26\nProject work\n\n\n\n13\nNov 28\nProject work\n\n\n\n14\nDec 03\nGroup presentations\nEveryone",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#assessment-schedule",
    "href": "index.html#assessment-schedule",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Assessment schedule",
    "text": "Assessment schedule\n\n\n\n\n\n\n\n\n\n\nAssignment\nType\nSubmitted on\nDue date\nMarks\n\n\n\n\nIndividual interest description\nIndividual\nQuercus\nSep 12\n1\n\n\nBasic R, command line, & Git\nIndividual\nQuercus\nSep 19\n8\n\n\nData wrangling\nIndividual\nQuercus\nSep 26\n8\n\n\nData visualization and exploration\nIndividual\nQuercus\nOct 03\n8\n\n\nIntroduction to inference\nIndividual\nQuercus\nOct 15\n8\n\n\nProject proposal\nGroup\nGitHub\nOct 17\n3\n\n\nLMs, model selection, and multivariate\nIndividual\nQuercus\nOct 24\n8\n\n\nChallenge assignment\nIndividual\nQuercus\nNov 17\n20\n\n\nMid-project update\nGroup\nGitHub\nNov 21\n6\n\n\nPresentation\nGroup\nIn-class\nDec 03\n10\n\n\nFinal report\nGroup\nGitHub\nDec 10\n20\n\n\n\nThere are 100 marks in total. Your final course mark will be the sum of your assignment scores, which will be translated to a letter grade according to the official grading scale of the Faculty of Arts and Science.\nAssignments will be distributed and submitted in the R Markdown format via Quercus. Assignments will typically be handed out on Thursdays after class and are due at 8:00 PM on the following Thursday.\nThe Challenge Assignment is equivalent to a take home exam. The format will be the same as the other assignments, but this assignment is designed challenge you to go a little beyond what was taught in class. It will be distributed on 9:00 AM on Nov 11, and it will be due 11:59 PM on Nov 17. Students are welcome to work in a group on this assignment, but each student must submit their own original work. No extensions will be granted on this assignment except under the same extra-ordinary circumstances akin to those under which an exam might be deferred. We only expect you to do your best!\nPer our stance on supporting student’s mental health, we are happy to accommodate a 72-hour extension for one of the assignments, no questions asked. Otherwise, except under extenuating circumstances, there will be a penalty of 5% per day (including weekends) for all late submissions. If you foresee needing an extension, please email Zoë AND Mete as soon as possible. This policy does not apply to the Challenge Assignment, Presentation, or Final Report.\nAll submissions to Quercus/GitHub must be submitted as PDFs (i.e., knitted).",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#improving-your-writing-skills",
    "href": "index.html#improving-your-writing-skills",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Improving your writing skills",
    "text": "Improving your writing skills\nEffective communication is crucial in science. The University of Toronto provides services to help you improve your writing, from general advices on effective writing to writing centers and writing courses. The Faculty of Arts & Science also offers an English Language Learning (ELL) program, which provides free individualized instruction in English skills. Take advantage of these!",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#academic-integrity",
    "href": "index.html#academic-integrity",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Academic integrity",
    "text": "Academic integrity\nYou should be aware of the University of Toronto Code of Behaviour on Academic Matters. Also see How Not to Plagiarize. Notably, it is NOT appropriate to use large sections from internet sources, and inserting a few words here and there does not make it an original piece of writing. Be careful in using internet sources – most online material are not reviewed and there are many errors out there. Make sure you read material from many sources (published, peer-reviewed, trusted internet sources) and that you write an original text using this information. Always cite your sources. In case of doubt about plagiarism, talk to your instructors and TAs. Please make sure that what you submit for the final project does not overlap with what you submit for other classes, such as the 4th-year research project.\n\nOn the use of generative AI\nWe recognize that students use generative artificial intelligence tools such as ChatGPT. If you use such tools in this course, we ask that you let us know. Given the limitations of these tools, and the fact we will be available to support your learning without the use of AI, we would recommend that you be very cautious when using generative AI. On any submissions where, e.g., ChatGPT, is used please indicate in the answer to the question how the tool was used.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#fas-student-engagement-programs",
    "href": "index.html#fas-student-engagement-programs",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "FAS student engagement programs",
    "text": "FAS student engagement programs\nThere are a few programs on campus aimed at increasing student engagement with their coursework and keeping them socially connected. Recognized Study Groups are voluntary, peer-led study groups of up to 8 students enrolled in the same course. Meet to Complete are online drop-in study sessions for A&S undergrads. These are worth checking out if you are interested in participating in a study group.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "about-us.html",
    "href": "about-us.html",
    "title": "2024 teaching team",
    "section": "",
    "text": "Course instructors\nMete is a 3rd year PhD student in the Dept. Ecology and Evolutionary Biology at UTSG, and is co-advised by Matt Osmond & Nicole Mideo. He uses mathematical models to address questions in evolutionary genetics and ecology. He is currently developing theory to understand patterns, causes, and consequences of recombination rate variation. Mete was an undergraduate math & statistics student at the University of Idaho. There, he worked on the ecology of gene drive interventions against vectored diseases, understanding how continuous spatial structure can affect species coexistence, and forecasting the dynamics of Chinook salmon in the Willamette River system. He loves R - and teaching this course! Outside of science, Mete enjoys listening to podcasts, cooking, and cycling between Toronto neighborhoods in search of coffee.\nZoë is a PhD student in the Wright and Barrett labs at UTSG. She studies the genome of a weedy plant to better understand how transposable elements affect sex chromosome evolution. She taught herself how to use R during her honours thesis and fell in love because it was much kinder than Lisp or C++ and, most importantly, because aesthetics. Zoë makes plots from aggressively large genomic data sets and spends a lot of time literally bash-ing her data into a file small enough for her computer to load into RStudio. For Zoë, the best way to work in R is while patting her puppies.",
    "crumbs": [
      "2024 teaching team"
    ]
  },
  {
    "objectID": "about-us.html#course-instructors",
    "href": "about-us.html#course-instructors",
    "title": "2024 teaching team",
    "section": "",
    "text": "Mete Yuksel (mete.yuksel@mail.utoronto.ca)\n\n\n\n\n\n\nZoe Humphries (zoe.humphries@mail.utoronto.ca)",
    "crumbs": [
      "2024 teaching team"
    ]
  },
  {
    "objectID": "about-us.html#teaching-assistants",
    "href": "about-us.html#teaching-assistants",
    "title": "2024 teaching team",
    "section": "Teaching assistants",
    "text": "Teaching assistants\n\n\n\nGavia Lertzman-Lepofsky (gavia.lertzmanlepofsky@mail.utoronto.ca)\n\n\nGavia is a PhD candidate in the Mahler lab on the StG campus. She works with the tropical Anolis lizards to understand how macroevolution shapes local patterns of community diversity and structure. When she’s not wandering around Caribbean islands catching lizards or spraying them with paint, she is happily ensconced on her couch working in R or exploring the many bakeries and bookstores of Toronto.\n\n\n\nJessie Wang (jae.wang@mail.utoronto.ca)\n\n\nJessie is a PhD student in the Frederickson lab at UTSG. She studies plant-microbe interactions using high- throughput experimentation in duckweeds. She fell in love with R during her time as an undergraduate and took EEB313 in 2020, simultaneously sharpening her coding skills while conducting research alone in the lab. Jessie loves to spend too much money on fancy coffee as she types away, making sure her code is well-annotated and her figures look beautiful. Outside of work, she enjoys caring for her many houseplants and aquariums, finding new delicious eats, and admiring other people’s pets.",
    "crumbs": [
      "2024 teaching team"
    ]
  },
  {
    "objectID": "downloadingR.html",
    "href": "downloadingR.html",
    "title": "Installing R & Ubuntu",
    "section": "",
    "text": "Introduction\nThis course makes use of R and RStudio, as well as the command line. The goal of these notes is to help you install R and Rstudio and, if you use a Windows machine, to install Ubuntu. If you have any issues, let’s resolve them in advance so that you can make the most of our time together! The goal here is to ensure that your computer is set up for the rest of the course. If you can’t complete these steps, please email Mete and Zoë as soon as possible.",
    "crumbs": [
      "Installing R & Ubuntu"
    ]
  },
  {
    "objectID": "downloadingR.html#installing-r-rstudio",
    "href": "downloadingR.html#installing-r-rstudio",
    "title": "Installing R & Ubuntu",
    "section": "Installing R & RStudio",
    "text": "Installing R & RStudio\n\nDownload R, a free software environment for statistical computing and graphics from CRAN, the Comprehensive R Archive Network. We recommend you install a precompiled binary distribution for your operating system – use the links up at the top of the CRAN page!\n\nNote: MacBook users with an Apple Silicon chip (e.g., M1 or M2) should install the “arm64” version of R, while MacBook users with an Intel chip should install the regular (64-bit) version of R. You can check your laptop’s hardware specifications by clicking the Apple icon (top left corner) \\&gt; About This Mac.\n\nInstall RStudio, a graphical user interface (GUI) for R. Click the link under “2: Install RStudio”. RStudio comes with a text editor, so there is no immediate need to install a stand-alone editor.\n\nIf R is already installed, ensure that the R version is 4.0 or higher. You can do this by opening RStudio, where you should see a multi-section window like below. Locate the quadrant named “Console”, and put your cursor at the start of the prompt indicated by the &gt; symbol. Type sessionInfo() - make sure that only the I at the start of Info is capitalized and you are including the round brackets. Press enter to run this command and R should return an output to you. The first line shows what version of R is installed. Ensure that the R version installed is at least 4.0.",
    "crumbs": [
      "Installing R & Ubuntu"
    ]
  },
  {
    "objectID": "downloadingR.html#installing-r-packages",
    "href": "downloadingR.html#installing-r-packages",
    "title": "Installing R & Ubuntu",
    "section": "Installing R packages",
    "text": "Installing R packages\n\nTinyTex\nThere is one package we have to install first before we can create PDF reports, which will be necessary for assignments and the project. Copy and paste into the console (where the \\&gt; symbol is) the two lines of code below to install a package called tinytex.\n\ninstall.packages(\"tinytex\") \ntinytex::install_tinytex()\n\n\n\nTidyverse\n\nCopy and paste the below code into your console.\n\n\ninstall.packages(c(\"tidyverse\", \"data.table\"), dependencies = TRUE)\n\nDuring installation, if you ever get the below message, click “No”.\n\nIf you get the message “Do you want to install from sources the packages which need compilation? (Yes/no/cancel)” in the Console, type “Yes” and press enter.\n\nCheck that the tinytex and tidyverse packages have been installed correctly. To do this, go to the bottom right pane and click the tab for “Packages”. If you can search for and find the below packages, then they have been installed! They do not need to be checked off. Alternatively, go to the Console and type library(tidyverse) to verify that the package is installed. An error along the lines “there is no package called tidyverse” will be returned if the package is not installed.",
    "crumbs": [
      "Installing R & Ubuntu"
    ]
  },
  {
    "objectID": "downloadingR.html#installing-ubuntu",
    "href": "downloadingR.html#installing-ubuntu",
    "title": "Installing R & Ubuntu",
    "section": "Installing Ubuntu",
    "text": "Installing Ubuntu\nIf you are a Windows user, you will need to install Ubuntu before the command line and GitHub lecture. (If you are a Mac user, it is safe to stop here.) The steps to install Unbuntu are as follows. Do not attempt to follow these instructions while your computer is plugged into an electrical outlet.\n\nSearch for “Turn Windows features on or off” in the Windows search bar and ensure that “Windows Subsystem for Linux” is turned on. This will force your machine to restart.\nDownload Ubuntu from this link.\nOpen the app once installed. The app will say it is installing and, once finished, will prompt you to make a username and password. (The password won’t show up, but the keystrokes are being recognized. You will be asked to confirm the password, too.)\n\nImportantly, you can open Ubuntu via Command Prompt by typing ubuntu. Here is a picture illustrating how to do this:\n\nAfter following these steps, you can check everything has been installed correctly by going to File Explorer and verifying that there is a Linux tab – scroll all the way down and look for a penguin!",
    "crumbs": [
      "Installing R & Ubuntu"
    ]
  },
  {
    "objectID": "lectures/lec01-r_and_rstudio.html",
    "href": "lectures/lec01-r_and_rstudio.html",
    "title": "1  Introduction to the course",
    "section": "",
    "text": "1.1 Introduction to R\nR is a computing environment that combines numerical analysis tools for linear algebra; functions for classical and modern statistical analysis; and functions for graphics and data visualization. It is based on the programming language S, developed by John Chambers in the 1970s.\nThere are two ways to start R:\nWe will use the graphical user interface RStudio throughout this course. Although the GUI makes certain things easier, it is not necessary to use it when running an R script. For example, running the following code in R returns the sum of the numbers 1 and 2.\n1+2\n\n[1] 3\nRunning the code with a # at the beginning of the line results in the line being read as a comment. This means that the calculation which is specified in the line is not processed and the output not returned. Comments are a useful way to keep track of what line(s) of code do, multiple versions of the same code, etc.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the course</span>"
    ]
  },
  {
    "objectID": "lectures/lec01-r_and_rstudio.html#introduction-to-r",
    "href": "lectures/lec01-r_and_rstudio.html#introduction-to-r",
    "title": "1  Introduction to the course",
    "section": "",
    "text": "Run R on the command line. On a Mac, you would do this in the Terminal application. On a Windows machine, you would do this using, e.g., Ubuntu. We will cover the command line next week.\nClick the R icon on your desktop, assuming the software has already been installed.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the course</span>"
    ]
  },
  {
    "objectID": "lectures/lec01-r_and_rstudio.html#rstudio-and-the-r-notebook",
    "href": "lectures/lec01-r_and_rstudio.html#rstudio-and-the-r-notebook",
    "title": "1  Introduction to the course",
    "section": "1.2 RStudio and the R Notebook",
    "text": "1.2 RStudio and the R Notebook\nRStudio includes the R console, but also many other convenient functionalities, which makes it easier to get started and to work with R. When you launch RStudio, you will see four panels. Starting at the top left and going clockwise, these panels are:\n\nThe text editor panel. This is where we can write scripts, i.e. putting several commands of code together and saving them as a text document so that they are accessible for later and so that we can execute them all at once by running the script instead of typing them in one by one.\nThe environment panel, which shows us all the files and objects we currently loaded into R.\nThe files-plots-help panel. This panel shows the files in the current directory (the folder we are working out of), any plots we make later, and also documentation for various packages and functions. Here, the documentation is formatted in a way that is easier to read and also provides links to the related sections.\nThe console is another space we can input code, only now the code is executed immediately and doesn’t get saved at the end.\n\nTo change the appearance of your RStudio, navigate to Tools &gt; Global Options &gt; Appearance. You can change the font and size, and the editor theme. The default is “Textmate”, but if you like dark mode, a good option is “Tomorrow Night Bright”. You can also change how your panels are organized.\nIn the RStudio interface, we will be writing code in a format called the R Notebook. As the name entails, this interface works like a notebook for code, as it allows us to save notes about what the code is doing, the code itself, and any output we get, such as plots and tables, all together in the same document.\nWhen we are in the Notebook, the text we write is normal plain text, just as if we would be writing it in a text document. If we want to execute some R code, we need to insert a code chunk.\nYou insert a code chunk by either clicking the “Insert” button or pressing Ctrl/Command + Alt + i simultaneously. You could also type out the surrounding backticks, but this would take longer. To run a code chunk, you press the green arrow, or Ctrl/Command + Shift + Enter.\n\n1+2\n\n[1] 3\n\n\nAs you can see, the output appears right under the code block.\nThis is a great way to perform explore your data, since you can do your analysis and write comments and conclusions right under it all in the same document. A powerful feature of this workflow is that there is no extra time needed for code documentation and note-taking, since you’re doing your analyses and taking notes at the same time. This makes it great for both taking notes at lectures and to have as a reference when you return to your code in the future.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the course</span>"
    ]
  },
  {
    "objectID": "lectures/lec01-r_and_rstudio.html#r-markdown",
    "href": "lectures/lec01-r_and_rstudio.html#r-markdown",
    "title": "1  Introduction to the course",
    "section": "1.3 R Markdown",
    "text": "1.3 R Markdown\nThe text format we are using in the R Notebook is called R Markdown. This format allows us to combine R code with the Markdown text format, which enables the use of certain characters to specify headings, bullet points, quotations and even citations. A simple example of how to write in Markdown is to use a single asterisk or underscore to emphasize text (*emphasis*) and two asterisks or underscores to strongly emphasize text (**strong emphasis**). When we convert our R Markdown text to other file formats, these will show up as italics and bold typeface, respectively. If you have used WhatsApp, you might already be familiar with this style of writing. In case you haven’t seen it before, you have just learned something about WhatsApp in your quantitative methods class…\nTo learn more about R Markdown, check out this reference.\n\n1.3.1 Saving data and generating reports\nTo save our notes, code, and graphs, all we have to do is to save the R Notebook file, and the we can open it in RStudio next time again. However, if we want someone else to look at this, we can’t always just send them the R Notebook file, because they might not have RStudio installed. Another great feature of R Notebooks is that it is really easy to export them to HTML, Microsoft Word, or PDF documents with figures and professional typesetting. There are actually many academic papers that are written entirely in this format and it is great for assignments and reports. (You might even use it to communicate with your collaborators!) Since R Notebook files convert to HTML, it is also easy to publish simple and good-looking websites in it, in which code chunks are embedded nicely within the text.\nLet’s try to create a document in R.\nFirst, let’s set up the YAML block. This is found at the top of your document, and it is where you specify the title of your document, what kind of output you want, etc.\n\n---\ntitle: \"Your title here\"\nauthor: \"Your name here\"\ndate: \"Insert date\"\noutput:\n  pdf_document: default\n---\n\nNext, let’s type code to perform the calculation we did above:\n\n1+2\n\n[1] 3\n\n\nTo create the output document, we say that we “knit” our R Markdown file into, e.g., a PDF. Simply press the Knit button here and the new document will be created.\nAs you can see in the knitted document, the title showed up as we would expect, and lines with pound sign(s) in front of them were converted into headers. Most importantly, we can see both the code and its output! Plots are generated directly in the report without us having to cut and paste images! If we change something in the code, we don’t have to find the new images and paste it in again, the correct one will appear right in your code.\nWhen you quit, R will ask you if you want to save the workspace (that is, all of the variables you have defined in this session); in general, you should say “no” to avoid clutter and unintentional confusion of results from different sessions. Note: When you say “yes” to saving your workspace, it is saved in a hidden file named .RData. By default, when you open a new R session in the same directory, this workspace is loaded and a message informing you so is printed: [Previously saved workspace restored]. It is often best practice to turn this feature off completely.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the course</span>"
    ]
  },
  {
    "objectID": "lectures/lec01-r_and_rstudio.html#practice-knitting",
    "href": "lectures/lec01-r_and_rstudio.html#practice-knitting",
    "title": "1  Introduction to the course",
    "section": "1.4 Practice knitting!",
    "text": "1.4 Practice knitting!\nFor your first assignment, you will need to write a short description of your interests in EEB and submitted the knitted document on Quercus. This assignment is due September 12. If you are having trouble knitting, which you will have to do throughout the course, come find us and we will help you troubleshoot.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the course</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html",
    "href": "lectures/lec02-basic-r.html",
    "title": "2  Introduction to R: assignment, vectors, functions, strings, loops",
    "section": "",
    "text": "2.1 Lesson Preamble",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: assignment, vectors, functions, strings, loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#lesson-preamble",
    "href": "lectures/lec02-basic-r.html#lesson-preamble",
    "title": "2  Introduction to R: assignment, vectors, functions, strings, loops",
    "section": "",
    "text": "2.1.1 Learning Objectives\n\nDefine the following terms as they relate to R: call, function, arguments, options.\nUse comments within code blocks.\nDo simple arithmetic operations in R using values and objects.\nCall functions and use arguments to change their default options.\nDefine our own functions\nInspect the content of vectors and manipulate their content.\nCreate for-loops\nDescribe what a data frame is.\nLoad external data from a .csv file into a data frame in R.\n\n2.1.2 Lecture outline\n\nSetting up your R Notebook (10 min)\nCreating objects/variables in R (10 min)\nUsing and writing functions (15 min)\nVectors and data types (10 min)\nSubsetting vectors (15 min)\nMissing data (10 min)\nLoops and vectorization (10 min)\nData set background (10 min)\nWhat are data frames? (10 min)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: assignment, vectors, functions, strings, loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#setting-up-the-r-notebook",
    "href": "lectures/lec02-basic-r.html#setting-up-the-r-notebook",
    "title": "2  Introduction to R: assignment, vectors, functions, strings, loops",
    "section": "2.2 Setting up the R Notebook",
    "text": "2.2 Setting up the R Notebook\nLet’s remove the template RStudio gives us, and add a title of our own.\n---\ntitle: Introduction to R\n---\nThis header block is called the YAML header. This is where we specify whether we want to convert this file to a HTML or PDF file. This will be discussed in more detail in another class. For now, we just care about including the lecture title here. If you are interested in playing with other YAML options, check out this guide.\nUnder this sentence, we will insert our first code chunk. Remember that you insert a code chunk by either clicking the “Insert” button or pressing Ctrl/Cmd + Alt + i simultaneously. To run a code chunk, you press the green arrow, or Ctrl/Cmd + Shift + Enter.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: assignment, vectors, functions, strings, loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#creating-objects-in-r",
    "href": "lectures/lec02-basic-r.html#creating-objects-in-r",
    "title": "2  Introduction to R: assignment, vectors, functions, strings, loops",
    "section": "2.3 Creating objects in R",
    "text": "2.3 Creating objects in R\nAs we saw in our first class, you can get output from R simply by typing math in the console:\n\n3 + 5\n\n[1] 8\n\n12 / 7\n\n[1] 1.714286\n\n\nHowever, to do useful and interesting things, we need to assign values to objects.\n\nx &lt;- 3\nx + 5\n\n[1] 8\n\n\nYou can name an object in R almost anything you want:\n\njoel &lt;- 3\njoel + 5\n\n[1] 8\n\n\n\n2.3.0.1 Challenge\nSo far, we have created two variables, joel and x. What is the sum of these variables?\n\n\n2.3.1 Some tips on naming objects\n\nObjects can be given any name: x, current_temperature, thing, or subject_id.\nYou want your object names to be explicit and not too long.\nObject names cannot start with a number: x2 is valid, but 2x is not.\nR is also case sensitive: joel is different from Joel.\nAvoid using the names of existing functions (e.g. mean, df). You can check whether the name is already in use by using tab completion\nGenerally good to use underscores (_) to separate words in variable and function names\n\nIt is also recommended to use nouns for variable names, and verbs for function names. It’s important to be consistent in the styling of your code (where you put spaces, how you name variables, etc.). Using a consistent coding style1 makes your code clearer to read for your future self and your collaborators. RStudio will format code for you if you highlight a section of code and press Ctrl/Cmd + Shift + a.\n\n\n2.3.2 Preforming calculations\nWhen assigning a value to an object, R does not print anything. You can force R to print the value by using parentheses or by typing the object name:\n\nweight_kg &lt;- 55    # doesn't print anything\n(weight_kg &lt;- 55)  # but putting parentheses around the call prints the value of `weight_kg`\n\n[1] 55\n\nweight_kg          # and so does typing the name of the object\n\n[1] 55\n\n\nThe variable weight_kg is stored in the computer’s memory where R can access it, and we can start doing arithmetic with it efficiently. For instance, we may want to convert this weight into pounds (weight in pounds is 2.2 times the weight in kg):\n\n2.2 * weight_kg\n\n[1] 121\n\n\nWe can also change a variable’s value by assigning it a new one:\n\nweight_kg &lt;- 57.5\n2.2 * weight_kg\n\n[1] 126.5\n\n\nThis means that assigning a value to one variable does not change the values of other variables. For example, let’s store the animal’s weight in pounds in a new variable, weight_lb:\n\nweight_lb &lt;- 2.2 * weight_kg # Actually, 1 kg = 2.204623 lbs\n\nand then change weight_kg to 100.\n\nweight_kg &lt;- 100\n\n\n2.3.2.1 Challenge\nWhat do you think is the current content of the object weight_lb? 126.5 or 220?\n\nweight_lb\n\n\n\n2.3.2.2 Challenge\nWhat are the values after each statement in the following?\n\nmass &lt;- 47.5\nage  &lt;- 122\nmass &lt;- mass * 2.0      # mass?\nage  &lt;- age - 20        # age?\nmass_index &lt;- mass/age  # mass_index?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: assignment, vectors, functions, strings, loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#functions-and-their-arguments",
    "href": "lectures/lec02-basic-r.html#functions-and-their-arguments",
    "title": "2  Introduction to R: assignment, vectors, functions, strings, loops",
    "section": "2.4 Functions and their arguments",
    "text": "2.4 Functions and their arguments\n\n2.4.1 Understanding functions\nFunctions can be thought of as recipes. You give a few ingredients as input to a function, and it will generate an output based on these ingredients. Just as with baking, both the ingredients and the actual recipe will influence what comes out of the recipe in the end: will it be a cake or a loaf of bread? In R, the inputs to a function are not called ingredients, but rather arguments, and the output is called the return value of the function. A function does not technically have to return a value, but often does so. Functions are used to automate more complicated sets of commands and many of them are already predefined in R. A typical example would be the function sqrt(). The input (the argument) must be a number, and the return value (in fact, the output) is the square root of that number. Executing a function (‘running it’) is called calling the function. An example of a function call is:\n\nsqrt(9)\n\n[1] 3\n\n\nWhich is the same as assigning the value to a variable and then passing that variable to the function:\n\na &lt;- 9\nb &lt;- sqrt(a)\nb\n\n[1] 3\n\n\nHere, the value of a is given to the sqrt() function, the sqrt() function calculates the square root, and returns the value which is then assigned to variable b. This function is very simple, because it takes just one argument.\nThe return ‘value’ of a function need not be numerical (like that of sqrt()), and it also does not need to be a single item: it can be a set of things, or even a dataset, as we will see later on.\nArguments can be anything, not only numbers or filenames, but also other objects. Exactly what each argument means differs per function, and must be looked up in the documentation (see below). Some functions take arguments which may either be specified by the user, or, if left out, take on a default value: these are called options. Options are typically used to alter the way the function operates, such as whether it ignores ‘bad values’, or what symbol to use in a plot. However, if you want something specific, you can specify a value of your choice which will be used instead of the default.\n\n\n2.4.2 Tab-completion\nTo access help about sqrt, we are first going to learn about tab-completion. Type s and press Tab.\n\ns&lt;tab&gt;q\n\nYou can see that R gives you suggestions of what functions and variables are available that start with the letter s, and thanks to RStudio they are formatted in this nice list. There are many suggestions here, so let’s be a bit more specific and append a q, to find what we want. If we press enter or tab again, R will insert the selected option.\nYou can see that R inserts a pair of parentheses together with the name of the function. This is how the function syntax looks for R and many other programming languages, and it means that within these parentheses, we will specify all the arguments (the ingredients) that we want to pass to this function.\nIf we press tab again, R will helpfully display all the available parameters for this function that we can pass an argument to. The word parameter is used to describe the name that the argument can be passed to. More on that later.\n\nsqrt(&lt;tab&gt;\n\nThere are many things in this list, but only one of them is marked in purple. Purple here means that this list item is a parameter we can use for the function, while yellow means that it is a variable that we defined earlier.2\n\n\n2.4.3 Help with defined functions\nTo read the full help about sqrt, we can use the question mark, or type it directly into the help document browser.\n\n?sqrt\n\nAs you can see, sqrt() takes only one argument, x, which needs to be a numerical vector. Don’t worry too much about the fact that it says vector here; we will talk more about that later. Briefly, a numerical vector is one or more numbers. In R, every number is a vector, so you don’t have to do anything special to create a vector. More on vectors later.\nLet’s try a function that can take multiple arguments: round().\n\nround(&lt;tab&gt;)\n?round\n\nIf we try round with a value:\n\nround(3.14159)\n\n[1] 3\n\n\nHere, we’ve called round() with just one argument, 3.14159, and it has returned the value 3. That’s because the default is to round to the nearest whole number, or integer. If we want more digits we can pass an argument to the digits parameter, to specify how many decimals we want to round to.\n\nround(3.14159, digits = 2)\n\n[1] 3.14\n\n\nSo, above we pass the argument 2, to the parameter digits. Knowing this nomenclature is not essential for doing your own data analysis, but it will be very helpful when you are reading through help documents online and in RStudio.\nWe can leave out the word digits since we know it comes as the second parameter, after x.\n\nround(3.14159, 2)\n\n[1] 3.14\n\n\nAs you notice, we have been leaving out x from the beginning. If you provide the names for both the arguments, we can switch their order:\n\nround(digits = 2, x = 3.14159)\n\n[1] 3.14\n\n\nIt’s good practice to put the non-optional arguments (like the number you’re rounding) first in your function call, and to specify the names of all optional arguments. If you don’t, someone reading your code might have to look up the definition of a function with unfamiliar arguments to understand what you’re doing.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: assignment, vectors, functions, strings, loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#writing-functions",
    "href": "lectures/lec02-basic-r.html#writing-functions",
    "title": "2  Introduction to R: assignment, vectors, functions, strings, loops",
    "section": "2.5 Writing functions",
    "text": "2.5 Writing functions\nIn this class, you will be working a lot with functions, especially those that someone else has already written. When you type sum, c(), or mean(), you are using a function that has been made previously and built into R. To remove some of the magic around these functions, we will go through how to make a basic function of our own. Let’s start with a simple example where we add two numbers together:\n\nadd_two_numbers &lt;- function(num1, num2) {\n    return(num1 + num2)\n}\nadd_two_numbers(4, 5)\n\n[1] 9\n\n\nAs you can see, running this function on two numbers returns their sum. We could also assign to a variable in the function and return the function.\n\nadd_two_numbers &lt;- function(num1, num2) {\n    my_sum &lt;- num1 + num2\n    return(my_sum)\n}\nadd_two_numbers(4, 5)\n\n[1] 9\n\n\n\n2.5.0.1 Challenge\nCan you write a function that calculates the mean of 3 numbers?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: assignment, vectors, functions, strings, loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#vectors-and-data-types",
    "href": "lectures/lec02-basic-r.html#vectors-and-data-types",
    "title": "2  Introduction to R: assignment, vectors, functions, strings, loops",
    "section": "2.6 Vectors and data types",
    "text": "2.6 Vectors and data types\nA vector is the most common and basic data type in R, and is pretty much the workhorse of R. A vector is composed by a series of values, which can be either numbers or characters. We can assign a series of values to a vector using the c() function, which stands for “concatenate (combine/connect one after another) values into a vector” For example we can create a vector of animal weights and assign it to a new object weight_g:\n\nweight_g &lt;- c(50, 60, 65, 82) # Concatenate/Combine values into a vector\nweight_g\n\n[1] 50 60 65 82\n\n\nYou can also use the built-in command seq, to create a sequence of numbers without typing all of them in manually.\n\nseq(0, 30) # This is the same as just `0:30`\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n[26] 25 26 27 28 29 30\n\nseq(0, 30, 3) # Every third number\n\n [1]  0  3  6  9 12 15 18 21 24 27 30\n\n\nA vector can also contain characters:\n\nanimals &lt;- c('mouse', 'rat', 'dog')\nanimals\n\n[1] \"mouse\" \"rat\"   \"dog\"  \n\n\nThe quotes around “mouse”, “rat”, etc. are essential here and can be either single or double quotes. Without the quotes R will assume there are objects called mouse, rat and dog. As these objects don’t exist in R’s memory, there will be an error message.\nThere are many functions that allow you to inspect the content of a vector. length() tells you how many elements are in a particular vector:\n\nlength(weight_g)\n\n[1] 4\n\nlength(animals)\n\n[1] 3\n\n\nAn important feature of a vector is that all of the elements are the same type of data. The function class() indicates the class (the type of element) of an object:\n\nclass(weight_g)\n\n[1] \"numeric\"\n\nclass(animals)\n\n[1] \"character\"\n\n\nThe function str() provides an overview of the structure of an object and its elements. It is a useful function when working with large and complex objects:\n\nstr(weight_g)\n\n num [1:4] 50 60 65 82\n\nstr(animals)\n\n chr [1:3] \"mouse\" \"rat\" \"dog\"\n\n\nYou can use the c() function to add other elements to your vector:\n\nweight_g &lt;- c(weight_g, 90) # add to the end of the vector\nweight_g &lt;- c(30, weight_g) # add to the beginning of the vector\nweight_g\n\n[1] 30 50 60 65 82 90\n\n\nIn the first line, we take the original vector weight_g, add the value 90 to the end of it, and save the result back into weight_g. Then we add the value 30 to the beginning, again saving the result back into weight_g.\nWe can do this over and over again to grow a vector, or assemble a dataset. As we program, this may be useful to add results that we are collecting or calculating.\nAn atomic vector is the simplest R data type and it is a linear vector of a single type, e.g. all numbers. Above, we saw 2 of the 6 main atomic vector types that R uses: \"character\" and \"numeric\" (or \"double\"). These are the basic building blocks that all R objects are built from.\nVectors are one of the many data structures that R uses. Other important ones are lists (list), matrices (matrix), data frames (data.frame), factors (factor) and arrays (array). In this class, we will focus on data frames, which is most commonly used one for data analyses.\n\n2.6.0.1 Challenge\nWe’ve seen that atomic vectors can be of type character, numeric (or double), integer, and logical. But what happens if we try to mix these types in a single vector? Find out by using class to test these examples.\n\nnum_char &lt;- c(1, 2, 3, 'a')\nnum_logical &lt;- c(1, 2, 3, TRUE)\nchar_logical &lt;- c('a', 'b', 'c', TRUE)\ntricky &lt;- c(1, 2, 3, '4')\n\nThis happens because vectors can be of only one data type. Instead of throwing an error and saying that you are trying to mix different types in the same vector, R tries to convert (coerce) the content of this vector to find a “common denominator”. A logical can be turn into 1 or 0, and a number can be turned into a string/character representation. It would be difficult to do it the other way around: would 5 be TRUE or FALSE? What number would ‘t’ be?\nIn R, we call converting objects from one class into another class coercion. These conversions happen according to a hierarchy, whereby some types get preferentially coerced into other types. Can you draw a diagram that represents the hierarchy of how these data types are coerced?\nThis can be important to watch for in data sets that you import.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: assignment, vectors, functions, strings, loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#subsetting-vectors",
    "href": "lectures/lec02-basic-r.html#subsetting-vectors",
    "title": "2  Introduction to R: assignment, vectors, functions, strings, loops",
    "section": "2.7 Subsetting vectors",
    "text": "2.7 Subsetting vectors\nIf we want to extract one or several values from a vector, we must provide one or several indices in square brackets. For instance:\n\nanimals &lt;- c(\"mouse\", \"rat\", \"dog\", \"cat\")\nanimals[2]\n\n[1] \"rat\"\n\nanimals[c(3, 2)]\n\n[1] \"dog\" \"rat\"\n\n\nWe can also repeat the indices to create an object with more elements than the original one:\n\nmore_animals &lt;- animals[c(1, 2, 3, 2, 1, 4)]\nmore_animals\n\n[1] \"mouse\" \"rat\"   \"dog\"   \"rat\"   \"mouse\" \"cat\"  \n\n\nR indices start at 1. Programming languages like Fortran, MATLAB, Julia, and R start counting at 1, because that’s what human beings typically do. Languages in the C family (including C++, Java, Perl, and Python) count from 0 because that was historically simpler for computers and can allow for more elegant code.\n\n2.7.1 Conditional subsetting\nAnother common way of subsetting is by using a logical vector. TRUE will select the element with the same index, while FALSE will not:\n\nweight_g &lt;- c(21, 34, 39, 54, 55)\nweight_g[c(TRUE, FALSE, TRUE, TRUE, FALSE)]\n\n[1] 21 39 54\n\n\nTypically, these logical vectors are not typed by hand, but are the output of other functions or logical tests. For instance, if you wanted to select only the values above 50:\n\nweight_g &gt; 50    # will return logicals with TRUE for the indices that meet the condition\n\n[1] FALSE FALSE FALSE  TRUE  TRUE\n\n## so we can use this to select only the values above 50\nweight_g[weight_g &gt; 50]\n\n[1] 54 55\n\n\nWe will consider conditions in more detail in the next few lectures.\n\n\n2.7.2 Strings (character vectors)\nJust a small note about character vectors, also called strings. There are built-in packages for subsetting them that we’ll learn about later. They can be particularly relevant for ecological and genomic data because important data can be nested in complicated strings of text (ex: extracting only the observations that occurred in wet habitats from a column of habitat descriptions or only genes with functions related to drought tolerance).\n\nstring1 &lt;- \"This is a string\" # you can include spaces between your quotes\nstring2 &lt;- c(string1, \"so is this\") # concatenate with another string\nstring2[2] # can access the second string via subsetting\n\n[1] \"so is this\"\n\n# Playing a bit with declaring variables\n\"You can include 'quotes' in a string\"\n\n[1] \"You can include 'quotes' in a string\"\n\nstring3 &lt;- 'You can include \"quotes\" in a string'\nstring3\n\n[1] \"You can include \\\"quotes\\\" in a string\"\n\n\"You can include \\\"matching quotes\\\" if you 'escape' them with a backslash (\\\\)\"\n\n[1] \"You can include \\\"matching quotes\\\" if you 'escape' them with a backslash (\\\\)\"",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: assignment, vectors, functions, strings, loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#missing-data",
    "href": "lectures/lec02-basic-r.html#missing-data",
    "title": "2  Introduction to R: assignment, vectors, functions, strings, loops",
    "section": "2.8 Missing data",
    "text": "2.8 Missing data\nAs R was designed to analyze datasets, it includes the concept of missing data (which is uncommon in other programming languages). Missing data are represented in vectors as NA.\nWhen doing operations on numbers, most functions will return NA if the data you are working with include missing values. This feature makes it harder to overlook the cases where you are dealing with missing data. You can add the argument na.rm = TRUE to calculate the result while ignoring the missing values.\n\nheights &lt;- c(2, 4, 4, NA, 6)\nmean(heights)\n\n[1] NA\n\nmax(heights)\n\n[1] NA\n\nmean(heights, na.rm = TRUE)\n\n[1] 4\n\nmax(heights, na.rm = TRUE)\n\n[1] 6\n\n\n\n## Extract those elements which are not missing values.\nheights[!is.na(heights)]\n\n[1] 2 4 4 6\n\n## Returns the object with incomplete cases removed. The returned object is an atomic vector of type `\"numeric\"` (or `\"double\"`).\nna.omit(heights)\n\n[1] 2 4 4 6\nattr(,\"na.action\")\n[1] 4\nattr(,\"class\")\n[1] \"omit\"\n\n## Extract those elements which are complete cases. The returned object is an atomic vector of type `\"numeric\"` (or `\"double\"`).\nheights[complete.cases(heights)]\n\n[1] 2 4 4 6\n\n\nRecall that you can use the class() function to find the type of your atomic vector.\n\n2.8.0.1 Challenge\n\nUsing this vector of length measurements, create a new vector with the NAs removed.\n\n\nlengths &lt;- c(10, 24, NA, 18, NA, 20)\n\n\nUse the function median() to calculate the median of the lengths vector.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: assignment, vectors, functions, strings, loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#loops-and-vectorization",
    "href": "lectures/lec02-basic-r.html#loops-and-vectorization",
    "title": "2  Introduction to R: assignment, vectors, functions, strings, loops",
    "section": "2.9 Loops and vectorization",
    "text": "2.9 Loops and vectorization\nLoops, specifically for-loops, are essential to programming in general. However, in R, you should avoid them as often as possible because there are more efficient ways of doing things that you should use instead. It is still important that you understand the concept of loops and you might also use them in some of your own functions if there is no vectorized way of going about what you want to do.\nYou can think of a for-loop as: “for each number contained in a list/vector, perform this operation” and the syntax basically says the same thing:\n\nv &lt;- c(2, 4, 6)\nfor (num in v) {\n    print(num)\n}\n\n[1] 2\n[1] 4\n[1] 6\n\n\nInstead of printing out every number to the console, we could also add numbers cumulatively, to calculate the sum of all the numbers in the vector:\n\n# To increment `w` each time, we must first create the variable,\n# which we do by setting `w &lt;- 0`, referred to as initializing.\n# This also ensures that `w` is zero at the start of the loop and\n# doesn't retain the value from last time we ran this code.\nw &lt;- 0\nfor (num in v) {\n    w &lt;- w + num\n}\nw\n\n[1] 12\n\n\nIf we put what we just did inside a function, we have essentially recreated the sum function in R.\n\nmy_sum &lt;- function(input_vector) {\n    vector_sum &lt;- 0\n    for (num in input_vector){\n        vector_sum &lt;- vector_sum + num\n    }\n    return(vector_sum)\n}\n\nmy_sum(v)\n\n[1] 12\n\n\nAlthough this gives us the same output as the built-in function sum, the built-in function has many more optimizations so it is much faster than our function. In R, it is always faster to try to find a way of doing things without writing a loop yourself. When you are reading about R, you might see suggestions that you should try to vectorize your code to make it faster. What people are referring to, is that you should not write for loops in R and instead use the ready-made functions that are much more efficient in working with vectors and essentially performs operations on entire vector at once instead of one number at a time. Conceptually, loops operate on one element at a time while vectorized code operates on all elements of a vector at once.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: assignment, vectors, functions, strings, loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#dataset-background",
    "href": "lectures/lec02-basic-r.html#dataset-background",
    "title": "2  Introduction to R: assignment, vectors, functions, strings, loops",
    "section": "2.10 Dataset background",
    "text": "2.10 Dataset background\nToday, we will be working with real data from a longitudinal study of the species abundance in the Chihuahuan desert ecosystem near Portal, Arizona, USA. This study includes observations of plants, ants, and rodents from 1977 - 2002, and has been used in over 100 publications. More information is available in the abstract of this paper from 2009. There are several datasets available related to this study, and we will be working with datasets that have been preprocessed by the Data Carpentry to facilitate teaching. These are made available online as The Portal Project Teaching Database, both at the Data Carpentry website, and on Figshare. Figshare is a great place to publish data, code, figures, and more openly to make them available for other researchers and to communicate findings that are not part of a longer paper.\n\n2.10.1 Presentation of the survey data\nWe are studying the species and weight of animals caught in plots in our study area. The dataset is stored as a comma separated value (CSV) file. Each row holds information for a single animal, and the columns represent:\n\n\n\nColumn\nDescription\n\n\n\n\nrecord_id\nunique id for the observation\n\n\nmonth\nmonth of observation\n\n\nday\nday of observation\n\n\nyear\nyear of observation\n\n\nplot_id\nID of a particular plot\n\n\nspecies_id\n2-letter code\n\n\nsex\nsex of animal (“M”, “F”)\n\n\nhindfoot_length\nlength of the hindfoot in mm\n\n\nweight\nweight of the animal in grams\n\n\ngenus\ngenus of animal\n\n\nspecies\nspecies of animal\n\n\ntaxa\ne.g. rodent, reptile, bird, rabbit\n\n\nplot_type\ntype of plot\n\n\n\nTo read the data into R, we are going to use a function called read_csv. This function is contained in an R-package called readr. R-packages are a bit like browser extensions; they are not essential, but can provide nifty functionality. We will go through R-packages in general and which ones are good for data analyses. One useful option that read_csv includes, is the ability to read a CSV file directly from a URL, without downloading it in a separate step:\n\nsurveys &lt;- readr::read_csv('https://ndownloader.figshare.com/files/2292169')\n\nHowever, it is often a good idea to download the data first, so you have a copy stored locally on your computer in case you want to do some offline analyses, or the online version of the file changes or the file is taken down. You can either download the data manually or from within R:\n\ndownload.file(\"https://ndownloader.figshare.com/files/2292169\",\n              \"data/portal_data.csv\") # Saves to current directory with this name\n\nThe data is read in by specifying its local path.\n\nsurveys &lt;- readr::read_csv('data/portal_data.csv')\n\nRows: 34786 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): species_id, sex, genus, species, taxa, plot_type\ndbl (7): record_id, month, day, year, plot_id, hindfoot_length, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis statement produces some output regarding which data type it found in each column. If we want to check this in more detail, we can print the variable’s value: surveys.\n\nsurveys\n\n# A tibble: 34,786 × 13\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2        72     8    19  1977       2 NL         M                  31     NA\n 3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n 4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n 5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n 6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n 7       435    12    10  1977       2 NL         &lt;NA&gt;               NA     NA\n 8       506     1     8  1978       2 NL         &lt;NA&gt;               NA     NA\n 9       588     2    18  1978       2 NL         M                  NA    218\n10       661     3    11  1978       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 34,776 more rows\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\nThis displays a nice tabular view of the data, which also includes pagination when there are many rows and we can click the arrow to view all the columns. Technically, this object is actually a tibble rather than a data frame, as indicated in the output. The reason for this is that read_csv automatically converts the data into to a tibble when loading it. Since a tibble is just a data frame with some convenient extra functionality, we will use these words interchangeably from now on.\nIf we just want to glance at how the data frame looks, it is sufficient to display only the top (the first 6 lines) using the function head():\n\nhead(surveys)\n\n# A tibble: 6 × 13\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1         1     7    16  1977       2 NL         M                  32     NA\n2        72     8    19  1977       2 NL         M                  31     NA\n3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: assignment, vectors, functions, strings, loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#what-are-data-frames",
    "href": "lectures/lec02-basic-r.html#what-are-data-frames",
    "title": "2  Introduction to R: assignment, vectors, functions, strings, loops",
    "section": "2.11 What are data frames?",
    "text": "2.11 What are data frames?\nData frames are the de facto data structure for most tabular data, and what we use for statistics and plotting. A data frame can be created by hand, but most commonly they are generated by the function read_csv(); in other words, when importing spreadsheets from your hard drive (or the web).\nA data frame is a representation of data in the format of a table where the columns are vectors that all have the same length. Because the columns are vectors, they all contain the same type of data as we discussed in last class (e.g., characters, integers, factors). We can see this when inspecting the structure of a data frame with the function str():\n\nstr(surveys)\n\nspc_tbl_ [34,786 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ record_id      : num [1:34786] 1 72 224 266 349 363 435 506 588 661 ...\n $ month          : num [1:34786] 7 8 9 10 11 11 12 1 2 3 ...\n $ day            : num [1:34786] 16 19 13 16 12 12 10 8 18 11 ...\n $ year           : num [1:34786] 1977 1977 1977 1977 1977 ...\n $ plot_id        : num [1:34786] 2 2 2 2 2 2 2 2 2 2 ...\n $ species_id     : chr [1:34786] \"NL\" \"NL\" \"NL\" \"NL\" ...\n $ sex            : chr [1:34786] \"M\" \"M\" NA NA ...\n $ hindfoot_length: num [1:34786] 32 31 NA NA NA NA NA NA NA NA ...\n $ weight         : num [1:34786] NA NA NA NA NA NA NA NA 218 NA ...\n $ genus          : chr [1:34786] \"Neotoma\" \"Neotoma\" \"Neotoma\" \"Neotoma\" ...\n $ species        : chr [1:34786] \"albigula\" \"albigula\" \"albigula\" \"albigula\" ...\n $ taxa           : chr [1:34786] \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ...\n $ plot_type      : chr [1:34786] \"Control\" \"Control\" \"Control\" \"Control\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   record_id = col_double(),\n  ..   month = col_double(),\n  ..   day = col_double(),\n  ..   year = col_double(),\n  ..   plot_id = col_double(),\n  ..   species_id = col_character(),\n  ..   sex = col_character(),\n  ..   hindfoot_length = col_double(),\n  ..   weight = col_double(),\n  ..   genus = col_character(),\n  ..   species = col_character(),\n  ..   taxa = col_character(),\n  ..   plot_type = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nInteger refers to a whole number, such as 1, 2, 3, 4, etc. Numbers with decimals, 1.0, 2.4, 3.333, are referred to as floats. Factors are used to represent categorical data. Factors can be ordered or unordered, and understanding them is necessary for statistical analysis and for plotting. Factors are stored as integers, and have labels (text) associated with these unique integers. While factors look (and often behave) like character vectors, they are actually integers under the hood, and you need to be careful when treating them like strings.\n\n2.11.1 Inspecting data.frame objects\nWe already saw how the functions head() and str() can be useful to check the content and the structure of a data frame. Here is a non-exhaustive list of functions to get a sense of the content/structure of the data. Let’s try them out!\n\nSize:\n\ndim(surveys) - returns a vector with the number of rows in the first element and the number of columns as the second element (the dimensions of the object)\nnrow(surveys) - returns the number of rows\nncol(surveys) - returns the number of columns\n\nContent:\n\nhead(surveys) - shows the first 6 rows\ntail(surveys) - shows the last 6 rows\n\nNames:\n\nnames(surveys) - returns the column names (synonym of colnames() for data.frame objects)\nrownames(surveys) - returns the row names\n\nSummary:\n\nstr(surveys) - structure of the object and information about the class, length, and content of each column\nsummary(surveys) - summary statistics for each column\n\n\nNote: most of these functions are “generic”, they can be used on other types of objects besides data.frame.\n\n2.11.1.1 Challenge\nBased on the output of str(surveys), can you answer the following questions?\n\nWhat is the class of the object surveys?\nHow many rows and how many columns are in this object?\nHow many species have been recorded during these surveys?\n\n\n\n\n2.11.2 Indexing and subsetting data frames\nOur survey data frame has rows and columns (it has 2 dimensions). If we want to extract some specific data from it, we need to specify the “coordinates” we want from it. Row numbers come first, followed by column numbers. When indexing, base R data frames return a different format depending on how we index the data (i.e. either a vector or a data frame), but with enhanced data frames, tibbles, the returned object is almost always a data frame.\n\nsurveys[1, 1]   # first element in the first column of the data frame\n\n# A tibble: 1 × 1\n  record_id\n      &lt;dbl&gt;\n1         1\n\nsurveys[1, 6]   # first element in the 6th column\n\n# A tibble: 1 × 1\n  species_id\n  &lt;chr&gt;     \n1 NL        \n\nsurveys[, 1]    # first column in the data frame\n\n# A tibble: 34,786 × 1\n   record_id\n       &lt;dbl&gt;\n 1         1\n 2        72\n 3       224\n 4       266\n 5       349\n 6       363\n 7       435\n 8       506\n 9       588\n10       661\n# ℹ 34,776 more rows\n\nsurveys[1]      # first column in the data frame\n\n# A tibble: 34,786 × 1\n   record_id\n       &lt;dbl&gt;\n 1         1\n 2        72\n 3       224\n 4       266\n 5       349\n 6       363\n 7       435\n 8       506\n 9       588\n10       661\n# ℹ 34,776 more rows\n\nsurveys[1:3, 7] # first three elements in the 7th column\n\n# A tibble: 3 × 1\n  sex  \n  &lt;chr&gt;\n1 M    \n2 M    \n3 &lt;NA&gt; \n\nsurveys[3, ]    # the 3rd element for all columns\n\n# A tibble: 1 × 13\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\nsurveys[1:6, ]  # equivalent to head(surveys)\n\n# A tibble: 6 × 13\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1         1     7    16  1977       2 NL         M                  32     NA\n2        72     8    19  1977       2 NL         M                  31     NA\n3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\n: is a special operator that creates numeric vectors of integers in increasing or decreasing order; test 1:10 and 10:1 for instance. This works similarly to seq, which we looked at earlier in class:\n\n0:10\n\n [1]  0  1  2  3  4  5  6  7  8  9 10\n\nseq(0, 10)\n\n [1]  0  1  2  3  4  5  6  7  8  9 10\n\n# We can test if all elements are the same\n0:10 == seq(0,10)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\nall(0:10 == seq(0,10))\n\n[1] TRUE\n\n\nYou can also exclude certain parts of a data frame using the “-” sign:\n\nsurveys[,-1]    # All columns, except the first\n\n# A tibble: 34,786 × 12\n   month   day  year plot_id species_id sex   hindfoot_length weight genus  \n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  \n 1     7    16  1977       2 NL         M                  32     NA Neotoma\n 2     8    19  1977       2 NL         M                  31     NA Neotoma\n 3     9    13  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 4    10    16  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 5    11    12  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 6    11    12  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 7    12    10  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 8     1     8  1978       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 9     2    18  1978       2 NL         M                  NA    218 Neotoma\n10     3    11  1978       2 NL         &lt;NA&gt;               NA     NA Neotoma\n# ℹ 34,776 more rows\n# ℹ 3 more variables: species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\nsurveys[-c(7:34786),] # Equivalent to head(surveys)\n\n# A tibble: 6 × 13\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1         1     7    16  1977       2 NL         M                  32     NA\n2        72     8    19  1977       2 NL         M                  31     NA\n3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\nAs well as using numeric values to subset a data.frame (or matrix), columns can be called by name, using one of the four following notations: \n\nsurveys[\"species_id\"]       # Result is a data.frame\n\n# A tibble: 34,786 × 1\n   species_id\n   &lt;chr&gt;     \n 1 NL        \n 2 NL        \n 3 NL        \n 4 NL        \n 5 NL        \n 6 NL        \n 7 NL        \n 8 NL        \n 9 NL        \n10 NL        \n# ℹ 34,776 more rows\n\nsurveys[, \"species_id\"]     # Result is a data.frame\n\n# A tibble: 34,786 × 1\n   species_id\n   &lt;chr&gt;     \n 1 NL        \n 2 NL        \n 3 NL        \n 4 NL        \n 5 NL        \n 6 NL        \n 7 NL        \n 8 NL        \n 9 NL        \n10 NL        \n# ℹ 34,776 more rows\n\n\nFor our purposes, these notations are equivalent. RStudio knows about the columns in your data frame, so you can take advantage of the autocompletion feature to get the full and correct column name.\nAnother syntax that is often used to specify column names is $. In this case, the returned object is actually a vector. We will not go into detail about this, but since it is such common usage, it is good to be aware of this.\n\n# We use `head()` since the output from vectors are not automatically cut off\n# and we don't want to clutter the screen with all the `species_id` values\nhead(surveys$species_id)          # Result is a vector\n\n[1] \"NL\" \"NL\" \"NL\" \"NL\" \"NL\" \"NL\"\n\n\n\n2.11.2.1 Challenge\n\nCreate a data.frame (surveys_200) containing only the observations from row 200 of the surveys dataset.\nNotice how nrow() gave you the number of rows in a data.frame?\n\nUse that number to pull out just that last row in the data frame.\nCompare that with what you see as the last row using tail() to make sure it’s meeting expectations.\nPull out that last row using nrow() instead of the row number.\nCreate a new data frame object (surveys_last) from that last row.\n\nUse nrow() to extract the row that is in the middle of the data frame. Store the content of this row in an object named surveys_middle.\nCombine nrow() with the - notation above to reproduce the behavior of head(surveys) keeping just the first through 6th rows of the surveys dataset.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: assignment, vectors, functions, strings, loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#exporting-data",
    "href": "lectures/lec02-basic-r.html#exporting-data",
    "title": "2  Introduction to R: assignment, vectors, functions, strings, loops",
    "section": "2.12 Exporting data",
    "text": "2.12 Exporting data\nAs you begin to play with your raw data, you may want to export these new, processed, datasets to share them with your collaborators or for archival. Similar to the read_csv() function used for reading CSV files into R, there is a write_csv() function that generates CSV files from data frames.\nManually create a new folder called “data-processed” in your directory. Alternatively, get R to help you with it.\n\ndir.create(\"Processed data\")\n\nWe are going to prepare a cleaned up version of the data without NAs.\n\n# Note that this omits observations with NA in *any* column.\n# There is no way to control which columns to use.\nsurveys_complete_naomit &lt;- na.omit(surveys)\n\n# Compare the dimensions of the original and the cleaned data frame\ndim(surveys)\n\n[1] 34786    13\n\ndim(surveys_complete_naomit)\n\n[1] 30676    13\n\n\nNow that our dataset is ready, we can save it as a CSV file in our Processed data folder.\n\n# To save to current directory\nwrite_csv(surveys_complete_naomit, \"surveys_complete_naomit.csv\")\n\n# To save to newly created directory\nwrite_csv(surveys_complete_naomit, \n          file.path(\"~/Processed data\", \"surveys_complete_naomit.csv\"))\n\nNext lecture, we’re going to discuss collaboration with GitHub and go over an intro to the command line.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: assignment, vectors, functions, strings, loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#footnotes",
    "href": "lectures/lec02-basic-r.html#footnotes",
    "title": "2  Introduction to R: assignment, vectors, functions, strings, loops",
    "section": "",
    "text": "Refer to the tidy style guide for which style to adhere to.↩︎\nThere are a few other symbols as well, all of which can be viewed at the end of this post about RStudio code completion.↩︎",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: assignment, vectors, functions, strings, loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec03-command_git.html",
    "href": "lectures/lec03-command_git.html",
    "title": "3  Command line & Git(Hub)",
    "section": "",
    "text": "3.1 Lesson preamble",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Command line & Git(Hub)</span>"
    ]
  },
  {
    "objectID": "lectures/lec03-command_git.html#lesson-preamble",
    "href": "lectures/lec03-command_git.html#lesson-preamble",
    "title": "3  Command line & Git(Hub)",
    "section": "",
    "text": "3.1.1 Lesson objectives\n\nDevelop familiarity with the logic of the command line, including how to\n\nNavigate between directories\nCreate, copy, move, delete and compare files and directories\nSearch, edit, and read content from files\nChange file permissions\nInstall, update and remove packages\nUse NCBI’s BLAST!\n\nLearn how Git(Hub) works and the logic of version control\n\nDevelop proficiency using basic Git commands\nDevelop understanding of the Git workflow and recommended practices\nPractice using Git commands at the command line\n\n\n3.1.2 Lesson outline\n\nIntroduction to the command line and where/why/when we use it (10 mins)\nA tour through different command line tools (25 mins)\nRunning software tools: BLAST (25 mins)\nCollaborating with GitHub using the command line (30 min)\nWorkflows and recommended practices (10 mins)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Command line & Git(Hub)</span>"
    ]
  },
  {
    "objectID": "lectures/lec03-command_git.html#intro-to-the-command-line",
    "href": "lectures/lec03-command_git.html#intro-to-the-command-line",
    "title": "3  Command line & Git(Hub)",
    "section": "3.2 Intro to the command line",
    "text": "3.2 Intro to the command line\n\n3.2.1 Why use the command line?\nIncreasing amounts of data in biology (especially genomic data stored in databases like the UK Biobank and GenBank) are transforming the field. These data have allowed us to ask and answer questions that would otherwise be impossible to address, and have motivated extensive cross-talk between biology, computer science, mathematics, and statistics. To analyze large-scale datasets in a way that is efficient, robust, reproducible, and scale-able, researchers turn to the command line. The command line interface allows researches to pass commands to the shell.\nSome advantages of working are the commend line are:\n\nEfficient parallelization: where it would not be possible to preform certain calculations due to time and memory limitations, the command line allows us to interface with computing clusters which can run many calculations or processes simultaneously. For example, efficient parallelization can allow a user to run many simulations of a model (each run corresponding to a different combination of parameter values) simultaneously. This can cut run times from months to seconds.\nControl over data: the command line allows users to view, parse, transform, and transfer large files (such as genome sequences) efficiently. Such control is simply not possible with a GUI like RStudio.\nAutomation, reproducibility, and computational ease: shell commands automate tasks which would otherwise be 1) computationally expensive, 2) error-prone, and 3) emotionally taxing.1 Shell commands can be scripted, shared, version-controlled, and made to run at certain times. (For example, shell scripts designed to scrape the web for data are often made to run during off-peak hours.) Because the command line makes automating complex tasks possible, software used to analyze large data is often written for use on the command line.\n\nAll that said, shell commands appear quite scary. Indeed, the command line is unforgiving–cryptic error messages abound when incorrect commands are entered. One can do serious damage without understanding the basis of the shell. The goal of this lecture is to provide you all with knowledge of 1) the logic of the command line, 2) important shell commands, 3) how to run NCBI’s BLAST from the command line, and 4) how to use the command line to interface with GitHub. The first half of the lecture will focus on the shell, and the second half on Git.\n\n\n3.2.2 Logic of the command line\nThe command line interface takes commands of the following form:\n\ncommand param1 param2 param3 … paramN\n\nwhere param1, param2, \\(\\dots,\\) and paramN are parameters provided by the user. This is a simplification, but one can get by thinking of commands as having this form, with the command line interpreter providing an interface between what is entered and the machine. Commands can range in what they do (navigation between directories, editing of files and of file permissions, etc.) but will not do what they are intended to if the relevant parameters are not specified, or if files which are called by the command are not accessible.\n\n\n3.2.3 Where are we?\nFor this portion of the lecture, we will move to the terminal on Mete’s machine. The commands that we execute there are given and described below:\n\ncal # returns this month's calender\ndate # returns today's date\npwd # print working directory\nls # returns contents of the working directory\nls -l # returns contents of wd and information about those contents\n\n\nmkdir newdirectory # makes new directory called newdirectory\ncd newdirectory # change directory to newdirectory\ntouch emptytextfile.txt # make new .txt file with name emptytextfile\nless emptytextfile.txt # view emptytextfile.txt\nq # stop viewing emptytextfile.txt\ntouch emptytextfile2.txt\nnano emptytextfile2.txt # edit emptytextfile2.txt\nmv emptytextfile2.txt file_nolongerempty.txt \n# make new file called file_nolongerempty.txt from emptytextfile2.txt\nless file_nolongerempty.txt\ncp file_nolongerempty.txt file_nolongerempty_todelete.txt\n# copy file_nolongerempty.txt and make new file called file_nolongerempty_todelete.txt\nrm file_nolongerempty_todelete.txt # delete file_nolongerempty_todelete.txt\n\n\ncd .. # go back to preview working directory\nmkdir newdirectory2\ncp -r newdirectory2 newdirectory3 # copy directory\nrm -r newdirectory3 # remove directory that was just made\n\n\ndiff -qr newdirectory/ newdirectory2/ \n# assess and return differences between directories specified\n# r recursively searches subdirectories, q reports 'briefly', when files differ\n# -arq is also valid, treats all files as text\n  \ncd newdirectory\ndiff file_nolongerempty.txt emptytextfile.txt \n# assess and return differences between files specified\n# -w ignores white space\n\n\n\n3.2.4 All about file permissions\nTo see what the permissions of a specific file are, one can use the command ls followed by -l (for all files within a directory) or -la for specific files within that directory.\n\nls -la file_nolongerempty.txt\n# multiple instances of r, w, and x reflect different levels of ownership\n# r = can read the file, can ls the directory\n# w = can write the file, can modify the directory's contents\n# x = can execute the file, can cd to the directory\n\nFor example, the rw that appears first in -rw-r--r-- indicates the owner (user) can can read and write to the file but can’t execute it as a program. The r that appears next indicates group members can read the file. drwxr-xr-x indicates group members can view as well as enter the directory.\nThe command cmod for “change mode” allows one to modify file permissions.\n\nchmod a+r file_nolongerempty.txt\n# a stands for all (default, so can be omitted), +r = add read permission\n# g = group, o = other, u = user\n# - = remove access, = sets exact access\nchmod go-rw file_nolongerempty.txt # removes group read and write permissions\n\nchmod also acts on directories but requires -R argument. chmod -R o+x would grant execution permissions for other users to a directory and its subdirectories.\nFor more about the chmod command (e.g., specifying the entire state of a file or directories permissions by providing the command numbers rather than combinations of r,x,w), one can run the command man chmod. The man command displays the manual page for a particular command.\nTo see what a shell command does, we also reccomend you check out https://explainshell.com.\n\n3.2.4.1 Challenge\nWhat steps, in order, would you perform in order to create a file called “test.txt” with the text “Hello World!” in a new folder called “challenge” and make it executable for all users?\n\n\n\n3.2.5 apt, sudo, get, and all that jazz\nAn important but unfortunately tricky part of using the command line is using the utility apt (or one of its cousins) to install, update, remove, or otherwise manage packages on a Linux distribution. More information how apt works and can be used can be found at https://ubuntu.com/server/docs/package-management. Since there are some steps to get apt to work on a Mac, Mete will illustrate how to do package installation using the free and open source package manager Homebrew.\nOne can install Homebrew by running\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# if you're on Linux or Windows (with Ubuntu installed with WSL 2):\napt update\napt upgrade\n\nbrew can be used to install packages the machine does not have. For example, running\n\nbrew install wget\n\ninstalls the package wget, a free package for retrieving files using HTTP, HTTPS, FTP and FTPS. Homebrew does this by running Ruby and Git, which you can see for yourseld by entering the following command:\n\nbrew edit wget\n\nHomebrew also allows users to create their own packages, has very through documentation, and even facilitates the installation of new fonts and non-open source software.\n\nbrew install --cask firefox\n\nA list of packages that are installed can be found by running\n\nbrew list\nbrew deps --tree --installed # illustrates dependencies\n\nA particular piece of software can be found by using brew search:\n\nbrew search google\n\napt-cache search google\n\nbrew install will install software such as the highly useful Basic Local Alignment Search Tool (BLAST):\n\nbrew install blast\n\napt-get install ncbi-blast+\n\nAlong these lines, it is important to mention that one sometimes will run into issues installing packages using brew or apt. To get around this, a very handy (but at times dangerous) command to have ready is sudo. For example, trying to run shutdown, Mete gets an error. But if we run\n\nsudo shutdown\n\none can run … just about anything. sudo stands for superuser do.\n\n\n3.2.6 Running software tools\n\n3.2.6.1 Examples of software tools\nThere’s a lot of software available for biological research, but much of it can only be used from the command line. For example, if you wanted to identify all of the transposable elements (“selfish genes” that make copies of themselves in their host genomes) in a genome, you’d want to run something like EDTA but it does not offer a graphical interface. If you had a sequence that you knew was associated with herbicide resistance, you could use NCBI’s BLAST (Basic Local Alignment Search Tool) to compare your nucleotide or protein sequence against a large database to find regions of similarity in order to determine the potential functional role of the region or if the region is shared among similar species. The has an online graphical interface but is also available to use via the command line. The command line provides some additional functionality, such as the time-saving option of performing searches in parallel.\n\n\n3.2.6.2 Step 1: Understand the software\nYour first step to running something is always going to be reading documentation. You need to understand what the software tool is actually doing and whether that methods it uses is appropriate for your data.\nThe inputs and outputs of a programme are an important starting place. BLAST’s required inputs are -query, a “query sequence” (what you’re looking for), and -db, a “database” (where you’re hoping to find a match). Ideally, you’ve read the paper associated with the software’s creation, you’ve understood all of the arguments that can be passed to the software, and, most importantly, you understand the software’s weaknesses and strengths with regards to your particular type of data.\nThis is the most important step for analysing your data. Getting the software running can be tricky and often more time-consuming than this step, but understanding what’s happening to your data is the most important thing you’re going to do.\nOne way to interact with help documentation is to use -h or -help. Many software tools will display their documentation after such an argument.\n\nbash --help\n\n\n\n3.2.6.3 Installing and running the software\nPackage managers like conda are really useful for more complex software tools that have dependencies. Luckily, tools like BLAST can be simply installed with apt-get and/or brew (as outlined earlier).\n\nbrew install blast\n# earlier code for reference\n\napt-get install ncbi-blast+\n\nIn the below code chunk, we use curl to download a mouse amino acid sequences that have been compressed (notice the file extension!) into our current working directory, and check it downloaded with ls.\n\ncurl -o mouse.protein.faa.gz -L https://osf.io/v6j9x/download\n\nls\n\nFilenames that end in .gz have been compressed with gzip (similar in theory to compressing with zip). We can de-compress the files to be able to view them.\n\ngunzip mouse.protein.faa.gz\n\nhead -n 6 mouse.protein.faa # head works the same in R as the command line\nless mouse.protein.faa # less will show the whole file. You can quit with `q`.\n\n# Let's take the first two sequences and save them to a different file\nhead -n 11 mouse.protein.faa &gt; mm.first.faa\n\n# You can check this worked with `less mm.first.faa`\n\nWe can search this mouse sequence against a protein data set with BLAST. First, we need to make a protein sequence database for searching with makeblastdb.\n\nmakeblastdb -in mouse.protein.faa -dbtype prot\n\nThen we can call BLAST to do a protein search:\n\n# Note that the outfile specified references the query and db names\nblastp -query mm.first.faa -db mouse.protein.faa -out mm.first.mouse.txt\n\n# Check the output with head or less\nless mm.first.mouse.txt\n\nYou can use spacebar to move down a page. q exits less.\n\n\n3.2.6.4 Challenge\nCreate a new query file from the first 498 lines of the file mouse.protein.faa, name it mm.second.faa, and use that to search mouse.protein.faa with blastp. Did BLAST take longer?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Command line & Git(Hub)</span>"
    ]
  },
  {
    "objectID": "lectures/lec03-command_git.html#reproducible-science-and-collaboration-with-github",
    "href": "lectures/lec03-command_git.html#reproducible-science-and-collaboration-with-github",
    "title": "3  Command line & Git(Hub)",
    "section": "3.3 Reproducible science and collaboration with Git(Hub)",
    "text": "3.3 Reproducible science and collaboration with Git(Hub)\n\n3.3.1 Introduction to version control using Git\nGit is a version control system that tracks changes in files. Although it is primarily used for software development, Git can be used to track changes in any files, and is an indispensable tool for collaborative projects. Using Git, we effectively create different versions of our files and track who has made what changes. The complete history of the changes for a particular project and their metadata make up a repository.\nTo sync the repository across different computers and to collaborate with others, Git is often used via GitHub, a web-based service that hosts Git repositories. In the spirit of “working open” and ensuring scientific reproducibility, it has also become increasingly common for scientists to upload scripts and related files to GitHub for others to use.\n\n\n3.3.2 Intro to GitHub\nGitHub allows for easy use of Git for collaborative purposes using a primarily point-and-click interface, in addition to providing a web-based hosting service for Git repositories (or “repos”). If you have not already made a GitHub account, do so now here.\nA new repository can be made by clicking on the + in the top right of the page and selecting “New repository”. For now, however, navigate to your provided group repo. All members of the group have been given admin access to a pre-made repository in the 2023-GroupX projects repo. All groups have been provided with existing repositories, but a new repository can be made by clicking on the + in the top right of the page and selecting “New repository”. For now, however, navigate to your provided group repo.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Command line & Git(Hub)</span>"
    ]
  },
  {
    "objectID": "lectures/lec03-command_git.html#version-control-with-git",
    "href": "lectures/lec03-command_git.html#version-control-with-git",
    "title": "3  Command line & Git(Hub)",
    "section": "3.4 Version Control with Git",
    "text": "3.4 Version Control with Git\n\n3.4.1 Setup and Installation\nGit is primarily used on the command line. The implementation of the command line that we’ll be using is known as a “bash interpreter”, or “shell”. While bash interpreters are natively available on Mac and Linux operating systems, Windows users may have to externally install a bash shell.\nTo install Git on a Mac, you will have to install Xcode, which can be downloaded from the Mac App Store. Be warned: Xcode is a very large download (approximately 6 GB)! Install Xcode and Git should be ready for use. (Note: most students will have already installed Xcode already for some R packages we used earlier in the course. If you’re not sure whether this is the case, run the git --version command described below)\nTo install Git (and Bash) on Windows, download Git for Windows from its website. At the time of writing, 2.19.1 is the most recent version. Download the exe file to get started. Git for Windows provides a program called Git Bash, which provides a bash interpreter that can also run Git commands.\nTo install Git for Linux, use the preferred package manager for your distribution and follow the instructions listed here.\nTo test whether Git has successfully been installed, open a bash interpreter, type:\n\ngit --version\n\nand hit Enter. If the interpreter returns a version number, Git has been installed.\n\n\n3.4.2 Getting started with Git\nFirst, we have to tell Git who we are. This is especially important when collaborating with Git and keeping track of who did what! Keep in mind that this only has to be done the first time we’re using Git.\nThis is done with the git config command. When using Git, all commands begin with git, followed by more specific subcommands.\n\ngit config --global user.name \"My Name\"\ngit config --global user.email \"myemail@example.com\"\n\nFinally, the following command can be used to review Git configurations:\n\ngit config --list\n\n\n\n3.4.3 Cloning local from main\nAfter configuring our name and email, we are ready for version control!\nFirst, we need to initialize our local repository, so we need to tell Git where to store our files on our computer. At the same time, we can also connect the two repositories: the local one on your computer, and the remote one on GitHub. We do both by making the GitHub repository a remote for the local repository.\nFirst, head to your group’s repo on GitHub, and click on the green “Clone or download” button on the right side of the page. This yields a link to your fork. Copy this link to your clipboard.\nOn the command line, run\n\ngit clone [repo link]\n\nwith the link in place of [repo link]. This process, known as cloning, will create a new folder in your current working directory that contains the contents of your GitHub folder. This also initializes your local repo.\nEnter this new folder with cd and type git status to make sure the repo has been cloned properly. git status should output that the branch is even with origin/main, indicating that it is currently the same as the current state of your fork.\n\n\n3.4.4 Adding and commiting files\nFor your projects, you will be making edits to your .Rmd file in RStudio, or you will be writing your report in text editors. For today, let’s use the file we created earlier and commit it.\nThe “untracked files” message means that there’s a file in the directory that Git isn’t keeping track of. We can tell Git to track a file using git add:\n\ngit add draft-yn.txt\n\nGit now knows that it’s supposed to keep track of mars.txt, but it hasn’t recorded these changes as a commit yet. To get it to do that, we need to run one more command:\n\ngit commit -m \"intro about my project\"\n\nWhen we run git commit, Git takes everything we have told it to save by using git add and stores a copy permanently inside the special .git directory. This permanent copy is called a commit (or revision) and its short identifier is a series of letters and numbers. Each commit has a unique identifier.\nWe use the -m flag to write a message that describes our edits specifically.\n\n\n3.4.5 Pushing changes\nNow, let’s push our changes from the local repo to the main repo.\nWe can push our changes:\n\ngit push origin main\n\nThe first time you run the push subcommand, you may get a prompt asking you to enter your GitHub username and password. If you are entering your password and nothing pops up, don’t worry! Your keystrokes are being recognized, although there is no visual cue for it.\nNow, running git status shows us that our local repo is up-to- date with origin/main.\nIf we navigate to GitHub, we now see that we have our updates in the main repo, and there is a comment “intro about my project” associated with the commit.\nGit commit history is a directed acyclic graph (or DAG), which means that every single commit always has at least one “parent” commit (the previous commit(s) in the history), and any individual commit can have multiple “children”. This history can be traced back through the “lineage” or “ancestry”.\n\n\n3.4.6 Pulling changes\nWhen other group members add to the shared repo, you have to make sure those edits have been incorporated into your repo before making new changes of your own. This ensures that there aren’t any conflicts within files, wherein your edits clash with someone else’s if one of you is working with an earlier version of the file.\nTo remain up to date, navigate to the local copy of your repo.\nFirst, you have to fetch the new changes that are in the shared origin repo.\n\ngit fetch origin\n\nOnce the edits have been downloaded from the origin, merge them into your local main repo:\n\ngit merge origin/main\n\nNote that the git pull command combines two other commands, git fetch and git merge.\n\ngit pull\n\nThis will download any changes your group members may have made and update your local versions of your fork accordingly.\nYour local copy is now even with the shared repo!\nBefore starting any edits of your own, it’s usually a good idea to start off by checking to see whether anything’s been added to the main repo and, if needed, pulling those changes\n\n\n3.4.7 Summary\nBasic Git commands:\n\ngit init (or git clone)\ngit status\ngit add\ngit commit\ngit push\ngit log\ngit checkout\ngit help\n\nThe “GitHub flow” (so far):\n\nMake sure your local is up to date using git pull\nMake your edits\nAdd your edits with git add\nCommit your edits with an informative commit message\nPush your edits\nRepeat\n\n\n3.4.7.1 GitHub Issues\nFinally, each repo on GitHub also has an Issues tab at the top of the page. Here, you and your group can create posts regarding the content of the repo that highlight issues with code or serve as to-do lists to manage outstanding tasks with.\nAlthough issues aren’t needed for any of the steps we discussed above, it can be useful to create a roadmap of your project with them and assign group members to specific tasks if need be.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Command line & Git(Hub)</span>"
    ]
  },
  {
    "objectID": "lectures/lec03-command_git.html#extra-material-best-practices-for-collaboration",
    "href": "lectures/lec03-command_git.html#extra-material-best-practices-for-collaboration",
    "title": "3  Command line & Git(Hub)",
    "section": "3.5 EXTRA MATERIAL: best practices for collaboration",
    "text": "3.5 EXTRA MATERIAL: best practices for collaboration\nUsually, there are several other steps involved during collaboration. At this point, we are able to push our edits from our local machine straight to the final version on the GitHub repo. This can be dangerous if no one else is checking over your code, especially if every team member has direct access to change the contents of the main repo, and all of your project files. These additional steps include using forks and branches.\n\n3.5.1 Creating a fork\nThe repos that have already been created can be thought of as “main repos”, which will contain the “primary” version of the repo at any given point in time. However, instead of directly uploading and editing files right within this main repo itself, usually collaborators will be begin by forking the repo. When a given user forks a repo, GitHub creates a user-specific copy of the repo and all its files in a remote location.\nHaving a forked copy means that the developer who performs the fork will have complete control over the newly copied codebase. Developers who contributed to the Git repository that was forked will have no knowledge of the newly forked repo. Previous contributors will have no means with which they can contribute to or synchronize with the Git fork unless the developer who performed the fork operation provides access to them.\nSome famous examples of Git forks include:\n\nFire OS for Kindle Fire, which is a fork of Android\nLibreOffice, which is a fork of OpenOffice\nUbuntu, which is a fork of Debian\nMariaDB, from MySQL\n\nYou’ll notice that on the top left of this repo page, the repo’s name will be “[your username] / 2023-GroupX”, as opposed to “EEB313 / 2023-GroupX”. Furthermore, GitHub will indicate that this is a fork right underneath said repo name (“forked from eeb313-[year]/ [repo name]”).\n\n\n3.5.2 Setting up your remotes\nNow we connect the three repositories: the local repo on your computer, the forked repo on GitHub, and the main group remote repo on GitHub. We do this by making the GitHub repository a remote for the local repository.\nTo get your fork up to date with the main repo, you next have to add a remote linking to the main repo. Head to your group’s repo and once again click on “Clone or download” to grab its link. Then, using the main repo link, run:\n\ngit remote add upstream [repo link]\n\nThen, we have to update our forked repo. Earlier, since we didn’t have a forked repo, we only had a link between our local computer version of the folder and the shared version. When we cloned the shared version, we created that remote link. Now, we have changed that shared repo as the upstream remote, and we have a fork that acts as an intermediate step. Let’s change our fork to become the main remote.\nRun:\n\ngit remote -v\n\nto get a list of existing remotes. This should return four links, two of which are labelled origin and two of which are labelled upstream. At this point, they should be the same link. We need to update our main remote by removing it and adding our fork link.\nTo remove our origin remote, run:\n\ngit remote rm origin\n\nThen, let’s add our fork:\n\ngit remote add origin [repo link]\n\nNow, when you run :\n\ngit remote -v\n\nyou should find two links for origin and two links for upstream. These two links are used to fetch from upstream, and one to push from main to upstream. The links for origin are your main forked repo on your own GitHub, while the links for upstream are the EEB313-2023-GroupX repo.\n\n\n3.5.3 Syncing your fork\nNext, you have to fetch the new changes that are in the shared repo.\n\ngit fetch upstream\n\nOnce the edits have been downloaded from upstream, merge them into your local repo:\n\ngit merge upstream/main\n\nYour local copy is now even with the main repo! Finally, push these changes to the GitHub version of your fork (origin) from your main local repo.\n\ngit push origin main\n\nNow the GitHub version of the fork is all synced up, ready for your next batch of edits, and eventually another pull request!\n\n\n3.5.4 Making edits\nEvery time you make edits to your local files, first make sure you are first syncing any changes from upstream to your local to your origin (fork).\nThen, go ahead and make your edits! After you are done your batch of editing, you can add and commit those changes, still using git add and git commit.\nThese commits do not go to the upstream EEB313 repo, but instead end up in your forked repo. In order to contribute your changes from your fork to upstream, you will need to make a Pull Request (PR).\n\n\n3.5.5 Pull requests\nAfter a commit has been made, head to your fork. GitHub will have noticed that there are new edits that you can contribute. Click “Contribute” and “Open Pull Request”. A PR is GitHub’s way of neatly packaging one or more commits that have been made into a single request, and then you can merge said commits into upstream. In our case, a PR is essentially you saying: “Here are all the edits I’ve made. Have a look, and add them to main if you think they’re ready to go.”\nFollowing a pull request pending, GitHub takes you to the “Pull Requests” tab of the repo and prompts you to write about your pull request (i.e., describe the changes you’re attempting to merge). Here, you can (and should) explain the changes you’ve made for your collaborators, so that they know what to look for and review. Be specific and detailed to save your group members’ time – it’s a good idea to start off your pull request message with an overall summary (“adding dplyr code to clean dataset”) followed by a point-form list of what changes have been made, if necessary.\nOnce the pull request has been made, GitHub will list both your message and your commit messages below. Clicking on any of these commits opens up a new page highlighting the changes made in that specific commit. You also have the option of merging the pull request yourself – but don’t do this! When collaborating, always have someone else review and merge your pull request.\nIf all does not look good, your team members can add messages below, and tag others using the @ symbol, similarly to most social networks. If more changes are needed before the pull request is ready to merge, any new commits you make to the main branch on your fork will automatically be added on to the pull request. This way, you can incorporate any changes or fixes suggested by your team members simply by continuing to work in your fork until your changes are ready to merge. For line-specific edits, if a file is opened up (i.e., by clicking on one of the commits), clicking on the + button that appears when hovering over a line number will allow you or a group member to add a comment specifically attached to that line. This can be useful when pointing out typos, for instance, among other things.\n\n\n3.5.6 Creating a branch\nA branch is used to isolate development work without affecting other branches in the repository. Each repository has one default branch, and can have multiple other branches. Branches allow you to develop features, fix bugs, or safely experiment with new ideas in a contained area of your repository.\nThe main branch should be thought of as the actual current state of your project – branches are meant, by design, to be temporary, and exist only to facilitate edits and experimental work while avoiding any risk of breaking the original codebase. Branches can keep experimental work separate; for example, you can create a separate branch from your main branch so any trials or bugs only exist in your branch.\nBranches are simply a named pointer to a commit in the DAG, which automatically move forward as commits are made. Divergent commits (two commits with the same parent) could be considered “virtual” branches. Since they are simply pointers, branches in Git are very lightweight.\nYou always create a branch from an existing branch, typically, the default branch of your repository. You can then work on this new branch in isolation from changes that other people are making to the repository. A branch you create to build a feature is commonly referred to as a feature branch or topic branch.\nTo create a new branch, run:\n\ngit branch &lt;branch-name&gt;\n\nFor example:\n\ngit branch new-feature\n\nThe repository history remains unchanged. Then, we need to record all new commits to that branch.\n\ngit checkout new-feature\n\nNote that you can combine branch creation and checkout by using only one command:\n\ngit checkout -b new-feature\n\nYou should see the confirmation: Switched to a new branch \"new-feature\"\nMake your edits, and commit them to your branch using git add and git commit. When you are ready to merge your new feature to your local main branch, head back to your main branch:\n\ngit checkout main\n\nAnd merge your new features from the new-feature branch:\n\ngit merge new-feature\n\nNow you can delete your branch, since it has been successfully merged:\n\ngit branch -d new-feature\n\nThis is a common workflow for short-lived topic branches that are used more as an isolated development than an organizational tool for longer-running features.\nNote that this is all happening locally. You now push your changes from your local main to upstream or origin. Don’t forget to create a pull request from your fork to the upstream EEB313 repo if you are using forks!\nHowever, you are also able to create a new branch and push that local branch to upstream or origin. To do this, make sure you are in your new branch:\n\ngit checkout new-feature\n\nThen, git add and git commit as you normally would. This time, instead of merging your new branch to your local main, git push to push them to your forked repo or upstream.\nNow, running git status shows us that our new-feature branch on our local repo is up-to- date with origin/main.\nIf we navigate to GitHub, we now see that we have a new branch in our forked repo (and remember that there should a comment associated with the commit)/\nNow, you can submit a pull request for the rest of your team to check on the repo. Once they review the changes, they can merge the PR, and the final version will show up on the upstream repo!\nOnce a pull request has been merged into the main repo, the new-feature branch (or whatever you have named your branch) isn’t needed anymore. Because of this, GitHub will immediately prompt you to delete the this branch as soon as the merge has been completed right in the PR.\nTo list the branches that are currently being used, use this for local branches:\n\ngit branch\ngit branch --list\n\nThe git branch command lets you create, list, rename, and delete branches. It doesn’t let you switch between branches or put a forked history back together again. For this reason, git branch is tightly integrated with the git checkout and git merge commands.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Command line & Git(Hub)</span>"
    ]
  },
  {
    "objectID": "lectures/lec03-command_git.html#tldr-refer-back-to-this",
    "href": "lectures/lec03-command_git.html#tldr-refer-back-to-this",
    "title": "3  Command line & Git(Hub)",
    "section": "3.6 TL;DR (refer back to this!)",
    "text": "3.6 TL;DR (refer back to this!)\nThe general collaborative workflow is as follows:\n\nFirst, create a branch of the main repo.\nMake edits in the branch. These could involve adding/deleting lines of code or even adding/removing entire files. Keep in mind that the branch is separate from the main codebase, so don’t worry too much about deleting things or making large changes.\nOnce you have made your changes in this branch, submit what’s known as a pull request (PR) from this edited branch to main. A PR neatly packages all the edits that have been made in your branch for review by other members of your group.\nOnce your changes have been approved, merge the PR. It’s good practice to have group members merge your PRs instead of doing it yourself.\n\nAlthough this process may seem a bit laborious, using this method (also known as the “GitHub flow”) minimizes chances of error and ensures that all code is reviewed by at least one other person. Understanding how and why this process works is key to collaborative work in software development and the like, and is used by all sorts of open source projects on GitHub (including dplyr itself!)\n\n3.6.1 Git Terminology\n\nrepository (short form: repo): a storage area for a project containing all the files for the project and the history of all the changes made to those files\nlocal copy: the version of file stored on your own computer\nremote copy: the version of a file stored outside of your own computer, for example stored on an external server, perhaps at GitHub. Remotes are referenced by nicknames, e.g., origin or upstream.\nbranch: a named series of commits. The default branch that you download is usually called gh-pages or main. Creating a new branch makes a parallel version of a repository where changes can be made that affect the new branch only and not the original (base) version of the repository. New branches are often used to test changes or new ideas, which can later be merged with the base branch. Moving from one branch to another is called checking out a new branch.\nfork (GitHub-specific term): to copy a repository from another user and store it under your own GitHub account. Can also refer to the copied repo itself.\ngh-pages (GitHub-specific term): stands for “GitHub Pages”. This is often the default branch in repositories. Branches called gh-pages can be published as webpages hosted by GitHub.\norigin: the main remote repository you want to download files from or compare local changes you have made to. When you’ve forked a repository, your origin is your new copy of the repository in your account.\nupstream: the original repository you made your fork from. Both origin and upstream are remote repositories.\ncommit: to save changes in your working directory to your local repository\npush: send committed changes you have made on your local computer to a remote repository. For a change to show up on GitHub, the committed changes must be pushed from your computer to the remote repository.\npull: download changes from a remote repository to your local version of the same repository. This is useful when other people have made changes to a shared project, and you want to download (pull) the changes from the shared remote repository to your own computer.\npull request (GitHub-specific term, abbreviated as “PR”): send proposed changes from a specific version of a repository back to the main version of a repository to be considered for incorporation by the people maintaining the repository (the maintainers). You are requesting that the maintainers pull your changes into their repository.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Command line & Git(Hub)</span>"
    ]
  },
  {
    "objectID": "lectures/lec03-command_git.html#additional-resources",
    "href": "lectures/lec03-command_git.html#additional-resources",
    "title": "3  Command line & Git(Hub)",
    "section": "3.7 Additional resources:",
    "text": "3.7 Additional resources:\n\nA visual demonstration of the GitHub flow.\nA useful Git command cheat sheet.\nGuide to a good Git Workflow\nThis textbook",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Command line & Git(Hub)</span>"
    ]
  },
  {
    "objectID": "lectures/lec03-command_git.html#footnotes",
    "href": "lectures/lec03-command_git.html#footnotes",
    "title": "3  Command line & Git(Hub)",
    "section": "",
    "text": "Trust us on this!↩︎",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Command line & Git(Hub)</span>"
    ]
  },
  {
    "objectID": "lectures/lec04-data-wrangling.html",
    "href": "lectures/lec04-data-wrangling.html",
    "title": "4  Data wrangling!",
    "section": "",
    "text": "4.1 Lesson preamble",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling!</span>"
    ]
  },
  {
    "objectID": "lectures/lec04-data-wrangling.html#lesson-preamble",
    "href": "lectures/lec04-data-wrangling.html#lesson-preamble",
    "title": "4  Data wrangling!",
    "section": "",
    "text": "4.1.1 Learning objectives\n\nUnderstand the purpose of the dplyr package.\nLearn to use data wrangling commands select, filter, %&gt;%, and mutate from the dplyr package.\nUnderstand the split-apply-combine concept for data analysis.\nUse summarize, group_by, and tally to split a data frame into groups of observations, apply a summary statistics for each group, and then combine the results.\nLearn to switch between long and wide format\n\n4.1.2 Lesson outline\n\nR packages for data analyses (10 min)\nData wrangling in dplyr (40 min)\nSplit-apply-combine techniques in dplyr (25 min)\nUsing group_by and tally to summarize categorical data (20 mins)\nReshaping data (15 mins)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling!</span>"
    ]
  },
  {
    "objectID": "lectures/lec04-data-wrangling.html#r-packages-for-data-analyses",
    "href": "lectures/lec04-data-wrangling.html#r-packages-for-data-analyses",
    "title": "4  Data wrangling!",
    "section": "4.2 R packages for data analyses",
    "text": "4.2 R packages for data analyses\nR packages are basically bundles of functions that perform related tasks. There are many some that will be come with a base install of R since they are considered critical for using R, such as c(), mean(), +, -, etc.\nThere is an official repository for R-packages beyond the base packages called CRAN (Comprehensive R Archive Network). CRAN has thousands of packages, and all these cannot be installed by default, because then base R installation would be huge and most people would only be using a fraction of everything installed on their machine. It would be like if you downloaded the Firefox or Chrome browser and you would get all extensions and add-ons installed by default, or as if your phone came with every app ever made for it already installed when you bought it: quite impractical.\nWe can install new packages using the function install.packages(). You only need to do this once, so we’ll pass eval=FALSE to knitr at the top of our code chunk to make sure that the chunk won’t be evaluated when we knit the document. You can find other possible options to pass that can be helpful for formatting your output document.\nWhile we’re looking at the {} section of our code chunk, we might note that it starts with “r”. This specifies that this chunk is written in R and you could tell RStudio to to instead interpret the code in the chunk as a different language like bash (command line) or python. You can also specify names for the chunks. The knitr options are probably the most useful part of this section, though!\n\ninstall.packages('tidyverse')\n\ntidyverse1 is a large collection of packages with similar functions, similar to the way Microsoft Word is part of Microsoft Office. tidyverse, as its name may suggest, contains many packages that makes data cleaning and exploring more intuitive and effective. It is basically an entire philosophy on how to handle data and has a massive following.\nThe two tidyverse packages we will be using the most frequently in this course is dplyr and ggplot2. dplyr is great for data wrangling (Lecture 2) and ggplot2 makes killer plots (Lecture 3).\nTo use functions in the dplyr package, type dplyr:: and then the function name.\n\ndplyr::glimpse(cars) # `glimpse` is similar to `str`\n\nRows: 50\nColumns: 2\n$ speed &lt;dbl&gt; 4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13…\n$ dist  &lt;dbl&gt; 2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34…\n\n# cars is a built-in data set\n\nSince we will be using this package a lot, it would be a little annoying to have to type dplyr:: every time. We can bypass this step by loading the package into our current environment. Think of this is “opening” the package for your work session.\n\n# We could also do `library(dplyr)`, but we need the rest of the\n# tidyverse packages later, so we might as well import the entire collection.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nglimpse(cars)\n\nRows: 50\nColumns: 2\n$ speed &lt;dbl&gt; 4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13…\n$ dist  &lt;dbl&gt; 2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34…\n\n\nThis needs to be done once for every new R session, and so it is common practice to keep a list of all the packages used at the top of your script or notebook for convenience and load all of it at start up.\nThat’s a lot of red though! What are these warning signs and checks?\nAll the warning signs indicate are the version of R that they were built under. They can frequently be ignored unless your version of R is so old that the packages can no longer be run on R! Note that packages are frequently updated, and functions may become deprecated.\nNext, the warning shows you all the packages that were successfully installed.\nFinally, there are some conflicts! All this means is that there are multiple functions with the same name that may do different things. R prioritizes functions from certain packages over others. So, in this case, the filter() function from dplyr will take precedent over the filter() function from the stats package. If you want to use the latter, use double colons :: to indicate that you are calling a function from a certain package:",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling!</span>"
    ]
  },
  {
    "objectID": "lectures/lec04-data-wrangling.html#data-wrangling-with-dplyr",
    "href": "lectures/lec04-data-wrangling.html#data-wrangling-with-dplyr",
    "title": "4  Data wrangling!",
    "section": "4.3 Data wrangling with dplyr",
    "text": "4.3 Data wrangling with dplyr\nWrangling here is used in the sense of maneuvering, managing, controlling, and turning your data upside down and inside out to look at it from different angles in order to understand it. The package dplyr provides easy tools for the most common data manipulation tasks. It is built to work directly with data frames, with many common tasks optimized by being written in a compiled language (C++), this means that many operations run much faster than similar tools in R. An additional feature is the ability to work directly with data stored in an external database, such as SQL-databases. The ability to work with databases is great because you are able to work with much bigger datasets (100s of GB) than your computer could normally handle. We will not talk in detail about this in class, but there are great resources online to learn more (e.g. this lecture from Data Carpentry).\n\n4.3.1 Selecting columns and filtering rows\nWe’re going to learn some of the most common dplyr functions: select(), filter(), mutate(), group_by(), and summarise(). To select columns of a data frame, use select(). The first argument to this function is the data frame (surveys), and the subsequent arguments are the columns to keep. Note that we don’t need quotation marks around the column names here like with did with base R. You do still need quotation marks around strings, though!\n\nselect(surveys, plot_id, species_id, weight, year) %&gt;% head()\n\n  plot_id species_id weight year\n1       2         NL     NA 1977\n2       2         NL     NA 1977\n3       2         NL     NA 1977\n4       2         NL     NA 1977\n5       2         NL     NA 1977\n6       2         NL     NA 1977\n\n# head explained below, allows us to see first couple of rows of the data frame\n\nTo choose rows based on a specific criteria, use filter():\n\nfilter(surveys, year == 1995) %&gt;% head()\n\n  record_id month day year plot_id species_id sex hindfoot_length weight\n1     22314     6   7 1995       2         NL   M              34     NA\n2     22728     9  23 1995       2         NL   F              32    165\n3     22899    10  28 1995       2         NL   F              32    171\n4     23032    12   2 1995       2         NL   F              33     NA\n5     22003     1  11 1995       2         DM   M              37     41\n6     22042     2   4 1995       2         DM   F              36     45\n      genus  species   taxa plot_type\n1   Neotoma albigula Rodent   Control\n2   Neotoma albigula Rodent   Control\n3   Neotoma albigula Rodent   Control\n4   Neotoma albigula Rodent   Control\n5 Dipodomys merriami Rodent   Control\n6 Dipodomys merriami Rodent   Control\n\n\n\n4.3.1.1 An aside on conditionals\nNote that to check for equality, R requires two equal signs (==). This is to prevent confusion with object assignment, since otherwise year = 1995 might be interpreted as ‘set the year parameter to 1995’, which is not what filter does!\nBasic conditionals in R are broadly similar to how they’re already expressed mathematically:\n\n2 &lt; 3\n\n[1] TRUE\n\n5 &gt; 9\n\n[1] FALSE\n\n\nHowever, there are a few idiosyncrasies to be mindful of for other conditionals:\n\n2 != 3 # not equal\n\n[1] TRUE\n\n2 &lt;= 3 # less than or equal to\n\n[1] TRUE\n\n5 &gt;= 9 # greater than or equal to\n\n[1] FALSE\n\n\nFinally, the %in% operator is used to check for membership:\n\n2 %in% c(2, 3, 4) # check whether 2 in c(2, 3, 4)\n\n[1] TRUE\n\n\nAll of the above conditionals are compatible with filter, with the key difference being that filter expects column names as part of conditional statements instead of individual numbers.\n\n\n\n4.3.2 Chaining functions together using pipes\nBut what if you wanted to select and filter at the same time? There are three ways to do this: use intermediate steps, nested functions, or pipes. With intermediate steps, you essentially create a temporary data frame and use that as input to the next function. This can clutter up your workspace with lots of objects:\n\ntemp_df &lt;- select(surveys, plot_id, species_id, weight, year)\nfilter(temp_df, year == 1995) %&gt;% head()\n\n  plot_id species_id weight year\n1       2         NL     NA 1995\n2       2         NL    165 1995\n3       2         NL    171 1995\n4       2         NL     NA 1995\n5       2         DM     41 1995\n6       2         DM     45 1995\n\n\nYou can also nest functions (i.e. one function inside of another). This is handy, but can be difficult to read if too many functions are nested as things are evaluated from the inside out. Readability can be mildly improved by enabling “rainbow parentheses” (open settings &gt; Code &gt; Display and check rainbow parentheses), but it’s still basically impossible to document and effectively convey your work with this method.\n\nfilter(select(surveys, plot_id, species_id, weight, year), year == 1995) %&gt;% head()\n\n  plot_id species_id weight year\n1       2         NL     NA 1995\n2       2         NL    165 1995\n3       2         NL    171 1995\n4       2         NL     NA 1995\n5       2         DM     41 1995\n6       2         DM     45 1995\n\n\nThe last option, pipes, are a fairly recent addition to R. Pipes let you take the output of one function and send it directly to the next, which is useful when you need to do many things to the same dataset. Pipes in R look like %&gt;% and are made available via the magrittr package that also is included in the tidyverse. If you use RStudio, you can type the pipe with Ctrl/Cmd + Shift + M.\n\nsurveys %&gt;% \n    select(., plot_id, species_id, weight, year) %&gt;% \n    filter(., year == 1995) %&gt;% head()\n\n  plot_id species_id weight year\n1       2         NL     NA 1995\n2       2         NL    165 1995\n3       2         NL    171 1995\n4       2         NL     NA 1995\n5       2         DM     41 1995\n6       2         DM     45 1995\n\n\nThe . refers to the object that is passed from the previous line. In this example, the data frame surveys is passed to the . in the select() statement. Then, the modified data frame which is the result of the select() operation, is passed to the . in the filter() statement. Put more simply: whatever was the result from the line above the current line, will be used in the current line.\nSince it gets a bit tedious to write out all the dots, dplyr allows for them to be omitted. By default, the pipe will pass its input to the first argument of the right hand side function; in dplyr, the first argument is always a data frame. The chunk below gives the same output as the one above:\n\nsurveys %&gt;% \n    select(plot_id, species_id, weight, year) %&gt;% \n    filter(year == 1995) %&gt;% \n  head()\n\n  plot_id species_id weight year\n1       2         NL     NA 1995\n2       2         NL    165 1995\n3       2         NL    171 1995\n4       2         NL     NA 1995\n5       2         DM     41 1995\n6       2         DM     45 1995\n\n\nAnother example:\n\nsurveys %&gt;%\n  filter(weight &lt; 5) %&gt;%\n  select(species_id, sex, weight) %&gt;% head()\n\n  species_id sex weight\n1         PF   F      4\n2         PF   F      4\n3         PF   M      4\n4         RM   F      4\n5         RM   M      4\n6         PF          4\n\n\nIn the above code, we use the pipe to send the surveys dataset first through filter() to keep rows where weight is less than 5, then through select() to keep only the species_id, sex, and weight columns. Since %&gt;% takes the object on its left and passes it as the first argument to the function on its right, we don’t need to explicitly include it as an argument to the filter() and select() functions anymore.\nIf this runs off your screen and you just want to see the first few rows, you can use a pipe to view the head() of the data. (Pipes work with non-dplyr functions, too, as long as either the dplyr or magrittr package is loaded).\n\nsurveys %&gt;%\n  filter(weight &lt; 5) %&gt;%\n  select(species_id, sex, weight) %&gt;% \n  head()\n\n  species_id sex weight\n1         PF   F      4\n2         PF   F      4\n3         PF   M      4\n4         RM   F      4\n5         RM   M      4\n6         PF          4\n\n\nIf we wanted to create a new object with this smaller version of the data, we could do so by assigning it a new name:\n\nsurveys_sml &lt;- surveys %&gt;%\n  filter(weight &lt; 5) %&gt;%\n  select(species_id, sex, weight)\n\nsurveys_sml\n\n   species_id sex weight\n1          PF   F      4\n2          PF   F      4\n3          PF   M      4\n4          RM   F      4\n5          RM   M      4\n6          PF          4\n7          PP   M      4\n8          RM   M      4\n9          RM   M      4\n10         RM   M      4\n11         PF   M      4\n12         PF   F      4\n13         RM   M      4\n14         RM   M      4\n15         RM   F      4\n16         RM   M      4\n17         RM   M      4\n\n\nNote that the final data frame is the leftmost part of this expression.\nA single expression can also be used to filter for several criteria, either matching all criteria (&) or any criteria (|):\n\nsurveys %&gt;% \n    filter(taxa == 'Rodent' & sex == 'F') %&gt;% \n    select(sex, taxa) %&gt;% head()\n\n  sex   taxa\n1   F Rodent\n2   F Rodent\n3   F Rodent\n4   F Rodent\n5   F Rodent\n6   F Rodent\n\n\n\nsurveys %&gt;% \n    filter(species == 'clarki' | species == 'leucophrys') %&gt;% \n    select(species, taxa) %&gt;% head()\n\n     species    taxa\n1 leucophrys    Bird\n2     clarki Reptile\n3 leucophrys    Bird\n\n\n\n4.3.2.1 Challenge\nUsing pipes, subset the survey data to include individuals collected before 1995 and retain only the columns year, sex, and weight.\n\n\n\n4.3.3 Creating new columns with mutate\nFrequently, you’ll want to create new columns based on the values in existing columns. For instance, you might want to do unit conversions, or find the ratio of values in two columns. For this we’ll use mutate().\nTo create a new column of weight in kg:\n\nsurveys %&gt;%\n    mutate(weight_kg = weight / 1000) %&gt;% head()\n\n  record_id month day year plot_id species_id sex hindfoot_length weight\n1         1     7  16 1977       2         NL   M              32     NA\n2        72     8  19 1977       2         NL   M              31     NA\n3       224     9  13 1977       2         NL                  NA     NA\n4       266    10  16 1977       2         NL                  NA     NA\n5       349    11  12 1977       2         NL                  NA     NA\n6       363    11  12 1977       2         NL                  NA     NA\n    genus  species   taxa plot_type weight_kg\n1 Neotoma albigula Rodent   Control        NA\n2 Neotoma albigula Rodent   Control        NA\n3 Neotoma albigula Rodent   Control        NA\n4 Neotoma albigula Rodent   Control        NA\n5 Neotoma albigula Rodent   Control        NA\n6 Neotoma albigula Rodent   Control        NA\n\n\nYou can also create a second new column based on the first new column within the same call of mutate():\n\nsurveys %&gt;%\n    mutate(weight_kg = weight / 1000,\n           weight_kg2 = weight_kg * 2) %&gt;% head()\n\n  record_id month day year plot_id species_id sex hindfoot_length weight\n1         1     7  16 1977       2         NL   M              32     NA\n2        72     8  19 1977       2         NL   M              31     NA\n3       224     9  13 1977       2         NL                  NA     NA\n4       266    10  16 1977       2         NL                  NA     NA\n5       349    11  12 1977       2         NL                  NA     NA\n6       363    11  12 1977       2         NL                  NA     NA\n    genus  species   taxa plot_type weight_kg weight_kg2\n1 Neotoma albigula Rodent   Control        NA         NA\n2 Neotoma albigula Rodent   Control        NA         NA\n3 Neotoma albigula Rodent   Control        NA         NA\n4 Neotoma albigula Rodent   Control        NA         NA\n5 Neotoma albigula Rodent   Control        NA         NA\n6 Neotoma albigula Rodent   Control        NA         NA\n\n\nThe first few rows of the output are full of NAs, so if we wanted to remove those we could insert a filter() in the chain:\n\nsurveys %&gt;%\n    filter(!is.na(weight)) %&gt;%\n    mutate(weight_kg = weight / 1000) %&gt;% head()\n\n  record_id month day year plot_id species_id sex hindfoot_length weight\n1       588     2  18 1978       2         NL   M              NA    218\n2       845     5   6 1978       2         NL   M              32    204\n3       990     6   9 1978       2         NL   M              NA    200\n4      1164     8   5 1978       2         NL   M              34    199\n5      1261     9   4 1978       2         NL   M              32    197\n6      1453    11   5 1978       2         NL   M              NA    218\n    genus  species   taxa plot_type weight_kg\n1 Neotoma albigula Rodent   Control     0.218\n2 Neotoma albigula Rodent   Control     0.204\n3 Neotoma albigula Rodent   Control     0.200\n4 Neotoma albigula Rodent   Control     0.199\n5 Neotoma albigula Rodent   Control     0.197\n6 Neotoma albigula Rodent   Control     0.218\n\n\nis.na() is a function that determines whether something is an NA. The ! symbol negates the result, so we’re asking for everything that is not an NA.\n\n4.3.3.1 Challenge\nCreate a new data frame from the surveys data that meets the following criteria: contains only the species_id column and a new column called hindfoot_half containing values that are half the hindfoot_length values. In this hindfoot_half column, there are no NAs and all values are less than 30.\nHint: think about how the commands should be ordered to produce this data frame!",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling!</span>"
    ]
  },
  {
    "objectID": "lectures/lec04-data-wrangling.html#split-apply-combine-techniques-in-dplyr",
    "href": "lectures/lec04-data-wrangling.html#split-apply-combine-techniques-in-dplyr",
    "title": "4  Data wrangling!",
    "section": "4.4 Split-apply-combine techniques in dplyr",
    "text": "4.4 Split-apply-combine techniques in dplyr\nMany data analysis tasks can be approached using the split-apply-combine paradigm: split the data into groups, apply some analysis to each group, and then combine the results.\ndplyr facilitates this workflow through the use of group_by() to split data and summarize(), which collapses each group into a single-row summary of that group. The arguments to group_by() are the column names that contain the categorical variables for which you want to calculate the summary statistics. Let’s view the mean weight by sex.\n\nsurveys %&gt;%\n    group_by(sex) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n# A tibble: 3 × 2\n  sex   mean_weight\n  &lt;chr&gt;       &lt;dbl&gt;\n1 \"\"             NA\n2 \"F\"            NA\n3 \"M\"            NA\n\n\nThe mean weights become NA since there are individual observations that are NA. Let’s remove those observations.\n\nsurveys %&gt;%\n    filter(!is.na(weight)) %&gt;%\n    group_by(sex) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n# A tibble: 3 × 2\n  sex   mean_weight\n  &lt;chr&gt;       &lt;dbl&gt;\n1 \"\"           64.7\n2 \"F\"          42.2\n3 \"M\"          43.0\n\n\nThere is one row here that is neither male nor female, these are observations where the animal escaped before the sex could not be determined. Let’s remove those as well.\n\nsurveys %&gt;%\n    filter(!is.na(weight) & !is.na(sex)) %&gt;%\n    group_by(sex) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n# A tibble: 3 × 2\n  sex   mean_weight\n  &lt;chr&gt;       &lt;dbl&gt;\n1 \"\"           64.7\n2 \"F\"          42.2\n3 \"M\"          43.0\n\n\nYou can also group by multiple columns:\n\nsurveys %&gt;%\n    filter(!is.na(weight) & !is.na(sex)) %&gt;%\n    group_by(genus, sex) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n`summarise()` has grouped output by 'genus'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 28 × 3\n# Groups:   genus [10]\n   genus       sex   mean_weight\n   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;\n 1 Baiomys     \"F\"          9.16\n 2 Baiomys     \"M\"          7.36\n 3 Chaetodipus \"\"          19.8 \n 4 Chaetodipus \"F\"         23.8 \n 5 Chaetodipus \"M\"         24.7 \n 6 Dipodomys   \"\"          81.4 \n 7 Dipodomys   \"F\"         55.2 \n 8 Dipodomys   \"M\"         56.2 \n 9 Neotoma     \"\"         168.  \n10 Neotoma     \"F\"        154.  \n# ℹ 18 more rows\n\n\nSince we will use the same filtered and grouped data frame in multiple code chunks below, we could assign this subset of the data to a new variable and use this variable in the subsequent code chunks instead of typing out the functions each time.\n\nfiltered_surveys &lt;- surveys %&gt;%\n    filter(!is.na(weight) & !is.na(sex)) %&gt;%\n    group_by(genus, sex)\n\nIf you want to display more data, you can use the print() function at the end of your chain with the argument n specifying the number of rows to display.\n\nfiltered_surveys %&gt;%\n    summarize(mean_weight = mean(weight)) %&gt;%\n    print(n = 15) # Will change the knitted output, not the notebook\n\n`summarise()` has grouped output by 'genus'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 28 × 3\n# Groups:   genus [10]\n   genus       sex   mean_weight\n   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;\n 1 Baiomys     \"F\"          9.16\n 2 Baiomys     \"M\"          7.36\n 3 Chaetodipus \"\"          19.8 \n 4 Chaetodipus \"F\"         23.8 \n 5 Chaetodipus \"M\"         24.7 \n 6 Dipodomys   \"\"          81.4 \n 7 Dipodomys   \"F\"         55.2 \n 8 Dipodomys   \"M\"         56.2 \n 9 Neotoma     \"\"         168.  \n10 Neotoma     \"F\"        154.  \n11 Neotoma     \"M\"        166.  \n12 Onychomys   \"\"          23.4 \n13 Onychomys   \"F\"         26.8 \n14 Onychomys   \"M\"         26.2 \n15 Perognathus \"\"           6   \n# ℹ 13 more rows\n\n\nOnce the data are grouped, you can also summarize multiple variables at the same time. For instance, we could add a column indicating the minimum weight for each species for each sex:\n\nfiltered_surveys %&gt;%\n    summarize(mean_weight = mean(weight),\n              min_weight = min(weight))\n\n`summarise()` has grouped output by 'genus'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 28 × 4\n# Groups:   genus [10]\n   genus       sex   mean_weight min_weight\n   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;      &lt;int&gt;\n 1 Baiomys     \"F\"          9.16          6\n 2 Baiomys     \"M\"          7.36          6\n 3 Chaetodipus \"\"          19.8          10\n 4 Chaetodipus \"F\"         23.8           5\n 5 Chaetodipus \"M\"         24.7           4\n 6 Dipodomys   \"\"          81.4          24\n 7 Dipodomys   \"F\"         55.2          10\n 8 Dipodomys   \"M\"         56.2          12\n 9 Neotoma     \"\"         168.           83\n10 Neotoma     \"F\"        154.           32\n# ℹ 18 more rows\n\n\n\n4.4.0.1 Challenge\n\nUse group_by() and summarize() to find the mean, min, and max hindfoot length for each species.\nWhat was the heaviest animal measured in each year? Return the columns year, genus, species, and weight.\n\n\n\n4.4.1 Using tally to summarize categorical data\nWhen working with data, it is also common to want to know the number of observations found for each factor or combination of factors. For this, dplyr provides tally(). For example, if we want to group by taxa and find the number of observations for each taxa, we would do:\n\nsurveys %&gt;%\n    group_by(taxa) %&gt;%\n    tally()\n\n# A tibble: 4 × 2\n  taxa        n\n  &lt;chr&gt;   &lt;int&gt;\n1 Bird      450\n2 Rabbit     75\n3 Reptile    14\n4 Rodent  34247\n\n\nWe can also use tally() when grouping on multiple variables:\n\nsurveys %&gt;%\n    group_by(taxa, sex) %&gt;%\n    tally()\n\n# A tibble: 6 × 3\n# Groups:   taxa [4]\n  taxa    sex       n\n  &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;\n1 Bird    \"\"      450\n2 Rabbit  \"\"       75\n3 Reptile \"\"       14\n4 Rodent  \"\"     1209\n5 Rodent  \"F\"   15690\n6 Rodent  \"M\"   17348\n\n\nHere, tally() is the action applied to the groups created by group_by() and counts the total number of records for each category.\nIf there are many groups, tally() is not that useful on its own. For example, when we want to view the five most abundant species among the observations:\n\nsurveys %&gt;%\n    group_by(species) %&gt;%\n    tally()\n\n# A tibble: 40 × 2\n   species             n\n   &lt;chr&gt;           &lt;int&gt;\n 1 albigula         1252\n 2 audubonii          75\n 3 baileyi          2891\n 4 bilineata         303\n 5 brunneicapillus    50\n 6 chlorurus          39\n 7 clarki              1\n 8 eremicus         1299\n 9 flavus           1597\n10 fulvescens         75\n# ℹ 30 more rows\n\n\nSince there are 40 rows in this output, we would like to order the table to display the most abundant species first. In dplyr, we say that we want to arrange() the data.\n\nsurveys %&gt;%\n    group_by(species) %&gt;%\n    tally() %&gt;%\n    arrange(n)\n\n# A tibble: 40 × 2\n   species          n\n   &lt;chr&gt;        &lt;int&gt;\n 1 clarki           1\n 2 scutalatus       1\n 3 tereticaudus     1\n 4 tigris           1\n 5 uniparens        1\n 6 viridis          1\n 7 leucophrys       2\n 8 savannarum       2\n 9 fuscus           5\n10 undulatus        5\n# ℹ 30 more rows\n\n\nStill not that useful. Since we are interested in the most abundant species, we want to display those with the highest count first, in other words, we want to arrange the column n in descending order:\n\nsurveys %&gt;%\n    group_by(species) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n)) %&gt;%\n    head(5)\n\n# A tibble: 5 × 2\n  species          n\n  &lt;chr&gt;        &lt;int&gt;\n1 merriami     10596\n2 penicillatus  3123\n3 ordii         3027\n4 baileyi       2891\n5 megalotis     2609\n\n\nIf we want to include more attributes about these species, we can include these in the call to group_by():\n\nsurveys %&gt;%\n    group_by(species, taxa, genus) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n)) %&gt;%\n    head(5)\n\n# A tibble: 5 × 4\n# Groups:   species, taxa [5]\n  species      taxa   genus               n\n  &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;           &lt;int&gt;\n1 merriami     Rodent Dipodomys       10596\n2 penicillatus Rodent Chaetodipus      3123\n3 ordii        Rodent Dipodomys        3027\n4 baileyi      Rodent Chaetodipus      2891\n5 megalotis    Rodent Reithrodontomys  2609\n\n\nBe careful not to include anything that would split the group into subgroups, such as sex, year etc.\n\n4.4.1.1 Challenge\n\nHow many individuals were caught in each plot_type surveyed?\nYou saw above how to count the number of individuals of each sex using a combination of group_by() and tally(). How could you get the same result using group_by() and summarize()? Hint: see ?n.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling!</span>"
    ]
  },
  {
    "objectID": "lectures/lec04-data-wrangling.html#reshaping-with-pivot_wider-and-pivot_longer",
    "href": "lectures/lec04-data-wrangling.html#reshaping-with-pivot_wider-and-pivot_longer",
    "title": "4  Data wrangling!",
    "section": "4.5 Reshaping with pivot_wider and pivot_longer",
    "text": "4.5 Reshaping with pivot_wider and pivot_longer\n\n4.5.1 Defining wide vs long data\nThe survey data presented here is almost in what we call a long format – every observation of every individual is its own row. This is an ideal format for data with a rich set of information per observation. It makes it difficult, however, to look at the relationships between measurements across plots/trials. For example, what is the relationship between mean weights of different genera across all plots?\nTo answer that question, we want each plot to have its own row, with each measurements in its own column. This is called a wide data format. For the surveys data as we have it right now, this is going to be one heck of a wide data frame! However, if we were to summarize data within plots and species, we can reduce the dataset and begin to look for some relationships we’d want to examine. We need to create a new table where each row is the values for a particular variable associated with each plot. In practical terms, this means the values in genus would become the names of column variables and the cells would contain the values of the mean weight observed on each plot by genus.\nWe can use the functions called pivot_wider() and pivot_longer() (these are newer replacements for spread() and gather(), which were the older functions). These can feel tricky to think through, but do not feel alone in this! Many others have squinted at their data, unsure exactly how to reshape it, so there are many guides and cheatsheets available to help!\n\n\n4.5.2 Summary of long vs wide formats\nLong format:\n\nevery column is a variable\n\nfirst column(s) repeat\n\nevery row is an observation\n\nWide format:\n\neach row is a measured thing\neach column is an independent observation\n\nfirst column does not repeat\n\n\n\n\n4.5.3 Long to Wide with pivot_wider\nLet’s start by using dplyr to create a data frame with the mean body weight of each genus by plot.\n\nsurveys_gw &lt;- surveys %&gt;%\n    filter(!is.na(weight)) %&gt;%\n    group_by(genus, plot_id) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n`summarise()` has grouped output by 'genus'. You can override using the\n`.groups` argument.\n\nsurveys_gw %&gt;% head()\n\n# A tibble: 6 × 3\n# Groups:   genus [1]\n  genus   plot_id mean_weight\n  &lt;chr&gt;     &lt;int&gt;       &lt;dbl&gt;\n1 Baiomys       1        7   \n2 Baiomys       2        6   \n3 Baiomys       3        8.61\n4 Baiomys       5        7.75\n5 Baiomys      18        9.5 \n6 Baiomys      19        9.53\n\n\nNow, to make this long data wide, we use pivot_wider() from tidyr to spread out the different taxa into columns. pivot_wider() takes 3 arguments: the data , the names_from column variable that will eventually become the column names, and the values_from column variable that will fill in the values. We’ll use a pipe so we don’t need to explicitly supply the data argument.\n\nsurveys_gw_wide &lt;- surveys_gw %&gt;% \n  pivot_wider(names_from = genus, values_from = mean_weight)\n\nhead(surveys_gw_wide)\n\n# A tibble: 6 × 11\n  plot_id Baiomys Chaetodipus Dipodomys Neotoma Onychomys Perognathus Peromyscus\n    &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1       1    7           22.2      60.2    156.      27.7        9.62       22.2\n2       2    6           25.1      55.7    169.      26.9        6.95       22.3\n3       3    8.61        24.6      52.0    158.      26.0        7.51       21.4\n4       5    7.75        18.0      51.1    190.      27.0        8.66       21.2\n5      18    9.5         26.8      61.4    149.      26.6        8.62       21.4\n6      19    9.53        26.4      43.3    120       23.8        8.09       20.8\n# ℹ 3 more variables: Reithrodontomys &lt;dbl&gt;, Sigmodon &lt;dbl&gt;, Spermophilus &lt;dbl&gt;\n\n\nNow we can go back to our original question: what is the relationship between mean weights of different genera across all plots? We can easily see the weights for each genus in each plot! Notice that some genera have NA values. That’s because some genera were not recorded in that plot.\nYou may have used spread()in the past, which also takes three arguments: the data, the key column (or column with identifying information), and the values column (the one with the numbers/values).\n\nsurveys_gw_wide0 &lt;- surveys_gw %&gt;%\n  spread(key = genus, value = mean_weight) \n\nhead(surveys_gw_wide0)\n\n# A tibble: 6 × 11\n  plot_id Baiomys Chaetodipus Dipodomys Neotoma Onychomys Perognathus Peromyscus\n    &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1       1    7           22.2      60.2    156.      27.7        9.62       22.2\n2       2    6           25.1      55.7    169.      26.9        6.95       22.3\n3       3    8.61        24.6      52.0    158.      26.0        7.51       21.4\n4       4   NA           23.0      57.5    164.      28.1        7.82       22.6\n5       5    7.75        18.0      51.1    190.      27.0        8.66       21.2\n6       6   NA           24.9      58.6    180.      25.9        7.81       21.8\n# ℹ 3 more variables: Reithrodontomys &lt;dbl&gt;, Sigmodon &lt;dbl&gt;, Spermophilus &lt;dbl&gt;\n\n\n\n\n4.5.4 Wide to long with gather and pivot_longer\nWhat if we had the opposite problem, and wanted to go from a wide to long format? For that, we can use pivot_longer() to gather a set of columns into one key-value pair. To go backwards from surveys_gw_wide, we should exclude plot_id.\npivot_longer() takes 4 arguments: the data, the names_to column variable that comes from the column names, the values_to column with the values, and cols which specifies which columns we want to keep or drop. Again, we will pipe from the dataset so we don’t have to specify the data argument:\n\nsurveys_gw_long2 &lt;- surveys_gw_wide %&gt;% \n  pivot_longer(names_to = \"genus\", values_to = \"mean_weight\", cols = -plot_id)\n\nsurveys_gw_long2\n\n# A tibble: 240 × 3\n   plot_id genus           mean_weight\n     &lt;int&gt; &lt;chr&gt;                 &lt;dbl&gt;\n 1       1 Baiomys                7   \n 2       1 Chaetodipus           22.2 \n 3       1 Dipodomys             60.2 \n 4       1 Neotoma              156.  \n 5       1 Onychomys             27.7 \n 6       1 Perognathus            9.62\n 7       1 Peromyscus            22.2 \n 8       1 Reithrodontomys       11.4 \n 9       1 Sigmodon              NA   \n10       1 Spermophilus          NA   \n# ℹ 230 more rows\n\n\nIf the columns are directly adjacent as they are here, we don’t even need to list the all out: we can just use the : operator, as before.\n\nsurveys_gw_wide %&gt;% \n  pivot_longer(names_to = \"genus\", values_to = \"mean_weight\", cols = Baiomys:Sigmodon)\n\n# A tibble: 216 × 4\n   plot_id Spermophilus genus           mean_weight\n     &lt;int&gt;        &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt;\n 1       1           NA Baiomys                7   \n 2       1           NA Chaetodipus           22.2 \n 3       1           NA Dipodomys             60.2 \n 4       1           NA Neotoma              156.  \n 5       1           NA Onychomys             27.7 \n 6       1           NA Perognathus            9.62\n 7       1           NA Peromyscus            22.2 \n 8       1           NA Reithrodontomys       11.4 \n 9       1           NA Sigmodon              NA   \n10       2           NA Baiomys                6   \n# ℹ 206 more rows\n\n\nNote that now the NA genera are included in the long format.\nIn the past, you may have used gather(). We give it the arguments of a new key and value column name, and then specify which columns we either want or do not want gathered up. So, togo backwards from surveys_gw_wide, and exclude plot_id from the gathering, we would do the following:\n\nsurveys_gw_long1 &lt;- surveys_gw_wide0 %&gt;%\n  gather(genus, mean_weight, -plot_id) \n\nhead(surveys_gw_long1)\n\n# A tibble: 6 × 3\n  plot_id genus   mean_weight\n    &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n1       1 Baiomys        7   \n2       2 Baiomys        6   \n3       3 Baiomys        8.61\n4       4 Baiomys       NA   \n5       5 Baiomys        7.75\n6       6 Baiomys       NA   \n\n\n\n4.5.4.1 Challenge\nStarting with the surveys_gw_wide dataset, how would you display a new dataset that gathers the mean weight of all the genera (excluding NAs) except for the genus Perognathus?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling!</span>"
    ]
  },
  {
    "objectID": "lectures/lec04-data-wrangling.html#footnotes",
    "href": "lectures/lec04-data-wrangling.html#footnotes",
    "title": "4  Data wrangling!",
    "section": "",
    "text": "This course is focused on tidyverse functions, because that seems to be the trend these days. Although all of our teaching material is written in tidy lingo, it is mostly for the sake of consistency. In all honesty, tidy is pretty great, but some functions are more intuitive in base, so most people code in a mix of the two. If you learned base R elsewhere and prefer to use those functions instead, by all means, go ahead. The correct code is code that does what you want it to do.↩︎",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling!</span>"
    ]
  },
  {
    "objectID": "lectures/lec05-data-visualization.html",
    "href": "lectures/lec05-data-visualization.html",
    "title": "5  Data visualization with ggplot2",
    "section": "",
    "text": "5.1 Lesson preamble\nNow we have seen how to get our dataset in our desired shape and form (aka “tidy”, where every column is a variable, and every row is an observation), we are of course itching to actually see what the data actually looks like. Luckily, our favourite package-of-packages tidyverse got us covered – it comes with a wonderful package for generating graphics called ggplot2!\nlibrary(tidyverse)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "lectures/lec05-data-visualization.html#lesson-preamble",
    "href": "lectures/lec05-data-visualization.html#lesson-preamble",
    "title": "5  Data visualization with ggplot2",
    "section": "",
    "text": "5.1.1 Learning Objectives\n\nProduce scatter plots, line plots, and histograms using ggplot.\nSet universal plot settings.\nUnderstand how to combine dplyr and ggplot.\nUnderstand and apply faceting in ggplot.\n\n5.1.2 Lesson outline\n\nLoading our data (10 min)\nPlotting with ggplot2 (15 mins)\nBuilding plots iteratively (30 mins)\nSplit-apply-combine… plot! (30 mins)\nFaceting (15 mins)\nExporting (10 mins)\n\n\n\n\n\n\n5.1.3 Loading data using relative file pathways\nWe’re going to install one new package today to help with loading and saving files. There are built-in functions, setwd() and getwd(), for setting and getting your working directory, but they’re fragile and depend on the way you organise your files. Setting the working directory for one code chunk in your Rmd file may not carry through to the next chunk.\nThe here package enables easy file referencing. It uses the top-level directory of a project to build paths to files.\n\ninstall.packages(\"here\")\n\nUse here::i_am('path/to/this/file') at the top of your R scripts to establish the root directory relative to your current file. Subsequent file paths can be made using the here function.\n\nlibrary(here)\n\nhere() starts at /Users/meteyuksel/eeb313website\n\nhere()\n\n[1] \"/Users/meteyuksel/eeb313website\"\n\nhere::i_am(\"lectures/lec05-data-visualization.qmd\")\n\nhere() starts at /Users/meteyuksel/eeb313website\n\n\nThis established path remains stable even if the working directory is changed.\nNow to load our portal data file using here().\n\n# Download file if you can't find it on your computer\n#download.file(\"https://ndownloader.figshare.com/files/2292169\",\n              # Pass a call to here() instead of the raw file path\n#              here(\"lectures/data/portal_data.csv\"))\n\n# Read the locally-stored file into R\nsurveys &lt;- read_csv(\n  # Pass a call to here() instead of the raw file path\n  here('lectures/data/portal_data.csv'))\n\nRows: 34786 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): species_id, sex, genus, species, taxa, plot_type\ndbl (7): record_id, month, day, year, plot_id, hindfoot_length, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# It's good practice to inspect your data frame to check for errors\nhead(surveys)\n\n# A tibble: 6 × 13\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1         1     7    16  1977       2 NL         M                  32     NA\n2        72     8    19  1977       2 NL         M                  31     NA\n3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\nstr(surveys)\n\nspc_tbl_ [34,786 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ record_id      : num [1:34786] 1 72 224 266 349 363 435 506 588 661 ...\n $ month          : num [1:34786] 7 8 9 10 11 11 12 1 2 3 ...\n $ day            : num [1:34786] 16 19 13 16 12 12 10 8 18 11 ...\n $ year           : num [1:34786] 1977 1977 1977 1977 1977 ...\n $ plot_id        : num [1:34786] 2 2 2 2 2 2 2 2 2 2 ...\n $ species_id     : chr [1:34786] \"NL\" \"NL\" \"NL\" \"NL\" ...\n $ sex            : chr [1:34786] \"M\" \"M\" NA NA ...\n $ hindfoot_length: num [1:34786] 32 31 NA NA NA NA NA NA NA NA ...\n $ weight         : num [1:34786] NA NA NA NA NA NA NA NA 218 NA ...\n $ genus          : chr [1:34786] \"Neotoma\" \"Neotoma\" \"Neotoma\" \"Neotoma\" ...\n $ species        : chr [1:34786] \"albigula\" \"albigula\" \"albigula\" \"albigula\" ...\n $ taxa           : chr [1:34786] \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ...\n $ plot_type      : chr [1:34786] \"Control\" \"Control\" \"Control\" \"Control\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   record_id = col_double(),\n  ..   month = col_double(),\n  ..   day = col_double(),\n  ..   year = col_double(),\n  ..   plot_id = col_double(),\n  ..   species_id = col_character(),\n  ..   sex = col_character(),\n  ..   hindfoot_length = col_double(),\n  ..   weight = col_double(),\n  ..   genus = col_character(),\n  ..   species = col_character(),\n  ..   taxa = col_character(),\n  ..   plot_type = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nThis can help you check why your call to read_csv() can’t find the data file you’re looking for.\n\n\n5.1.4 Challenge\nHow would you establish the root directory in a file “data_tidying.Rmd” in a new project directory “Group_Project” using the here package? How would you then load your data file “raw_data.csv” if it was contained in the same “Group_Project” directory along with “data_tidying.Rmd”?\nNote: You may want to set “eval=FALSE” in the code chunk header for your notes so that this code chunk doesn’t run and mess up your root directory for this lecture.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "lectures/lec05-data-visualization.html#plotting-with-ggplot2",
    "href": "lectures/lec05-data-visualization.html#plotting-with-ggplot2",
    "title": "5  Data visualization with ggplot2",
    "section": "5.2 Plotting with ggplot2",
    "text": "5.2 Plotting with ggplot2\nggplot2 is a plotting package that makes it simple to create complex plots from data frames. The name ggplot2 comes from its inspiration, the book A Grammar of Graphics, and the main goal is to allow coders to distill complex data structure and express their desired graphical outcome in a concise manner instead of telling the computer every detail about what should happen. For example, you would say “colour my data by species” instead of “go through this data frame and plot any observations of species1 in blue, any observations of species2 in red, etc”. Thanks to this functional way of interfacing with data, only minimal changes are required if the underlying data change or if you want to try a different type of visualization. Publication-quality plots can be created with minimal amounts of adjustment and tweaking.\nggplot2 graphics are built step by step by adding new elements, or layers. Adding layers in this fashion allows for extensive flexibility and customization of plots. To build a ggplot, we need to:\n1. Use the ggplot() function and bind the plot to a specific data frame using the data argument\n\nggplot(data = surveys)\n\n\n\n\n\n\n\n\nRemember, if the arguments are provided in the right order then the names of the arguments can be omitted.\n\nggplot(surveys)\n\n\n\n\n\n\n\n# You can also use the %&gt;% operator to pass the data to ggplot\nsurveys %&gt;% \n  ggplot()\n\n\n\n\n\n\n\n\n2. Define aesthetics (aes), by selecting the columns to be plotted and the presentation variables (ex: point size, shape, colour, etc.)\n\nggplot(surveys, aes(x = weight, y = hindfoot_length))\n\n\n\n\n\n\n\n\n3. Add geoms – geometrical objects as a graphical representation of the data in the plot (points, lines, bars). ggplot2 offers many different geoms. We will use a few common ones today, including: * geom_point() for scatter plots, dot plots, etc. * geom_line() for trend lines, time-series, etc. * geom_histogram() for histograms\nTo add a geom to the plot use + operator. Because we have two continuous variables (weight and hindfoot_length), let’s use geom_point() first:\n\nggplot(surveys, aes(x = weight, y = hindfoot_length)) +\n  geom_point()\n\nWarning: Removed 4048 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNote: Notice that triangle-! warning sign above the plot? ggplot is telling you that it wasn’t able to plot all of your data. Typically this means that there are NAs in the data, or that some data points lie outside of the bounds of the axes. Can you figure what it is in this instance?\nThe + in the ggplot2 package is particularly useful because it allows you to modify existing ggplot objects. This means you can easily set up plot “templates” and conveniently explore different types of plots. The above plot can be generated with code like this:\n\n# Assign plot to a variable\nsurveys_plot &lt;- ggplot(surveys, aes(x = weight, y = hindfoot_length))\n\n# Draw the plot\nsurveys_plot + geom_point()\n\nWarning: Removed 4048 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThree notes:\n\nAnything you put in the top ggplot() call can be seen/used by any geom layers that you add, including the x and y axis variables you set up in aes(). These are essentially universal plot settings.\nYou can specify aesthetics for a geom independently of the aesthetics defined by ggplot(), which is particularly handy when you’re layering data from different data frames\nThe + sign used to add layers must be placed at the end of each line containing a layer. If it’s used at the start of line, ggplot2 will not add the new layer and R will return an error message.\n\n\n5.2.1 Building plots iteratively\nBuilding plots with ggplot is typically an iterative process. Start simply. We will define the dataset to use, lay the axes, and choose one geom, as we just did:\n\nggplot(surveys, aes(x = weight, y = hindfoot_length)) +\n    geom_point()\n\nWarning: Removed 4048 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThen, we start modifying this plot to extract more information from it. For instance, we can add the argument for transparency (alpha) to reduce overplotting:\n\nggplot(data = surveys, aes(x = weight, y = hindfoot_length)) +\n    geom_point(alpha = 0.2)\n\nWarning: Removed 4048 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nBased on the hindfoot length and the weights, there appears to be 4 clusters in this data. Potentially, one of the categorical variables we have in the data could explain this pattern. Colouring the data points according to a categorical variable is an easy way to find out if there seems to be correlation. Let’s try colouring this points according to plot_type.\n\nggplot(surveys, aes(x = weight, y = hindfoot_length, colour = plot_type)) +\n    geom_point(alpha = 0.2)\n\nWarning: Removed 4048 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIt seems like the type of plot the animal was captured on correlates well with some of these clusters, but there are still many that are quite mixed. Let’s try to do better! This time, the information about the data can provide some clues to which variable to look at. The plot above suggests that there might be 4 clusters, so a variable with 4 values is a good guess for what could explain the observed pattern in the scatter plot.\n\nsurveys %&gt;%\n    summarize_all(n_distinct) \n\n# A tibble: 1 × 13\n  record_id month   day  year plot_id species_id   sex hindfoot_length weight\n      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt;           &lt;int&gt;  &lt;int&gt;\n1     34786    12    31    26      24         48     3              57    256\n# ℹ 4 more variables: genus &lt;int&gt;, species &lt;int&gt;, taxa &lt;int&gt;, plot_type &lt;int&gt;\n\n# `n_distinct` is a function that counts unique values in a set of vectors\n\nRemember that there are still NA values here, that’s why there are 3 unique sexes although only male and female were coded in our original data set. There are four taxa so that could be a good candidate, let’s see which those are.\n\nsurveys %&gt;%\n    distinct(taxa)\n\n# A tibble: 4 × 1\n  taxa   \n  &lt;chr&gt;  \n1 Rodent \n2 Rabbit \n3 Bird   \n4 Reptile\n\n\nIt seems reasonable that these taxa contain animals different enough to have diverse weights and length of their feet. Lets use this categorical variable to colour the scatter plot.\n\nggplot(surveys, aes(x = weight, y = hindfoot_length, colour = taxa)) +\n    geom_point(alpha = 0.2)\n\nWarning: Removed 4048 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nOnly rodents? That was unexpected… Let’s check what’s going on.\n\nsurveys %&gt;%\n    group_by(taxa) %&gt;%\n    tally()\n\n# A tibble: 4 × 2\n  taxa        n\n  &lt;chr&gt;   &lt;int&gt;\n1 Bird      450\n2 Rabbit     75\n3 Reptile    14\n4 Rodent  34247\n\n\nDefinitely mostly rodents in our data set…\n\nsurveys %&gt;%\n    filter( !is.na(hindfoot_length) ) %&gt;% # control by removing `!`\n    group_by(taxa) %&gt;%\n    tally()\n\n# A tibble: 1 × 2\n  taxa       n\n  &lt;chr&gt;  &lt;int&gt;\n1 Rodent 31438\n\n\n…and it turns out that only rodents have had their hindfeet measured! Rats.\nLet’s remove all records of animals without hindfoot measurements, including rodents. We’ll also remove any observations that did not include weights.\n\nsurveys_hf_wt &lt;- surveys %&gt;%\n    filter(!is.na(hindfoot_length) & !is.na(weight))\n\nsurveys_hf_wt %&gt;%\n    summarize_all(n_distinct)\n\n# A tibble: 1 × 13\n  record_id month   day  year plot_id species_id   sex hindfoot_length weight\n      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt;           &lt;int&gt;  &lt;int&gt;\n1     30738    12    31    26      24         24     3              55    252\n# ℹ 4 more variables: genus &lt;int&gt;, species &lt;int&gt;, taxa &lt;int&gt;, plot_type &lt;int&gt;\n\n\nMaybe the genus of the animals can explain what we are seeing.\n\nggplot(surveys_hf_wt, aes(x = weight, y = hindfoot_length, colour = genus)) +\n    geom_point(alpha = 0.2)\n\n\n\n\n\n\n\n\nNow this looks good! There is a clear separation between different genera but also significant spread within genus. For example, in the weight of the green Neotoma observations. There are also two clearly separate clusters that are both coloured in olive green (Dipodomys). Maybe separating the observations into different species would be better?\n\nggplot(surveys_hf_wt, aes(x = weight, y = hindfoot_length, colour = species)) +\n    geom_point(alpha = 0.2)\n\n\n\n\n\n\n\n\nGreat! Together with the genus plot, this definitely seems to explain most of the variation we see in the hindfoot length and weight measurements. It is still a bit messy as it appears like we have around five clusters of data points but there are 21 species in the legend.\n\nsurveys %&gt;%\n    filter(!is.na(hindfoot_length) & !is.na(weight)) %&gt;%\n    group_by(species) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n))\n\n# A tibble: 21 × 2\n   species          n\n   &lt;chr&gt;        &lt;int&gt;\n 1 merriami      9739\n 2 penicillatus  2978\n 3 baileyi       2808\n 4 ordii         2793\n 5 megalotis     2429\n 6 torridus      2085\n 7 spectabilis   2026\n 8 flavus        1471\n 9 eremicus      1200\n10 albigula      1046\n# ℹ 11 more rows\n\n\nThere is a big drop from 838 to 159, let’s include only those with more than 800 observations.\n\nsurveys_abun_species &lt;- surveys %&gt;%\n    filter(!is.na(hindfoot_length) & !is.na(weight)) %&gt;%\n    group_by(species) %&gt;%\n    mutate(n = n()) %&gt;% # add count value to each row\n    filter(n &gt; 800) %&gt;%\n    select(-n)\n\nsurveys_abun_species %&gt;%\n  # Remember, print limits lines displayed when knitted\n  print(10)\n\n# A\n#   tibble:\n#   30,320\n#   × 13\n# Groups:  \n#   species\n#   [12]\n# ℹ 30,310\n#   more\n#   rows\n# ℹ 13\n#   more\n#   variables:\n#   record_id &lt;dbl&gt;, …\n\n\nStill has almost 31k observations, so only ~3k observations were removed.\n\nsurveys_abun_species %&gt;%\n  ggplot(aes(x = weight, y = hindfoot_length, colour = species)) +\n  geom_point(alpha = 0.2)\n\n\n\n\n\n\n\n\nThe plot is now cleaner; there are fewer species and so fewer colours and the clusters are more distinct.\n\n\n5.2.2 Challenge\nCreate a scatter plot of hindfoot_length against species with the weight data displayed using colours. If you’re unsure of which variable to put on which axis, Y variables are generally “against” X variables. Also, continuous variables are generally plotted on the Y axis.\nDo you notice any potential issues with this plot given the sheer number of observations we know exist in the data?\n(This is further illustrating the iterative nature of constructing plots)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "lectures/lec05-data-visualization.html#split-apply-combine-plot",
    "href": "lectures/lec05-data-visualization.html#split-apply-combine-plot",
    "title": "5  Data visualization with ggplot2",
    "section": "5.3 Split-apply-combine… plot!",
    "text": "5.3 Split-apply-combine… plot!\nIn this section, we will learn how to work with dplyr and ggplot together. Aided by the pipes (%&gt;%), we can create a powerful data exploration workflow using these two packages.\nLet’s calculate number of counts per year for each species. First, we need to group the data and count records within each group:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n)) # Adding arrange just to compare with histogram\n\n# A tibble: 275 × 3\n# Groups:   year [26]\n    year species      n\n   &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;\n 1  2002 baileyi    868\n 2  1985 merriami   653\n 3  1997 merriami   572\n 4  2000 baileyi    545\n 5  1982 merriami   532\n 6  2001 baileyi    522\n 7  1983 merriami   521\n 8  1996 merriami   484\n 9  1998 merriami   447\n10  1990 merriami   425\n# ℹ 265 more rows\n\n\nWe could assign this table to a variable, and then pass that variable to ggplot().\n\nyearly_counts &lt;- surveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n))\n\nggplot(yearly_counts, aes(x = n)) +\n    geom_histogram()\n\n\n\n\n\n\n\n\nRemember that a histogram plots the number of observations based on a variable, so you only need to specify the x-axis in the ggplot() call. Also, that a histogram’s bin size can really change what you might understand about the data. The histogram geom has a bins argument that allows you to specify the number of bins and a binwidth argument that allows you to specify the size of the bins.\n\nggplot(yearly_counts, aes(x = n)) +\n    geom_histogram(bins=10)\n\n\n\n\n\n\n\n\nCreating an intermediate variable would be preferable for time consuming calculations, because you would not want to do that operation every time you change the plot aesthetics.\nIf it is not a time consuming calculation or you would like the flexibility of changing the data summary and the plotting options in the same code chunk, you can pipe the output of your split-apply-combine operation to the plotting command:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = n)) +\n        geom_histogram()\n\n\n\n\n\n\n\n\nWe can perform a quick check that the plot corresponds to the table by colouring the histogram by species:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n  # We are using \"fill\" here instead of \"colour\"\n    ggplot(aes(x = n, fill = species)) + \n        geom_histogram()\n\n\n\n\n\n\n\n\nNote: Here we are using fill to assign colours to species rather than colour. In general colour refers to the outline of points/bars or whatever it is you are plotting and fill refers to the colour that goes inside the point or bar. If you are confused, try switching out fill for colour to see what looks best!\nLet’s explore how the number of each genus varies over time. Longitudinal data can be visualized as a line plot with years on the x axis and counts on the y axis:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n)) +\n        geom_line()\n\n\n\n\n\n\n\n\nUnfortunately, this does not work because we plotted data for all the species together as one line. We need to tell ggplot to draw a line for each species by modifying the aesthetic function to include group = species:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n, group = species)) +\n        geom_line()\n\n\n\n\n\n\n\n\nWe will be able to distinguish species in the plot if we add colours (using colour also automatically groups the data):\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n  # `colour` groups automatically\n    ggplot(aes(x = year, y = n, colour = species)) +\n        geom_line()",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "lectures/lec05-data-visualization.html#faceting",
    "href": "lectures/lec05-data-visualization.html#faceting",
    "title": "5  Data visualization with ggplot2",
    "section": "5.4 Faceting",
    "text": "5.4 Faceting\nggplot has a special technique called faceting that allows the user to split one plot into multiple subplots based on a variable included in the dataset. This allows us to examine the trends associated with each grouping variable more closely. We will use it to make a time series plot for each species:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n)) + \n        geom_line() +\n        facet_wrap(~species)\n\n\n\n\n\n\n\n\nNow we would like to split the line in each plot by the sex of each individual measured. To do that we need to make counts in the data frame after grouping by year, species, and sex:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species, sex) %&gt;%\n    tally()\n\n# A tibble: 575 × 4\n# Groups:   year, species [275]\n    year species     sex       n\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt; &lt;int&gt;\n 1  1977 eremicus    M         2\n 2  1977 flavus      F        14\n 3  1977 flavus      M         8\n 4  1977 leucogaster F         1\n 5  1977 leucogaster &lt;NA&gt;      1\n 6  1977 megalotis   F         1\n 7  1977 megalotis   M         1\n 8  1977 merriami    F        75\n 9  1977 merriami    M       106\n10  1977 ordii       F        10\n# ℹ 565 more rows\n\n\nWe can reflect this grouping by sex in the faceted plot by splitting further with colour (within a single plot):\n\nsurveys_abun_species %&gt;%\n    group_by(year, species, sex) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n, colour = sex)) +\n        geom_line() +\n        facet_wrap(~species)\n\n\n\n\n\n\n\n\nThere are several observations where sex was not recorded. Let’s filter out those values.\n\nsurveys_abun_species %&gt;%\n    filter(!is.na(sex)) %&gt;%\n    group_by(year, species, sex) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n, color = sex)) +\n        geom_line() +\n        facet_wrap(~species)\n\n\n\n\n\n\n\n\nIt is possible to specify exactly which colors1 to use and to change the thickness of the lines to make the them easier to distinguish.\n\nsurveys_abun_species %&gt;%\n    filter(!is.na(sex)) %&gt;%\n    group_by(year, species, sex) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n, colour = sex)) +\n        geom_line(size = 1) +\n        scale_colour_manual(values = c(\"black\", \"orange\")) +\n        facet_wrap(~species) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nNot sure what colours would look good on your plot? The R Community got you covered! Check out these awesome color palettes where nice-looking color combos come predefined. We especially recommend the viridis color palettes. These palettes are not only pretty, they are specifically designed to be easier to read by those with colourblindness.\nLastly, let’s change the x labels so that they don’t overlap, and remove the grey background to increase contrast with the lines. To customize the non-data components of the plot, we will pass some theme statements2 to ggplot.\n\nsurveys_abun_species %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  group_by(year, species, sex) %&gt;%\n  tally() %&gt;%\n  ggplot(aes(x = year, y = n, color = sex)) +\n  geom_line(size = 1) +\n  scale_colour_viridis_d() +\n  facet_wrap(~species) +\n  theme_classic() +\n  theme(text = element_text(size = 12),\n        axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n\nThere are other popular theme options, such as theme_bw().\nOur plot looks pretty polished now! It would be difficult to share with other, however, given the lack of information provided on the Y axis. Let’s add some meaningful axis labels.\n\nsurveys_abun_species %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  group_by(year, species, sex) %&gt;%\n  tally() %&gt;%\n  ggplot(aes(x = year, y = n, color = sex)) +\n  geom_line(size = 1) +\n  scale_colour_viridis_d() +\n  facet_wrap(~species) +\n  theme_classic() +\n  theme(text = element_text(size = 12),\n        axis.text.x = element_text(angle = 30, hjust = 1)) +\n  labs(title = \"Rodent abundance over time\",\n       x = \"Year\",\n       y = \"Number observed\",\n       colour = \"Sex\")\n\n\n\n\n\n\n\n\n\n5.4.1 Challenge\nUse the filtered data frame (surveys_abun_species) for part 2.\n1. Remember the histogram coloured according to each species? Starting from that code, how could we separate each species into its own subplot? Hint: look in the aplit-apply-comine section\n2.a. Create a plot that shows the average weight over years. Which year was the average weight of all animals the highest?\n2.b. Iterate on the plot so it shows differences among species of their average weight over time. Is the yearly trend the same for all species?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "lectures/lec05-data-visualization.html#export-plots",
    "href": "lectures/lec05-data-visualization.html#export-plots",
    "title": "5  Data visualization with ggplot2",
    "section": "5.5 Export plots",
    "text": "5.5 Export plots\nLet’s save our polished faceted plot. We will assign the entire plot to a new object.\n\npolished_plot &lt;- surveys_abun_species %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  group_by(year, species, sex) %&gt;%\n  tally() %&gt;%\n  ggplot(aes(x = year, y = n, color = sex)) +\n  geom_line(size = 1) +\n  scale_colour_viridis_d() +\n  facet_wrap(~species) +\n  theme_classic() +\n  theme(text = element_text(size = 12),\n        axis.text.x = element_text(angle = 30, hjust = 1)) +\n  labs(title = \"Rodent abundance over time\",\n       x = \"Year\",\n       y = \"Number observed\",\n       colour = \"Sex\")\n\nIf we don’t already have a figures directory to save our plot in, we can create one in R.\n\n# Reminding ourselves where we are\nhere()\n\n[1] \"/Users/meteyuksel/eeb313website\"\n\ndir.create( here(\"lectures/figures\") )\n\nWarning in dir.create(here(\"lectures/figures\")):\n'/Users/meteyuksel/eeb313website/lectures/figures' already exists\n\n\nWe can use a conditional expression with if() to create the directory only if it doesn’t already exist. The condition is inside the brackets “()” and the action to take if that’s true is in the curly braces “{}”.\n\nif( !dir.exists(\"lectures/figures\")) {\n  dir.create(\"lectures/figures\")\n}\n\nWarning in dir.create(\"lectures/figures\"): cannot create dir\n'lectures/figures', reason 'No such file or directory'\n\n\nWith this, we can use ggsave() to save our plot. The first argument is a path to the filename we want to use, including the file extension you want. You can use .png, .jpg, .pdf, .tiff, and others. You’ll need to specify the path relative to the current working directory, so we’ll use here() to ensure the new file ends up where we expect.\nThe next argument is the name of the plot object in our environment that we want to save.\nWe then have optional arguments such as the width and height of the image to be saved. You can also specify the units for the width and height, as “in”, “cm”, “mm”, or “px”.\n\nggsave( here(\"lectures/figures/survey_yearly_abundance.png\"), \n        polished_plot, width = 8, height = 6, units = \"in\" )\n\n\n5.5.1 Challenge\nSave the polished plot as a .jpg that’s 300 by 250 pixels inside a directory called “figures”.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "lectures/lec05-data-visualization.html#footnotes",
    "href": "lectures/lec05-data-visualization.html#footnotes",
    "title": "5  Data visualization with ggplot2",
    "section": "",
    "text": "There are so many colors to chose from in R. Check out the R Color doc to find something that brings you joy.↩︎\nThe amount of control over various plot elements in ggplot is truly astonishing. Check out the complete list of themes here. Have fun!↩︎",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "lectures/lec06-exploratory-data-analysis.html",
    "href": "lectures/lec06-exploratory-data-analysis.html",
    "title": "6  Exploratory data analysis",
    "section": "",
    "text": "6.1 Lesson preamble:",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "lectures/lec06-exploratory-data-analysis.html#lesson-preamble",
    "href": "lectures/lec06-exploratory-data-analysis.html#lesson-preamble",
    "title": "6  Exploratory data analysis",
    "section": "",
    "text": "6.1.1 Lesson objectives:\n\nImplications of (not) understanding your data\n\nHow did you collect your data?\nWhat are the properties of your data?\n\nExploring and asking questions about your data with graphing/visualization\nUsing insights from exploratory analysis to clean up data:\n\nDealing with unusual values/outliers\nDealing with missing values (NAs)\n\n\n6.1.2 Lesson outline:\n\nData properties, initial predictions (15 min)\nPlotting and exploring data (45 min)\nDealing with unusual values (15 min)\nRe-connecting with our predictions (30 min)\nDealing with missing values (15 min)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "lectures/lec06-exploratory-data-analysis.html#introduction",
    "href": "lectures/lec06-exploratory-data-analysis.html#introduction",
    "title": "6  Exploratory data analysis",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nExploratory data analysis is your exciting first look at your data! It’s a chance to develop a better understanding of the variables in your data set and the relationships between them. You can check your assumptions, find outliers, and possible errors. But THEN you’ll get to ask your questions! Yay!!\nYou need to understand your data you before you analyze it.\n\nWhat kind of data is it?\nWhat variation is present in my data?\nAre there any data points with values beyond the limits I anticipated?\nDo you notice any patterns?\n\nThe patterns you see can lead you to exciting new questions you may not have anticipated!",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "lectures/lec06-exploratory-data-analysis.html#setup",
    "href": "lectures/lec06-exploratory-data-analysis.html#setup",
    "title": "6  Exploratory data analysis",
    "section": "6.3 Setup",
    "text": "6.3 Setup\nWe’ll use what you’ve learned in past lectures about summarizing and visualizing data with dplyr and ggplot to get to know some data!\n\nlibrary(tidyverse)\n\n\ndownload.file(\"https://uoftcoders.github.io/rcourse/data/pseudo.ara.busco\", \n              \"data/pseudo.ara.busco\")\ndownload.file(\"https://uoftcoders.github.io/rcourse/data/pseudo.LTRs\", \n              \"data/pseudo.LTRs\")\n\n# note: data/ provides a relative path to file; same below\n\nWe’re going to load the genomic data we have on the frequency of highly conserved genes and the frequency of a type of repetitive element (LTRs stands for Long Terminal Repeat - there’s some more info on them coming up in the ‘predictions’ section).\n\ngeneDensity &lt;- read_tsv(\"data/pseudo.ara.busco\", \n                        col_names = c(\"chromosome\", \"start\", \"end\", \"winNum\", \n                                      \"numElements\", \"numBases\", \"winSize\", \n                                      \"density\"))\n\nRows: 48952 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): chromosome\ndbl (7): start, end, winNum, numElements, numBases, winSize, density\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nltrDensity &lt;- read_tsv(\"data/pseudo.LTRs\", \n                       col_names = c(\"chromosome\", \"start\", \"end\", \"winNum\", \n                                     \"numElements\", \"numBases\", \"winSize\", \n                                     \"density\"))\n\nRows: 48952 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): chromosome\ndbl (7): start, end, winNum, numElements, numBases, winSize, density\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe’re using “read_tsv” because the columns in this file are separated by tabs instead of commas or white space. Our two data sets need some column information to make it more interpretable",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "lectures/lec06-exploratory-data-analysis.html#what-is-my-data-actually",
    "href": "lectures/lec06-exploratory-data-analysis.html#what-is-my-data-actually",
    "title": "6  Exploratory data analysis",
    "section": "6.4 What is my data, actually?",
    "text": "6.4 What is my data, actually?\nBefore we do anything else, we have to think about where this came from & whether the data is appropriate for the kinds of questions we might have.\nThis data describes a two of the genetic units (we’ll call them “elements”) that live in one plant genome: a set of highly conserved genes and one type of transposon (a “selfish” gene that makes copies of itself at the expense of its host genome). The chromosomes have been broken down into 1Mb pieces (“windows”) that overlap each other. In each window, we know the number and size (base pairs occupied) of the conserved genes and transposons.\n\n6.4.1 Predictions\nIt’s always good to lay out your hypotheses first. It can help you figure out how you need to assemble your data in order to test those predictions effectively.\n\nIn areas where gene density is high, LTR density is low\n\nLTRs are a type of transposable element, aka “genomic parasite”\n\nThey make copies of themselves at the expense of their host genome\nThey make up a large portion of plant genomes (can be &gt;40%!)\nThe host genome wants to prevent them from replicating\n\nCertain regions of a chromosome are more tightly wound up with histones\n\nThis makes them less accessible to molecular machinery\nIf polymerases aren’t likely to access a region, the region can’t be expressed\nIf a region is unexpressed, you don’t want genes there!!\nLTRs tend to accumulate in these regions\n\nMore accessible, active regions of a chromosome have higher gene content\n\nThese regions can be expressed effectively!\nLTRs that insert into these regions have a worse impact on the host\n\nOther factors like recombination rate and methylation also support this pattern\n\nThe sex chromosome (LG_X) will have higher LTR density\n\nLarger proportions of sex chromosomes are less accessible\nSex chromosomes experience lower rates of recombination relative to autosomes\n\nAlso correlated with higher transposon density and lower gene density\n\nThese trends are more true for non-recombining Y chromosomes than X chromosomes\n\nRecombination can occur between the two X chromosomes in females\n\n\n\n\n\n6.4.2 First Peek\nFirst, let’s just take a quick look at the gene density data set and ask ourselves what we’re dealing with. On a very basic level, what kind of variables do we have?\nWhat is one way to view a data frame?\n\n#head(geneDensity) # prints the first 6 rows\n#tail(geneDensity) #prints the last 6 rows\nglimpse(geneDensity) #prints number of rows and columns, column names, types, and several entries\n\nRows: 48,952\nColumns: 8\n$ chromosome  &lt;chr&gt; \"LG_N\", \"LG_N\", \"LG_N\", \"LG_N\", \"LG_N\", \"LG_N\", \"LG_N\", \"L…\n$ start       &lt;dbl&gt; 0, 20000, 40000, 60000, 80000, 100000, 120000, 140000, 160…\n$ end         &lt;dbl&gt; 1000000, 1020000, 1040000, 1060000, 1080000, 1100000, 1120…\n$ winNum      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ numElements &lt;dbl&gt; 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ numBases    &lt;dbl&gt; 2499, 2499, 10158, 11583, 11583, 11583, 11583, 11583, 1158…\n$ winSize     &lt;dbl&gt; 1e+06, 1e+06, 1e+06, 1e+06, 1e+06, 1e+06, 1e+06, 1e+06, 1e…\n$ density     &lt;dbl&gt; 0.002499, 0.002499, 0.010158, 0.011583, 0.011583, 0.011583…\n\n\nWhat are your first impressions of the data?\nWhich variables will be relevant for testing our predictions?\n\n\n6.4.3 Basic Variable Categories\nCommon variable types:\n\nIndependent vs dependent\nContinuous vs discrete\nQualitative: categorical/nominal, ranked/ordinal, dichotomous\nQuantitative: interval, ratio\n\nThis matters because the type of data tells us the appropriate way to visualize it:\n\nQualitative data: pie charts or bar charts\nQuantitative data: histograms, box plots",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "lectures/lec06-exploratory-data-analysis.html#visualizing-your-data",
    "href": "lectures/lec06-exploratory-data-analysis.html#visualizing-your-data",
    "title": "6  Exploratory data analysis",
    "section": "6.5 Visualizing Your Data",
    "text": "6.5 Visualizing Your Data\n\n6.5.1 Describing Patterns in Histograms\nFor a given variable, you’re generally looking at the range of values and where the majority of the data lies. This gives you an idea of the distribution of your data. As you’re aware, many statistical tests make assumptions about the distribution of your input data - it’s very important to make note of the shapely properties of your data.\n\nAverage (mean, median, mode)\nRange (max, min)\nSkewness: how symmetrical is your data around the average?\n\nClassic bell curve has a skew of zero\nIf your data isn’t symmetrical, that can give you important info!\nSkewed distributions aren’t likely to be normally distributed\n\nKurtosis: how sharp is your central peak?\n\nIf your distribution is basically flat, its kurtosis is negative\nIf your distribution has a huge spike, its kurtosis will be positive\n\n\n\n\n6.5.2 Qualitative Data with Histograms\nHistograms are great for qualitative data because they visualize the number of times a given value appears in your data.\n\n\n6.5.3 Quantitative Data with Histograms\nHistograms can provide a useful view into continuous data, providing that you tell ggplot how to group your data into discrete bins. Here, we can look at our data’s values for gene density. This density is a measurement of the number of base pairs in a 1Mb window that are part of a gene divided by the total number of base pairs in the window (1 000 000).\n\nggplot(geneDensity, aes(density)) +\n  geom_histogram(binwidth = 0.01) + \n  labs(title = \"Distribution of gene density values\",\n       x = \"Gene density\", y = \"Count (bin size = 0.01)\") # Adding labels helps!\n\n\n\n\n\n\n\n\nWhat are some words you’d use to describe this distribution?\n\n\n6.5.4 Binning Quantitative Data\nWhen you’re subsetting continuous data into discrete bin widths, it’s important to try out different values because different bin sizes can give vastly different impressions of your data’s distribution.\n\nggplot(geneDensity, aes(density)) +\n  geom_histogram(binwidth = 0.001) +  # Teeny tiny bins\n  labs(title = \"Distribution of gene density values\",\n       x = \"Gene density\", y = \"Count (bin size = 0.001)\")\n\n\n\n\n\n\n\nggplot(geneDensity, aes(density)) +\n  geom_histogram(binwidth = 0.1) +  # Huge bins! (for this data)\n  labs(title = \"Distribution of gene density values\",\n       x = \"Gene density\", y = \"Count (bin size = 0.1)\")\n\n\n\n\n\n\n\n\nIt’s also interesting to see whether your data’s distribution is different among the categories you’re looking at. Is there variation in the species diversity in Canadian tundra environments different when compared to all areas sampled in Canada? (Do be careful with this, because looking for patterns by poking your data into a bunch of different subsets will basically guarantee you’ll find a pattern, whether or not it’s biologically relevant.)\n\n6.5.4.1 Histogram for One Chromosome\nLet’s see whether the gene density on one of the autosomes (how about LG_2) fits the general pattern.\n(Based on our initial hypotheses, would you predict that it would?)\nIt is important to consider how your predictions may affect the way you filter your data, so be mindful about tweaking parameters (like bin width) to fit the output you expect!\n\ngeneDensity %&gt;%\n  filter(chromosome == \"LG_2\") %&gt;%\n  ggplot(aes(density)) +\n  geom_histogram(binwidth = 0.01) +\n  labs(title = \"Distribution of gene density values on LG_2\",\n       x = \"Gene density\", y = \"Count\")\n\n\n\n\n\n\n\n\nThe range for the x axis is much smaller! The maximum gene density here (~12%) is much smaller than the highest value in the full genome data set (~40/50%).\n(Why might this be?)\nOne of the aspects of your data that you can’t visualize well with a histogram is whether there are any values that exceed the limits you expected for your data.\n\n\n\n6.5.5 Scatterplots & Box plots\n\n6.5.5.1 More info! Less bias!\nWith quantitative data, we can get more information by looking at scatterplots and box plots. Not only are they immune to bin size bias, they can help us find outliers and let us make initial visual comparisons of averages across categories.\n\n\n6.5.5.2 Visualize raw data as a scatterplot\nWe know that “chromosome” is a categorical, independent variable appropriate for our X axis and that “density” is a continuous, dependent variable that will be appropriate for the Y.\n\nggplot(geneDensity, aes(x = chromosome, y = density)) +\n  geom_point() +\n  labs(title = \"Comparison of gene density across chromosomes\",\n       x = \"Chromosome\", y = \"Gene density\")\n\n\n\n\n\n\n\n\nAlready, we can see that there different maximum gene density values on each chromosome. Because the points are overlapping, it’s hard to evaluate what the average or skewness might be for any of the categories.\n\n\n6.5.5.3 Boxplots for better comparisons\nBecause boxplots display the median and quartile limits, it’s much easier to evaluate the properties of the distribution.\n\nggplot(geneDensity, aes(x = chromosome, y = density)) +\n  geom_boxplot() +\n  labs(title = \"Comparison of gene density across chromosomes\",\n       x = \"Chromosome\", y = \"Gene density\")\n\n\n\n\n\n\n\n\nThere’s definitely a value that jumps out immediately. It’s stretching the scale of the Y axis so that it’s hard to effectively compare the medians of each of the chromosomes.\nBefore we officially decide what to do with this outlier, we’ll visually set it aside for now by re-scaling our Y axis, which we’ve already learned how to do!\n\nggplot(geneDensity, aes(x = chromosome, y = density)) +\n  geom_boxplot() +\n  ylim(0, 0.125) + #other methods possible\n  labs(title = \"Comparison of gene density across chromosomes\",\n       x = \"Chromosome\", y = \"Gene density\")\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nLook at that handy warning! It lets us know that one value was thrown out: “removed 1 rows”. This view helps us to get a better general understanding of how this categorical “chromosome” value might relate to gene density. However! It’s important not to throw out data unless you have good reason to believe it doesn’t belong in the data set.\nBonus note: you can use the coord_cartesian function instead of ylim. It won’t warn you if any of your data points are beyond the limits of the axes, though.\n\n\n6.5.5.4 Boxplot augmentations\nThere are a few additional things we can do that might make boxplots even more informative for our data.\n\nViolin plots - boxplots but with curves instead of boxes\nAdding a scatterplot behind the boxplot\nAdding “jitter” to scatterplots so the points are offset\n\nAdditionally, you can make the points more transparent (change the alpha value)\n\nYou can also add a trend line to help you visualize potential relationships\n\n\nggplot(geneDensity, aes(x = chromosome, y = density)) +\n  geom_jitter(alpha = 0.1, width = 0.3) +\n  geom_violin() + \n  labs(title = \"Comparison of gene density across chromosomes\",\n       x = \"Chromosome\", y = \"Gene density\")\n\n\n\n\n\n\n\n\nMaking the points more transparent gives us a better idea of what density values are most common. You can see this at the bottom of the graph, where the points don’t look transparent at all - so many data points!!\n\n\n6.5.5.5 What about the other variables?\n\nThis data describes a few of the genetic “bits” (we generally call them “elements”) live in one plant genome. The chromosomes have been broken down into 1Mb pieces (“windows”) that overlap each other and the contents of each window have been averaged. We’ve got information on the density of conserved genes and one type of transposon for each window.\n\nAverage number of genes in bins along chromosomes.\nDefinitely more interesting to compare across the categories built into our data (here, chromosomes) to see how the gene density looks in each one separately. We can see whether the global pattern is present in each category. But how can we get all that info in one graph??\nFirst step is to ask ourselves what we currently have in our data. If our category for comparison is chromosome, what independent variables are shared among them that could facilitate comparison of the dependent gene density variable?\n\nhead(geneDensity)\n\n# A tibble: 6 × 8\n  chromosome  start     end winNum numElements numBases winSize density\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 LG_N            0 1000000      1           1     2499 1000000 0.00250\n2 LG_N        20000 1020000      2           1     2499 1000000 0.00250\n3 LG_N        40000 1040000      3           2    10158 1000000 0.0102 \n4 LG_N        60000 1060000      4           2    11583 1000000 0.0116 \n5 LG_N        80000 1080000      5           2    11583 1000000 0.0116 \n6 LG_N       100000 1100000      6           2    11583 1000000 0.0116 \n\n\nStart, end, and winNum would all be reasonable proxies for position along the chromosome.\n\ngeneDensity %&gt;%\n  filter(chromosome == \"LG_2\") %&gt;%\n  ggplot(aes(x = start, y = density)) +\n  geom_point() +\n  labs(title = \"Comparison of gene density along LG_2\",\n       x = \"Chromosomal position (bp)\", y = \"Gene density\")\n\n\n\n\n\n\n\n\nThis gives us an overview of how many of the conserved genes are found in which region of this LG_2 chromosome.\nTo be able to compare all the chromosomes at the same time, we can split our graph into “facets” so there’s one per chromosome, as you’ve learned how to in the last lecture.\n\nggplot(geneDensity, aes(x=start, y=density)) +\n  geom_point() +\n  labs(title=\"Comparison of gene density across chromosomes\",\n       x=\"Chromosomal position (bp)\", y=\"Gene density\") +\n  facet_wrap( vars(chromosome) )\n\n\n\n\n\n\n\n\nBecause not all of the chromosomes are the same length, the data appears more squished in some of the panels. We can adjust that by telling facet wrap to scale the X axis per-panel instead of globally.\nIf we want to be able to visually compare the densities across chromosomes, we should not allow the Y axis to scale freely. We can, however, set a limit for the Y axis values, as we’ve done before.\nUse different command for scaling the Y axis\n\nggplot(geneDensity, aes(x = start, y = density)) +\n  geom_point() +\n  coord_cartesian( ylim = c(0,0.13) ) +\n  labs(title = \"Comparison of gene density across chromosomes\",\n       x = \"Chromosomal position (bp)\", y = \"Gene density\") +\n  facet_wrap( vars(chromosome), scales = \"free_x\" )\n\n\n\n\n\n\n\n\nCool, eh?? The chromosomes have very different patterns! The range and distribution of values differs considerably!\nWhat are some reasons for gene density to change along a chromosome?\n\nCentromeres are mostly made up of repeats - very low gene content\n\nCentromeres can be in the middle or near one end of a chromosome\nWhere do you think the centromeres are in these chromosomes?\n\nCertain regions of a chromosome are more tightly wound up with histones\n\nMakes them less accessible to molecular machinery\nIf polymerases don’t reach a region, that region can’t be expressed\nIf a region is unexpressed, you don’t want genes there!\nCentromeres are generally one of these ‘inactive’ regions\n\nMore accessible, active regions of a chromosome have higher gene content\n\nThese regions are generally along chromosome arms\n\n\n\n\n\n6.5.6 Challenge!\nHow could you visualize the LTR data across chromosomes? Don’t forget to use axis labels.\nWhat is the range of LTR density for the LG_2 chromosome?\n\nggplot(ltrDensity, aes(x = chromosome, y = density)) +\n  geom_violin() + #boxplot also valid\n  geom_point(alpha = 0.01, position = \"jitter\") +\n  labs(title = \"Comparison of LTR density across Chromosomes\",\n       x = \"Chromosome\", y = \"LTR density\")\n\n\n\n\n\n\n\nltrDensity %&gt;%\n  group_by(chromosome) %&gt;%\n  summarize(mean = mean(density), median = median(density), \n            n = n(), max = max(density))\n\n# A tibble: 5 × 5\n  chromosome  mean median     n   max\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 LG_2       0.324  0.331 11234 0.488\n2 LG_4       0.303  0.320  6520 0.973\n3 LG_7       0.331  0.339 15938 0.503\n4 LG_N       0.318  0.332  8551 0.476\n5 LG_X       0.307  0.315  6709 0.477\n\n\nNow it’s time to start thinking about what to do with rebellious outliers!",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "lectures/lec06-exploratory-data-analysis.html#outliers",
    "href": "lectures/lec06-exploratory-data-analysis.html#outliers",
    "title": "6  Exploratory data analysis",
    "section": "6.6 Outliers",
    "text": "6.6 Outliers\n\n6.6.1 But why are you like this?\nThere could be many reasons why your data has values that exceed the limits you expected it would have. It basically comes down to error, whether in your data or in the expectation you had for its limits. Consider error carefully.\n\nIncorrect prediction of what the limits should be\n\nMaybe your study system has different qualities than literature spp.\n\nSystematic error is predictable and affects a measurement’s accuracy\n\nIncorrectly calibrated lab equipment (pH meter, thermometer, etc.)\nGenomics - your gene annotation can be biased by repetitive elements\nCan be very difficult to compensate for this kind of error\n\nRandom error affects measurement precision (think significant figures)\n\nWriting down measurements incorrectly in your notes\nA lab scale can only weigh samples to a given decimal point\nSimple human fallibility when it comes to assessing measurements\nTake multiple measurements and average the results to compensate\n\nCommon sources\n\nThis (exploratory data analysis) is a great time to look for issues!\nError in previous analysis steps (code that produced LTR density)\nErroneous assumptions about your sample sites or methods\n\nDon’t just throw out a point because you can ascribe it to error\n\nIMPORTANT NOTE: if you do end up removing any data, you MUST disclose the removal and your justification for the removal. Your reasons could be great, but you need to make sure your audience has access to those reasons.\nThis consideration of weird values brings up an interesting point: you’re doing these checks because your values are different than what you expected. It’s important to think about analytical ‘controls’ to look for potential errors even when your data looks the way you expect it to! Steps towards this can be as simple as sharing your code publicly.\n\n\n6.6.2 Let’s take a look!\nWe had that weird really high gene density value on the LG_X chromosome. Let’s look at what’s happening there.\n\nfilter(geneDensity, density &gt; 0.13)\n\n# A tibble: 1 × 8\n  chromosome     start       end winNum numElements numBases winSize density\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 LG_X       134160000 134165151   6709           1     2655    5151   0.515\n\n\nWhat do the other variables tell us about this data point?\nThis data point has a really high winNum, so it’s likely located near the end of the chromosome. But importantly, our windows are supposed to be 1Mb in size (1 000 000 value in the winSize column). The winSize value for this outlier is tiny in comparison!!\n\n\n6.6.3 Wholesome thoughts about your data\nAverages – how was your data collected & what biases might be inherent? The data I’m showing you is a pretty clear example of how important (and difficult) it is to understand what the variables mean in your data sets. What might cause outliers in the kind of data you’re interested in?\nThat last look showed us that it’s definitely very important to consider our data as a whole: to think not only about the variables relevant to our hypotheses, but the way in which it was collected (how that was reflected in the other properties of the data).\nSo. Let’s try to understand more about the interaction between gene density and window size in the rest of our data. Visualization time!\n\nggplot(geneDensity, aes(x = start,y = winSize, colour = chromosome)) +\n  geom_point() +\n  labs(title = \"Window sizes along the chromosome\",\n       x = \"Chromosomal position (bp)\", y = \"Window size (bp)\")\n\n\n\n\n\n\n\n\nIt looks like all of the chromosomes get this trail-off in window size near their ends. This is not what we expected!! All of the squish at the end is basically just error from a previous analysis.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "lectures/lec06-exploratory-data-analysis.html#compare-density-with-and-without-outliers",
    "href": "lectures/lec06-exploratory-data-analysis.html#compare-density-with-and-without-outliers",
    "title": "6  Exploratory data analysis",
    "section": "6.7 Compare density with and without outliers",
    "text": "6.7 Compare density with and without outliers\nHow does this winSize variable impact gene density? What do we predict based on what we know thus far?\nIt’s much esier to compare when everything is in the same plot. This will be somewhat tricky with the data the way it is. We will need to label each density as either belonging in a “small window” or a “normal window”. We can create a new “winCategory” variable using mutate() and assign the value of “small” to windows with winSize less than 1Mb and “normal” to all the other windows (which we expect will have a value of 1Mb). We can use the function case_when() within mutate() to provide different values depending on conditional statements (much like “if” statements).\n\ngeneDensity2 &lt;- geneDensity %&gt;%\n  mutate( winCategory = case_when(winSize&lt;1000000 ~ \"small\",\n                                  TRUE ~ \"normal\") ) %&gt;%\n  group_by(winCategory, chromosome)\n\nsummarize(geneDensity2,\n          mean = mean(density), median = median(density), n = n(), \n          max = max(density), sd = sd(density), .groups = \"keep\")\n\n# A tibble: 10 × 7\n# Groups:   winCategory, chromosome [10]\n   winCategory chromosome    mean  median     n    max     sd\n   &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 normal      LG_2       0.0104  0.00481 11184 0.119  0.0168\n 2 normal      LG_4       0.0123  0.00730  6470 0.0900 0.0146\n 3 normal      LG_7       0.00909 0.00284 15888 0.123  0.0151\n 4 normal      LG_N       0.0111  0.00464  8501 0.0690 0.0143\n 5 normal      LG_X       0.0119  0.00449  6659 0.0960 0.0175\n 6 small       LG_2       0.0443  0.0472     50 0.0964 0.0299\n 7 small       LG_4       0       0          50 0      0     \n 8 small       LG_7       0.0443  0.0370     50 0.122  0.0233\n 9 small       LG_N       0       0          50 0      0     \n10 small       LG_X       0.0358  0.0226     50 0.515  0.0705\n\n\nWhat can we take away from this table?\nThe n values are considerably larger for the normal-sized windows group. LG_4 and LG_N had 0 gene density in their small windows but have some of the highest median gene densities in the normal-sized windows.\nThe standard deviation of the small windows is much higher. Is that what we would expect for that data category? Perhaps most importantly for our purposes, the mean and median are quite different. These smaller windows have considerably different values.\nWhat does this look like in an actual plot? This is going to take a bit of black magic in the form of two separate calls to geom_boxplot(). The first will use all the windows (setting it to colour values by ‘all’) and the second will actually create (and colour) different box plots based on their winCategory value.\n\nggplot(geneDensity2, aes(x = chromosome, y = density, colour = winCategory)) +\n  geom_boxplot( aes(x = chromosome, y = density, colour = \"all\") ) + \n  geom_boxplot() +\n  ylim(0, 0.125) +\n  labs(title=\"Visualizing gene density across window size and chromosome\",\n       x=\"Chromosome\", y=\"Gene density\", colour=\"Window\\nCategory\")\n\nWarning: Removed 1 row containing non-finite outside the scale range (`stat_boxplot()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nThe small window values seem quite different than the global gene density results!\nWhy do you think this might be? Looking back on the summaries, we can see that there aren’t many data points in the ‘small’ window category.\nIn conclusion!!\nThese small windows do seem to contain interesting information, but they are over-weighted given the amount of information they’re based on. Based on the analysis conducted to create the windows, it might be appropriate to discard the small windows on the ends of the chromosomes. Each windowed data point is weighted equally, even though these smaller windows contain less information, which creates a bias.\nWhat do you think is the most appropriate way to deal with this data?\nIs there a way to weight the gene density by window size?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "lectures/lec06-exploratory-data-analysis.html#so-what-does-this-mean-for-our-predictions",
    "href": "lectures/lec06-exploratory-data-analysis.html#so-what-does-this-mean-for-our-predictions",
    "title": "6  Exploratory data analysis",
    "section": "6.8 So what does this mean for our predictions?",
    "text": "6.8 So what does this mean for our predictions?\nRight. The reason we collected this data in the first place!\n\nIn areas where gene density is high, LTR density is low\nThe sex chromosome (LG_X) will have higher LTR density\n\nNote: preparing data for analysis is generally the most time-consuming part of any project. Establishing what you need to do with your data in order to test your hypotheses & thoroughly exploring your data and its properties are extremely useful steps in this process.\n\n6.8.1 How do we explore these questions?\nWe need to relate gene density with LTR density. Do we have this data?\nIs it currently in a form where we can make comparisons?\nBased on the properties of our gene and LTR density data sets, what are the shared “units”? Essentially, what are we trying to compare within and among each chromosome?\n\nhead(geneDensity)\n\n# A tibble: 6 × 8\n  chromosome  start     end winNum numElements numBases winSize density\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 LG_N            0 1000000      1           1     2499 1000000 0.00250\n2 LG_N        20000 1020000      2           1     2499 1000000 0.00250\n3 LG_N        40000 1040000      3           2    10158 1000000 0.0102 \n4 LG_N        60000 1060000      4           2    11583 1000000 0.0116 \n5 LG_N        80000 1080000      5           2    11583 1000000 0.0116 \n6 LG_N       100000 1100000      6           2    11583 1000000 0.0116 \n\nas_tibble(ltrDensity)\n\n# A tibble: 48,952 × 8\n   chromosome  start     end winNum numElements numBases winSize density\n   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 LG_N            0 1000000      1         396   258184 1000000   0.258\n 2 LG_N        20000 1020000      2         400   274748 1000000   0.275\n 3 LG_N        40000 1040000      3         390   271692 1000000   0.272\n 4 LG_N        60000 1060000      4         390   271540 1000000   0.272\n 5 LG_N        80000 1080000      5         386   268414 1000000   0.268\n 6 LG_N       100000 1100000      6         380   268519 1000000   0.269\n 7 LG_N       120000 1120000      7         396   285314 1000000   0.285\n 8 LG_N       140000 1140000      8         396   284053 1000000   0.284\n 9 LG_N       160000 1160000      9         396   270360 1000000   0.270\n10 LG_N       180000 1180000     10         388   267071 1000000   0.267\n# ℹ 48,942 more rows\n\n\nThe basic “unit” in this data is the 1Mb window. Because this is shared across the two data sets, we can use it to join them together. Excellent!\n\n6.8.1.1 Actual wrangling\nWe’re also going to pull out only the variables we now know we’ll need (what’s shared among the data sets and what will be used to try and test our predictions), just because of how large this data frame will be. It’s not a good idea to do this before looking at all the variables together.\n\nsimpleGeneDensity &lt;- geneDensity %&gt;%\n  mutate(elementType = \"gene\") %&gt;%\n  select(chromosome, start, elementType, density)\n\nsimpleLTRdensity &lt;- ltrDensity %&gt;%\n  mutate(elementType = \"LTR\") %&gt;%\n  select(chromosome, start, elementType, density)\n\nhead(simpleLTRdensity)\n\n# A tibble: 6 × 4\n  chromosome  start elementType density\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 LG_N            0 LTR           0.258\n2 LG_N        20000 LTR           0.275\n3 LG_N        40000 LTR           0.272\n4 LG_N        60000 LTR           0.272\n5 LG_N        80000 LTR           0.268\n6 LG_N       100000 LTR           0.269\n\n\nAt this point, are these data “long” or “wide”?\n\n\n6.8.1.2 ? Knowledge Check Challenge\nJoin the two data sets (simpleLTRdensity and simpleGeneDensity) into one data frame called “densities”. As a bonus, try mutating the start variable so that it’s measured in 10kb increments instead of 1bp. This will just make our X axis labels are easier to interpret.\n\ndensities &lt;- full_join(simpleLTRdensity, simpleGeneDensity,\n                       by = c(\"chromosome\", \"start\", \"elementType\", \"density\")) %&gt;%\n  mutate(start = start / 10000) %&gt;%\n  group_by(chromosome, elementType)\n  \nhead(densities)\n\n# A tibble: 6 × 4\n# Groups:   chromosome, elementType [1]\n  chromosome start elementType density\n  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 LG_N           0 LTR           0.258\n2 LG_N           2 LTR           0.275\n3 LG_N           4 LTR           0.272\n4 LG_N           6 LTR           0.272\n5 LG_N           8 LTR           0.268\n6 LG_N          10 LTR           0.269\n\nrm(simpleGeneDensity, simpleLTRdensity)\n\nWe’ve got two independent categorical variables, an independent numerical variable, and a dependent numerical variable. It’s beautiful.\n\n\n\n6.8.2 Is gene density high when LTR density is low? (hyp #1)\nWhat variables do we want to plot?\n\nChromosome\nStart position (bp)\nElement type\nElement density\n\n\n\n6.8.2.1 Challenge\nOf the plot types we’ve used so far, what would you use to try and compare gene densities along the chromosomal positions on each chromosome?\n\nggplot(densities, aes(x = density, fill = elementType)) +\n  geom_histogram( binwidth = 0.03 ) +\n  facet_wrap( vars(chromosome), scales = \"free_y\" ) +\n  coord_cartesian(xlim = c(0,0.6)) +\n  labs(x = \"Element Density\", y = \"Count\", fill = \"Element\\nType\",\n       title = \"Element densities among chromosomes\")\n\n\n\n\n\n\n\n\nPoking at the histogram shows us some interesting things about differences in the frequencies of LTRs and genes. Gene values have extremely high kurtosis near 0. LG_4 may have the highest median/mode LTR density.\nThe Y axis can be free-scaled here because all of the counts are based on the size of their chromosome. We don’t want one chromosome to seem like it has a much higher LTR count just because it has more windows (greater n) than the other chromosomes.\nThis was an interesting plot, but it compares densities across chromosomes more than it looks at differences in LTR/gene patterns within chromosomes.\n\nggplot(densities, aes(x = start,y = density,colour = elementType)) +\n  geom_point(alpha = 0.3) +\n  facet_wrap( vars(chromosome), scales = \"free_x\" ) +\n  ylim(0, 0.5) +\n  labs(title = \"Element densities along chromosomes\",\n       x = \"Chromosomal position (10kb)\", y = \"Element density\",\n       colour = \"Element\\nType\")\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThis looks like the kind of information we want! If we squint, we can almost see that increases in gene density seem to correlate with decreases in LTR density.\nIf you can remember how to add a smooth line to show broad patterns, this will be the easiest view.\n\nggplot(densities, aes(x = start,y = density,colour = elementType)) +\n  geom_smooth() +\n  ylim(0, 0.4) +\n  facet_wrap( vars(chromosome), scales = \"free_x\" ) +\n  labs(title = \"Element densities along chromosomes\",\n       x = \"Chromosomal position (10kb)\", y = \"Element density\",\n       colour = \"Element\\nType\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 3700 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_smooth()`).\n\n\n\n\n\n\n\n\n\nBroadly, we can see that when the LTR density plummets, gene density smudges upward.\n\n\n\n6.8.3 Does the sex chromosome (LG_X) have higher LTR density? (hyp #2)\nWe want to compare LTR densities across chromosomes. How would you do this?\nLet’s make a box plot of the LTR densities among chromosomes\n\nggplot(ltrDensity, aes(x = chromosome, y = density)) +\n  geom_boxplot() +\n  labs(title = \"LTR densities among chromosomes\",\n       x = \"Chromosome\", y = \"LTR density\")\n\n\n\n\n\n\n\n\nWhat unusual things do you notice about this plot?\nDoes this initial exploration lead us to think that the sex chromosome, with its reduced rate of recombination, has accumulated more LTRs?\nBased on what we know about the gene density data, what would you suggest we might need to do with this LTR density data?\nThere is one extreme outlier again - let’s see what happens if we colour the data based on window size. You may notice that this offers a simpler method for quickly visualizing given our outliers. Good eye!\n\nltrDensity %&gt;%\n  ggplot( aes(x = chromosome, y = density, colour = winSize==1000000) ) +\n  geom_boxplot() +\n  labs(title = \"LTR densities among chromosomes\",\n       x = \"Chromosome\", y = \"LTR density\", colour = \"Normal\\nWindow\\nSize\")\n\n\n\n\n\n\n\n\nThere are fewer outliers for the windows that are the expected size, but the pattern remains the same. GOOD SIGN! :)\nThe LTR density does not appear to differ much between the sex chromosome (LG_X) relative to the autosomes.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "lectures/lec06-exploratory-data-analysis.html#references",
    "href": "lectures/lec06-exploratory-data-analysis.html#references",
    "title": "6  Exploratory data analysis",
    "section": "6.9 References",
    "text": "6.9 References\nZuur, A. F., Ieno, E. N., Elphick, C. S. 2010. A protocol for data exploration to avoid common statistical problems. Methods in Ecology and Evolution 1: 3-14.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "lectures/lec07-self-directed-analysis.html",
    "href": "lectures/lec07-self-directed-analysis.html",
    "title": "7  Review using Farrell and Davies (2019) dataset",
    "section": "",
    "text": "7.1 Introduction\nThe goal of this class is to review concepts from the first half of the course, and to become comfortable writing your own code to wrangle and explore data. It also provides an opportunity to discuss project ideas with your class-mates. (We would like you to form groups before the class on Thursday Sept. 26, which will be devoted to project work, or by the end of that class at the latest.) Since the 2nd half of the course requires familiarity with the basics of R, this is a good point to assess if there are programming principles or specific concepts which you need to brush up on.\nHere is how the class will work: you will split up into groups for each of the questions below, briefly discuss your interests for the group project, and work on the questions collaboratively. The questions will guide you through an exploratory analysis of a nice dataset generated by Farrell and Davies. In their paper, Farrell and Davies seek to test the hypothesis that disease-induced mortality (or virulence of a pathogen) increases with the degree to which an infected host species is diverged from other mammalian hosts. Your answers will NOT be marked, but Mete and Zoë are happy to check your answers and to provide guidance if you become stuck. We have also provided some tips and tricks regarding best practices for troubleshooting your code, and encourage you to try to solve the problems by applying some of those practices.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Review using Farrell and Davies (2019) dataset</span>"
    ]
  },
  {
    "objectID": "lectures/lec07-self-directed-analysis.html#learning-preamble",
    "href": "lectures/lec07-self-directed-analysis.html#learning-preamble",
    "title": "7  Review using Farrell and Davies (2019) dataset",
    "section": "7.2 Learning preamble",
    "text": "7.2 Learning preamble\n\n7.2.1 Learning objectives\n\nBecome more confortable writing code on your own\nReview concepts from the first half of the course, including\n\nReading datasets into R\nNavigating between directories, including absolute and relative file paths\nSummarizing information about the structure of data\nUsing ggplot to visualize data\nCommon uses of dplyr functions: mutate, select, subset, summarize, group_by, pivot_longer, etc.\nExporting plots\n\nBecome more comfortable with troubleshooting code\nDiscuss project ideas with your class-mates!\n\n7.2.2 Lesson outline:\n\nTroubleshooting (5 min)\nIntro to the data (5 min)\nExercise 1 (20 min)\nExercise 2 (15 min)\nExercise 3 (35 min)\nExercise 4 (15 min)\nExercise 5 (15 min)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Review using Farrell and Davies (2019) dataset</span>"
    ]
  },
  {
    "objectID": "lectures/lec07-self-directed-analysis.html#best-practices-for-troubleshooting-code",
    "href": "lectures/lec07-self-directed-analysis.html#best-practices-for-troubleshooting-code",
    "title": "7  Review using Farrell and Davies (2019) dataset",
    "section": "7.3 Best practices for troubleshooting code",
    "text": "7.3 Best practices for troubleshooting code\n\n7.3.1 Tips\nIf you have an error message, look for the FIRST occurrence of an error in the output and Google that line.\nTips for Googling:\n\nInclude the name of the function that failed in your query\n\nIf you’re not sure what failed, look for function names in that first error line (names with () after them) or the line above it\n\nRemove any variable names or values from your query\nIf you’re given line numbers, use them to double-check your script at that line location for syntax mistakes\n\nIf you got the error after running a whole code chunk, you can also try running your code line by line (with cmd+enter on Mac or ctrl+enter on PC) to pinpoint the source of the problem\n\n\nIf you’re not getting output you expect, test what you’re trying to do with toy data (make a tiny data frame or vector) and/or use your actual data but perform the operations one at a time.\nGenerally, ensure you understand any warning messages that printed by your code. You may decide to change your script to eliminate the message, but make that decision carefully. This could be affecting analyses you’re performing later in your script. Google is your friend here as well.\nIt is absolutely disheartening to spend a lot of time trying to fix something. We’re not here to dismiss that feeling. It’ll take a while to get good at recognizing what keywords will be important for your Google searches & being able to fully understand the Stack Overflow posts. Troubleshooting and debugging are very valuable skills, though, and it’s worth the effort!\n\n\n7.3.2 Quick Examples\n\nlibrary(tdyverse)\n\noutput: Error in library(tdyverse) : there is no package called ‘tdyverse’\nWhat would you search?\n\n\nx = 7\n\nif (x &lt; 5) {\n  print(\"x is less than 5\")\nelse {\n  print(\"x is greater than 5\")\n}\n  \nrm(x)\n\noutput: Error: unexpected 'else' in:\"  print(\"x is less than 5\") else\"\nWhat would you search?\n\n\nmean(iris$Species)\n\noutput: Warning: argument is not numeric or logical: returning NA\nWhat would you search?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Review using Farrell and Davies (2019) dataset</span>"
    ]
  },
  {
    "objectID": "lectures/lec07-self-directed-analysis.html#the-dataset",
    "href": "lectures/lec07-self-directed-analysis.html#the-dataset",
    "title": "7  Review using Farrell and Davies (2019) dataset",
    "section": "7.4 The dataset",
    "text": "7.4 The dataset\nInformation regarding the columns of the dataset are below. Each row corresponds to a particular host-parasite combination, and includes information about the order of the host, type of parasite, number of documented host species per parasite, the number of reported cases and deaths in the focal host, if transmission of the parasite is vector-borne, the mean phylogenetic distance of the focal host species from all species known to host the parasite, and information regarding the times and locations of case and fatality count data.\n\nYear – Year of report\nCountry – Country name\nDisease – OIE reported disease name\nParasite – Parasite Latin binomial\nHost – Latin binomial of host reported by World Organisation for Animal Health\nCases – Number of reported cases\nDeaths – Number of reported deaths due to disease\nDestroyed – Number of animals reported to have been destroyed\nSlaughtered – Number of animals reported to have been slaughtered\nHostOrder – Taxonomic order of host species\nSR – The number of documented host species per parasite, also referred to as “host species richness”\nEvoIso – Host evolutionary isolation (in millions of years); see Figure 1 and SI 1.3 for details.\nTaxonomic Range – The largest taxonomic range among documented host species (1-6 representing parasites restricted to a single species, genus, family, order, class, or multiple classes, respectively)\nParaType – Gross parasite taxonomic group (arthropod, bacteria, fungi, helminth, protozoa, virus)\nParaFamily – Parasite taxonomic family\nVector – Binary variable (0/1) indicating whether parasite can be transmitted by an arthropod vector\nEnviroRestingStage – Binary variable (0/1) indicating whether parasite has a resting stage capable of persisting for long periods of time in the environment (typically months to years)\nAvianReservoir –Binary variable (0/1) indicating whether parasite has been documented to use avian species as reservoir hosts\nReproduction – Binary variable (0/1) indicating whether parasite can be vertically transmitted as a function of reproduction (either vertically transmitted, sexually transmitted, or passed from mother to offspring via ingestion of milk or colostrum\nWOK_citations – number of Web of Knowledge citations per parasite\nWOK_citations_noHuman – number of Web of Knowledge citations per parasite excluding those that reference humans\nlatitude – latitude of country in degrees\ngdp – Gross Domestic Product of country (current US$); WDI code “NY.GDP.MKTP.CD”\ngdp_pcap – Gross Domestic Product of country per capita (current US$); WDI code “NY.GDP.PCAP.CD”",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Review using Farrell and Davies (2019) dataset</span>"
    ]
  },
  {
    "objectID": "lectures/lec07-self-directed-analysis.html#exercise-1-20-min",
    "href": "lectures/lec07-self-directed-analysis.html#exercise-1-20-min",
    "title": "7  Review using Farrell and Davies (2019) dataset",
    "section": "7.5 Exercise 1 (20 min)",
    "text": "7.5 Exercise 1 (20 min)\na. Use download.file() to download the disease_distance.csv dataset from the EEB313 website. Call the computer on your file disease_distance.csv.\nb. Clear your environment and console.\nc. Read the disease_distance.csv dataset into R in two ways. Name the data object disease_data.\n\nChange your working directory to the location where the file is saved using setwd(), and provide read_csv() with the relative path to the data.\nProvide the absolute file path to the Farrell and Davies data when you call read_csv().\n\nd. View aspects of the data in the console.\n\nRun str() on the data and interpret the output.\nPrint the first 100 rows of the data. Print the last 100 rows.\nAssign to an object new_df a data frame with 1000 randomly selected rows. Do this using the tidy function slice_sample(). Hint: use the documentation for the slice() function found here.\nAssign to an object new_df2 a data frame with every 10th row of the data. Do this using base R. Hint: use seq() to create a vector whose entries are 10, 20, 30, … and use this vector to extract the rows of interest.\nSelect columns Reproduction, Host, Cases, Country, and Deaths from the new_df2 data frame you just made.\nRun class() on the following columns: Reproduction, Deaths, and Country. Which of these columns can be effectively treated — or, in other words, coerced — into a logical vector (i.e., a vector whose entries are either TRUE or FALSE)?\n\nFrom here on out, use the FULL data frame (i.e., disease_data, where all columns and rows present). To ensure that you do not call new_df, run new_df &lt;- NULL. Try to think of a few reasons why, in writing a program for a research project, one would write over an intermediate data frame or file.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Review using Farrell and Davies (2019) dataset</span>"
    ]
  },
  {
    "objectID": "lectures/lec07-self-directed-analysis.html#exercise-2-15-min",
    "href": "lectures/lec07-self-directed-analysis.html#exercise-2-15-min",
    "title": "7  Review using Farrell and Davies (2019) dataset",
    "section": "7.6 Exercise 2 (15 min)",
    "text": "7.6 Exercise 2 (15 min)\na. Remove any rows with missing data using the tidy function drop_na(). Documentation can be found here. Are there any missing rows? Why or why not?\nb. A calculation and some coersion!\n\nUse the function mutate to estimate the case fatality rate by dividing number of individuals that die of a disease by the number of confirmed cases. Return a data frame with one column, corresponding to the case fatality rates that were just calculated.\nCoerce the case fatality rate data column into a numeric vector using as.numeric(). What function might you use to coerce something into a character vector? (R is nice in that the naming conventions are very natural…when you get the hang of them!)\nWhat do you notice about the distribution of case fatality rates?\nTry to identify conditions under which the case fatality rate would be a problematic measure of parasite virulence?\n\nc. Use tidyverse to count the number of observations associated to each parasite. Do the same thing for each host, and then for each host-country combination.\nd. Compute mean, median, max, and min of Cases for each host in each country. Then, filter the resulting data frame so that rows where the mean number is &gt;5000 are kept. What countries are such that the mean number is &gt;5000? Think about why some countries may have fewer reported cases (key word: reported).",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Review using Farrell and Davies (2019) dataset</span>"
    ]
  },
  {
    "objectID": "lectures/lec07-self-directed-analysis.html#exercise-3-35-min",
    "href": "lectures/lec07-self-directed-analysis.html#exercise-3-35-min",
    "title": "7  Review using Farrell and Davies (2019) dataset",
    "section": "7.7 Exercise 3 (35 min)",
    "text": "7.7 Exercise 3 (35 min)\na. Histograms, histograms, histograms!\n\nMake a histogram of the number of cases, and another for the number of deaths. What do you notice?\nMake histograms of the numbers of the log-transformed cases and deaths. What do you notice? By log transformed, we mean the logarithm (base 10) of the variable. Are there any warnings which appear and, if so, why?\nUse the function unique() to determine the unique host species represented in the dataset. How many are there?\nMake a histogram of the log-transformed number of deaths, faceted by host. Play with the parameter bins. The default for this parameter is bins = 30. What happens if you set bins = 1 and bins = 100?\nMake a histogram of the number of deaths over the number of cases by host order and parasite type. Hint: look at the documentation for facet_grid().\nInterpret the histograms you just made. Do you notice anything interesting? (For example, think about what parasite types hosts that die in large proportions relative to the number of reported cases…)\n\nb. Boxplots (and a bunch of other geoms…)\n\nExplore the documentation for ggplot “geoms”, which can be found here here.\nMake a boxplot with host on the x axis and the log-transformed number of deaths on the y axis. Call this plot p.\nExplore the structure of the plot object p using the global environment and, after this, the str() function?\nTry changing the theme. Classics include bw(), classic(), light(), and void(). Which ones do you like? (More on ggplot themes can be found here.) Also, check out the GitHub repo for the newly-developed Barbie and Oppenheimer themes here! You can use these themes by first installing the package ThemePark.\nUse Google to detemrine what to add to the ggplot so the text on the x axis is rotated by 90 degrees (i.e., is readable).\nExport the plot as a .png and then as a .pdf.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Review using Farrell and Davies (2019) dataset</span>"
    ]
  },
  {
    "objectID": "lectures/lec07-self-directed-analysis.html#exercise-4-15-min",
    "href": "lectures/lec07-self-directed-analysis.html#exercise-4-15-min",
    "title": "7  Review using Farrell and Davies (2019) dataset",
    "section": "7.8 Exercise 4 (15 min)",
    "text": "7.8 Exercise 4 (15 min)\na. Visualize the data to see if there is relationship between the case fatality rate, the number of deaths over the number of cases, for a host and evolutionary isolation from the other hosts?\nBased on your visualization of the data, what you predict the relationship between these variables is? How would you test your understand and/or visualize the relationship in a different way? If constructing a statistical model to determine if there is relationship, are there variables in the dataset that you think would be important to control for?\nAfter thinking about these questions, read the abstract and significance statement for the paper from which this dataset is from. Discuss with your group what what exploratory analysis of data can and cannot do, based on the information in the paper and what you noticed about your visualization.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Review using Farrell and Davies (2019) dataset</span>"
    ]
  },
  {
    "objectID": "lectures/lec07-self-directed-analysis.html#exercise-5-15-min",
    "href": "lectures/lec07-self-directed-analysis.html#exercise-5-15-min",
    "title": "7  Review using Farrell and Davies (2019) dataset",
    "section": "7.9 Exercise 5 (15 min)",
    "text": "7.9 Exercise 5 (15 min)\na. Check out this documentation for the package ggridges.\nb. Install ggridges and make sure to load it using library() or require().\nc. Play around with ggridges! Using the disease_distance dataset which you have been exploring this lecture, make a ridgeline plot with a continuous variable on the x axis and discrete variable on the y. If you like, try to customize it (e.g., with fun colors)!\nd. Run the following code chunk :)\n\ninstall.packages(\"praise\")\nlibrary(praise)\nfor (i in 1:10) { print(praise()) }",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Review using Farrell and Davies (2019) dataset</span>"
    ]
  },
  {
    "objectID": "lectures/lec08-intro-inference-1.html",
    "href": "lectures/lec08-intro-inference-1.html",
    "title": "8  Introduction to inference I",
    "section": "",
    "text": "8.1 Lesson preamble\nlibrary(tidyverse)\nAs we begin to learn about probability and statistical inference, a fun quote:",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to inference I</span>"
    ]
  },
  {
    "objectID": "lectures/lec08-intro-inference-1.html#lesson-preamble",
    "href": "lectures/lec08-intro-inference-1.html#lesson-preamble",
    "title": "8  Introduction to inference I",
    "section": "",
    "text": "8.1.1 Lesson objectives\n\nDevelop understanding of random variables, probability distributions, and likelihood.\nBecome familiar with how to simulate realizations of a random variable.\nDevelop familiarity with the likelihood function and the logic of maximum likelihood.\n\n8.1.2 Lesson outline\n\nRandom variables, probability distributions, and likelihood\n\nInterpretations of probability, sources of uncertainty\nDiscrete vs continuous RVs\nCommon discrete and continuous probability distributions\nSimulating random variables in R\nEvaluating probabilities (and probability densities) of RVs\n\nLikelihood estimation and inference\n\nUnderstanding the likelihood function\nThe relationship between the likelihood and log-likelihood\nPractice writing down likelihoods\n\n\n\n\n\n\n\nProbability is the most important concept in modern science, especially as nobody has the slightest notion what it means. —Bertrand Russell, 1929 Lecture",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to inference I</span>"
    ]
  },
  {
    "objectID": "lectures/lec08-intro-inference-1.html#introduction-whats-chance-got-to-do-got-to-do-with-it",
    "href": "lectures/lec08-intro-inference-1.html#introduction-whats-chance-got-to-do-got-to-do-with-it",
    "title": "8  Introduction to inference I",
    "section": "8.2 Introduction: what’s chance … got to do … got to do … with it?",
    "text": "8.2 Introduction: what’s chance … got to do … got to do … with it?\nProbability and statistics are disciplines which seek to provide scientists with the tools to characterize uncertainty, and to make decisions (from betting on a particular horse in a race to deciding how to conserve a a species) under uncertainty (about the running speeds of horses in the race to the dynamics of an endangered population). How does something work, given the (finite, imperfect, incomplete, and noisy) data which are available to us? In science, we often are interested in some version of this question and use probabilistic/statistical models to help arrive at informed answers which, as more data arise, are updated. Ideally, such a process brings us closer to “the truth” even if there are issues with the modeling, the data, particular studies or contexts, etc.\nRandomness that is inherit in many physical processes (especially those in ecology and evolution) makes it difficult to, given data, choose between alternative explanations for how something (say, the world) works. What are some sources of uncertainty you can think of? From measurement error to differences in how a process unfolds in time or space, probability theory and statistics provides us with tools to confront various kinds of uncertainty — and to arrive at principled, informed conclusions.\nIn this lecture, we will introduce key concepts in probability and statistics. These concepts (in particular, likelihood) form the backbone for future lectures and will be invaluable if you do research in quantitative disciplines such as ecology and/or evolutionary biology. The focus of this lecture is a review of probability, some important probability distributions, simulating realizations of random variables (or, equivalently, draws from their probability distributions), and the basic idea of point estimation. Next class, we will extend what we cover today to describe the uncertainty/confidence around our “best” guess(es) for a parameter (or set of parameters) – that is, we will cover the basic theory of interval estimation and its connections to hypothesis testing.\n\n8.2.1 Interpretations of probability\nBefore we dive into a review of probability, it is useful to step back and ask: what do we mean when we make probabilistic statements? (Think about it for a minute and discuss with your neighbor.)\nSome common interpretations include:\n\nFinite frequency: probability is the empirical frequency at which particular outcomes or events unfold. If I flip a coin 10 times and get heads each time, the probability of getting heads is 10/10 and tails is 0/10. Basically, in this interpretation, a probability measures of the possibilities how many are “favorable” (in that a particular outcome or set of outcomes occured). This is not how scientists commonly think about probability, but it does bear resemblance to how they do inference.\nLimiting frequency: probability is the long-run frequency at which particular outcomes or events unfold. The probability of flipping a fair coin and landing heads is 1/2 if we flip the coin over and over and over again and, for each flip, we tally if the coin landed on heads or tails. If we keep adding to a string of Hs and Ts, the change in the frequency of Hs in the string will become smaller and smaller; the value to which it converges is what we would call the probability of flipping a coin and it landing on heads. This a widely-accepted interpretation of probability, and comes in a few flavors…\nPropensity: probability is a physical disposition of an aspect of the world to yield a particular set of outcomes. The probability that the coin turns up heads is a reflection of its disposition (or habit) to do so.\nBelief: probability reflects a measure of personal (or objective, under some interpretations) confidence that a particular set of outcomes or events have unfolded, or will unfold. This is the basis for Bayesian statistics, and it provides a very natural way to think about uncertainty in parameters (such as the mutation rate in humans). Under frequency-based interpretations of probability, parameters are not random – they are fixed, and we try to estimate what they are from the data. For Bayesians, parameters can be random because our belief about the values they assume are not fixed, but changing depending on the data which are available.\n\nFor more on this, check out the Stanford Encyclopedia of Philosophy.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to inference I</span>"
    ]
  },
  {
    "objectID": "lectures/lec08-intro-inference-1.html#review-of-probability",
    "href": "lectures/lec08-intro-inference-1.html#review-of-probability",
    "title": "8  Introduction to inference I",
    "section": "8.3 Review of probability!",
    "text": "8.3 Review of probability!\nConsider an experiment which gives rise to a set \\(\\Omega\\) of outcomes. This set of outcomes is called the sample space. The space of all events is formed by considering all possible combinations of outcomes (or, in some cases, some combinations but not all for technical reasons). Suppose the sample space is \\(\\Omega = \\{1,2,3,4\\}\\), so that the experiment might be rolling a four-sided die. One event is observing 2 or 3 upon rolling the die; this event is denoted \\(\\{2,3\\}\\), and is only one of many such events!\n\n8.3.1 Challenge question\nWhat are the other events that can happen, if the outcomes are 1, 2, 3, and 4? Don’t forget the event where nothing happens!\n\n\n8.3.2 Axioms of probability\nProbability allows us to assign to each event (\\(\\{2,3\\}, \\{\\}, \\{1,2,3,4\\}\\), etc.) a value between zero and one capturing how likely that event is to occur under the experiment we have preformed. We also need certain conditions on this function to be met:\n\nThe probability of any event must be \\(\\geqslant 0\\). Negative probabilities make no sense!\nThe probability of the event \\(\\Omega\\) must be \\(=1\\). Something must have happened!\nCountably many mutually exclusive events \\(A_1,A_2,\\dots,\\) must satisfy the following:\n\n\\[\\Pr (A_1 \\text{ or } A_2 \\text{ or } A_3 \\text{ or } \\cdots) = \\sum_{i=1}^\\infty \\Pr (A_i)\\]\n\\(A_1,\\dots,A_n,\\dots\\) are mutually exclusive if \\(A_i, A_j\\) do not share outcomes for all \\(i \\neq j\\). (The events \\(A_1 = \\{1,2\\}\\) and \\(A_2 = \\{3,4\\}\\) are distinct, while the events \\(A_1 = \\{1,3\\}\\) and \\(A_2 = \\{3,4\\}\\) are not..)\nA sample space, set of events formed from individual outcomes in the sample space, and a function on those events which satisfies the conditions above are the ingredients we need to define probability. Moreover, this mathematical definition is comparable with all of the interpretations above. Differences between interpretations are primarily philosophical in nature.\n\n\n8.3.3 An example\nIn the case of rolling a four-sided die, the probability of all events can be calculated (due to third condition) by specifying what the probability of each outcome is. When there are countably many outcomes (i.e., we can count them even if there are infinitely many), the probabilities of individual outcomes tell us about probabilities of events. For example,\n\\[\\Pr(\\{1,2,3,4\\}) = \\Pr(1 \\text{ or } 2 \\text{ or } 3 \\text{ or } 4) = \\Pr(\\{1\\}) + \\Pr(\\{2\\}) + \\Pr(\\{3\\}) + \\Pr(\\{4\\}) = 1\\] \\[\\Pr(\\{1,2,3\\}) = \\Pr(1 \\text{ or } 2 \\text{ or } 3) = \\Pr(\\{1\\}) + \\Pr(\\{2\\}) + \\Pr(\\{3\\}) = 0.75\\]\n\\[\\Pr(\\{1,3\\}) = \\Pr(1 \\text{ or } 3) = \\Pr(\\{1\\}) + \\Pr(\\{3\\}) = 0.5\\]\n\n\n8.3.4 Challenge question\nSuppose \\(\\Pr(\\{1\\}) = 0.2, \\Pr(\\{2\\}) = 0.3, \\Pr(\\{3\\}) = 0.25,\\) and \\(\\Pr(\\{4\\}) = 0.25\\), i.e., the die is not fair but not horribly biased to one side. What is the probability of getting a 2 or 3 if we were to roll a die with these properties?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to inference I</span>"
    ]
  },
  {
    "objectID": "lectures/lec08-intro-inference-1.html#random-variables",
    "href": "lectures/lec08-intro-inference-1.html#random-variables",
    "title": "8  Introduction to inference I",
    "section": "8.4 Random variables!",
    "text": "8.4 Random variables!\nA random variable is the (uncertain) outcome of a random experiment; more precisely, it is a function from the sample space (i.e., outcomes in the sample space) to the real numbers. Random variables are often denoted with capital letters, like \\(X, Y, B\\), and \\(T\\). In the previous example, the random variable which we were (secretly!) working with was a function that indicated what the outcome of the die-rolling experiment was – i.e., if we got a 1, 2, 3, or 4. We can summarize the probability the random variable, \\(X\\), assumes particular values using its probability mass function:\n\\[\\Pr(X=k) = p_k \\hspace{12pt} (p_k =0.25 \\text{ when the die is fair})\\]\nCan you think of other ways to summarize this information in this function? There are several, but a common choice is the cumulative probability distribution function, which is defined as the cumulative probability \\(X\\) assumes a value up to \\(x\\). When \\(X\\) is discrete, this is\n\\[F(x) = \\sum_{k=1,2,\\dots,x} \\Pr(X=k).\\]\nWhen \\(X\\) assumes values in a continuous space, the sum is replaced with an integral of something called a probability density function. Integrals are just fancy types of sums, so we don’t have to worry too much about the differences. (If you are interested in learning more about the math, ask Mete! He loves this stuff…)\nThe key difference between discrete and continuous random variables and probability distributions is the following. For a discrete random variable, as we have seen, the probability of an event is a sum over the outcomes associated to that event. For continuous random variable \\(Y\\), the probability of an event \\(A\\) (for example, observing any number between 0.3 and 0.7) is\n\\[\\Pr(Y \\in A) = \\int_A f(y|\\theta) \\text{d} y.\\]\n\\(f(y|\\theta)\\) called the probability density function for the random variable \\(Y\\). It is the continuous analogue of the probability mass function for discrete random variables. (Note: we can express the above in terms of \\(Y\\)’s cumulative distribution function, too.)\n\n8.4.1 Challenge question\nSuppose we roll a 6-sided die such that, with probability \\(p_k\\), it lands on \\(k\\). The outcome of this experiment is denoted \\(X\\). Suppose \\(p_1 = p_2 = p_3 = 0.1\\). Use the cumulative distribution function for \\(X\\) to get the probability it lands on \\(k \\leq 3\\).",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to inference I</span>"
    ]
  },
  {
    "objectID": "lectures/lec08-intro-inference-1.html#examples-of-random-variables",
    "href": "lectures/lec08-intro-inference-1.html#examples-of-random-variables",
    "title": "8  Introduction to inference I",
    "section": "8.5 Examples of random variables",
    "text": "8.5 Examples of random variables\nCommon examples of random variables include:\n\nThe time between events. For example, the time (in seconds or minutes) between cars that pass Mete on Harbord is a random variable. Is it discrete or continuous?\nThe number of individuals in a sample of fixed size with a specific property. For example, the number of lizards on an island that are of reproductive age, in a sample of \\(N = 100\\) individuals collected, is a random variable. Same for the the number of birds in North America that are currently infected with H5N1, highly pathogenic avian influenza. Are these variables discrete or continuous?\nThe number of occurrences of a rare event. For example, the number mutations on a newly-replicated strand of viral DNA or RNA, which are induced by errors in the polymerase and associated proof-reading machinery, is a random variable. Is it discrete or continuous?\nTraits which vary in a population of individuals. For example, the mass (in g) of penguins in Antarctica is a random variable. Is it discrete or continuous?\n\nAll of these random variables have probability distributions.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to inference I</span>"
    ]
  },
  {
    "objectID": "lectures/lec08-intro-inference-1.html#common-probability-distributions",
    "href": "lectures/lec08-intro-inference-1.html#common-probability-distributions",
    "title": "8  Introduction to inference I",
    "section": "8.6 Common probability distributions",
    "text": "8.6 Common probability distributions\nExamples of discrete probability distributions include:\n\nThe Poisson distribution. This distribution models the number of occurrences of a rare event. It has one parameter: the rate \\(\\lambda\\) at which such events occur.\nThe Binomial distribution. This distribution is associated to a random variable which counts the number of “successes” or “failures” in \\(N\\) trials, where the success probability is \\(p\\).\nThe Hypergeometric distribution models the probability that, if we draw \\(n\\) objects from a population of size \\(N\\) where \\(K\\) of the objects have a specific attribute without replacement, that we see \\(k\\) objects in our sample with the attribute.\nThe Geometric distribution. If we do many trials, each of which are independent and have probability of “success” \\(p\\), a random variable that has a \\(\\text{Geometric}(p)\\) distribution counts the number of tries until a success occurs.\nThe Negative binomial distribution models the number of trials until \\(r\\) successes occur, if the success probability in each trial is \\(p\\).\n\nNote: when \\(N\\), \\(K\\) are large relative to \\(n\\) and \\(K/N\\) is not too close to 0 or 1, the Binomial and Hypergeometric distributions are similar and its often advantageous to use the former (because it has a nicer probability distribution function, fewer parameters).\nExamples of continuous probability distributions include:\n\nThe Exponential distribution models the time in-between events, if the events occur at rate \\(\\lambda\\). What is means for events to occur at rate \\(\\lambda\\) is that, over a small interval of time \\(\\Delta t\\), an event occurs with probability approximately \\(\\lambda \\Delta t\\). The larger the \\(\\lambda\\), the more frequently the event occurs…\nThe Uniform distribution models outcomes where all occur with equal probability.\nThe Normal distribution is often used to model continuous observations that are sums of a large number of small, random contributions (e.g, individual alleles to a trait).\nThe Log-normal distribution models continuous observations such that their natural logarithm follows a Normal distribution. In other words, if \\(Y \\sim \\text{Lognormal}(\\mu,\\sigma^2)\\), then \\(\\ln Y \\sim \\text{Normal}(\\mu,\\sigma^2)\\) where \\(\\mu\\) is the mean.\nThe Gamma distribution is commonly used to describe the waiting time to the \\(r\\)th event of a particular type. It is a flexible distribution which can be used in a number of contexts, especially when it is not clear what the right probabilistic model for observations may be.\n\n\n8.6.1 Challenge\nFor each of the distributions above, think of random variables (e.g., the number of brown bears with body sizes \\(&gt;\\) 100 kg) with that distribution? Discuss with your neighbors.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to inference I</span>"
    ]
  },
  {
    "objectID": "lectures/lec08-intro-inference-1.html#simulating-random-variables",
    "href": "lectures/lec08-intro-inference-1.html#simulating-random-variables",
    "title": "8  Introduction to inference I",
    "section": "8.7 Simulating random variables",
    "text": "8.7 Simulating random variables\nOne of the most useful things about R is the fact it allows for efficient and rapid simulation and evaluation of random variables we care about. This means that we can assess the probability that our data would arise, ask questions about sample sizes required to realiably estimate parameters we care about, etc.\nFor example, can simulate a large realizations of a uniform random variable using runif().\n\nN &lt;- 10000\nrealizations_N_uniform &lt;- runif(n = N, min = 0, max = 1)\n\ndata.frame(value = realizations_N_uniform) %&gt;% ggplot(aes(x = value, y = ..count../N)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can evaluate the probability one realization arose from, say, the Uniform(0,100) distribution using dunif(). By evaluate, we mean return the probability density the observation would arise.1\n\ndunif(realizations_N_uniform[1], min = 0, max = 100)\n\n[1] 0.01\n\n\nSimilar syntax (r followed by the name of the distribution when we simulate and d followed by the name of the distribution) for other commonly distributions. For example, dbinom() evaluates the probability an observation arose from the Binomial distribution with a specific number of trials and success probability, and rnorm() samples from the Normal distribution with a specific mean and variance. Parameters of the focal distribution are supplied as arguments to the r___() and d___() functions…\n\n8.7.1 Challenge\nWrite a function which returns \\(n\\) realizations of a random variable that is 0 with probability \\(1-p\\) and 1 with probability \\(p\\). Such a random variable is said to have a Bernoulli distribution, or be Bernoulli. Call the function bernouli and give it two inputs, \\(p\\) and \\(n\\).\nHint: the Bernouli distribution is a special case of the Binomial with \\(N=1\\), i.e., one trial. Read the documentation for rbinom() to figure out what the names for \\(N\\) and \\(p\\) are…\n\n\n8.7.2 Challenge\nWrite a function to, for a vector of observations \\(x\\), evaluate the “probability” (i.e., the probability density) that each observation arises from a Gamma distribution with shape and scale parameters \\(\\alpha\\) and \\(\\beta\\), respectively. Call the arguments of the function x, alpha, and beta.\nTest the function using the vector x &lt;- c(2.77776, 3.56, 9.11) and \\(\\alpha = \\beta = 0.2\\).",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to inference I</span>"
    ]
  },
  {
    "objectID": "lectures/lec08-intro-inference-1.html#all-about-likelihood",
    "href": "lectures/lec08-intro-inference-1.html#all-about-likelihood",
    "title": "8  Introduction to inference I",
    "section": "8.8 All about likelihood!",
    "text": "8.8 All about likelihood!\nSo far we have seen:\n\nRandom variables model experiments with uncertain outcomes and come in many flavors. They are the main workhorse of probability and statistics.\nFunctions in base R allow us to conveniently and easily simulation realizations (i.e., draws) of many random variables. When the probability distribution of a random variable may be hard to write down, transformations of other random variables may provide a means to generate random numbers.\n\nNow, we turn our attention to the central problem of statistics: determining what processes and parameters gave rise to data (estimation), and quantifying uncertainty in those estimates (inference). Estimation and inference based on the likelihood function is the basis/foundation for most statistical procedures used in the sciences (including all kinds of linear models).",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to inference I</span>"
    ]
  },
  {
    "objectID": "lectures/lec08-intro-inference-1.html#the-likelihood-function",
    "href": "lectures/lec08-intro-inference-1.html#the-likelihood-function",
    "title": "8  Introduction to inference I",
    "section": "8.9 The likelihood function",
    "text": "8.9 The likelihood function\nThe idea is as follows. Given data \\(x_1,x_2,\\dots,x_n \\sim f(x|\\theta)\\), we want to estimate \\(\\theta\\), i.e., to determine what parameters were mostly likely to have given rise to the data (under the assumption \\(f\\) models the data generating process). We do this by maximizing the likelihood function\n\\[L(x_1,\\dots,x_n|\\theta) = f(x_1|\\theta) \\cdots f(x_n|\\theta) = \\prod_{i=1}^n f(x_i|\\theta),\\]\nwhich is formed under the assumption \\(x_1,\\dots,x_n\\) are independent. (There are methods that accommodate for dependent data, but we will not get into them here.) A couple things to notice:\n\nThe likelihood is formed by simply plugging in the data into the probability distribution function from which they jointly arose. (When the data are independent, the joint probability distribution function is the product of the individual distribution functions.)\nThis formulation of the likelihood is agnostic to weather or not the observations are discrete or continuous — regardless, the distribution of the data is denoted \\(f(\\cdot|\\theta)\\)!\nThe likelihood function is a function of the parameters \\(\\theta\\), but NOT of the data. This is because the data has already been collected and is, thus, fixed.\nAmazingly, the likelihood contains all of the information in the data about the parameters.\nViewed as a function of \\(\\theta\\), the likelihood tells us how likely each set of parameter values is to have given rise to the data, given the data. (The \\(|\\) symbol means given…)\n\nMost importantly, the value(s) of parameter(s) \\(\\theta\\) which jointly maximize \\(L\\) (i.e., have the highest likelihood of generating the observed data) is called the maximum likelihood estimator. It is often denoted \\(\\hat{\\theta}_{\\text{MLE}}\\), and is our best guess for the parameter(s) which generated the data.\n\n8.9.1 The log-likelihood\nIt is often easier to work with the log-transformed likelihood function\n\\[\\ln L(x_1,\\dots,x_n|\\theta) = \\ln \\prod_{i=1}^n f(x_i|\\theta) = \\sum_{i=1}^n \\ln f(x_i|\\theta).\\]\nIt has a maximum where the regular likelihood does, because the logarithm is an increasing function. Also, products are much more unruly (not to mention numerically unstable) than sums…\n\n\n8.9.2 Challenge\nWrite down the likelihood function for \\(x_1 = 12, x_2 = 14, x_3 = 14\\), assuming that the data are independent and identically distributed according to a \\(\\text{Binomial}(N,p)\\). Then, write the log-likelihood function for these data.\nThe probability mass function for the Binomial distribution is\n\\[\\Pr(X=x) = f(x|N,p) = {N \\choose x} (1-p)^{N-x} p^x.\\]\n\n\n8.9.3 Challenge\nWrite down the likelihood function for \\(x_1 = -1.23, x_2 = 0.2\\), assuming that the data are independent and identically distributed according to a \\(\\text{Normal}(\\mu,\\sigma^2)\\). Then, write the log-likelihood function for these data.\nThe probability density function for the Normal distribution is\n\\[f(x|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\text{exp} \\bigg(-\\frac{(x-\\mu)^2}{2\\sigma^2} \\bigg)\\]",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to inference I</span>"
    ]
  },
  {
    "objectID": "lectures/lec08-intro-inference-1.html#footnotes",
    "href": "lectures/lec08-intro-inference-1.html#footnotes",
    "title": "8  Introduction to inference I",
    "section": "",
    "text": "A technical and somewhat confusing thing about probability: if a continuous random variable assumes the value \\(x\\), the probability density function at \\(x\\) can assume a value greater than 1, but the probability of drawing \\(x\\) exactly is zero. This means, if Mete throws a dart at a board, the probability it lands at a particular point is zero — even though the probability density associated to this experiment would likely assume a value \\(&gt;0\\). While this seems strange, and it is, there is a simple way to understand what’s going on. The density must be integrated over a small region around the outcome to get an honest-to-god probability. In other words, the probability of getting \\(x\\) exactly is zero, but the probability of getting a value from \\(x-\\delta\\) to \\(x+\\delta\\) is the integral of the probability density over this region.↩︎",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to inference I</span>"
    ]
  },
  {
    "objectID": "lectures/lec09-intro-inference-2.html",
    "href": "lectures/lec09-intro-inference-2.html",
    "title": "9  Introduction to inference II",
    "section": "",
    "text": "9.1 Lesson preamble\nlibrary(tidyverse)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to inference II</span>"
    ]
  },
  {
    "objectID": "lectures/lec09-intro-inference-2.html#lesson-preamble",
    "href": "lectures/lec09-intro-inference-2.html#lesson-preamble",
    "title": "9  Introduction to inference II",
    "section": "",
    "text": "9.1.1 Lesson objectives\n\nLearn how to estimate parameters by numerically maximizing the (log-)likelihood function.\nImplement numerical maximization using dataset of Farrell and Davies (2019).\nReview of confidence intervals and hypothesis testing, focusing on their relationship with each other and the likelihood function.\nLearn how to preform interval estimation using the log-likelihood.\nPractice constructing confidence intervals using dataset of Farrell and Davies.\n\n9.1.2 Lesson outline\n\nPoint estimation\n\nReview of the idea of maximum likelihood estimation\nExample of likelihood maximizion using Farrell and Davies dataset\n\nHypothesis testing and interval estimation\n\nReview of hypothesis testing\nHypothesis testing & confidence interval construction using the log-likelihood ratio test statistic",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to inference II</span>"
    ]
  },
  {
    "objectID": "lectures/lec09-intro-inference-2.html#review-of-the-likelihood-function",
    "href": "lectures/lec09-intro-inference-2.html#review-of-the-likelihood-function",
    "title": "9  Introduction to inference II",
    "section": "9.2 Review of the likelihood function",
    "text": "9.2 Review of the likelihood function\nGiven data \\(x_1,x_2,\\dots,x_n \\sim f(x|\\theta)\\), likelihood-based statistical methods allow us to estimate parameters of of the model, \\(\\theta\\), which we use to explain the data. (If the data are, say, present-day genome sequences, parameters of the model could include historical mutation and recombination rates and effective population sizes. If the data were occurrence-absence data for a species in a given location, parameters of the model could include immigration and emigration rates.)\nTo determine what parameters were mostly likely to have given rise to the data under the assumption \\(f\\) models the data generating process, we find what value of \\(\\theta\\) maximizes\n\\[ \\ln L(x_1,\\dots,x_n|\\theta) = \\sum_{i=1}^n \\ln f(x_i|\\theta).\\]\nRemember that the logarithm shows up because \\(\\ln L\\) has nice statistical properties, and there may be numerical issues if probabilities (or probability densities) are multiplied.\nThe intuition for the method is quite simple: the value of a parameter (or, in the multi-parameter setting, the combination of values for the parameters) which gives rise to the highest probability of observing our data was most likely to have given rise to that data.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to inference II</span>"
    ]
  },
  {
    "objectID": "lectures/lec09-intro-inference-2.html#numerical-maximization-of-the-log-likelihood",
    "href": "lectures/lec09-intro-inference-2.html#numerical-maximization-of-the-log-likelihood",
    "title": "9  Introduction to inference II",
    "section": "9.3 Numerical maximization of the (log-)likelihood",
    "text": "9.3 Numerical maximization of the (log-)likelihood\nNumerical methods are often used to evaluate and maximize the likelihood. When you use functions in many packages, they use numerical techniques to maximize the likelihood function or something like it. So that you know WHAT they are actually doing, the following code chunk illustrates how to evaluate the likelihood function and identify where it assumes a maximum value — using the Farrell and Davies data-set which you analyzed in the self-directed lecture a couple weeks ago.\n\nread_csv(\"data/disease_distance.csv\") -&gt; disease_distance\n\nRows: 4157 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (7): Country, Disease, Parasite, Host, HostOrder, ParaType, ParaFamily\ndbl (17): Year, Cases, Deaths, Destroyed, Slaughtered, SR, EvoIso, Taxonomic...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNote: we will not do anything fancy to maximize the likelihood. People have thought up clever ways to maximize functions, but we will do a brute-force search of parameter space to find the parameter(s) most likely to explain the data. This always works, but if there are a lot of combinations of parameters which need to be tested, this is intractable and more sophisticated methods are needed.\n\n9.3.1 Step 1: specify the likelihood (i.e., the distribution of the data)\nThe likelihood one writes down will depend, in general, on the available data and the question that a researcher wants to answer. For this example, we will ignore very important complexities in the data (e.g., the fact that case and death count data are uneven between countries and years, the fact some hosts and parasites are represented more than others) to make progress.\nOur goal will be to estimate, for the host species Sus scrofa (the wild boar) and parasite family Coronavirinae (which includes SARS-CoV-1 and -2), the probability that a case results in a death. We will group the data, regardless of the year, sampling location, etc.\nWhat distribution makes sense to model the number of Sus scrofa deaths due to viruses in the family Coronavirinae? The Binomial! One can think of the number of (confirmed) cases of infection with a member of Coronavirinae as the “number of trials” and probability death as a “success probability”. Written out, \\(\\text{deaths}_{ij} \\sim \\text{Binomial}(\\text{cases}_{ij}, p)\\) for each country-year \\(ij\\).\n\ndata &lt;- disease_distance |&gt; subset(ParaFamily == \"Coronavirinae\" & Host == \"Sus_scrofa\")\n\nHow many observations are there?\n\n\n9.3.2 Step 2: evaluate the likelihood of individual observations\nSince we will need to evaluate the probability of all observations in order to estimate \\(p\\), the first thing to figure out to evaluate likelihood when we have only one observation (i.e., the first row of the previous data frame). The below function evaluate the probability there are exactly \\(d\\) deaths when there are \\(N\\) cases and a probability \\(p\\) that a boar dies due to infection (somewhere, at some time) with a member of the Coronavirinae.\n\nbinomial_evaluator &lt;- function(d, N, p){\n  return(dbinom(x = d, size = N, prob = p)) # log=T returns the log-likelihood of the observation\n}\n\n# let's figure out the likelihood of a single observation -- the first row\n\ncases &lt;- as.numeric(data[1,\"Cases\"])\ndeaths &lt;- as.numeric(data[1,\"Deaths\"])\n\n# make sure that your observations are numeric vectors, not data frames!\n\nbinomial_evaluator(d = deaths, N = cases, p = 0.01)\n\n[1] 0.9227447\n\nbinomial_evaluator(d = deaths, N = cases, p = 0.1)\n\n[1] 0.4304672\n\nbinomial_evaluator(d = deaths, N = cases, p = 0.2)\n\n[1] 0.1677722\n\n\nOf these values of \\(p\\), which is most likely to explain the observation in the first row?\n\n\n9.3.3 Step 3: evaluate the likelihood of all observations\n\ndataALL &lt;- data[,c(\"Cases\", \"Deaths\")]\n\nbinomial_evaluator(d = dataALL$Deaths, N = dataALL$Cases, p = 0.01)\n\n [1] 9.227447e-01 0.000000e+00 9.043821e-01 0.000000e+00 9.043821e-01\n [6] 1.000000e-12 2.084925e-01 9.509900e-01 9.900000e-01 6.670506e-21\n[11] 0.000000e+00 9.509900e-01 8.179069e-01 9.065279e-55\n\nbinomial_evaluator(d = dataALL$Deaths, N = dataALL$Cases,p = 0.1)\n\n [1] 4.304672e-01 1.693764e-02 3.486784e-01 0.000000e+00 3.486784e-01\n [6] 1.000000e-06 7.274974e-08 5.904900e-01 9.000000e-01 1.596866e-07\n[11] 0.000000e+00 5.904900e-01 1.215767e-01 1.551125e-13\n\nbinomial_evaluator(d = dataALL$Deaths, N = dataALL$Cases,p = 0.2)\n\n [1]  1.677722e-01  1.516705e-96  1.073742e-01 3.860350e-306  1.073742e-01\n [6]  6.400000e-05  7.621456e-16  3.276800e-01  8.000000e-01  4.470958e-04\n[11] 8.870964e-220  3.276800e-01  1.152922e-02  1.208139e-04\n\n\n\n\n9.3.4 Step 4: put the (log-)likelihoods together!\n\n# numerical problems can arise when you take products of small numbers\n# this is one reason why we need to take the log of the likelihood\n# product of individual likelihoods (for each observation) = the sum of log-likelihoods\n\nprod(binomial_evaluator(d = dataALL$Deaths, N = dataALL$Cases, p = 0.2))\n\n[1] 0\n\nsum(log(binomial_evaluator(d = dataALL$Deaths, N = dataALL$Cases, p = 0.2)))\n\n[1] -1502.624\n\n\n\n\n9.3.5 Step 5: identify the value of the parameter(s) most likely to explain the data\nSo far, we have only evaluated the log-likelihood (of one and all of observations) at specific values of the parameter of interest, \\(p\\). To identify what value of \\(p\\) is most likely to explain the data, we need to evaluate the log-likelihood across a RANGE of \\(p\\) values and identify when the function is maximized.\nWe can do this in a couple ways, but here is how using a for loop:\n\nLogLik &lt;- NULL\nindex &lt;- 1\np_values_to_test &lt;- seq(0, 1, by = 0.001)\n\nfor (p in p_values_to_test){\n  LogLik[index] &lt;- sum(log(binomial_evaluator(d = dataALL$Deaths, N = dataALL$Cases, p = p)))\n  index &lt;- index + 1\n}\n\nLLs &lt;- data.frame(LogLik, p = p_values_to_test)\n\nLLs |&gt; ggplot(aes(x = p, y = LogLik)) + geom_line()\n\n\n\n\n\n\n\n\nValues of \\(p\\) where \\(\\ln L(p)= -\\infty\\) are very unlikely to explain the data. Since we are interested in the value of \\(p\\) which gives rise to the largest \\(\\ln L\\), and these values are infinitely negative, they are not candidates for our estimator of \\(p\\). As a result, they are suppressed in the plot.\nAnother thing to notice is the curvature of the likelihood function around the value where it assumes a maximum. This tells us something about how confident we should be about our “best guess” for the parameter, or the maximum likelihood estimate of that parameter. More on this in a couple minutes…\n\nLLs |&gt; subset(LogLik == max(LogLik))\n\n       LogLik     p\n235 -1476.786 0.234\n\n\n\n\n9.3.6 Homework question…\nIn the next homework, you will be asked to write a function with arguments that specify the host species and the parasite family. Then, using this function on all host and parasite family combinations, you will be asked to estimate the most likely values of \\(p\\), i.e., the value which maximizes the likelihood function that arises from the assumption that, for a combination of host and parasite family, the number of deaths of the host is Binomial with a probability of death that does not depend on time or sampling location. If there are no observations for a specific host and parasite family combination, the function should return NA; else, the function should return the value of \\(p\\) which maximizes the likelihood function formed from the data specific to that host and parasite family. You may also run into issues where the log-likelihood is negative infinity at all \\(p\\); in this case, the function should return NA. This is a really challenging exercise, but we think it will be (hopefully) equally as rewarding!\nHint: this exercise will involve modifying the code that you have been provided in this lecture. In particular, you will have to wrap the code above in a function and loop over all combinations of hosts and parasite families.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to inference II</span>"
    ]
  },
  {
    "objectID": "lectures/lec09-intro-inference-2.html#review-of-hypothesis-testing",
    "href": "lectures/lec09-intro-inference-2.html#review-of-hypothesis-testing",
    "title": "9  Introduction to inference II",
    "section": "9.4 Review of hypothesis testing",
    "text": "9.4 Review of hypothesis testing\nOften, the objective of a study is not to estimate a parameter but to decide which of two (or more) contradictory claims about the parameter is consistent with the data. This part of statistics is called hypothesis testing. Hypothesis testing is intimately connected to the construction and interpretation of confidence intervals which quantify estimation uncertainty.\n\n\n9.4.1 Null and alternative hypotheses\nA statistical hypothesis is an assertion about the values of one or more parameters, or the form of a probability distribution that is used to model the data.\nTwo contradictory hypotheses of the first kind are\n\\[H_0: \\theta = \\theta_0\\] \\[H_1: \\theta \\neq \\theta_0\\]\nThe first hypothesis is called the null hypothesis and may correspond to an expectation we have about the parameter (from, e.g., prior data). The second hypothesis is called the alternative hypothesis. The data are used to make a principle conclusion about if the null hypothesis is consistent with the data; if so, we reject the alternative hypothesis and, if not, we reject the null hypothesis as an explanation for the data generative process. In the following sections, we will describe the process for conducting such a hypothesis test.\n\n\n9.4.2 Test statistics\nSuppose we have data \\(x_1,\\dots,x_n \\sim f(x|\\theta)\\) and wish to test the above hypotheses, i.e., to decide if \\(\\theta \\neq \\theta_0\\). We do this by constructing a test statistic, i.e., function of the data, and assessing if the realized value of statistic is consistent with its distribution under the null hypothesis. Is the value of the statistic, at some level of significance, different from what we would expect if the null hypothesis were true?\nMany choices of test statistic are possible, but the likelihood ratio is one that is commonly used:\n\\[\\lambda_{LR} = -2 (\\ln L(\\theta_0)-\\ln L(\\hat{\\theta}_{\\text{MLE}})).\\]\nThe statistic is based on the likelihood function, and its asymptotic distribution (as the sample size becomes large) under the null hypothesis is known. \\(\\lambda_{LR}\\) has an approximate \\(\\chi^2\\) distribution under \\(H_0\\). (Recall that one must know the approximate distribution of a test statistic to preform a hypothesis test.) Given the value and distribution of our test statistic under the null hypothesis, we can determine which two competing hypothesis is consistent with the data.\n\n\n9.4.3 How do we do this?\nTo decide between the null and alternative hypothesis, given a test statistic and its distribution under \\(H_0\\), we must specify a significance level \\(\\alpha\\). The significance level measures how likely we are to reject the null hypothesis, given that it is true: \\(\\alpha = P(\\text{reject } H_0 | H_0)\\). The significance level is chosen before data collection, and is typically set to 0.05 or smaller. By a similar token, the power of a statistical test is defined as the probability of rejecting the alternative hypothesis, given it is true: \\(1-\\beta = \\Pr(\\text{reject } H_0 | H_1).\\) Many factors affect the power of a test, but a test based on the likelihood ratio test statistic is the uniformly most powerful among all alternatives to test the above hypothesis. In general, one can preform (and it is best practice to preform!) analyses ahead of data collection to ensure power at a certain level.1\nWe conduct a hypothesis test at significance level \\(\\alpha\\) as follows:\n\nState the null and alternative hypothesis and significance level \\(\\alpha\\).\nCollect data, possibly with knowledge of the sample size required to achieve a certain power.\nCalculate the realized value \\(s\\) of a test statistic \\(S\\), e.g., \\(\\lambda_{LR}\\). The test statistic must have a known distribution under the null hypothesis. The likelihood ratio has a \\(\\chi^2\\) distribution under \\(H_0\\) above.\nCompute the probability of observing the realized value of the test statistic or something more extreme, given the null hypothesis is true, i.e., \\(p = \\Pr(S &gt; s | H_0)\\). This probability is called a \\(p\\) value. If \\(p &lt; \\alpha\\), we reject the null hypothesis at significance level \\(\\alpha\\) and, if not, we fail to reject \\(H_0\\).\n\nTo illustrate how this works, we will return to the previous example.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to inference II</span>"
    ]
  },
  {
    "objectID": "lectures/lec09-intro-inference-2.html#hypothesis-testing-using-the-log-likelihood-ratio",
    "href": "lectures/lec09-intro-inference-2.html#hypothesis-testing-using-the-log-likelihood-ratio",
    "title": "9  Introduction to inference II",
    "section": "9.5 Hypothesis testing using the (log-)likelihood ratio",
    "text": "9.5 Hypothesis testing using the (log-)likelihood ratio\nUsing data of the number of cases and deaths of the wild boar infected with members of the viral family Coronavirinae, we estimated that the probability of death given infection is \\(\\hat{p}_{\\text{MLE}} = 0.234\\). Is this estimate significantly different than say 0.2?\nA test of \\(H_0: p = 0.2\\) vs \\(H_1: p \\neq 0.2\\) at significance level \\(\\alpha = 0.05\\) can be preformed by\n\ncalculating the log-likelihood ratio test statistic\ndetermining the probability of obtaining a statistic more extreme, under the expected distribution for the statistic. For the log-likelihood statistic, as it has a \\(\\chi^2\\) distribution with one degree of freedom, this can be done as follows:\n\n\nLL_at_null &lt;- LLs |&gt; subset(p == 0.2) |&gt; select(LogLik)\nLL_at_max &lt;- LLs |&gt; subset(LogLik == max(LogLik)) |&gt; select(LogLik)\n\ntest_statistic &lt;- as.numeric(-2*(LL_at_null - LL_at_max))\ntest_statistic ### realized value of the likelihood ratio test statistic\n\n[1] 51.67639\n\npchisq(test_statistic, df = 1, lower.tail = FALSE)\n\n[1] 6.544503e-13\n\n### if p &lt; 0.05, then we reject H0; otherwise, we fail to reject H0\n\n\n9.5.1 Challenge\nHow might we extend this operation – of calculating the \\(p\\)-value, or the probability of a the log-likelihood ratio test statistic more extreme than the one we observed – to determine the set of \\(p_0\\) such that we fail to reject \\(H_0: p = p_0\\)?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to inference II</span>"
    ]
  },
  {
    "objectID": "lectures/lec09-intro-inference-2.html#confidence-intervals",
    "href": "lectures/lec09-intro-inference-2.html#confidence-intervals",
    "title": "9  Introduction to inference II",
    "section": "9.6 Confidence intervals",
    "text": "9.6 Confidence intervals\nHaving gotten an estimate for \\(p\\), the probability a wild boar dies when infected with a member of the Coronavirinae, you might be asking if it is possible to get an estimate of the uncertainty around our “best guess” (i.e., maximum likelihood estimator) for \\(p\\). The answer is a resounding YES! It turns out the previous calculations enable us to construct a confidence interval for \\(p\\). This is because the confidence interval for a parameter is nothing but the set of values of the parameter for which we cannot reject the null hypothesis!2 That is, there is a duality between confidence intervals and hypothesis testing. When you do one, you (secretly) do the other. In the previous example, we showed that \\(p=0.2\\) is not in the confidence interval for \\(p\\)!\nThere are other ways to construct confidence intervals, but the one below involves finding which \\(\\theta_0\\) are such that \\(\\lambda_{LR} -2(\\ln L(\\theta_0)-\\ln L(\\hat{\\theta}_{\\text{MLE}})) &lt; \\chi^2_c\\), where \\(\\chi^2_c\\) a cutoff based on \\(100(1-\\alpha)\\%\\)-ile for a \\(\\chi^2\\) distribution with one degree of freedom.3\n\ncutoff &lt;- qchisq(0.95,df=1)/2 \n# cutoff for admissible values based on 95%-ile for a chi-squared dist df=1\n\nMLE &lt;- LLs |&gt; subset(LogLik == max(LogLik)) \n\nLLs %&gt;% subset(abs(LogLik - MLE$LogLik) &lt; cutoff) -&gt; values_inconfidence_interval\n\nc(min(values_inconfidence_interval$p),\n  max(values_inconfidence_interval$p)) ### confidence interval!\n\n[1] 0.225 0.243\n\n\nNow, let’s display our point estimate for \\(p\\) as well as the confidence interval we have just constructed.\n\nLLs %&gt;% ggplot(aes(x = p, y = LogLik)) + geom_line() +\n  geom_vline(xintercept = MLE$p, color = \"red\") +\n  geom_vline(xintercept = min(values_inconfidence_interval$p), \n              color = \"red\", linetype = \"dashed\") +\n   geom_vline(xintercept = max(values_inconfidence_interval$p), \n              color = \"red\", linetype = \"dashed\") +\n  labs(x = \"p\", y = \"log-likelihood\") +\n  xlim(c(0.2,0.3))\n\n\n\n\n\n\n\n\nThe limits for the \\(95\\%\\) confidence interval for \\(p\\) are given by the dashed red lines. The maximum likelihood estimate for \\(p\\) is represented by the solid red line. The key thing to note is that the width confidence region is determined by the curvature of the log-likelihood function about the maximum likelihood estimate for the focal parameter.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to inference II</span>"
    ]
  },
  {
    "objectID": "lectures/lec09-intro-inference-2.html#footnotes",
    "href": "lectures/lec09-intro-inference-2.html#footnotes",
    "title": "9  Introduction to inference II",
    "section": "",
    "text": "We will learn how to do power analyses in the linear models lectures!↩︎\nThere are other ways to define and understand confidence intervals but they are somewhat oblique.↩︎\nNote: the code in the previous section assumed \\(\\alpha = 0.05\\). The functions we use in this section, to keep things general, are a bit different.↩︎",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to inference II</span>"
    ]
  },
  {
    "objectID": "lectures/lec10-linear-models-1.html",
    "href": "lectures/lec10-linear-models-1.html",
    "title": "10  Linear models I",
    "section": "",
    "text": "10.1 Lesson preamble:\nlibrary(tidyverse)\nSSDdata &lt;- read_csv(\"data/SSDinMammals.csv\")\n\nRows: 691 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): Order, Family, Species, Scientific_Name, Comments, Source\ndbl (12): massM, SDmassM, massF, SDmassF, lengthM, SDlengthM, lengthF, SDlen...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec10-linear-models-1.html#lesson-preamble",
    "href": "lectures/lec10-linear-models-1.html#lesson-preamble",
    "title": "10  Linear models I",
    "section": "",
    "text": "10.1.1 Lesson objectives\n\nUnderstand the logic of the general linear model, including the assumptions that are placed on the data, parameters, and errors.\nUnderstand the meaning of regression coefficients and how they are estimated.\nUnderstand how to implement linear models in R.\nLearn how to respond when data violate assumptions, visualize fitted models, etc.\n\n10.1.2 Lesson outline\n\nTheory\n\nStructure and assumptions, including interpretation of the effects\nLikelihood-based estimation and inference\nTransformations of the response, dummy variables, and interactions between covariates\n\nPractice\n\nImplement multivariate linear model using sexual size dimorphism dataset\nDevelop intuition for model diagnostics\nVisualizing fitted models",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec10-linear-models-1.html#linear-models-why-we-care",
    "href": "lectures/lec10-linear-models-1.html#linear-models-why-we-care",
    "title": "10  Linear models I",
    "section": "10.2 Linear models: why we care",
    "text": "10.2 Linear models: why we care\nLinear models are at the heart of statistical practice in the physical, life, and social sciences! Linear regression actually refers to a family of modeling approaches that attempt to learn how the mean and/or variance of a response variable \\(\\boldsymbol{y} = (y_1,\\dots,y_n)\\) depend on (linear) combinations of variables \\(\\boldsymbol{x}_i = (x_{i1},\\dots,x_{in})\\) called predictors. In this lecture, we will discuss various forms of the linear model and assumptions placed on the data to make estimation and inference of the relationships between variables tractable. Our goal will be to become familiar with how these models work – namely, how their parameters are inferred using maximum likelihood — and practice fitting them in R.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec10-linear-models-1.html#the-simple-linear-model",
    "href": "lectures/lec10-linear-models-1.html#the-simple-linear-model",
    "title": "10  Linear models I",
    "section": "10.3 The simple linear model",
    "text": "10.3 The simple linear model\nThe simple linear models describes how a response \\(Y\\) depends on a contentious explanatory variable, \\(x\\), which is fixed. The model is\n\\[Y \\sim \\text{Normal}(\\beta_0 + \\beta_1 x, \\sigma^2).\\]\nIn turn, this specifies what the likelihood function have data \\((y_1,x_1),\\dots,(y_n,x_n)\\). (As usual, we assume the \\(y\\)s are independent.)\nBelow is a visual representation of how the data generative process for \\(Y\\) is modeled. At each \\(x\\), \\(Y\\) is Normal with a mean that depends on the explanatory variable and fixed variance (i.e., changing \\(x\\) does not change the variance in the observed response).\n\n\n10.3.1 Challenge\nForm the likelihood \\(L(\\beta_0,\\beta_1,\\sigma^2|\\boldsymbol{x}_i,\\boldsymbol{y}_i)\\). Recall that the probability distribution of the \\(Normal(\\mu,\\sigma^2)\\) distribution is\n\\[\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-(y-\\mu)^2/2\\sigma^2}.\\]\nHint: notice how, in the regression model, the mean value of the response is \\(\\beta_0 + \\beta_1 x.\\)\n\n\n10.3.2 Challenge\nWhat assumptions are we making about the data when we fit simple linear model? What must be true of the data? Discuss.\n\n\n10.3.3 Challenge\nSuppose we think \\(Y\\) is not normally distributed (and constant variance \\(\\sigma^2\\)), but that it’s logarithm (or, say, square root) is. How might we adjust the model to account for this?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec10-linear-models-1.html#multiple-regression",
    "href": "lectures/lec10-linear-models-1.html#multiple-regression",
    "title": "10  Linear models I",
    "section": "10.4 Multiple regression",
    "text": "10.4 Multiple regression\nIt is straightforward to extend the simple regression model to include multiple covariates/predictors. Suppose, for each realization of \\(Y\\), we have associated measurements \\(x_1,\\dots,x_p\\). We can model how \\(Y\\) changes with these predictors as follows:\n\\[Y \\sim \\text{Normal}(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p, \\sigma^2).\\] The likelihood that arises from data \\((y_i,x_{1i},\\dots,x_{pi})\\) where \\(i=1,\\dots,n\\) is\n\\[L(\\beta_0,\\dots,\\beta_p,\\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-(y_i-\\beta_0-\\beta_1 x_{1i}-\\cdots-\\beta_p x_{pi})^2/2\\sigma^2}.\\]\nAgain, assumptions of this model include\n\nThe data, i.e., observations of the response \\(Y\\), are independent and Normally distributed.\nThe mean response is a linear function of the predictors.\nThe error variance \\(\\sigma^2\\) is constant and, thus, does not depend on the predictors.\nThe parameters \\(\\beta_0,\\dots,\\beta_p\\) (called regression coefficients or effect sizes) are non-random.\nThe predictors are known with certainty.\n\nMaximum likelihood estimation gives rise to point and interval estimates for \\(\\beta_1,\\dots,\\beta_p,\\sigma^2\\).1\n\n10.4.1 Interpreting the regression coefficients\nWe must be careful how we interpret the parameters of any statistic model after we fit the model to data. This is definitely true of the regression coefficients \\(\\beta_1,\\dots,\\beta_p\\). The estimates are not the same as the “true” values the parameters assume; they are our best guess of the “true” regression coefficients, given the (limited, imperfect, incomplete, noisy) data that we have. Moreover, \\(\\beta_j\\) must be understood as the amount the average value of the response variable changes when the predictor \\(x_j\\) increases by one unit, assuming all else is constant.\nThat is, \\(\\beta_j\\) is a measure of the sensitivity of \\(E(Y)\\) to \\(x_j\\) — how much does \\(Y\\) change, on average, if we increase \\(x_j\\) by one unit.\n\n\n10.4.2 Transformations\nIf you suspect the raw data are not normally distributed, but transformed versions of the data are, you can replace \\(Y\\) with \\(f(Y)\\) where \\(f(\\cdot)\\) is the transformation and proceed with the analysis. The only thing to keep in mind is how to interpret the regression coefficients.\nIf \\(\\beta_1 = 0.1\\) when regression \\(f(Y)\\) on \\(x_1,\\dots,x_p\\), then that means _per unit change in \\(x_1\\), all else constant, \\(f(Y)\\) increases, on average, by unit 0.1. This does NOT mean that \\(Y\\), on the raw scale, increases by that amount.\n\n\n10.4.3 Categorical predictors\nWe can regress \\(Y\\) on discrete, as well as continuous, predictor variables. To see how this is done, and how to interpret the resulting regression coefficients, suppose a predictor has \\(K\\) levels. To estimate the effect of one of these levels on the response variable (say, of sampling individuals in a particular country), one of the levels of the discrete variable is treated as a “reference” and effects are estimated for all other levels. That is, we define the model in terms of a baseline and to interpret the regression coefficients relative to this baseline. This involves coding “dummy variables” for all but one the \\(K\\) values the predictor can take assume and estimating regression coefficients for each of these variables.\nThe regression coefficient for a “dummy variable” (associated to one of the values a categorical predictor) measures the expected change in the response, all else constant, if we were to change from the baseline to the level of interest.\n\n\n10.4.4 Interactions, higher order terms\nFinally, linear models can accommodate non-linear interactions between explanatory variables. If \\(x_2 = x_1^2\\), one can estimate an effect for \\(x_2\\) (and do the same for all higher order terms for \\(x_1\\) and the other covariates). This effect sizes that are estimated have to be interpreted carefully but does not pose a difficulty to forming the likelihood nor maximizing it.\nOther interactions can also be modeled. For example, suppose we were interested in the combined effects of changing salinity and temperature on the number of species in an ecosystem because we suspect that changing salinity has little effect on the effect on the number of species if temperature in high (e.g., there are no species left). Then, letting \\(x_1\\) be salinity and \\(x_2\\) temperature, their interaction would be included as a covariate \\(x_3 = x_1 x_2\\) and the associated effect estimated from the data.\nWhen there are interactions, coefficients are interpreted as follows. We can say that \\(\\beta_1\\) is the expected change in the response if we increase \\(x_1\\) by one unit and set all covariates with which it interacts equal to zero. The effect size for the interaction between \\(x_1\\) and \\(x_2\\) measures the change in the the expected response if \\(x_1\\) and \\(x_2\\) were to BOTH increase by one unit that is on top of or in addition to the change due each variable in isolation.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec10-linear-models-1.html#using-lm-to-fit-a-multivariate-regression-model",
    "href": "lectures/lec10-linear-models-1.html#using-lm-to-fit-a-multivariate-regression-model",
    "title": "10  Linear models I",
    "section": "10.5 Using lm to fit a multivariate regression model",
    "text": "10.5 Using lm to fit a multivariate regression model\nOne can use the likelihood above to derive estimates of \\(\\beta_0,\\dots,\\beta_p,\\sigma^2\\) and confidence intervals for those parameters. (In fact, one can use calculus to maximize the likelihood and derive expressions for the estimates which maximize the probability of the observed data.)\nMaximization of the likelihood, assuming the data are independent draws of a normal distribution with the mean and varience specified above, is exactly what happens when one fits a regression model in R using the function lm().\nFor example, using the sexual size dimorphism data set, one to estimate the effects of Order (a variable) on the logarithm mean male mass as follows.\n\nmodel &lt;- lm(log(massM) ~ Order, data = SSDdata)\nsummary(model)\n\n\nCall:\nlm(formula = log(massM) ~ Order, data = SSDdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9361 -0.8298 -0.1617  0.7102  6.4749 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            3.3101     0.2800  11.823  &lt; 2e-16 ***\nOrderArtiodactyla      7.5459     0.3584  21.054  &lt; 2e-16 ***\nOrderCarnivora         5.1631     0.3530  14.627  &lt; 2e-16 ***\nOrderChiroptera       -0.5865     0.2982  -1.967 0.049623 *  \nOrderCingulata         4.7213     0.6625   7.127 2.65e-12 ***\nOrderDasyuromorphia    2.8644     0.6625   4.324 1.77e-05 ***\nOrderDidelphimorphia   1.2473     0.4052   3.078 0.002169 ** \nOrderDiprotodontia     4.4899     0.4105  10.938  &lt; 2e-16 ***\nOrderEulipotyphla     -0.1853     0.3584  -0.517 0.605292    \nOrderLagomorpha        3.3687     0.5086   6.624 7.15e-11 ***\nOrderMacroscelidea     0.5654     0.6625   0.853 0.393752    \nOrderPeramelemorphia   3.4938     0.7274   4.803 1.92e-06 ***\nOrderPerissodactyla    9.1769     0.9898   9.271  &lt; 2e-16 ***\nOrderPilosa            5.4604     0.6155   8.872  &lt; 2e-16 ***\nOrderPrimates          4.1313     0.3152  13.108  &lt; 2e-16 ***\nOrderRodentia          1.1136     0.2943   3.784 0.000168 ***\nOrderScandentia        1.8380     0.8242   2.230 0.026065 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.343 on 674 degrees of freedom\nMultiple R-squared:  0.7588,    Adjusted R-squared:  0.7531 \nF-statistic: 132.5 on 16 and 674 DF,  p-value: &lt; 2.2e-16\n\n\nAfter a call to lm(), R returns a lot of information. Here is what is in the table above:\n\nDescriptive statistics for the “residuals” \\(\\varepsilon_i = y_i - \\widehat{\\beta_0} - \\widehat{\\beta_1} x_i\\), which tell us about how much variability there is in the data relative to the linear model specified and fitted.\nThe regression coefficients minimizing the likelihood of the data and \\(95\\%\\) confidence intervals for each. The CIs are expressed as standard errors, since the estimators have an approximate Normal distribution. A joint confidence region for the coefficients can also be found using the LLR statistic which have been using to construct confidence intervals.\nA suite of test statistics! The \\(t\\) statistics and their \\(p\\) values are associated to the test \\(H_0: \\beta_i = 0\\) vs \\(H_1: \\beta_i \\neq 0\\). Significance codes specify the level \\(\\alpha\\) at which we have evidence to reject the null hypothesis for each coefficient.\nMeasures of goodness-of-fit: the multiple \\(R^2\\) and the adjusted \\(R^2\\). These explain the proportion of variance that are explained by the model. The latter measures the proportion of variance explained by the linear model upon adjusting for sample size and \\(\\#\\) of predictors.\n\n\n10.5.0.1 Challenge\nHow would we interpret the coefficient associated to the Order Dasyuromorphia?\n\n\n10.5.0.2 Challenge\nWhy did we log-transform the mean mass of males in the previous call to lm()? Hint: think about the assumptions of the linear model.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec10-linear-models-1.html#checking-assumptions-of-the-model-are-satified",
    "href": "lectures/lec10-linear-models-1.html#checking-assumptions-of-the-model-are-satified",
    "title": "10  Linear models I",
    "section": "10.6 Checking assumptions of the model are satified",
    "text": "10.6 Checking assumptions of the model are satified\nOne can do a quick check to see if the assumptions of the regression model — e.g., that the data are normal — by calling the function plot() on the model fitted using lm.\n\nplot(model) # returns diagnostics (are the data normal, are there points with high leverage...)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore on how to read diagnostic plots can be found here.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec10-linear-models-1.html#an-example",
    "href": "lectures/lec10-linear-models-1.html#an-example",
    "title": "10  Linear models I",
    "section": "10.7 An example!",
    "text": "10.7 An example!\n\n10.7.1 In mammals, is there an effect of sex on body size?\nRecall how you cleaned the sexual size dimorphism dataset in the second homework. We will use the cleaned dataset to do a very crude test of the hypothesis that males and females differ in size. In particular, we will determine the quantitative effect of sex is on the size of a typical individual, regardless of their species, sampling effort, etc. Much more complicated models can be constructed, but it turns out these models give rise to qualitatively-similar conclusions.\n\nmammal_length &lt;- SSDdata %&gt;%\n  select(c(\"Order\", \"Scientific_Name\", \"lengthM\", \"lengthF\")) %&gt;%\n  pivot_longer(c(lengthM, lengthF), names_to = \"sex\", values_to = \"length\",\n               names_pattern = \"length(.)\")\n\nmammal_mass &lt;- SSDdata %&gt;%\n  select(c(\"Order\", \"Scientific_Name\", \"massM\", \"massF\")) %&gt;%\n  pivot_longer(c(massM, massF), names_to = \"sex\", values_to = \"mass\",\n               names_pattern = \"mass(.)\")\n\nmass_nodup &lt;- mammal_mass %&gt;% \n  group_by(Scientific_Name, sex) %&gt;%\n  distinct(Scientific_Name, sex, .keep_all = TRUE)\n\nlength_nodup &lt;- mammal_length %&gt;% \n  group_by(Scientific_Name, sex) %&gt;%\n  distinct(Scientific_Name, sex, .keep_all = TRUE)\n\nmammal_long &lt;- full_join(mass_nodup, length_nodup, \n                         by = join_by(\"Scientific_Name\", \"sex\", \"Order\"))\n\nglimpse(mammal_long)\n\nRows: 1,282\nColumns: 5\nGroups: Scientific_Name, sex [1,282]\n$ Order           &lt;chr&gt; \"Afrosoricida\", \"Afrosoricida\", \"Afrosoricida\", \"Afros…\n$ Scientific_Name &lt;chr&gt; \"Amblysomus hottentotus\", \"Amblysomus hottentotus\", \"E…\n$ sex             &lt;chr&gt; \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\",…\n$ mass            &lt;dbl&gt; 80.6, 66.0, 28.0, 23.1, 102.4, 99.9, 7.3, 7.0, 111.0, …\n$ length          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\nmodel &lt;- lm(log(mass) ~ sex, data = mammal_long)\nsummary(model)\n\n\nCall:\nlm(formula = log(mass) ~ sex, data = mammal_long)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0267 -2.0625 -0.7779  1.8266 11.2988 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.0586     0.1048   48.25   &lt;2e-16 ***\nsexM          0.0667     0.1483    0.45    0.653    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.655 on 1280 degrees of freedom\nMultiple R-squared:  0.000158,  Adjusted R-squared:  -0.0006231 \nF-statistic: 0.2023 on 1 and 1280 DF,  p-value: 0.6529\n\n\nRemarkably, no effect! Males are not larger than females. The \\(p\\) value is not only \\(&gt;\\alpha = 0.05\\), the size of the estimated effect is quite small. This is pretty suprising, given the large number of papers that claim, despite the lack of evidence, that there are stark differences in size between male and female individuals.\n\n\n10.7.2 Adding interactions…\nTo include multiple variables in a regression, without their interaction, one writes y ~ x + z. To include their interaction, the syntax is y ~ x*z. (This is equivalent to y ~ x + z + x:z where the color indicates to fit an interaction-only model. In general, it is best to avoid fitting interaction-only models. As this post explains, interactions can be due to dependence of the response on the covariates in the interaction, or non-linear functions of those covariates. Plus, interpreting the estimated coefficients is really difficult.)\nHow would you fit a model in which there is an effect of sex and, separately, effects of belonging to a given Order on mean body size? How would you fit a model where, in addition to these effects, an interaction between sex and Order?\n\nOrderSpecificModel &lt;- lm(log(mass) ~ sex+Order, data = mammal_long)\nsummary(OrderSpecificModel)\n\n\nCall:\nlm(formula = log(mass) ~ sex + Order, data = mammal_long)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.3025 -0.8503 -0.1681  0.7009  6.6448 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           3.26470    0.20065  16.271  &lt; 2e-16 ***\nsexM                  0.06670    0.07469   0.893 0.372003    \nOrderArtiodactyla     7.48041    0.25681  29.128  &lt; 2e-16 ***\nOrderCarnivora        4.98832    0.25238  19.765  &lt; 2e-16 ***\nOrderChiroptera      -0.51539    0.21144  -2.437 0.014926 *  \nOrderCingulata        4.75527    0.46652  10.193  &lt; 2e-16 ***\nOrderDasyuromorphia   2.67286    0.46652   5.729 1.26e-08 ***\nOrderDidelphimorphia  1.04768    0.29311   3.574 0.000364 ***\nOrderDiprotodontia    4.46041    0.28907  15.430  &lt; 2e-16 ***\nOrderEulipotyphla    -0.13484    0.25681  -0.525 0.599633    \nOrderLagomorpha       3.16969    0.38807   8.168 7.54e-16 ***\nOrderMacroscelidea    0.58336    0.46652   1.250 0.211367    \nOrderPeramelemorphia  3.38116    0.51219   6.601 5.98e-11 ***\nOrderPerissodactyla   9.19047    0.69700  13.186  &lt; 2e-16 ***\nOrderPilosa           5.56738    0.46652  11.934  &lt; 2e-16 ***\nOrderPrimates         4.06503    0.22279  18.246  &lt; 2e-16 ***\nOrderRodentia         1.12542    0.20796   5.412 7.46e-08 ***\nOrderScandentia       1.83724    0.58037   3.166 0.001584 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.337 on 1264 degrees of freedom\nMultiple R-squared:  0.7495,    Adjusted R-squared:  0.7461 \nF-statistic: 222.5 on 17 and 1264 DF,  p-value: &lt; 2.2e-16\n\nOrderSexInteractionModel &lt;- lm(log(mass) ~ Order*sex, data = mammal_long)\nsummary(OrderSexInteractionModel)\n\n\nCall:\nlm(formula = log(mass) ~ Order * sex, data = mammal_long)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.2349 -0.8424 -0.1661  0.6973  6.6406 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                3.286023   0.280406  11.719  &lt; 2e-16 ***\nOrderArtiodactyla          7.391554   0.365279  20.235  &lt; 2e-16 ***\nOrderCarnivora             4.864594   0.358973  13.551  &lt; 2e-16 ***\nOrderChiroptera           -0.480737   0.300745  -1.598 0.110187    \nOrderCingulata             4.789225   0.663561   7.217 9.17e-13 ***\nOrderDasyuromorphia        2.481360   0.663561   3.739 0.000193 ***\nOrderDidelphimorphia       0.985744   0.416903   2.364 0.018210 *  \nOrderDiprotodontia         4.430888   0.411156  10.777  &lt; 2e-16 ***\nOrderEulipotyphla         -0.168853   0.365279  -0.462 0.643977    \nOrderLagomorpha            3.206508   0.551980   5.809 7.96e-09 ***\nOrderMacroscelidea         0.601359   0.663561   0.906 0.364973    \nOrderPeramelemorphia       3.268528   0.728516   4.487 7.91e-06 ***\nOrderPerissodactyla        9.204062   0.991385   9.284  &lt; 2e-16 ***\nOrderPilosa                5.618670   0.663561   8.467  &lt; 2e-16 ***\nOrderPrimates              4.009450   0.316885  12.653  &lt; 2e-16 ***\nOrderRodentia              1.108223   0.295791   3.747 0.000187 ***\nOrderScandentia            1.836437   0.825493   2.225 0.026283 *  \nsexM                       0.024053   0.396554   0.061 0.951643    \nOrderArtiodactyla:sexM     0.177712   0.516582   0.344 0.730893    \nOrderCarnivora:sexM        0.247443   0.507665   0.487 0.626050    \nOrderChiroptera:sexM      -0.069299   0.425317  -0.163 0.870596    \nOrderCingulata:sexM       -0.067905   0.938418  -0.072 0.942326    \nOrderDasyuromorphia:sexM   0.382991   0.938418   0.408 0.683252    \nOrderDidelphimorphia:sexM  0.123879   0.589590   0.210 0.833615    \nOrderDiprotodontia:sexM    0.059034   0.581462   0.102 0.919148    \nOrderEulipotyphla:sexM     0.068022   0.516582   0.132 0.895261    \nOrderLagomorpha:sexM      -0.073637   0.780617  -0.094 0.924861    \nOrderMacroscelidea:sexM   -0.035996   0.938418  -0.038 0.969409    \nOrderPeramelemorphia:sexM  0.225269   1.030277   0.219 0.826959    \nOrderPerissodactyla:sexM  -0.027191   1.402030  -0.019 0.984530    \nOrderPilosa:sexM          -0.102580   0.938418  -0.109 0.912973    \nOrderPrimates:sexM         0.111163   0.448142   0.248 0.804134    \nOrderRodentia:sexM         0.034388   0.418312   0.082 0.934496    \nOrderScandentia:sexM       0.001603   1.167423   0.001 0.998904    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 1248 degrees of freedom\nMultiple R-squared:  0.7498,    Adjusted R-squared:  0.7432 \nF-statistic: 113.3 on 33 and 1248 DF,  p-value: &lt; 2.2e-16\n\n\nNotice that in the OrderSexInteractionModel model, there are significant effects of the Order on body size but the effect of sex, even when order-specific, on size is not significant. That is, the associated regression coefficients have confidence intervals overlapping zero, so that we fail to reject the null hypothesis \\(H_0: \\beta_j = 0\\).\n\n\n10.7.3 Challenge\nUse a for loop to regress log body size on sex for each Order? Print the coefficients from inside the loop.\n\n\n10.7.4 Visualizing fitted models\nFor the first model we fit, it is straightforward to visualize the (lack of an) effect of sex on size:\n\nmodel &lt;- lm(log(mass) ~ sex, data = mammal_long)\n\nmammal_long |&gt; ggplot(aes(x = sex, y = log(mass))) + \n  geom_violin() +\n  geom_jitter(alpha = 0.5, width = 0.1) + \n  geom_smooth(aes(group = 1), method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFor the model fitted for each Order, we can visualize the effects as follows:\n\nmammal_long |&gt; \n  ggplot(aes(x = sex, group = Order, y = log(mass))) + \n  geom_jitter(alpha = 0.5, width = 0.1) + \n  geom_smooth(method = \"lm\") +\n  facet_wrap(~Order)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNotice differences in sample size (i.e., number of sampled species per order).\n\n\n10.7.5 Challenge\nRegress log body size on log length, visualize the effect, and interpret the coefficient for length. Then do the same but including sex and its interaction with length as an effect.\nHint: when interpreting the coefficient for length, recall that it has been log-transformed and, in the interaction, everything is measured relative to a baseline (in this example, males relative or the other way around). A per unit increase in the covariate does not imply a per unit increase in length.\nCAUTION: do not fit your models using ggplot. Use lm(). Visualizing the model should be, well, a visualization exercise. It is quite dangerous to draw conclusions when you are not 100 percent sure what model (with or without interaction, by factor, etc.) was fit and then plotted. To avoid these issues, many folks visualize models fitted in lm() using base R.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec10-linear-models-1.html#footnotes",
    "href": "lectures/lec10-linear-models-1.html#footnotes",
    "title": "10  Linear models I",
    "section": "",
    "text": "It is worth noting that what is often used in theory and practice is a matrix formulation of the model we have written down: \\[\\boldsymbol{y} = \\begin{bmatrix}\ny_{1} \\\\\ny_{2} \\\\\n\\vdots \\\\\ny_{n}\n\\end{bmatrix} = \\begin{bmatrix}\n1 & x_{11} & x_{21} & \\cdots & x_{p1} \\\\\n1 & x_{12} & x_{22} & \\cdots & x_{p2} \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n1 & x_{1n} & x_{2n} & \\cdots & x_{pn}\n\\end{bmatrix} \\begin{bmatrix}\n\\beta_{1} \\\\\n\\beta_{2} \\\\\n\\vdots \\\\\n\\beta_{p}\n\\end{bmatrix} + \\begin{bmatrix}\n\\varepsilon_{1} \\\\\n\\varepsilon_{2} \\\\\n\\vdots \\\\\n\\varepsilon_{n}\n\\end{bmatrix} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}.\\] Here, \\(\\boldsymbol{y} = (y_1,\\dots,y_n)'\\) is a vector of measurements for the response, \\(\\boldsymbol{x_i} = (x_{i1},\\dots,x_{in})'\\) is a vector of measurements for the \\(k\\)th predictor, and \\(\\boldsymbol{\\varepsilon} = (\\varepsilon_1,\\dots,\\varepsilon_n)'\\) is a vector of measurement errors. The \\('\\) symbol denotes transposition, the operation where the rows and columns of a vector or matrix are interchanged. In R, the function t() can be used to transpose a vector or matrix.↩︎",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec11-linear-models-2.html",
    "href": "lectures/lec11-linear-models-2.html",
    "title": "11  Linear models II",
    "section": "",
    "text": "11.1 Lesson preamble\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(lme4)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear models II</span>"
    ]
  },
  {
    "objectID": "lectures/lec11-linear-models-2.html#lesson-preamble",
    "href": "lectures/lec11-linear-models-2.html#lesson-preamble",
    "title": "11  Linear models II",
    "section": "",
    "text": "11.1.1 Learning objectives\n\nUnderstand the structure, assumptions, and implementation of GLMs.\nLearn how dependent data can be modeled, and the situations in which one may encounter such data. Understand the structure, assumptions, and implementation of LLMs.\nLearn how to use simulation to inform what data that should be collected, and how.\n\n11.1.2 Lesson outline\n\nGeneralized linear models\n\nStructure and assumptions, including interpretation of the effects\nImplement logistic regression using dataset of Farrell and Davies (2019)\nOther types of GLMs (Poisson, negative binomial, etc.)\n\nPower analysis!\nDealing with dependent data\n\nSplitting the data into groups…\nControlling for phylogeny\nLinear mixed models: structure and assumptions\n\nDifference between fixed, random effects; examples\nImplement mixed models using sexual size dimorphism data",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear models II</span>"
    ]
  },
  {
    "objectID": "lectures/lec11-linear-models-2.html#generalized-linear-models-theory-and-examples",
    "href": "lectures/lec11-linear-models-2.html#generalized-linear-models-theory-and-examples",
    "title": "11  Linear models II",
    "section": "11.2 Generalized linear models: theory and examples",
    "text": "11.2 Generalized linear models: theory and examples\nSo far we have seen how linear models can be used to describe relationships between a response variable and predictors when the data is normally distributed or can be transformed so that this assumption is not violated. What if we were, say, interested in a binary response (e.g., infection status of a host with a particular parasite) and how it changes with a continuous predictor (e.g., age of the host)? In this case, one can use a special kind of linear model called logistic regression to estimate the additive effect of predictor(s) on the binary response. Logistic regression is a special kind of generalized linear model.\n\n11.2.1 GLMs: structure and assumptions\nGeneralized linear models describe how the mean of a response variable changes as a function of the predictors when important assumptions of the linear modeling framework (normality, constant error variance, etc.) are violated. In particular, GLMs allow us to work with data that are not normal, whose range is restricted, or whose variance depends on the mean. The latter might be important if, say, larger values of the response were also more variable.\nA GLM models the transformed mean of a response variable \\(Y_i\\) as a linear combination of the predictors \\(x_1,\\dots,x_p\\). The goal of using a GLM is often to estimate how the predictors (e.g., sex and previous exposure to the disease) affect the response (e.g., infection status). sThe transformation of the response is done using a “link” function \\(g(\\cdot)\\) that is specific to the distribution which used to model the data. Written out, a GLM is of the form\n\\[g(\\mu_i) = \\beta_0 + \\beta_1 x_{1i} + \\cdots \\beta_p x_{pi}.\\]\nThe link functions for the Normal, Gamma, Exponential, Poisson, and Multinomial distributions are known. In general, GLMs apply when the data are modeled using a member of the exponential family. The distributions will have a mean parameter \\(\\mu\\) and, sometimes, a parameter \\(\\tau\\) which characterizes the dispersion around the mean. GLMs are fitted using by maximizing the likelihood function that results from assuming the data arise from a distribution in the exponential family (with a mean and dispersion that depend on the predictors), using using numerical optimization methods.\n\n\n11.2.2 Interpretation of the effects\nNotice that, in a GLM, because the mean of the response been transformed in a particular way, the coefficients must be interpreted carefully. In particular, \\(\\beta_{j}\\) is how a per-unit change in \\(x_{j}\\) increases or decreases the transformed mean \\(g(\\mu_i)\\).\n\n\n11.2.3 Example: logistic regression\nIn a previous class, we estimated the probability of death given infection for the wild boar when infected with viruses in the family Coronaviridae. In your current homework, you have been tasked with extending that model to all host and parasite family combinations. Excitingly, the dataset which have used includes information about the mean evolutionary isolation of all hosts that are infected with a parasite from all other hosts which are known to be infected.\nFarrell and Davies tested if the mean evolutionary isolation affected the probability of death. They used a complex model to control for sampling bias and other confounding aspects of the data. We will ignore those complexities and see if, using a GLM, we can recapitulate their findings. To get started, load the disease_distance.csv dataset.\n\ndisease_distance &lt;- read_csv(\"data/disease_distance.csv\")\n\nRows: 4157 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (7): Country, Disease, Parasite, Host, HostOrder, ParaType, ParaFamily\ndbl (17): Year, Cases, Deaths, Destroyed, Slaughtered, SR, EvoIso, Taxonomic...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndisease_distance %&gt;% \n  mutate(AnyDeaths = case_when(Deaths &gt; 0 ~ 1,\n                               Deaths == 0 ~ 0)) -&gt; DataBern\n\nDataBern |&gt;\n  ggplot(aes(x = EvoIso, y = AnyDeaths)) + \n  geom_point()\n\n\n\n\n\n\n\n\nThese are binary data. We CANNOT use a linear model, as this assumes the data are Normal. In fact, these data are distributed according to a Bernoulli(\\(p\\)) distribution. This is a member of the exponential family and the type of generalized linear model when the data are distributed in this way (i.e., are binary) is called logistic regression.\nThe mean of Bernoulli(\\(p\\)) data is just \\(p\\), and the link function for \\(p\\) is\n\\[ \\text{logit}(p) = \\log\\frac{p}{1-p}.\\]\n\\(\\text{logit}(p)\\) is called the log-odds, which can be thought of as a likelihood the response takes on the value one. In logistic regression, the log-odds ratio is modeled as a linear combination in the predictors:\n\\[ \\text{logit}(p_i) =  \\beta_0 + \\beta_1 x_{1i} + \\cdots + \\beta_p x_{pi}.\\]\nNotice that increasing \\(x_j\\) by one unit results in change \\(\\beta_j\\) to the link-transformed response.This is how the effect sizes are interpreted for GLMs such as this one.\n\nmodel &lt;- glm(AnyDeaths ~ EvoIso, \n             family = \"binomial\", \n             data = DataBern)\nsummary(model)\n\n\nCall:\nglm(formula = AnyDeaths ~ EvoIso, family = \"binomial\", data = DataBern)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0874  -1.2607   0.6072   0.8802   1.4703  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.6665277  0.0805302  -8.277   &lt;2e-16 ***\nEvoIso       0.0141947  0.0007802  18.193   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5257.1  on 4156  degrees of freedom\nResidual deviance: 4890.5  on 4155  degrees of freedom\nAIC: 4894.5\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n11.2.3.1 Challenge\nHow do we interpret the regression coefficient above?\n\n\n11.2.3.2 Challenge\nWhat are the log-odds of death if the evolutionary isolation of hosts is \\(EI = 200\\)? How much does this quantity change if the evolutionary isolation were to increase by 20 million years?\n\n\n11.2.3.3 Visualizing the fitted model\n\nDataBern |&gt;\n  ggplot(aes(x = EvoIso, y = AnyDeaths)) + \n  geom_point() +\n  geom_smooth(method = \"glm\", \n              method.args = list(family = \"binomial\")\n              )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n### base R implementation\n\nEvoIso &lt;- seq(0, 200, 0.1)\npredicted_prob &lt;- predict(model, list(EvoIso = EvoIso), type = \"response\")\n\nplot(DataBern$EvoIso, DataBern$AnyDeaths)\nlines(EvoIso, predicted_prob)\n\n\n\n\n\n\n\n\n\n\n\n11.2.4 Other GLMs\nHere are some common types of response variables and their corresponding distributions:\n\nCount data: the Poisson distribution\nOver-dispersed count data (when the count data is more spread out than “expected”): the negative binomial distribution\nBinary data (two discrete categories): the binomial distribution\nCounts of occurrences of \\(K\\) different types: the multinomial distribution\nTimes between \\(r\\) events: the gamma distribution, which is equivilent to the exponential when \\(r=1\\)\n\nYou will have the opportunity to implement such models in your homework and on the challenge assignment!",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear models II</span>"
    ]
  },
  {
    "objectID": "lectures/lec11-linear-models-2.html#power-analysis",
    "href": "lectures/lec11-linear-models-2.html#power-analysis",
    "title": "11  Linear models II",
    "section": "11.3 Power analysis",
    "text": "11.3 Power analysis\nWhen designing experiments or how best to collect data, it is best to think about the model you will fit and the kind of experiment you need to design in order to have sufficient power to detect an effect of interest. For example, if we thought that age affected the likelihood that a host dies of a disease, then we would likely fit a logistic regression-type model. By simulating data from such a model, we can determine\n\nthe minimum sample size required to reliably estimate an effect of a certain size\nthe minimum sample size required to detect an effect of a certain size at a fixed significance level\nthe maximum effect size that can be detected at a fixed significance level and sample size\n\n\n11.3.1 Example\nBelow is an example of a simulation in which binary disease data (death/no death; 0/1) are simulated hosts of various ages, assuming the the log-odds of disease is a linear function of age. (Notice that we assume that host lifetimes are exponentially distributed with rate parameter \\(\\lambda = 1/10\\) years. This means that hosts, in this simulation, live for an average of 10 years.) We then fit a logistic regression to this data to determine the effect of age on disease risk.\n\ndata_generator &lt;- function(sample_size = 100, effect = 0.1){\n  age &lt;- rexp(n = sample_size, rate = 1/10)\n  linear_predictor &lt;- effect*age\n  prob &lt;- 1/(1+exp(-linear_predictor))\n  \n  disease_status &lt;- c()\n  \n  for (i in 1:length(prob)){\n  disease_status[i] &lt;- rbinom(n = 1, size = 1, prob = prob[i])\n  }\n  \n  return(data.frame(age = age, disease_status = disease_status))\n}\n\ndata &lt;- data_generator()\n\ndata %&gt;% pivot_longer(! age) %&gt;% \n  ggplot(aes(x = age, y = value)) + geom_point() + \n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\")\n              ) + labs(y = \"prob. of disease (i.e., disease status =1)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nmodel &lt;- glm(disease_status~scale(age), family = binomial, data = data)\nsummary(model)\n\n\nCall:\nglm(formula = disease_status ~ scale(age), family = binomial, \n    data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1723  -0.1777   0.6750   0.8387   0.9555  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.1962     0.2541   4.708  2.5e-06 ***\nscale(age)    0.6400     0.3249   1.970   0.0489 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 112.47  on 99  degrees of freedom\nResidual deviance: 107.44  on 98  degrees of freedom\nAIC: 111.44\n\nNumber of Fisher Scoring iterations: 4\n\n\nNext, we will write a function that performs a power analysis. We will use this function determine the sample size that is required so that simulating the age-disease data repeatedly we identify a significant effect of age on disease status (i.e., reject the null hypothesis) at level \\(\\alpha = 0.01\\) at least \\(95\\%\\) of the time.\n\npower_analysis_function &lt;- function(sample_size){\n  \n  sims &lt;- 1000\n  pvalues &lt;- c()\n  for (i in 1:sims){\n  data &lt;- data_generator(sample_size)\n  model &lt;- glm(disease_status~scale(age), family = binomial, data = data)\n  pvalues[i] &lt;- summary(model)$coefficients[2,4]\n  }\n  \n  power_estimate &lt;- length(which(pvalues &lt; 0.01))/length(pvalues)\n  \n  return(power_estimate)\n}\n\nsample_sizes &lt;- seq(150,200,10); power_estimates &lt;- c()\n\nfor (i in 1:length(sample_sizes)){\n  power_estimates[i] &lt;- power_analysis_function(sample_size = sample_sizes[i])\n}\n\nknitr::kable(cbind(n = sample_sizes, power = power_estimates))\n\n\n\n\nn\npower\n\n\n\n\n150\n0.890\n\n\n160\n0.926\n\n\n170\n0.925\n\n\n180\n0.956\n\n\n190\n0.968\n\n\n200\n0.966",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear models II</span>"
    ]
  },
  {
    "objectID": "lectures/lec11-linear-models-2.html#dealing-with-dependent-data",
    "href": "lectures/lec11-linear-models-2.html#dealing-with-dependent-data",
    "title": "11  Linear models II",
    "section": "11.4 Dealing with dependent data!",
    "text": "11.4 Dealing with dependent data!\nTo illustrate how mixed models work and what kinds of questions we can answer using them, we will use the sexual size dimorphism data which we analyzed last class. Recall that we did NOT find a significant effect of sex on the average body size. There was no effect of the interaction between Order and sex on body size. Indeed, this matches what you saw in the homework questions where you had to visualize the data — most of the variation in body size was between orders.\nIn the models we built, we ignored a pretty important fact about the data: species have a common history (i.e., phylogeny). Thus, observations are not independent! This can make drawing inferences from comparative data difficult. We will address how to deal with the non-independence of data (due to a common history of species, replication in blocks, etc.) using three approaches.\nBut, first, let’s download the data we will use in this section!\n\nSSDdata &lt;- read_csv(\"data/SSDinMammals.csv\")\n\nRows: 691 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): Order, Family, Species, Scientific_Name, Comments, Source\ndbl (12): massM, SDmassM, massF, SDmassF, lengthM, SDlengthM, lengthF, SDlen...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nmammal_length &lt;- SSDdata %&gt;%\n  select(c(\"Order\", \"Scientific_Name\", \"lengthM\", \"lengthF\")) %&gt;%\n  pivot_longer(c(lengthM, lengthF), names_to = \"sex\", values_to = \"length\",\n               names_pattern = \"length(.)\")\n\nmammal_mass &lt;- SSDdata %&gt;%\n  select(c(\"Order\", \"Scientific_Name\", \"massM\", \"massF\")) %&gt;%\n  pivot_longer(c(massM, massF), names_to = \"sex\", values_to = \"mass\",\n               names_pattern = \"mass(.)\")\n\nmass_nodup &lt;- mammal_mass %&gt;% \n  group_by(Scientific_Name, sex) %&gt;%\n  distinct(Scientific_Name, sex, .keep_all = TRUE)\n\nlength_nodup &lt;- mammal_length %&gt;% \n  group_by(Scientific_Name, sex) %&gt;%\n  distinct(Scientific_Name, sex, .keep_all = TRUE)\n\nmammal_long &lt;- full_join(mass_nodup, length_nodup, \n                         by = join_by(\"Scientific_Name\", \"sex\", \"Order\")) |&gt;\n  drop_na()\n\nglimpse(mammal_long)\n\nRows: 528\nColumns: 5\nGroups: Scientific_Name, sex [528]\n$ Order           &lt;chr&gt; \"Artiodactyla\", \"Artiodactyla\", \"Artiodactyla\", \"Artio…\n$ Scientific_Name &lt;chr&gt; \"Aepyceros melampus\", \"Aepyceros melampus\", \"Gazella r…\n$ sex             &lt;chr&gt; \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\",…\n$ mass            &lt;dbl&gt; 44180.00, 40480.00, 30700.00, 28100.00, 70280.00, 5206…\n$ length          &lt;dbl&gt; 93.400, 100.000, 89.200, 92.300, 159.450, 149.190, 181…\n\n\n\n11.4.1 Group-by-group analyses\nOne first way we can handle dependent data is to split observations into groups such that, within each group, observations are independent (or approximately so). This is what we did last class when we fit order-specific regression coefficients of sex on body size. We saw that ALL order-specific effects had confidence intervals which overlapped zero!\n\n# run linear model of size on sex for EACH Order\n\nOrder_specific_models &lt;- \n  mammal_long |&gt; \n  group_by(Order) |&gt;\n  do(model = tidy(lm(log(mass) ~ sex, data = .), conf.int = T))\n  \n# get coefficients for each Order\n\nOrder_specific_models |&gt;\n  unnest() |&gt;\n  select(Order, estimate)\n\n# A tibble: 30 × 2\n   Order          estimate\n   &lt;chr&gt;             &lt;dbl&gt;\n 1 Artiodactyla    11.5   \n 2 Artiodactyla     0.0327\n 3 Carnivora        7.68  \n 4 Carnivora        0.320 \n 5 Chiroptera       2.76  \n 6 Chiroptera      -0.0484\n 7 Cingulata       10.4   \n 8 Cingulata        0.0908\n 9 Dasyuromorphia   7.44  \n10 Dasyuromorphia   0.499 \n# ℹ 20 more rows\n\n# visualize effects, CIs, and p values\n\nOrder_specific_models |&gt;\n  unnest() |&gt;\n  subset(term == \"sexM\") |&gt;\n  group_by(Order) |&gt;\n  ggplot(aes(y = Order, x = estimate, \n             xmin = conf.low,\n             xmax = conf.high\n             )\n         ) +\n  geom_pointrange() +\n  geom_vline(xintercept = 0, lty = 2)\n\n\n\n\n\n\n\n\nNo effect of sex on body size, for any of the orders!\n\n11.4.1.1 Challenge\nHow would you adjust the previous plot to show the estimated intercepts AND the effects of sex?\n\n\n11.4.1.2 Challenge\nHow would you retrieve model fits for each Order in base R?\n\n\n11.4.1.3 Challenge\nAdjust the plot above so that it the size of the estimates depend on the number of observations in an Order? Hint: determine the number of observations per Order and then use the merge() function with the Order_specific_models tibble. To adjust the range of point sizes, use scale_size().\n\n\n11.4.1.4 Pros and cons\nThere are several advantages to preforming a group-by-group analysis:\n\nFitting more complex models (e.g., those with random effects) can be difficult. There may be convergence issues, and interpretation of effects and \\(p\\)-values can be tricky.\nThe group-by-group analysis is robust in the face of unbalanced data (i.e., when there are more observations for some groups than others).\nThe conclusions from a group-by-group analysis are conservative.\nThe group-by-group analysis is fairly easy to implement.\n\nAmong the disparages to this approach are the following:\n\nWith fewer samples per group, the analysis may be under-powered.\nSplitting the data into groups means there are more coefficients to estimate, and thus confidence intervals to be estimated and hypothesis tests to be performed. This means there is a greater chance that we obtain a spurious association.*\nIt is difficult to draw conclusions about the variance between groups. In some applications, this is of interest. In others, it is not.\n\n*One solution is to adjust the significant level based on the number of tests conducted. If \\(k\\) hypotheses are tested, an common adjustment is to set \\(\\alpha = 0.05/k\\).\n\n\n\n11.4.2 Controlling for phylogeny\nA common reason data are dependent in biology is that species share a common history of descent with modification. When we assume that the observations are independent, we make implicit assumptions about the degree to which the characters at the tips of a phylogeny have underwent independent evolution. Sometimes, when species are diverged by many millions of years and traits evolve quickly, it is reasonable to ignore the phylogenetic constraints on the data. In other cases, it is essential to consider the role of phylogeny.\nSeveral methods can control for phylogeny (if known). In fact, such methods can use information in the branching pattern of species to draw inferences about the evolution of ecologically-important traits (such as body and brain size, dispersal rate, etc.). We will not discuss how to implement phylogenetic comparative methods, but it is good to know they exist and how, at a surface level, they work.\nIntuition for how PCMs work is easiest to grasp when we consider a tree with \\(n\\) species at the tips. If we know the pairwise divergence times for all species, then we can transform the data so that observations are independent by looking at all differences of traits. Not all differences are equally informative; if species have diverged a long time ago, there has been more time for differences to build up. PCMs account for this by specifying how the distribution of trait differences between species \\(i\\) and \\(j\\) depends on the time that has elapsed since these species diverged, In particular, the larger the divergence time, the greater the variance in \\(Y_i-Y_j\\), the difference in trait values between species \\(i\\) and \\(j\\). A simple way to do this is to write \\(\\sigma_{ij} = \\sigma^2/T_{ij}\\).1\n\n\n11.4.3 Random effects!\nAnother way one can account for dependent data is by including random effects. Random effects are often used to control for the fact that observations are clustered (e.g., trait data for species belonging to a higher taxonomic unit, measurements of plant biomass from plots of land). Random effects are used to account for dependencies in hierarchical, longitudinal, and other dependent data.\n\n11.4.3.1 Structure and assumptions\nA common way models with random effects are formulated is as follows:\n\\[Y_{ij} \\sim \\text{Normal}(\\beta_0 + \\beta_1 x_{1ij} + \\cdots + \\beta_p x_{pij} + b_{0i} + b_{1i} z_{1ij} + \\cdots + b_{qi} z_{qij}, \\sigma_{ij}^2),\\]\nwhere \\(Y_{ij}\\) is the value of the response for the \\(j\\)th in \\(n_i\\) observations in the \\(i\\)th of \\(m\\) clusters, and\n\\[b_{0i},\\dots,b_{qi} \\sim \\text{Normal}(0,\\tau_i^2).\\]\nThis gives rise to a distribution for \\(Y\\) which depends on the values of the random effects \\(b_0,\\dots,b_q\\). Under the hood, methods that fit models of this form numerically maximize a version of the likelihood function that results from these assumptions.\n\n\n11.4.3.2 Challenge\nIn each of the following examples, which effects might be reasonably treated as fixed vs. random? Justify your answer.\n\nSuppose we are working with rodents that have been infected with an evolved strain of the parasite that causes malaria. Some rodents have been treated with a prospective vaccine and others sham-vaccinated. We are interested if a proxy for virulence (e.g., density of infected red blood cells) depends on vaccination status.\nSuppose we conduct the same experiment, except rodents are caged are sets of four.\nSuppose we measure the time between sightings of a raccoon in a Toronto neighborhood using six camera traps (strategically placed in the neighborhood). We do this for year, and want to ask if mean daily temperature predicts the frequency of raccoon occurrence.\n\nFor more on the difference between fixed and random effects, check out the following resources\n\nthis Cross Validated post\nthis Dynamic Ecology post\nthis book chapter\n\n\n\n11.4.3.3 Using lme4 to fit random and mixed effect models\nWe will start by fitting a model of log body size on log length where the intercept is random depending on the Order. Based on a quick visualization of the data, such a model may be appropriate.\n\nggplot(data = mammal_long, aes(y = log(mass), \n                               x = log(length)\n                               )\n       ) + \n    geom_point(aes(color = Order), size = 3) -&gt; p\n\np \n\n\n\n\n\n\n\n\nTo fit regress the response on a set of fixed effects with random intercepts that depends the values assumed by a random effect x, we call lmer and write a linear model with (1|x).\n\n## random intercept for Order\n\nmodel &lt;- lmer(formula = log(mass) ~ log(length) + (1|Order), data = mammal_long)\nsummary(model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log(mass) ~ log(length) + (1 | Order)\n   Data: mammal_long\n\nREML criterion at convergence: 1473\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4725 -0.6209 -0.0859  0.5464  4.5351 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Order    (Intercept) 4.256    2.0630  \n Residual             0.847    0.9203  \nNumber of obs: 528, groups:  Order, 15\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  2.72062    0.59488   4.573\nlog(length)  1.12363    0.07116  15.789\n\nCorrelation of Fixed Effects:\n            (Intr)\nlog(length) -0.412\n\nfixef(model)\n\n(Intercept) log(length) \n   2.720625    1.123628 \n\nranef(model)\n\n$Order\n                (Intercept)\nArtiodactyla      2.9590568\nCarnivora        -0.4243736\nChiroptera       -1.7590786\nCingulata         1.9715210\nDasyuromorphia    2.1671133\nDidelphimorphia  -1.4045738\nDiprotodontia     1.2066571\nEulipotyphla     -2.4045055\nLagomorpha        2.1519392\nMacroscelidea    -4.0757058\nPeramelemorphia   1.5377333\nPilosa           -0.1832817\nPrimates          0.8008374\nRodentia         -1.5015008\nScandentia       -1.0418383\n\nwith conditional variances for \"Order\" \n\n\nFrom top to bottom, the output is telling us that the model was fit using a method called REML (restricted maximum likelihood). It returns information about the residuals, random effects, and fixed effects. The output also tells us about the estimated variance for the random effects in the model. Here, the variance associated with Order is 4.256. The variance explained by Order is 4.256/(4.256+0.847) after controlling for the fixed effects. Note the denominator here is the total variance, including from the residuals. As usual, we also have information about the fixed effects.\nTo visualize the model, we can predict the overall and Order-specific relationship of log mass on log length for all of the Orders represented in the data.\n\nmammal_long |&gt; ungroup() |&gt; select(mass, length, Order) |&gt; \n  mutate(fit.m = predict(model, re.form = NA), # does not include random effects\n         fit.c = predict(model, re.form = NULL) # includes random effects\n         ) -&gt;\n  predicted_values\n\npredicted_values |&gt;\n  ggplot(aes(x = log(length), y = log(mass), color = Order)) +\n  geom_point(size = 3) +\n  geom_line(inherit.aes = F, aes(x = log(length), y = fit.c, color = Order), size = 1) +\n  geom_line(inherit.aes = F, aes(x = log(length), y = fit.m), color = \"black\", size = 2)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nThe thick black line corresponds to the fitted values associated with the fixed-effect component of the model. The colored lines correspond to the fitted values estimated for each Order.\nPerhaps a model with Order-specific random intercepts AND slopes would be better. We fit this model using the code below. The key difference in syntax is that we write (1+fixed effect|random effect) to indicate that the random effect has an effect on both the intercept and slope of the response on the fixed effect.\n\n## random intercept and slope for Order\n\nmodel2 &lt;- lmer(formula = log(mass) ~ log(length) + (1+log(length)|Order), data = mammal_long)\nsummary(model2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log(mass) ~ log(length) + (1 + log(length) | Order)\n   Data: mammal_long\n\nREML criterion at convergence: 1362.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.8608 -0.5198 -0.0610  0.4402  5.7352 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n Order    (Intercept) 19.7590  4.4451        \n          log(length)  1.3434  1.1591   -0.96\n Residual              0.6581  0.8112        \nNumber of obs: 528, groups:  Order, 15\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   3.6222     1.2800   2.830\nlog(length)   0.9398     0.3392   2.771\n\nCorrelation of Fixed Effects:\n            (Intr)\nlog(length) -0.954\n\nfixef(model2)\n\n(Intercept) log(length) \n  3.6222206   0.9397814 \n\nranef(model2)\n\n$Order\n                (Intercept) log(length)\nArtiodactyla      -5.086433   1.5697087\nCarnivora          3.517003  -0.7993673\nChiroptera        -3.227380   0.5425373\nCingulata         -2.437867   0.9014589\nDasyuromorphia     3.494122  -0.7550221\nDidelphimorphia   -2.096031   0.1145380\nDiprotodontia      3.737120  -0.7974711\nEulipotyphla      -6.677306   1.7068614\nLagomorpha         3.168664  -0.5891273\nMacroscelidea      4.576435  -1.8088720\nPeramelemorphia    2.085998  -0.4541549\nPilosa             4.823089  -0.9632143\nPrimates           1.578103  -0.2317300\nRodentia          -4.478792   0.9991504\nScandentia        -2.976726   0.5647040\n\nwith conditional variances for \"Order\" \n\n\n\nmammal_long |&gt; ungroup() |&gt; select(mass, length, Order) |&gt; \n  mutate(fit.m = predict(model2, re.form = NA),\n         fit.c = predict(model2, re.form = NULL)\n         ) -&gt;\n  predicted_values\n\npredicted_values |&gt;\n  ggplot(aes(x = log(length), y = log(mass), color = Order)) +\n  geom_point(size = 3) +\n  geom_line(inherit.aes = F, aes(x = log(length), y = fit.c, color = Order), size = 1) +\n  geom_line(inherit.aes = F, aes(x = log(length), y = fit.m), color = \"black\", size = 2)\n\n\n\n\n\n\n\n\nWhy does this look like … a little bit of a mess? One explanation is that, as stated above, mixed models do not work well when datasets are unbalanced. Indeed, the number of observations in the different Orders are quite variable.\n\n\n11.4.3.4 Challenge\nRegress log body size on sex with Order as a random effect that affects the intercept AND slope of the sex-size relationship.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear models II</span>"
    ]
  },
  {
    "objectID": "lectures/lec11-linear-models-2.html#footnotes",
    "href": "lectures/lec11-linear-models-2.html#footnotes",
    "title": "11  Linear models II",
    "section": "",
    "text": "Note: this gives rise to a likelihood function that is functionally VERY similar to the one for the linear model with constant error variance.↩︎",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear models II</span>"
    ]
  },
  {
    "objectID": "lectures/lec12-model-selection.html",
    "href": "lectures/lec12-model-selection.html",
    "title": "12  Model selection",
    "section": "",
    "text": "12.1 Lesson preamble",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "lectures/lec12-model-selection.html#lesson-preamble",
    "href": "lectures/lec12-model-selection.html#lesson-preamble",
    "title": "12  Model selection",
    "section": "",
    "text": "12.1.1 Learning objectives\n\nUnderstand the difference between likelihood estimation/inference and model selection\nDevelop familiarity with common model selection approaches\nUnderstand intuition for and the use of common information theoretic model selection criteria\nPerform model selection on LMs, GLMs, LMMs",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "lectures/lec12-model-selection.html#what-is-model-selection",
    "href": "lectures/lec12-model-selection.html#what-is-model-selection",
    "title": "12  Model selection",
    "section": "12.2 What is model selection?",
    "text": "12.2 What is model selection?\nSo far, we have covered central problems in statistics: determining what processes and parameters gave rise to data (estimation), and quantifying uncertainty in those estimates using confidence intervals (inference). As we have seen, however, to any confidence interval there is an associated hypothesis test. Inference and hypothesis testing, then, involve deciding if a particular set of parameter values could have plausibly given rise to the data.\nIt should come as no surprise, since both estimation and inference involve decision making, that in statistics we often interested in deciding if the models we build are appropriate descriptions of how the world works (given the data we have and use to fit those models), or what among a set of candidate models is the “best”. This practice is called model selection and is the focus of this lecture.\n\n12.2.1 Examples\n\nHypothesis testing is a kind of model selection. For example, for data \\(x_1,\\dots,x_n \\sim f(x|\\theta)\\), testing \\(H_0 \\colon \\theta = \\theta_0\\) vs \\(H_1 \\colon \\theta \\neq \\theta_0\\) is equivalent to choosing between models \\(\\mathcal{M}_0 \\colon f(x|\\theta)\\) and \\(\\mathcal{M}_1 \\colon f(x|\\theta_0)\\).\nSuppose we regress \\(y\\) on the 1st, 2nd, \\(\\dots\\), \\(p\\)th powers of a covariate \\(x\\):\n\n\\[y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_p x^p.\\]\nThis gives rise to a sequence of models \\(\\mathcal{M}_1,\\mathcal{M}_2,\\dots,\\mathcal{M}_p\\) of the data generative process. Which model is the best description of the data? Although the full model is more flexible in that it has more parameters than, say, the model in which all second and higher order terms are \\(=0\\), it is more prone to overfitting. Choosing between a sequence of nested linear models like this is a classic model selection problem.\n\nSuppose we would like to model the relationship between expression (i.e., the number of transcripts produced) of each coding gene in the human genome and, say, height. If there are \\(p\\) genes for which we have measurements and only \\(n \\ll p\\) observations, it is not possible to fit\n\n\\[y_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_p x_{pi}.\\]\n(The reason is that there are infinitely many solutions to the likelihood maximization problem.) In this case, we might want to select a subset of the covariates which best explain the available data. Of \\(x_1,\\dots,x_p\\), which are most informative? This too is a kind of model selection problem.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "lectures/lec12-model-selection.html#stepwise-regressionand-why-you-should-not-use-it.",
    "href": "lectures/lec12-model-selection.html#stepwise-regressionand-why-you-should-not-use-it.",
    "title": "12  Model selection",
    "section": "12.3 Stepwise regression…and why you should not use it.",
    "text": "12.3 Stepwise regression…and why you should not use it.\nSuppose you want to model the effects of multiple predictors and their interactions on a particular response variable of interest. It may be tempting, then, to test all possible combinations of predictors and see which set fits the data the best. Stepwise selection involves iteratively adding and removing predictors according to some criterion (e.g., variance explained, AIC). This can be down by adding predictors to a null model or by removing predictors from one which is saturated (i.e., where all predictors and their interactions are included.)\nIn general, we urge you to AVOID stepwise selection and to be critical of analyses which use it. No statistics can be a substitute for the inclusion of predictors which are biologically meaningful. Stupid models can fit the data well, even when accounting for the number of parameters, and good models can fit the data (relatively) poorly. When choosing between models, careful consideration of the what predictors are informative and why is key.\nFor more on the problems with automated stepwise selection methods, see here.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "lectures/lec12-model-selection.html#the-akaike-information-criterion",
    "href": "lectures/lec12-model-selection.html#the-akaike-information-criterion",
    "title": "12  Model selection",
    "section": "12.4 The Akaike information criterion",
    "text": "12.4 The Akaike information criterion\nRather than (mindlessly) adding and removing predictors, it is important to build models that are based in evidence from good theory and the literature. We can then weigh how much the data supports each model using the Aikake information criterion (AIC)\nSuppose we have data \\(y_1,\\dots,y_n\\) that are drawn from a distribution \\(p\\) and a set of candidate models\n\\[\\mathcal{M}_j = \\{p_j(y|\\theta_j)\\}.\\]\nIt is possible under this setup to find the maximum likelihood estimators for each of the candidate models; it is, however, difficult to compare these models in that the parameters underlying each model might not match (i.e., the models may not be nested). The AIC overcomes this issue, and despite having a lengthy and complicated derivation, is a metric which balances two things: 1) the goodness-of-fit of the model and 2) the number of parameters fitted. There are other methods for model selection that are similar to AIC (such as BIC, \\(C_p\\)) that follow similar principles but use different penalties.\nThe intuition and formula for the AIC is as follows. If \\(\\hat{\\theta}_{j,\\text{MLE}}\\) is the MLE for \\(\\theta\\) under model \\(\\mathcal{M}_j\\), then we can measure the distance between the ground truth (i.e., distribution \\(p\\) of the data) and fitted model \\(\\hat{p}_j = p_j(y|\\hat{\\theta}_{j,\\text{MLE}})\\) using a metric called the Kullback-Leibler divergence:\n\\[D_{KL}(p, \\hat{p}_j) = \\int p(y) \\log p(y) \\text{d} y - \\int p(y) \\log \\hat{p_j}(y) \\text{d} y. \\]\nMinimizing the Kullback-Leibler divergence (distance) between the ground truth and density \\(j\\) is a principled way to preform model selection, and forms the basis for the AIC. Note that minimizing only involves the second integral, and we can estimate the integral with an average\n\\[\\frac{1}{n} \\log L_j(\\hat{\\theta}_{j,\\text{MLE}}) = \\frac{1}{n} \\sum_{i=1}^n \\log p_j(y_i|\\hat{\\theta}_{j,\\text{MLE}})\\]\nImportantly, AIC corrects for the fact this is an unbiased estimator of the divergence by adding \\(d_j/n\\), where \\(d_j = \\text{dim}(\\mathcal{M}_j)\\). This term is what penalizes models with a large number of parameters. So,\n\\[\\text{AIC} = - 2 \\log L_j(\\hat{\\theta}_{j,\\text{MLE}}) + 2 d_j.\\]\nNotice we have multiplied the preceding quantities by \\(-2n\\) to get the above expression; this does not change anything, and is largely for historical reasons. Based on the AIC expression, it is clear 1) the higher the likelihood, the lower the AIC; 2) introducing more parameters into the model without changing the likelihood results in a greater value for the AIC. The balance between goodness-of-fit (likelihood) and the number of parameters (the potential to overfit) is what AIC tries to optimize in choosing between candidate models. As we have shown here, that balance is struck by minimizing the distance between candidate models and the ground truth, while correcting for the bias introduced by having models with different numbers of parameters.\n\n12.4.1 \\(\\text{AIC}_c\\) for small sample sizes\nIt is sometimes convenient to, when sample sizes are small (\\(n &lt; 40\\) is a commonly used rule-of-thumb) use the following metric to choose between candidate models:\n\\[\\text{AIC}_c = \\text{AIC} + \\frac{2d_j(d_j+1)}{n-d_j-1}\\]",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "lectures/lec12-model-selection.html#example-with-a-linear-model",
    "href": "lectures/lec12-model-selection.html#example-with-a-linear-model",
    "title": "12  Model selection",
    "section": "12.5 Example with a linear model",
    "text": "12.5 Example with a linear model\nIn 2016, Afkhami and Stinchcombe investigated the effects of multiple microbial mutualists on the performance of the model legume, Medicago truncatula. These microbial mutualists offer different rewards to their plant host; rhizobia bacteria provide fixed nitrogen; mycorrhizal fungi provide phosphorus. Plants were inoculated with either both microbial partners, one microbial partner, or none. Measures of plant performance such as aboveground and belowground biomass, and mutualist performance (nodule count and biomass) were collected.\n\nmedicago&lt;-read.csv(\"data/medicago.csv\")\n\n# convert to factor\ncols&lt;-c(\"Block\",\"Plant_ID\",\"Myco_fungi\",\"Rhizobia\")\nmedicago[cols]&lt;-lapply(medicago[cols],factor)\n\nstr(medicago)\n\n'data.frame':   120 obs. of  8 variables:\n $ Block       : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Plant_ID    : Factor w/ 120 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ Myco_fungi  : Factor w/ 2 levels \"-\",\"+\": 2 1 1 2 1 1 1 2 2 1 ...\n $ Rhizobia    : Factor w/ 2 levels \"-\",\"+\": 2 1 2 2 2 2 1 1 1 1 ...\n $ Shoot_mass  : num  18 14.5 23 25.3 17.8 ...\n $ Root_mass   : num  NA NA NA 28.1 NA ...\n $ Nodule_count: int  NA NA 40 39 NA 53 0 0 NA NA ...\n $ Nodule_mass : num  NA NA 0.463 0.726 0.33 NA NA NA NA NA ...\n\n\nBoth rhizobia and mycorrhizal fungi interact with the root structures of their plant host. Rhizobia are housed in specialized root structures called nodules while mycorrhizal fungi can make filamentous branches, hyphae, on and inside plant roots. Therefore, we could be interested in testing the effects of both microbial partners on belowground (root) biomass. Let’s plot the data!\n\nggplot(aes(x = Myco_fungi, y = Root_mass, colour = Rhizobia), data = medicago) +\n  geom_boxplot()\n\nWarning: Removed 67 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nIt looks like plants inoculated with rhizobia and fungi had higher root biomass. Let’s look at the model.\n\n# main effects + interaction\nroot1 &lt;- lm(Root_mass ~ Myco_fungi + Rhizobia + Myco_fungi:Rhizobia, data = medicago)\n# or simplify to Myco_fungi*Rhizobia\n\nsummary(root1)\n\n\nCall:\nlm(formula = Root_mass ~ Myco_fungi + Rhizobia + Myco_fungi:Rhizobia, \n    data = medicago)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.032  -8.557  -1.709   4.909  36.523 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            31.9963     3.5188   9.093 4.29e-12 ***\nMyco_fungi+            -2.8121     5.0711  -0.555    0.582    \nRhizobia+              -0.4129     5.1795  -0.080    0.937    \nMyco_fungi+:Rhizobia+   5.0162     7.2487   0.692    0.492    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.17 on 49 degrees of freedom\n  (67 observations deleted due to missingness)\nMultiple R-squared:  0.01675,   Adjusted R-squared:  -0.04345 \nF-statistic: 0.2782 on 3 and 49 DF,  p-value: 0.8409\n\n# no significant interaction! perhaps a model with just the main effects?\n\n# main effects only\nroot2 &lt;- lm(Root_mass ~ Myco_fungi + Rhizobia, data = medicago)\n\nsummary(root2)\n\n\nCall:\nlm(formula = Root_mass ~ Myco_fungi + Rhizobia, data = medicago)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.305  -7.941  -2.612   5.790  37.705 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  30.8142     3.0603  10.069 1.27e-13 ***\nMyco_fungi+  -0.3571     3.6046  -0.099    0.921    \nRhizobia+     2.1483     3.6046   0.596    0.554    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.1 on 50 degrees of freedom\n  (67 observations deleted due to missingness)\nMultiple R-squared:  0.007138,  Adjusted R-squared:  -0.03258 \nF-statistic: 0.1797 on 2 and 50 DF,  p-value: 0.836\n\n\nUnfortunately, the models do not show significant differences between inoculation treatments. But let’s compare these models using AIC.\n\nAICc(root1, root2)\n\n      df     AICc\nroot1  5 430.7550\nroot2  4 428.8273\n\n\nBased on the above, the model without the interaction term is a better fit to the data (lower AIC score). However, the scores for both models are really close! How do we decide which model(s) to interpret? Statisticians have thought about this problem and it turns out that models with delta AIC (or other criterion) less than 2 are considered to be just as good as the top model and thus we shouldn’t just discount them.\nPlant investment to the roots is important for microbial mutualist association, but what about aboveground biomass? Could this be impacted by microbial associations?\n\nggplot(aes(x = Myco_fungi, y = Shoot_mass, colour = Rhizobia), data = medicago)+\n  geom_boxplot()\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n# main effects + interaction\nshoot1 &lt;- lm(Shoot_mass ~ Myco_fungi + Rhizobia + Myco_fungi:Rhizobia, data = medicago)\n\nsummary(shoot1)\n\n\nCall:\nlm(formula = Shoot_mass ~ Myco_fungi + Rhizobia + Myco_fungi:Rhizobia, \n    data = medicago)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.9064  -4.9520   0.5606   3.9902  23.7974 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             20.114      1.289  15.610   &lt;2e-16 ***\nMyco_fungi+             -0.252      1.838  -0.137   0.8912    \nRhizobia+               -1.157      1.822  -0.635   0.5268    \nMyco_fungi+:Rhizobia+    5.807      2.588   2.244   0.0268 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.058 on 115 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.08824,   Adjusted R-squared:  0.06446 \nF-statistic:  3.71 on 3 and 115 DF,  p-value: 0.01366\n\n\nAlthough there are similar trends to the data as with root biomass, it looks like shoot biomass is significantly different between treatments.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "lectures/lec12-model-selection.html#another-example",
    "href": "lectures/lec12-model-selection.html#another-example",
    "title": "12  Model selection",
    "section": "12.6 Another example",
    "text": "12.6 Another example\nNote: this example comes from Dolph Schluter’s “Model Selection” R workshop.\nSavage et al. (2004) investigated competing claims for the value of the scaling parameter, \\(\\beta\\), in mammalian basal metabolic rate (BMR):\n\\[\\text{BMR} = \\alpha M^\\beta,\\]\nwhere \\(\\text{BMR}\\) is basal metabolic rate, \\(M\\) is body mass, and \\(\\alpha\\) is a constant.\nOn a log scale, this can be written as:\n\\[\\log \\text{BMR} = \\log(\\alpha) + \\beta\\log(M),\\]\nwhere \\(\\beta\\) is now a slope parameter of a linear model. Theory based on optimization of hydrodynamic flows through the circulation system predicts that the exponent should be \\(\\beta = \\frac{3}{4}\\) but we would expect \\(\\beta = \\frac{2}{3}\\) if metabolic rate scales with heat dissipation and therefore body surface area. These alternative scaling relationships represent distinct hypotheses. We will use them as candidate models and apply model selection procedures to compare their fits to data.\nSavage et al. compiled data from 626 species of mammals. To simplify, and reduce possible effects of non-independence of species data points, they took the average of \\(\\log(\\text{BMR})\\) among species in small intervals of \\(\\log(M)\\). Body mass is in grams, whereas basal metabolic rate is in watts.\n\n# download dataset\nbmr &lt;- read.csv(url(\"https://www.zoology.ubc.ca/~bio501/R/data/bmr.csv\"),\n                stringsAsFactors = FALSE)\n\nbmr$logmass&lt;-log(bmr$mass.g)\nbmr$logbmr&lt;-log(bmr$bmr.w)\nhead(bmr)\n\n  mass.g bmr.w   logmass    logbmr\n1    2.4 0.063 0.8754687 -2.764621\n2    3.7 0.027 1.3083328 -3.611918\n3    4.6 0.067 1.5260563 -2.703063\n4    5.6 0.108 1.7227666 -2.225624\n5    7.3 0.103 1.9878743 -2.273026\n6    8.9 0.102 2.1860513 -2.282782\n\n\n\n# plot the data on a log scale\nggplot(aes(x = logmass, y = logbmr), data = bmr) +\n  geom_point()\n\n\n\n\n\n\n\n\nLet’s fit a linear model on this data!\n\nmod &lt;- lm(logbmr ~ logmass, data = bmr)\nsummary(mod) # notice the estimate of the slope\n\n\nCall:\nlm(formula = logbmr ~ logmass, data = bmr)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.18771 -0.13741  0.01169  0.17836  0.62592 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -4.00329    0.09858  -40.61   &lt;2e-16 ***\nlogmass      0.73654    0.01261   58.42   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3243 on 50 degrees of freedom\nMultiple R-squared:  0.9856,    Adjusted R-squared:  0.9853 \nF-statistic:  3413 on 1 and 50 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s fit the two candidate models, where \\[\\beta = \\frac{3}{4}\\] or \\[\\beta = \\frac{2}{3}\\]\n\n# fit models\nmod1 &lt;- lm(logbmr ~ 1+offset((3/4)*logmass), data = bmr)\nmod2 &lt;- lm(logbmr ~ 1+offset((2/3)*logmass), data = bmr)\n\n# plot\nggplot(aes(x = logmass,y = logbmr), data = bmr)+\n  geom_point()+\n  geom_abline(intercept = coef(mod1), slope = 3/4, colour = \"blue\")+\n  geom_abline(intercept = coef(mod2), slope = 2/3, colour = \"red\")\n\n\n\n\n\n\n\n\nNow let’s compare the AIC scores of the two models.\n\nAICc(mod1, mod2)\n\n     df     AICc\nmod1  2 33.82759\nmod2  2 57.56350\n\n\nBy this criterion, which model is the best?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "lectures/lec12-model-selection.html#example-with-a-glm",
    "href": "lectures/lec12-model-selection.html#example-with-a-glm",
    "title": "12  Model selection",
    "section": "12.7 Example with a GLM",
    "text": "12.7 Example with a GLM\nLast lecture, we tried to see if we could recapture the findings of Farrell and Davies (2019) using a simple GLM to see if the mean evolutionary isolation affected the probability of death. Recall that the data is distributed according to a Bernoulli distribution, so we needed to conduct a logistic regression.\n\ndisease_distance &lt;- read_csv(\"data/disease_distance.csv\")\n\nRows: 4157 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (7): Country, Disease, Parasite, Host, HostOrder, ParaType, ParaFamily\ndbl (17): Year, Cases, Deaths, Destroyed, Slaughtered, SR, EvoIso, Taxonomic...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nDataBern&lt;-disease_distance %&gt;% \n  mutate(AnyDeaths = case_when(Deaths &gt; 0 ~ 1,\n                               Deaths == 0 ~ 0))\n\nmodel &lt;- glm(AnyDeaths ~ EvoIso, \n             family = \"binomial\", \n             data = DataBern)\n\nsummary(model)\n\n\nCall:\nglm(formula = AnyDeaths ~ EvoIso, family = \"binomial\", data = DataBern)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0874  -1.2607   0.6072   0.8802   1.4703  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.6665277  0.0805302  -8.277   &lt;2e-16 ***\nEvoIso       0.0141947  0.0007802  18.193   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5257.1  on 4156  degrees of freedom\nResidual deviance: 4890.5  on 4155  degrees of freedom\nAIC: 4894.5\n\nNumber of Fisher Scoring iterations: 4\n\n\nSuppose we want to see if other factors affect the probability of death, in addition to or outside of evolutionary isolation. What factors could we test?\nPerhaps the type of parasite also affects the probability of death. Let’s compare the models.\n\nmodel2 &lt;- glm(AnyDeaths ~ EvoIso + ParaType, \n              family = \"binomial\", \n              data = DataBern)\n\nAICc(model,model2)\n\n       df     AICc\nmodel   2 4894.539\nmodel2  7 4589.116\n\n\nNote that the we could continue to add terms to the model that would make it a better fit to the data. But this veers towards p-hacking territory! This section is to show you that AIC works the same for GLM. In practice, you should first build your models based on good theory and then compare them.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "lectures/lec12-model-selection.html#additional-reading",
    "href": "lectures/lec12-model-selection.html#additional-reading",
    "title": "12  Model selection",
    "section": "12.8 Additional reading",
    "text": "12.8 Additional reading\n\nJohnson & Omland 2004. Model selection in ecology and evolution. Trends in Ecology and Evolution.\nBurnham & Anderson 2002. Model Selection and Multimodel Inference (2nd ed.).\nZuur et al. 2009. Mixed Effects Models and Extensions in Ecology with R.\nTredennick et al. 2021. A practical guide to selecting models for exploration, inference, and prediction in ecology. Ecology.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "lectures/lec13-intro-to-multivariate-stats.html",
    "href": "lectures/lec13-intro-to-multivariate-stats.html",
    "title": "13  Multivariate statistics",
    "section": "",
    "text": "13.1 Lesson preamble:",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multivariate statistics</span>"
    ]
  },
  {
    "objectID": "lectures/lec13-intro-to-multivariate-stats.html#lesson-preamble",
    "href": "lectures/lec13-intro-to-multivariate-stats.html#lesson-preamble",
    "title": "13  Multivariate statistics",
    "section": "",
    "text": "13.1.1 Lesson objectives\n\nBe able to picture data as a multidimensional cloud\nGain familiarity with the terms, “trait space”, “morphospace”, “pairs plot”, “PC axes”, “cumulative variation explained”, “biplot”, and “scree plot”\nVerbally describe the process behind a PCA\nInterpret the output from a PCA\nIdentify when the output from a PCA is robust… or not\n\n13.1.2 Lesson outline\n\nIntroduce Pigot et al. 2020 data and motivate class with a scientific question\nExplore trait data in 2D and 3D while projecting example species into trait space\nIntroduce PCA\nRun PCA\nInterpret PCA\nChallenge questions",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multivariate statistics</span>"
    ]
  },
  {
    "objectID": "lectures/lec13-intro-to-multivariate-stats.html#background-information",
    "href": "lectures/lec13-intro-to-multivariate-stats.html#background-information",
    "title": "13  Multivariate statistics",
    "section": "13.2 Background information",
    "text": "13.2 Background information\nMost things in the natural world are inter-related and correlated, where many trends and observations tend to change in a predictable way together.\nWe use multivariate techniques to reduce complex situations into more manageable ones and to deal with the fact that patterns are correlated with other patterns, which are in turn driven by a number of interacting ecological processes.\nMultivariate statistics is a family of statistical tools used to to handle situations where the predictor and/or the response variables come in sets, allowing us to distill through some of that correlated (and sometimes redundant) information and to analyze these variables simultaneously. These tools are also useful for identifying substructure or groupings of your data.\nCommon biological questions that multivariate statistics can be used to answer:\n\nDo sites with similar disturbance regimes (history of fire, nutrient fertilization, soil chemicals) host similar biological communities? To answer this question, one could use a PCA, NMDS, PCoA).\nWhat is the effect of a set of environmental variables (temperature, salinity, current strength) on the composition of fish communities? To answer this question, one could use a RDA).\nDo communities in different habitats–with different structural complexity and environments– have different functional representation in trait space? To answer this question, one could use a PCA, NMDS, PCoA).",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multivariate statistics</span>"
    ]
  },
  {
    "objectID": "lectures/lec13-intro-to-multivariate-stats.html#bird-morphology-data",
    "href": "lectures/lec13-intro-to-multivariate-stats.html#bird-morphology-data",
    "title": "13  Multivariate statistics",
    "section": "13.3 Bird morphology data",
    "text": "13.3 Bird morphology data\nToday we will be introducing the world of multivariate statistics by working with a published data set from (Pigot et al. 2020). In this paper, the authors are asking if morphology is linked to ecological function in birds. We might expect this to be true if species have evolved traits (in this case morphological traits) to optimize their fitness in a specific biotic and abiotic environment. In this way, traits can be used to link species to theories of community assembly, niche partitioning, and local adaptation.\nThus we might expect that birds that have occupy specialized niches–say the pollinators or the deep sea divers–might have also evolved specialized morphology in order to exploit these niches. If this is true, we might also see that phylogenetically unrelated species, that occupy similar niches, might have evolved similar traits via convergent evolution.\nToday, we will be simplying the Pigot et al. (2020) analysis and to ask two questions among birds of the neotropics:\n\nDo birds that are more closely related (eg. within order) tend to have similar trait values ie occupy the same regions of morphospace (indication of phylogenetic inertia)?\nDo birds that occupy similar trophic niches have similar traits, as we might expect from trait-based theories of ecology and convergent evolution? While we will not be performing phylogenetic analyses in class, and so can’t explicitly test for the signal of convergent evolution, Pigot et al. (2020) do tackle this in the published paper.\n\nWe will be using the morphology data from Pigot et al. (2020), who measured 9 traits (8 morphology + mass) on museum specimens. These traits are previosly known to be important determinants of bird ecomorpholgy.\n\n\n\nExtended Figure 1, Pigot et al. 2020. a, Resident frugivorous tropical passerine (fiery-capped manakin, Machaeropterus pyrocephalus) showing four beak measurements: (1) beak length measured from tip to skull along the culmen; (2) beak length measured from the tip to the anterior edge of the nares; (3) beak depth; (4) beak width. b, Insectivorous migratory temperate-zone passerine (redwing, Turdus iliacus) showing five body measurements: (5) tarsus length; (6) wing length from carpal joint to wingtip; (7) secondary length from carpal joint to tip of the outermost secondary; (8) Kipp’s distance, calculated as wing length minus first-secondary length; (9) tail length. Illustration by Richard Johnson.\n\n\n\n# read in bird data\nbirds &lt;- read_csv(\"data/Pigot2020.csv\")\n\nRows: 9963 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (9): Binomial, Realm, TrophicLevel, TrophicNiche, ForagingNiche, Family...\ndbl (14): MinLatitude, MaxLatitude, CentroidLatitude, CentroidLongitude, Ran...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# subset to neotropics so we have a more tractable dataset for class\nbirds &lt;- subset(birds, Realm==\"Neotropic\")\n\n\n# explore and look at columns\nhead(birds)\n\n# A tibble: 6 × 23\n  Binomial    Realm TrophicLevel TrophicNiche ForagingNiche Family Order Habitat\n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  \n1 Abeillia_a… Neot… Herbivore    Nectarivore  Nectarivore … Troch… Apod… Forest \n2 Aburria_ab… Neot… Herbivore    Frugivore    Fugivore gle… Craci… Gall… Forest \n3 Acanthidop… Neot… Omnivore     Invertivore  Invertivore … Ember… Pass… Forest \n4 Accipiter_… Neot… Carnivore    Vertivore    Vertivore pe… Accip… Acci… Woodla…\n5 Accipiter_… Neot… Carnivore    Vertivore    Vertivore pe… Accip… Acci… Woodla…\n6 Accipiter_… Neot… Carnivore    Vertivore    Vertivore pe… Accip… Acci… Forest \n# ℹ 15 more variables: PrimaryLifestyle &lt;chr&gt;, MinLatitude &lt;dbl&gt;,\n#   MaxLatitude &lt;dbl&gt;, CentroidLatitude &lt;dbl&gt;, CentroidLongitude &lt;dbl&gt;,\n#   RangeSize &lt;dbl&gt;, Beak.Length_Culmen &lt;dbl&gt;, Beak.Length_Nares &lt;dbl&gt;,\n#   Beak.Width &lt;dbl&gt;, Beak.Depth &lt;dbl&gt;, Tarsus.Length &lt;dbl&gt;, Wing.Length &lt;dbl&gt;,\n#   Secondary1 &lt;dbl&gt;, Tail.Length &lt;dbl&gt;, Mass &lt;dbl&gt;\n\n# need to log transform bc morphological data are often not normally distributed\npar(mfrow=c(1,2))\nhist(birds$Beak.Length_Culmen, main=\"Beak.length\")\nhist(birds$Mass, main=\"Mass\")\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\n# log transform and overwrite original columns\nbirds[, 15:23] &lt;- log(birds[ , 15:23])\nnrow(birds) # each row is a species\n\n[1] 3566",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multivariate statistics</span>"
    ]
  },
  {
    "objectID": "lectures/lec13-intro-to-multivariate-stats.html#exploring-trait-space",
    "href": "lectures/lec13-intro-to-multivariate-stats.html#exploring-trait-space",
    "title": "13  Multivariate statistics",
    "section": "13.4 Exploring trait space",
    "text": "13.4 Exploring trait space\nWe want to identify if different groupings (eg orders, trophic niche) of species have more or less similar traits.\nHow might we go about doing this?\nIdeally, we would construct a 9 dimensional morphospace, where each dimension corresponds to one of our traits… but humans can’t visualize above 3 dimensions. So lets start by visualizing the relationships between our 9 traits, ie dimensions, in two dimensions using pairwise plots.\n\n13.4.1 2D\nWe can look at the pairwise relationships between traits in bivariate plots to understand how different traits are correlated with each other. Remember from previous lectures that high multicolinearity can introduce problems in univariate statistics, we can see that here. across traits\n\n# all highly correlated\n# each point here is a species\npar(mfrow=c(1,2))\nplot(birds$Beak.Depth, birds$Beak.Width)\nplot(birds$Wing.Length, birds$Mass)\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\n# can look at all variables together\n# can see that all morphological traits are highly linearly and positively correlated\n# ie species with large masses ALSO have high wing length and high tarsus length (lower leg bone, the bone below what looks like a backwards facing knee)\npairs(birds[, 15:23])\n\n\n\n\n\n\n\n# lets look at where some example species fall out on here\n# Anthracothorax_mango =  hummingbird from Jamaica\n# Harpia_harpyja = Harpy eagle \n# Junco_vulcani = Volcano Junko from costa rica and panama\ncols &lt;- ifelse(birds$Binomial== \"Anthracothorax_mango\", \"darkorange\" , \n               ifelse(birds$Binomial== \"Harpia_harpyja\", \"darkblue\", \n                      ifelse(birds$Binomial== \"Junco_vulcani\", \"#009900\", \n                             alpha(\"grey80\", .01))))\n\npairs(birds[, 15:23], col=cols, pch=19)\n\n\n\n\n\n\n\n\nWhat can we conclude from this figure about the relative morphological similarity/dissimilarity of the species?\n\n\n13.4.2 3D\nNow let’s expand this visualization to three dimensions–we still can’t capture the full 9 dimensions of our data in such a way that we can visualize it, but seeing the 3D plot can help us get a flavour of how our species are spread through morphospace.\nGenerally 3D plots are frowned upon as a data visualizing tool–they are complicated to interpret and there are usually simpler and clearer ways to show relationships between your variables in a manuscript, presentation, or talk. In this case, these 3D plots are a useful tool for illustrating the idea of higher dimensional data and the concepts of multivariate statistics.\n\n# lets pick 3 dimensions or traits to visualize, beak.Width, Wing.Length and Mass\n# rotate plot to see how each variable are related to eachother\nplot3d(birds$Mass, birds$Beak.Width, birds$Wing.Length, col=alpha(\"grey80\", .01))\n\n\n\n\n\nWhich dimensions have the tighter relationship? How can you tell?\nAlong which dimension is there the most variation?\nLets now add our 3 exemplar species to see how they fall in this morphospace.\n\nplot3d(birds$Mass, birds$Beak.Width, birds$Wing.Length, col=alpha(\"grey80\", .01))\npoints3d(subset(birds, Binomial==\"Anthracothorax_mango\" | Binomial==\"Harpia_harpyja\" |Binomial==\"Junco_vulcani\")$Mass, \n         subset(birds, Binomial==\"Anthracothorax_mango\" | Binomial==\"Harpia_harpyja\" |Binomial==\"Junco_vulcani\")$Beak.Width, \n         subset(birds, Binomial==\"Anthracothorax_mango\" | Binomial==\"Harpia_harpyja\" |Binomial==\"Junco_vulcani\")$Wing.Length, size=10, col=c(\"darkorange\", \"darkblue\", \"#009900\"))\n\n\n\n\n\nHow would you describe where these 3 species fall out in morphospace? Where is the centre of morphospace?\nHow could we quantify this? Remember that we have 3 dimensions (traits) here, but really we’re interested in all 9. And it is not uncommon to have even more.\n\n\n13.4.3 Euclidean distance\nThe solution is the Euclidean distance - which describes the closeness of points in a multidimensional space. If your data aren’t numeric and continuous, other notions of distance can be used in its place.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multivariate statistics</span>"
    ]
  },
  {
    "objectID": "lectures/lec13-intro-to-multivariate-stats.html#pca-principle-components-analysis",
    "href": "lectures/lec13-intro-to-multivariate-stats.html#pca-principle-components-analysis",
    "title": "13  Multivariate statistics",
    "section": "13.5 PCA: Principle Components Analysis",
    "text": "13.5 PCA: Principle Components Analysis\nRather than going through each set of traits and asking visually how they are correlated and where our species fall out across these highly correlated traits, we can run a PCA, which does all of this for us.\nA PCA (Principle components analysis) is one of the most common ordination techniques used in EEB. They are really popular in the community ecology literature, the functional morphology literature, and population genetics. They can be used as an exploratory tool, intermediate step in an analysis, or the final product of an analysis. There are also lots of variations that deal with phylogenetic structure or data that violates the assumptions (eg categorical data).\nPCAs work by identifying a new coordinate system (PC axes) that best account for the variation in a cloud of data across multiple dimensions. This process produces a set of “summary axes” that explain the variation in the data. Ideally, this has the effect of reducing the dimensionality of the data. In our example data in class, this would mean that we could describe the how our species are spread across trait space with less than 9 trait axes. This can be useful for summarizing information, understanding how traits are correlated with each other, provide simpler visualizations (ie in less dimensions), or can be necessary for downstream analyses.\nWhat this means is the a PCA is simply a rotation of your data–it doesn’t change the relative relationships between species or how they are distributed through trait space. It simply provides new axes through which to look at the relationships between the data (eg. species) and the dimensions (eg. traits).\nImportantly, a PCA assumes a linear relationship between your variables, can only take quantitative data, and requires more rows than columns in the data.\nThe procedure to conduct a PCA is as follows:\nImagine your data contains n objects and p variables. The n objects can be represented as a cluster of points in the p-dimensional space.\n\nImagine a 9-dimensional object with each of our trait measurements as an axis (k=9). We’ve already visualized this in 3 dimensions.\n\n\n\n\n\n\n\n\nPlot our species (n=3566) in this 9D space such that we are representing our data as cloud in high dimension space. Identify the centre of trait space using euclidean distances.\n\n\n\n\n\n\n\n\nThe cluster of data in trait space is generally not spheroid: it is elongated in some directions and flattened in others (recall our bird data in 3 dimensions). This step fits a line through the data points in multidimensional space that goes through the centre of trait space and that explains the most variance in our data. This is the first PC axis.\n\nRecall, in our 3D toy example, the major axis of variation is between mass and wing length. To actually calculate the math to add this line to the 3D figure, we would have to do the math behind a PCA or simply run the PCA and plot the resultant line… let’s do that now in order to visualize the process.\n\nmean_xyz &lt;- apply(birds[, c(23, 17, 20)], 2, mean)\nxyz_pca   &lt;- prcomp(birds[,c(23, 17, 20)], scale.=F)\ndirVector1 &lt;- xyz_pca$rotation[, 1]  # PC1\nplot3d(birds$Mass, birds$Beak.Width, birds$Wing.Length, col=alpha(\"grey80\", .01), size=1)\n#abclines3d(mean_xyz, a = t(dirVector1), col=\"darkorange\", lwd=2) # mean + t * direction_vector of PC1\nlines3d(c(1,9),  \n        c(.5,3.5), \n    c(3.5, 6.5),\n        col=\"darkorange\", lwd=8)\n\n\n\n\n\n\nFind a second line through the data under the condition that it also goes through the centre of trait space and is orthogonal to the first axes (i.e., at a 90° angle). This means that these two axes are uncorrelated. This will be our second PC axis.\n\n\n\n\nVisual representation of PC1 and PC2 in 2 dimensions.\n\n\nNote how all principle components are perpendicular (orthogonal) to each other.\n\nThis process continues until we have as many PC axes as we did starting dimensions. So in the case of our bird trait data, that would be 9.\n\n\n13.5.1 Run PCA\nNow that we’ve conceptually visualized the process of what a PCA is doing, let’s run it in R.\nWe will use the function prcomp–there are many functions that calculate a PCA in R that all vary slightly, but generally give consistent results.\nAt its most basic, prcomp takes two variables, the trait data and scale = TRUE/FALSE. The scale command specifies whether the PCA will run on a correlation or a covariance matrix. Choosing scale = TRUE selects a correlation matrix which standardizes your data to have unit variance. This puts all your variables on the same scale and is necessary if your data contain multiple types of units (eg mass measured in grams and wing length measured in cm).\n\n# just the trait data, scale = TRUE puts all trait values on unit variance, use when measure in multiple units, makes traits directly comparable\npca.out &lt;- prcomp(birds[, 15:23], scale.=T)\n\n\n\n13.5.2 Interpret PCA output\nNow let us interpret the PCA object.\n\nsummary(pca.out) \n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.5795 1.0550 0.68273 0.62615 0.46602 0.26437 0.19886\nProportion of Variance 0.7393 0.1237 0.05179 0.04356 0.02413 0.00777 0.00439\nCumulative Proportion  0.7393 0.8630 0.91478 0.95834 0.98247 0.99023 0.99463\n                          PC8     PC9\nStandard deviation     0.1723 0.13658\nProportion of Variance 0.0033 0.00207\nCumulative Proportion  0.9979 1.00000\n\n\nThis gives us a summary in table format of variance explained by each axes. This tells us how good a job each PC axis is doing at explaining the variation across our data. Note the second row (Proportion of Variance) that tells us how well each individual PC axis is doing while the third row (Cumulative Proportion) simply adds the individual contributions of variance explained by each axis to tell us cumulatively how much variation we are explaining with the addition of each new PC axis. What we want to see here is that the first several axes are explaining the majority of the variation in the data. If the first PC axis is only explaining 5% of the variation in the data, that means that the PCA is not describing the relationships between your data very well and any interpretations of the output won’t be meaningful.\nHow much variation is explained cumulatively by the first 3 PC axes in our data?\nLet’s have a look at this object in another way.\n\nstr(pca.out)\n\nList of 5\n $ sdev    : num [1:9] 2.58 1.055 0.683 0.626 0.466 ...\n $ rotation: num [1:9, 1:9] 0.309 0.27 0.343 0.349 0.308 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:9] \"Beak.Length_Culmen\" \"Beak.Length_Nares\" \"Beak.Width\" \"Beak.Depth\" ...\n  .. ..$ : chr [1:9] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n $ center  : Named num [1:9] 3.03 2.55 1.63 1.75 3.03 ...\n  ..- attr(*, \"names\")= chr [1:9] \"Beak.Length_Culmen\" \"Beak.Length_Nares\" \"Beak.Width\" \"Beak.Depth\" ...\n $ scale   : Named num [1:9] 0.508 0.59 0.578 0.687 0.645 ...\n  ..- attr(*, \"names\")= chr [1:9] \"Beak.Length_Culmen\" \"Beak.Length_Nares\" \"Beak.Width\" \"Beak.Depth\" ...\n $ x       : num [1:3566, 1:9] -5 5.15 -1.13 3.91 3.38 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:9] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n - attr(*, \"class\")= chr \"prcomp\"\n\n\nWe see that this object is structured as a five-item list. We mostly care about two of these items:\n\nLoadings – appropriately named rotation – these identify the PC axes (ie rotations) around which the data are organized.\nSpecies scores – lazily named x – these are the coordinates in PC space for each of the species in our data and tell us where in PC space each species falls.\n\n\nhead(pca.out$x) # this gives us the particular location (ie the coordinates) of each of our species in the multidimensional space formed by the PC axes\n\n           PC1       PC2        PC3        PC4         PC5         PC6\n[1,] -4.996801 -1.172868  0.8622206 -0.1550225 -0.23109589  0.81003047\n[2,]  5.150912  1.272725  1.5778845  0.5593605 -0.23613418 -0.12045828\n[3,] -1.133455  0.200266 -0.1704813 -0.2639140  0.31336225  0.24707581\n[4,]  3.914509  1.267083  0.5104558  0.4590751  0.13288949  0.06398866\n[5,]  3.376036  1.472642  0.7392281  0.3893921 -0.05490466  0.23730346\n[6,]  2.574368  1.615100  0.5074203  0.1862374 -0.17974063  0.20475730\n             PC7        PC8         PC9\n[1,] -0.08561238  0.4712667 -0.01224030\n[2,]  0.16270838 -0.2759536  0.05778076\n[3,] -0.15454788  0.0498536  0.03989184\n[4,] -0.20406768  0.1652683  0.02280116\n[5,]  0.05735235  0.2638578  0.12172863\n[6,]  0.28287395  0.3047981  0.18268409\n\nhead(pca.out$rotation) # this tells us which of our traits  are contributing most strongly to each PC axis (ie which variables are being summarized by each newly created PC axis)\n\n                         PC1         PC2          PC3        PC4         PC5\nBeak.Length_Culmen 0.3092990 -0.53227660  0.188578600 -0.2181403  0.09158808\nBeak.Length_Nares  0.2696209 -0.66835973  0.001967864 -0.1106845  0.15491394\nBeak.Width         0.3426905  0.03999976 -0.610281688  0.1667968 -0.01501396\nBeak.Depth         0.3490122  0.03462108 -0.564305388  0.1015702  0.05454900\nTarsus.Length      0.3076452  0.39691184  0.140452964 -0.5812005  0.47122204\nWing.Length        0.3589849  0.04897339  0.291811470  0.1331052 -0.60580349\n                           PC6        PC7         PC8         PC9\nBeak.Length_Culmen -0.11820103  0.1180164 -0.12558811 -0.69699967\nBeak.Length_Nares   0.05223567 -0.1744716  0.03992508  0.63998724\nBeak.Width         -0.66324179  0.1790129  0.08765516  0.02922599\nBeak.Depth          0.68964479 -0.1426081  0.08479110 -0.20523806\nTarsus.Length      -0.04341410  0.0149411  0.40423045  0.05228672\nWing.Length        -0.07557215 -0.3876020  0.48903369 -0.06133691\n\n\nWe can interpret these outputs directly, specifically the rotations.\n\npca.out$rotation[ ,1:4] # let's just look examine the first 4 PC axes\n\n                         PC1         PC2          PC3         PC4\nBeak.Length_Culmen 0.3092990 -0.53227660  0.188578600 -0.21814032\nBeak.Length_Nares  0.2696209 -0.66835973  0.001967864 -0.11068447\nBeak.Width         0.3426905  0.03999976 -0.610281688  0.16679682\nBeak.Depth         0.3490122  0.03462108 -0.564305388  0.10157019\nTarsus.Length      0.3076452  0.39691184  0.140452964 -0.58120050\nWing.Length        0.3589849  0.04897339  0.291811470  0.13310521\nSecondary1         0.3626428  0.28972162  0.083292817 -0.07918361\nTail.Length        0.3192785  0.09456601  0.380946021  0.71437740\nMass               0.3680056  0.11983129  0.128885207 -0.17294488\n\n\nThe way we interpret this is by looking, within each PC axis, for the variables that have the largest absolute values. These are the variables that are describing the most variation along this axis.\nFor example, all the traits along PC1 are positively correlated with each other (all values have the same sign). PC1 is a axis where at large values of PC1 species have long wings, big masses, long tails etc. PC2 is slightly more interesting to interpret: it is a beak length axis vs tarsus length. This suggests that species that have long beaks tend to also have a short tarsus (leg bone).\nWhich traits drive variation along PC3?\nWe can visualize these relationships using the base R function biplot.\n\npar(mar=c(5,5,5,5))\nbiplot(pca.out)\n\n\n\n\n\n\n\n\nOn this figure, there are 2 sets of information. Each species is plotted (these are the numbers). As well, the variable loadings are represented as arrows and tell us how each variable is contributing to each axis. The clustering of species along the axes (and rotations) tell us which species are similar in terms of their traits.\nWe can do this with the function autoplot in the ggfortify package, which takes advantage of ggplot style plotting.\n\nautoplot(pca.out)\n\n\n\n\n\n\n\n\nThe simplest version of autoplot displays less information than our biplot function. On this figure we can see that we are plotting PC1 by PC2 and each point on the figure is each of the rows aka species in our data. autoplot handily also shows us the proportion of variance explained by each of these PC axes. Now lets add the loadings.\n\n# lets add our 3 example species to see where they land in PC space\ncols &lt;- ifelse(birds$Binomial== \"Anthracothorax_mango\", \"darkorange\" , \n               ifelse(birds$Binomial== \"Harpia_harpyja\", \"darkblue\", \n                      ifelse(birds$Binomial== \"Junco_vulcani\", \"#009900\", \n                             alpha(\"grey30\", .1))))\n\nautoplot(pca.out, x=1,y=2,loadings=TRUE, loadings.label=TRUE, \n         loadings.colour=\"grey30\", \n         colour=cols, size=3) + \n  theme_bw()\n\n\n\n\n\n\n\n\nBased on where these species fall out in PC space, what can we conclude about the trait values of the hummingbird vs the junko vs the eagle?\nThe next step in the PCA workflow is to identify the important PC axes that give meaningful information about the data.\nWe do this with a scree plot. This shows us the amount of variation explained by each axis. As we know, each axis explains subsequently less variation. We use scree plots to help us determine at which point the axes stop adding much new information. We can do this visually or quantitatively. We will just go over the visual method as determining which axes to keep is pretty subjective and depends on your system and question of interest.\n\nplot(pca.out)\n\n\n\n\n\n\n\n\nEach bar is a PC axis, starting from PC 1. In a scree plot, what you want to look for is a big jump between subsequent axes. It’s subjective, but based on this scree plot I would probably be inclined to keep the first 2 PC axes. As well, we can look at the cumulative proportion of explained variance:\n\nsummary(pca.out)$importance[,1:6]\n\n                            PC1      PC2      PC3       PC4       PC5       PC6\nStandard deviation     2.579503 1.055004 0.682726 0.6261508 0.4660172 0.2643707\nProportion of Variance 0.739310 0.123670 0.051790 0.0435600 0.0241300 0.0077700\nCumulative Proportion  0.739310 0.862990 0.914780 0.9583400 0.9824700 0.9902300\n\n\nWe can see that by PC2 we have explained 86% of the variation in the data, which is pretty good. By the time we get to PC4, each individual axis is explaining less than 10% of variation in the data, so adding them, and conversely excluding them, isn’t going to have a large effect.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multivariate statistics</span>"
    ]
  },
  {
    "objectID": "lectures/lec13-intro-to-multivariate-stats.html#challenge-questions",
    "href": "lectures/lec13-intro-to-multivariate-stats.html#challenge-questions",
    "title": "13  Multivariate statistics",
    "section": "13.6 Challenge questions",
    "text": "13.6 Challenge questions\n\n13.6.1 Answer biological questions posed at start of class\n\nRecall the way we started this lecture, we want to be able to answer if:\n\n\n\nDo birds that are more closely related (eg. within order) tend to have similar trait values ie occupy the same regions of morphospace (indication of phylogenetic inertia)?\nDo birds that occupy similar trophic niches have similar traits, as we might expect from trait-based theories of ecology and convergent evolution?\n\nTo answer these questions let’s return to our autoplot and add some convex hulls that define different groupings of our data. First, we will group the species by their higher level taxonomic structure (order) and then by ecology (trophic niche).\n\n# answers first question\nautoplot(pca.out, x=1,y=2,\n         data=birds, \n         colour=\"Order\",\n         frame=TRUE) +\n  theme_classic() \n\n\n\n\n\n\n\n# answers second question\nautoplot(pca.out, x=1,y=2,\n         data=birds, \n         colour=\"TrophicNiche\",\n         frame=TRUE) +\n  theme_classic() \n\n\n\n\n\n\n\n\nWhat can we conclude from these figures?\n\n\n13.6.2 PCA limitations and strengths\nIn class we’ve discussed some of the assumptions behind a PCA and how we interpret a PCA. Is there any type of data for which a PCA might not be appropriate? What are some examples of a biological research question for which a PCA might NOT be appropriate, even if the data is conceivably multidimensional? What are some of the drawbacks of using a PCA? When might you want to consider using a PCA vs not?\n\n\n13.6.3 PCA in the literature\nPCAs are really common across lots of disciplines and they are very common to encounter when reading papers. Let’s look at some real examples of published PCAs and interpret them. Are they robust? Do you trust the authors conclusions?\n\n13.6.3.1 Pigot et al. 2020 example\nWhat can we conclude from this figure?\n\n\n\nFigure 6 from Pigot et al. 2020. Clustered points along each principal coordinate axis (PCoA) show the relative morphological similarity between trophic niches from different ecological theatres (biogeographical realms) based on the average pairwise distance between species (n=9,963 species) in nine-dimensional morphospace (see Methods). Individual trophic niches are omitted where they are absent or rare (&lt;6 species) within particular biogeographic realms (Na, Nearctic; Ne, Neotropic; Pa, Palaearctic; Af, Afrotropic; Ma, Madagascar; In, Indo-Malaya; Oc, Oceania; Au, Australasia; An, Antarctic). Bird images reproduced with permission from HBW, Lynx Edicions.\n\n\n\n\n13.6.3.2 Microplastics example\nIn this paper, the authors seek to identify if beaches along the Caribbean coast of Colombia vary in the quantity and type (fibers, pellets, fragments, or foam) of microplastic (MPs) pollution.\n\nGroup A - beaches in tolerable conditions with a “Moderate” MPs presence\nGroup B - beaches in bad conditions with a “High” MPs presence\nGroup C - beaches in extremely bad conditions due to “high” and “very high” MPs densities\n\nFocusing on the bottom left panel, what do you think of the authors groupings into these 3 categories?\n\n\n\nFigure 7 from Rangel-Buitrago et al. 2021. Groups by similarity levels between beaches and MP shapes presented by use of Agglomerative Hierarchical Clustering (AHC) and Principal Components Analysis (PCA). These analyses allow separation into three groups of beaches. The PCA scatterplot shows factors F1 and F2 as responsible for almost 81% of the total variance.\n\n\n\n\n13.6.3.3 Population genetics example\nThe authors in this paper, “Getting genetic ancestry right for science and society”, argue that in a human genetic context, PCAs can do more harm than good when artificial categories are applied to continuous data. For example, they show a PCA plot of human genetic ancestry–defined by migration and high mixing–and attempt to impost discrete clusters of continental origin on the PC space.\nBased on the figure, do you think that imposing categories on this data is reasonable? Why or why not? What does this suggest about when it might be (in)appropriate seek classifications of via a PCA?\n\n\n\nFigure 1 from Lewis et al. 2023. The continuous, category free, nature of genetic variation. Colored dots (N=4149) are reference panel individuals from 87 populations representing ancestry from 7 continental or subcontinental regions projected onto the first two principal components of genetic similarity. Gray dots (N=31705) are participants from BioMe, a diverse biobank based in New York City. Clearly delineated continental ancestry categories, the islands of color, are shown to be a by-product of sampling strategy. They are not reflective of the diversity in this real-world dataset, made evident by the continuous sea of gray. Reproduced/modified from Olalde I et al., Nature 555, 190–196 (2018).",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multivariate statistics</span>"
    ]
  },
  {
    "objectID": "lectures/lec13-intro-to-multivariate-stats.html#further-reading",
    "href": "lectures/lec13-intro-to-multivariate-stats.html#further-reading",
    "title": "13  Multivariate statistics",
    "section": "13.7 Further reading",
    "text": "13.7 Further reading\n\nChapter 5 of Numerical Ecology with R by Borcard, D., Gillet, F. and Legendre, P. (2011).\nPCA: a visual explanation\nhttps://setosa.io/ev/principal-component-analysis/\nPCA explained using a metaphor with wine, dinner, grandmothers, and animations\nhttps://stirlingcodingclub.github.io/ordination/#wasps",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multivariate statistics</span>"
    ]
  },
  {
    "objectID": "lectures/lec14-mathematical-models-1.html",
    "href": "lectures/lec14-mathematical-models-1.html",
    "title": "14  Mathematical models I",
    "section": "",
    "text": "14.1 Lesson preamble",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mathematical models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec14-mathematical-models-1.html#lesson-preamble",
    "href": "lectures/lec14-mathematical-models-1.html#lesson-preamble",
    "title": "14  Mathematical models I",
    "section": "",
    "text": "14.1.1 Lesson objectives\n\nDevelop familiarity with mathematical models and why/where they are used in EEB\nDevelop understanding of important models in EEB, including\n\nThe (geometric) growth of a population with constant birth and death rates\nExponential growth of a population\nThe spread of an infectious disease in a susceptible population\nCompetition between types, mutation-selection balance\n\nConceptually understand equilibria and their stability\nBecome familiar with the ways approximation is used in modeling complex systems\nBecome familiar with how mathematical models can be fit to data\n\n\n\n14.1.2 Lesson outline\n\nWhy model?\nGeometric and exponential growth\nFitting models to the data\nSIR model: when does an epidemic grow?\nEquilibria and stability\nBack to the SIR model\nMutation-selection balance",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mathematical models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec14-mathematical-models-1.html#why-model",
    "href": "lectures/lec14-mathematical-models-1.html#why-model",
    "title": "14  Mathematical models I",
    "section": "14.2 Why model?",
    "text": "14.2 Why model?\nMathematical models are descriptions of biological processes which describe idealized situations and can help us build intuition and understanding for how those processes may unfold in nature. They help us\n\nclarify assumptions we may have about how complex systems work\nbuild understanding of how complex systems behave and why\ngenerate predictions and hypothesis\ntest predictions and hypothesis based on analysis of the model (or fitting the model to data)\ndetermine what data is needed to learn about something (of if it is possible to, given the data that is available, reliably infer parameters of interest)\ndetermine in what ways certain kinds of data may be problematic or biased\n\nIn this lecture, we will discuss five models: (1) a deterministic, discrete-time model of geometric growth of a population with constant per-capita rates of birth and death. (2) A model of exponential growth, the continuous-time version of the geometric growth model. (3) A Susceptible-Infected-Recovered (SIR) model in which interactions between variables shape the transient and long-run the dynamics of the system. (4) A deterministic, continuous-time model of competitive exclusion between individuals of two types (e.g., different species, individuals of the same species carrying different alleles, etc.). (5) An extension of the previous model which maintains diversity via recurrent mutation between types.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mathematical models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec14-mathematical-models-1.html#geometric-growth",
    "href": "lectures/lec14-mathematical-models-1.html#geometric-growth",
    "title": "14  Mathematical models I",
    "section": "14.3 Geometric growth",
    "text": "14.3 Geometric growth\nLet \\(N_t\\) be the number of individuals at times \\(t=0,1,2,3,\\dots\\). Suppose we know \\(N_0\\), the initial number of individuals in the population. Assume\n\nFrom one time to the next, every individual reproduces at per-captia rate \\(b\\)\nFrom one time to the next, every individual dies at per-catpia rate \\(d\\)\nThe population is closed: there is no immigration and no emigration\nThere is no stochasticity: individuals do not, by chance, give birth to more/fewer offspring\n\nIn this case, we can write describe the population size at generation \\(t+1\\) in terms of the population size at the previous generation by keeping track of everything that could unfold.\n\\[N_{t+1} = N_t + b N_t - d N_t = (1+b-d)N_t, \\]\nwhere \\(b N_t\\) is the number of individuals born into the population and \\(d N_t\\) is the number of individuals who leave the population because they die. Births and deaths modify the previous population size such that, if \\(b &gt; d\\), then the population will grow. If \\(b &lt; d\\), the population will decline in size. If \\(b = d\\), then the population will stay constant in size. This model is unique insofar as it has a closed form solution. That is, we know the state of the system at any time in the future given knowledge of its state at time zero:\n\\[N_{1} = (1+b-d) N_0\\] \\[N_{2} = (1+b-d) N_1 = (1+b-d)(1+b-d) N_0\\] \\[N_{3} = (1+b-d) N_{2} = (1+b-d) (1+b-d)^2 N_0 = (1+b-d)^3 N_0\\]\nIn general, \\(N_{t} = (1+b-d)^t N_0\\) for all \\(t\\).\nWhen \\(b&gt;d\\), the population is said to grow geometrically, in that the growth rate \\(1+b+d\\) is being raised to a power equal to the number of timesteps (e.g., generations) that have passed. Moreover, it is sometimes convenient in discrete time models like this to describe the dynamics in terms of how variables change from one time to the next. In the case of the geometric growth model, we can express the dynamics in terms of change in absolute population size.\n\\[\\Delta N_t \\equiv N_{t+1} - N_t = (1+b-d) N_t - N_t = (b-d) N_t.\\]\nIn population ecology, \\((b-d)\\) is called the intrinsic rate of increase or decrease, and often denoted \\(r\\). If positive, the population grows in a given generation. If not, it becomes smaller.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mathematical models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec14-mathematical-models-1.html#exponential-growth",
    "href": "lectures/lec14-mathematical-models-1.html#exponential-growth",
    "title": "14  Mathematical models I",
    "section": "14.4 Exponential growth",
    "text": "14.4 Exponential growth\nContinuous-time analogues of discrete-time equations are written as differential equations. If we assume time-steps are small, we can replace the difference in the previous equation with a derivative to describe the dynamics of a population changing in continuous time:\n\\[\\frac{d N}{dt} = (b-d) N = r N\\] subject to \\(N(0) = N_0\\). Like the geometric growth equation, we can solve this equation exactly. Diving through by \\(N\\) and using properties of derivatives from calculus,\n\\[\\frac{1}{N} \\frac{d N}{d t} = \\frac{d \\ln N}{d t} = r.\\] This equation tells us that, in the continuous time version of the growth model above, the per-capita (i.e., logarithmic) rate of increase in the population size is constant. Integrating and applying the initial condition, one can show \\(N(t) = N_0 e^{r t}\\). If \\(r &gt; 0\\), i.e., \\(b &gt; d\\), then the population grows exponentially; if \\(r &lt; 0\\), the population will decay exponentially in size to zero. The trajectory of the population is completely determined in a deterministic model like this one by the parameters (i.e., the intrinsic growth rate) and initial conditions.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mathematical models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec14-mathematical-models-1.html#aside-fitting-mathematical-models-to-data",
    "href": "lectures/lec14-mathematical-models-1.html#aside-fitting-mathematical-models-to-data",
    "title": "14  Mathematical models I",
    "section": "14.5 Aside: fitting mathematical models to data",
    "text": "14.5 Aside: fitting mathematical models to data\nSuppose we measure the size of a population through time, and believe that the assumptions we have made (i.e., constant birth and death rates) are reasonable for the system.\nThe steps to fit a mathematical model to the data are as follow:\n\nDetermine the distribution of data around the solution of the model (difference or differential equation). A common convention is that the data are normally distributed around the solution to the model for a given initial condition and set of parameter values. If \\(Y_i\\) is the measurement at time \\(t_i\\) and \\(X_i\\) is the state of the system (e.g., the real population size or the true values of more variables, which influence each other and the dynamics), then this amounts to specifying the distribution of \\(Y_i\\) given \\(X_i\\):\n\n\\[f_{Y_i|X_i}(y_i|\\text{parameters})\\]\n\nWrite down and (numerically) maximize the log-likelihood function:\n\n\\[\\ln L(\\text{parameters } \\boldsymbol{\\theta} | \\text{data } y_1,\\dots,y_n) = \\sum_{i=1}^n \\ln f_{Y_i|X_i}(y_i|\\boldsymbol{\\theta}).\\]\nWhen the distribution of the measurements around the solution to the system (model) is normal with some variance (and mean equal to the model solution), maximizing the likelihood is equivalent to minimizing the sum of squared departures of the solution from the data.\nWe will return to fitting mathematical models to data next lecture.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mathematical models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec14-mathematical-models-1.html#the-sir-model",
    "href": "lectures/lec14-mathematical-models-1.html#the-sir-model",
    "title": "14  Mathematical models I",
    "section": "14.6 The SIR model",
    "text": "14.6 The SIR model\nThe Susceptible-Infected-Recovered (SIR) model describes how different processes (e.g., transmission, recovery, virulence, loss of immunity, demography) shape how a pathogen spreads in a population through time and if it is able to persist in the long-run. The model tracks the number of susceptible \\(S\\), infected \\(I\\), and recovered \\(R\\) individuals in continuous time.\nAssuming that the population size is constant (\\(N = S + I + R\\) does not change), susceptible individuals acquire the infection at rate \\(\\beta\\), and infected individuals recover at rate \\(\\gamma\\), the model equations are given by:\n\\[\\frac{dS}{dt} = - \\beta SI\\] \\[\\frac{dI}{dt} = \\beta SI - \\gamma I\\] \\[\\frac{dR}{dt} = \\gamma I\\]\nSince the population size is constant, one of these is not actually needed. Focusing in on the \\(I\\) equation, however, we notice that is of the same form as the geometric and exponential growth models: we simply track the flux in and out of the compartment due to, in the case of \\(I\\), infection and recovery.\n\n14.6.1 Challenge\n\nWhat are the assumptions of the model?\nWhat terms are missing?\nWhat is the model trying to capture?\nWhat dynamics and long run behavior might we expect of a disease that has been introduced to a largely susceptible population?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mathematical models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec14-mathematical-models-1.html#when-will-the-epidemic-grow",
    "href": "lectures/lec14-mathematical-models-1.html#when-will-the-epidemic-grow",
    "title": "14  Mathematical models I",
    "section": "14.7 When will the epidemic grow?",
    "text": "14.7 When will the epidemic grow?\nTo determine if an epidemic that begins from the rare introduction of a pathogen into a closed population will spread, we can use the following approximation. For the epidemic to grow, we need\n\\[\\frac{d I}{dt} = \\beta S I - \\gamma I &gt; 0\\]\nAt the beginning of the outbreak, \\(I \\approx 1\\) and so \\(S \\approx N\\) because no individuals have immunity to the disease under investigation. So the condition for the disease to spread is\n\\[\\frac{d I}{dt} = \\beta N - \\gamma &gt; 0 \\iff R_0 = \\frac{\\beta N}{\\gamma} &gt; 1.\\]\nThe quantity \\(R_0\\) measures the average number of secondary infections from a single infectious individual in an otherwise susceptible population. This measure of reproductive success or fitness is also common in population ecology. Intuitively, if the pathogen is unable to infect \\(&gt;1\\) individual on average when the susceptible pool has not yet been depleted (i.e., all individuals are susceptible), it will fail to spread. If \\(R_0 &gt; 1\\), then the disease will spread. Importantly, \\(R_0\\) does not provide a quantitative measure of how quickly this happens, and researchers have proposed using other metrics (like the intrinsic growth rate of the pathogen) to measure the speed of pathogen spread in a susceptible population.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mathematical models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec14-mathematical-models-1.html#what-happens-in-the-long-run",
    "href": "lectures/lec14-mathematical-models-1.html#what-happens-in-the-long-run",
    "title": "14  Mathematical models I",
    "section": "14.8 What happens in the long run?",
    "text": "14.8 What happens in the long run?\nTo understand the long-run behavior of a dynamical system (i.e., where key variables will go after enough time has passed), modelers will typically search for equilibria and characterize their stability properties. Intuitively, an equilibrium (or steady-state) is a point such that if the system starts there, it stays there. The stability of an equilibrium is determined by the behavior of the system (model) if perturbed.\nWhat are the equilibria of the SIR model above? They are exactly the points such that\n\\[\\frac{d S}{dt} = \\frac{d I}{d t} = \\frac{d R}{d t} = 0,\\] \ni.e., the variables of interest (number susceptible, infected, recovered) do not change. There are two such equillirbia, which we can find by solving for \\(S,I\\) in\n\\[\\frac{d S}{dt} = - \\beta S I = 0\\] \\[\\frac{d I}{dt} = \\beta S I - \\gamma I = 0.\\] \nThe two equilibria are \\((S^*,I^*,R^*) = (N,0,0)\\), which corresponds to the situation in which the disease is absent from the population (i.e., the disease-free equilibrium) and\n\\[ S^* = \\frac{\\gamma}{\\beta} = \\frac{N}{R_0}, \\hspace{12pt} I^* = 0, \\hspace{12pt} R^* = N - S^* = N (1- \\frac{1}{R_0}).\\] \nThe second equilibrium corresponds to the situation in which the disease has taken hold, but depleted the susceptible pool: although a fraction of individuals have experienced infection and recovered, a positive fraction of individuals did not acquire the disease. In other words, the disease burned itself out by depleting the resource pool (i.e., the number of susceptibles) such that some “resources” are still left.\nBut there are there are two equilibria — where will the system go? One way to answer this question is to determine the stability of each equilibrium point. Intuitively, a stable equilibrium is one such small perturbations from the equilibrium shrink the system returns to the equilibrium in question. An unstable equilibrium is one such small perturbations grow, leaving the system to go somewhere else. (There is, in fact, a third case that can arise. If the perturbation does not shrink or grow, the equilibrium is said to be neutrally stable.) The details are beyond the scope of this course but it turns out that it is often enough to look at the linear behavior of the system about the equilibrium to characterize stability. To gauge local stability of an equilibrium \\(x^* = (x_1^*,\\dots,x_n^*)\\) for a system of differential equations\n\\[\\frac{d x_i}{dt} = f_i(x_1,\\dots,x_n)\\]\n\nTake derivatives of of \\(f_i\\) with respect to all \\(x_j\\)s.\nPut these derivatives into a matrix called the Jacobian, whose columns correspond to the derivatives \\(\\partial f_i/\\partial x_1, \\dots, \\partial f_n/\\partial x_1\\).\nEvaluate the Jacobian at \\(x^*\\).\nDetermine the eigenvalues of the Jacobian at \\(x^*\\).\nIf the eigenvalues have strictly negative real part, then \\(x^*\\) is stable.\n\n\n14.8.1 A stability analysis of the disease-free equilibrium\nHere is how the local stability analysis would work in the case of the SIR model with \\((S^*,I^*) = (N,0)\\). Remember we only need two equations to describe the dynamics of the population, since the population is of constant size. Using the notation above,\n\\[\\frac{dS}{dt} = -\\beta S I = f_1(S,I)\\] \\[\\frac{dI}{dt} = \\beta S I - \\gamma I = f_2(S,I).\\] \nThe Jacobian \\(J\\) is formed by taking partial derivatives of \\(f_1,f_2\\) with respect to the state variables. (One follows the same procedure for systems with more variables.)\n\\[J = \\begin{pmatrix} \\partial f_1/\\partial S & \\partial f_1/\\partial I \\\\\n\\partial f_2/\\partial S & \\partial f_2/\\partial I \\end{pmatrix}\n= \\begin{pmatrix} - \\beta I & - \\beta S \\\\\n\\beta I & \\beta S - \\gamma \\end{pmatrix}\\]\nEvaluating \\(J\\) at the disease-free equilibrium and solving for the eigenvalues, one can show that a necessary and sufficient condition for the disease-free equilibrium to be stable is that\n\\[\\beta N - \\gamma &lt; 0 \\iff R_0 &lt; 1.\\] \nThis should make sense as, in our previous analysis, we showed that for an epidemic to grow from a small introduction (perturbation), we need \\(R_0 &gt; 1\\). Fewer than one secondary infection from a single infectious individual in an otherwise susceptible population will ensure the stability of the disease-free steady-state since the pathogen will be unable to sustain transmission!",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mathematical models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec14-mathematical-models-1.html#a-model-of-mutation-selection-balance",
    "href": "lectures/lec14-mathematical-models-1.html#a-model-of-mutation-selection-balance",
    "title": "14  Mathematical models I",
    "section": "14.9 A model of mutation-selection balance!",
    "text": "14.9 A model of mutation-selection balance!\nNow, we go about deriving from a set of ecological equations a model for the evolution of a population which consists of individuals carrying one of two alleles at a locus. \\(n_i\\) will denote the number of individuals carrying allele \\(i=1,2\\). If individuals of both types are exponentially growing at rates \\(r_1,r_2\\), a model of the population dynamics is given by\n\\[\\frac{d n_i}{dt} = r_i n_i.\\]\nSuppose there is also mutation which converts allele \\(i\\) to \\(j\\) at rate \\(\\mu_{ij}\\). The model equations would then include a second term to account for changes in each type due to mutation:\n\\[\\frac{d n_i}{dt} = r_i n_i - \\mu_{ij} n_i + \\mu_{ji} n_j.\\] \nConsider the fraction of individuals carrying allele 1, i.e., \\(p = n_1/(n_1+n_2)\\). An equation for this new variable can be found by using the quotient rule, substituting, and simplifying so that the equation is in terms of \\(p\\):\n\\[\\begin{align*}\n\\frac{dp}{dt} &= \\frac{d (\\frac{n_1}{n_1+n_2})}{dt} \\\\\n&= \\frac{(n_1+n_2) \\frac{d n_1}{dt} - n_1 \\frac{d(n_1+n_2)}{dt}}{(n_1+n_2)^2} \\\\\n&= \\frac{1}{n_1+n_2} (r_1 n_1 - \\mu_{12} n_1 + \\mu_{21} n_2) - \\frac{n_1(r_1 n_1 + r_2 n_2)}{(n_1+n_2)^2} \\\\\n&= r_1 p - \\mu_{12} p + \\mu_{21} (1-p) - p(r_1 p + r_2 (1-p)) \\\\\n&= (r_1-r_2) p(1-p) - \\mu_{12} p + \\mu_{21} (1-p) \\\\\n\\end{align*}\\]\nThe terms in the equation can be interpreted as follow. (1) The first term describes the change in allele 1 frequency due to selection imposed by differences in intrinsic growth. If \\(r_1 &gt; r_2\\), then allele 1 grows in frequency due to its competitive advantage. If \\(r_1 &lt; r_2\\), then allele 1 is purged from the population by purifying selection. In this case, and without mutation, individuals carrying allele 2 take over the population. (2) The second and third terms describe mutation from type 1 to type 2 and type 2 to 1, respectively. With mutation rates that are both \\(&gt;0\\), competitive exclusion cannot happen because individuals change type at rates \\(\\mu_{12},\\mu_{21}\\). The behavior of the system and the long-run predictions will depend on the relative strengths of mutation and selection.\nTo understand where selection and mutation take the system, we can preform an approximation. This is common in modeling: either by assuming certain parameters are small, or that certain processes unfold faster than others (e.g., a separation of ecological and evolutionary timescales), we can learn what complex systems do and disentangle the reasons why. The key takeaway is that problems that would otherwise be difficult to solve can be addressed via approximation.\nHere, we will assume \\(s = r_1 - r_2 &lt; 0\\) and preform an approximation when selection is strong and mutation is weak. (A similar approximation can be applied in the case mutation is strong and selection is weak.) If selection is much stronger than selection, i.e., \\(|s| \\gg \\mu_{12}, \\mu_{21}\\), then we expect allele 1 to be rare. Ignoring terms like the allele frequency squared, allele frequency times mutation rate, etc. (which are expected to be smaller still!), we arrive at an approximate equation for allele frequency change:\n\\[\\frac{d p}{dt} = s p - \\mu_{21} + \\text{small stuff}.\\] \nThe approximate equilibrium allele frequency is \\(p^* = \\mu_{21}/s\\). Indeed, one can solve the model equations numerically (as we will do next class!) to see that this approximation is very good when the assumptions are met, and is decent still when the assumptions begin to break down.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mathematical models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec14-mathematical-models-1.html#summary",
    "href": "lectures/lec14-mathematical-models-1.html#summary",
    "title": "14  Mathematical models I",
    "section": "14.10 Summary",
    "text": "14.10 Summary\nModeling is an indispensable tool in EEB, and a very creative endeavor! Here we have covered a lot of ground: geometric and exponential growth models in population ecology, an eco-evolutionary model of the balance between mutation and selection, the SIR model for the spread of an infectious disease, and the WF model of allele frequency change under drift. Next class will focus on simulating these models in R, and trying to use them to learn about interesting biology!",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mathematical models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec14-mathematical-models-1.html#resources",
    "href": "lectures/lec14-mathematical-models-1.html#resources",
    "title": "14  Mathematical models I",
    "section": "14.11 Resources",
    "text": "14.11 Resources\n\nOtto, S. & Day, T. (2007). A Biologist’s Guide to Mathematical Modeling in Ecology and Evolution.\nNuismer, S. (2017) Introduction to Coevolutionary Theory.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mathematical models I</span>"
    ]
  },
  {
    "objectID": "lectures/lec15-mathematical-models-2.html",
    "href": "lectures/lec15-mathematical-models-2.html",
    "title": "15  Mathematical models II",
    "section": "",
    "text": "15.1 Lesson preamble\nrequire(tidyverse)\nrequire(pbmcapply)\nrequire(deSolve)\ntheme_set(theme_bw())",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Mathematical models II</span>"
    ]
  },
  {
    "objectID": "lectures/lec15-mathematical-models-2.html#lesson-preamble",
    "href": "lectures/lec15-mathematical-models-2.html#lesson-preamble",
    "title": "15  Mathematical models II",
    "section": "",
    "text": "15.1.1 Lesson objectives\n\nDevelop familiarity with important mathematical models in EEB\nDevelop familiarity with how to fit mathematical models to data\nUnderstand how to numerically solve systems of differential equations in R\nPractice solving systems of ODEs\n\n\n\n15.1.2 Lesson outline\n\nUsing deSolve to solve systems of differential equations\nFitting models to data: a case study in the SIR model",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Mathematical models II</span>"
    ]
  },
  {
    "objectID": "lectures/lec15-mathematical-models-2.html#using-desolve-to-solve-systems-of-differential-equations",
    "href": "lectures/lec15-mathematical-models-2.html#using-desolve-to-solve-systems-of-differential-equations",
    "title": "15  Mathematical models II",
    "section": "15.2 Using deSolve to solve systems of differential equations",
    "text": "15.2 Using deSolve to solve systems of differential equations\nLast class we discussed how to formulate and analyze (by finding equilibria and determining their stability properties) systems of differential equations of the form\n\\[\\frac{d x_i}{dt} = f_i(x_1,\\dots,x_n|\\boldsymbol{\\theta}),\\] \nwhere \\(x_1,\\dots,x_n\\) are variables of interest (e.g., the number of susceptible and infected individuals in a population) and \\(\\boldsymbol{\\theta}\\) is the set of all parameters (e.g., the transmission and recovery rates) which shape the dynamics and long-run behavior of the system (e.g., if the disease successfully spreads in the population).\nIn this class we will discuss how to numerically solve systems differential equations using the package deSolve. Numerical analysis of differential equations is a big, active area of research in mathematics but we will not concern ourselves with the details of how these methods ensure convergence to the true solution to a given system of differential equations. (It is enough to recognize that approximating each derivative with a difference quotient will provide a means to iteratively update variables through time.)\nTo illustrate how to solve differential equations using deSolve, we will consider the following system, which describes how a pathogen spreads in a population of hosts that are born and die at per-capita rate \\(\\mu\\); this ensures that the population size is constant. We assume that all individuals are born susceptible to the disease, and that immunity is life-long. Transmission occurs at rate \\(\\beta\\) and recovery at rate \\(\\gamma\\). Finally, we include a term to capture disease-induced mortality (i.e., virulence).\nThe model is as follows:\n\\[\\frac{dS}{dt} = \\mu N - \\beta S I - \\mu S\\] \\[\\frac{dI}{dt} = \\beta S I - (\\mu+\\gamma+v) I\\] \\[\\frac{dR}{dt} = \\gamma I - \\mu R\\]\nSince \\(dN/dt = 0\\), the population size is constant and we can (as we did last class) ignore the \\(R\\) equation (since \\(R = N - S - I\\)). Setting the previous equations to zero and solving, one has that there are two equilibria: the disease-free equilibrium \\((S^*, I^*) = (N,0)\\), and the endemic equilibrium\n\\[(S^*,I^*) = (\\frac{\\mu+\\gamma+v}{\\beta}, \\frac{\\mu N}{\\mu+\\gamma + v}-\\frac{\\mu}{\\beta}).\\] \nThe endemic equilibrium is stable (and the disease-free equilibrium is unstable) whenever the basic reproductive number of the disease agent\n\\[R_0 = \\frac{\\beta N}{\\mu+\\gamma+v}\\]\nis \\(&gt;1\\), i.e., in an otherwise susceptible population a single infectious individual infects more than one individual on average. Notice how demography, recovery, and virulence all reduce the reproductive value of the pathogen; intuitively, this is because an infected individual can die or recover before transmitting.\nBelow we solve the system and plot the dynamics for a specific set of parameter values.\n\nN &lt;- 100 # population size\n\n# define parameters\ngamma &lt;- 1/14  # mean infectious period = 14 days\nv &lt;- 1/14  # mean time before individual dies to due disease = 14 days\nbeta &lt;- 0.01  # transmission rate\nmu &lt;- 1/200 # average life time of individual = 200 days\n\nR0 &lt;- beta*N/(mu+gamma+v); R0\n\n[1] 6.763285\n\n# put parameter values into vector params\nparams &lt;- c(mu = mu, gamma = gamma, beta = beta, v = v)\n\ninitialI &lt;- 10\n\nstate &lt;- c(S=N-initialI, I=initialI) # define initial conditions\n\n# define times to save\ntimes &lt;- seq(0, 500, 0.01)\n\n# define the model!\nsir &lt;- function(time, state, params){\n  with(as.list(c(state,params)),{\n    \n    dS &lt;- mu*N - beta*S*I - mu*S\n    dI &lt;- beta*S*I - gamma*I - v*I - mu*I\n    \n    return(list(c(dS, dI)))\n  })\n}\n\n# numerically integrate equations!\nout &lt;- as.data.frame(ode(state, times, sir, params))\n\nout %&gt;% pivot_longer(! time) %&gt;% \n  ggplot(aes(x = time, y = value, color = name)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\n15.2.1 Challenge\nDetermine at what time the number of infected individuals peaks.\n\n\n15.2.2 Challenge\nA common way to visualize the behavior of a mathematical model is to plot the variables (in the previous case, \\(S\\) and \\(I\\)) against each other in phase space. Doing this, one can determine how the variables jointly influence each other through time, and if/how they enter an equilibrium or limit cycle.\nBased on the phase portrait of the system, the number of susceptible and infected individuals spiral into the endemic equilibrium.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Mathematical models II</span>"
    ]
  },
  {
    "objectID": "lectures/lec15-mathematical-models-2.html#fitting-the-sir-model-to-data",
    "href": "lectures/lec15-mathematical-models-2.html#fitting-the-sir-model-to-data",
    "title": "15  Mathematical models II",
    "section": "15.3 Fitting the SIR model to data",
    "text": "15.3 Fitting the SIR model to data\nWe will now try to fit the above SIR model to (crudely) simulated case count data. We will assume \\(\\mu = v = 0\\), i.e., no births or deaths occur over the sampling period.\n\nread_csv(\"data/meas.csv\") -&gt; meas\n\nRows: 103 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): time, reports\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nggplot(meas, aes(x = time, y = reports)) + geom_line() + xlab(\"day\")\n\n\n\n\n\n\n\n\nTo fit the model to the data using ML, we will follow the steps outlined in the previous lecture.\n\n15.3.1 Step 1: determine the distribution of the data\nWe will assume the case count data are an imperfect and random sample of the true incidence. There are a couple ways to do this, but we will suppose the cases have a binomial distribution: \\(\\text{reports}_i \\sim \\text{Binomial}(\\kappa I(t_i),p)\\). In other words, a fraction \\(\\kappa\\) of infected individuals at an observation time are tested and the test comes up positive with probability \\(p\\). (Keep in mind there are other ways to specify the distribution of the data around the solution of the model!)\n\n\n15.3.2 Step 2: maximize the likelihood function\nWe will now write a function to solve the differential equation above for a fixed set of parameter values and evaluate the likelihood of the case count data for a set of parameters. Finally, looping over combinations of parameters we will determine where the likelihood assumes a maximum (i.e., the MLE).\n\ninitialI &lt;- 10\nstate &lt;- c(S=N-initialI, I=initialI) # define initial conditions\ntimes &lt;- seq(0, 75, 0.1)\n\nparams &lt;- expand.grid(mu = 0, \n                     gamma = seq(0.06,0.08,0.005),\n                     beta = seq(0.003,0.006,0.001),\n                     v = 0,\n                     p = seq(0.5,0.9,0.1),\n                     kappa = seq(0.4,1,0.1)\n                     )\n# many parameters fixed for convenience, but in general could / often should be fitted!\n\n\nreturn_LL_at_specific_combo_params &lt;- function(params_to_use){\n  \n  reporting_times &lt;- meas$time\n  \n  out &lt;- \n    as.data.frame(ode(state, times, sir, params_to_use)) %&gt;%\n    subset(time %in% reporting_times) \n  # solve model for particular set of parameters\n  # keep variables only at observation times\n  \n  LL &lt;- c()\n  \n  for (i in 1:length(reporting_times)){\n    LL[i] &lt;- dbinom(meas$reports[i], size = round(params_to_use$kappa*out$I[i]), \n                    prob = params_to_use$p)\n  }\n  # calculate likelihood of data at observation times\n  # based on assumption data are Normal around the solution at a given time\n  \n  LogLik &lt;- sum(log(LL))\n  # calculate log-likelihood of parameters given ALL data\n  \n  return(data.frame(params_to_use, LogLik = LogLik))\n}\n\n\nLogLikihoods &lt;- NULL\noutALL &lt;- NULL\n\nfor (i in 1:nrow(params)){\n  LogLikihoods &lt;- rbind(LogLikihoods, \n                        return_LL_at_specific_combo_params(params[i,])\n                        )\n  outALL[[i]] &lt;- data.frame(ode(state, times, sir, params[i,1:4]), index = i)\n}\n\nMLE &lt;- LogLikihoods %&gt;% subset(is.finite(LogLik)) %&gt;% \n  subset(LogLik == max(LogLik)); MLE\n\n    mu gamma  beta v   p kappa  LogLik\n573  0  0.07 0.005 0 0.8   0.9 -199.63\n\noutALL &lt;- do.call(rbind, outALL)\n\n# what does the solution at the MLE look like compared to the data?\nbest_solution &lt;- outALL %&gt;% \n  subset(index == which(LogLikihoods$LogLik == max(LogLikihoods$LogLik))) %&gt;%\n  group_by(time) %&gt;%\n  mutate(expected_measurement = MLE$kappa*MLE$p*I)\n\nbest_solution %&gt;% ggplot() + \n  geom_line(aes(x = time, y = expected_measurement), color = \"black\") +\n  geom_point(data = meas, aes(x = time, y = reports), size = 2) +\n  geom_line(aes(x = time, y = I), color = \"red\")\n\n\n\n\n\n\n\n\nThat’s a great fit to the data. It is rare to have sampling as frequent in this example, but the underlying methodology is still quite powerful in cases where there are such constraints!",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Mathematical models II</span>"
    ]
  },
  {
    "objectID": "assignments/assignment-00.html",
    "href": "assignments/assignment-00.html",
    "title": "Assignment00: individual interest description",
    "section": "",
    "text": "Download the .Rmd file here.\nTo submit this assignment, upload the full document on Quercus, including the original questions, your code, and the output. Submit your assignment as a knitted .pdf.\n\nGet set up at home (or on a lab computer after hours)\n\nIf you have not already done so, install R and RStudio (already installed on the lab computers).\nOpen a new R Notebook and read the instructions about how to use the R Markdown syntax.\nOpen this assignment file in RStudio or copy its content into an empty R Notebook.\nInsert a code chunk below, above Question 2. Set eval=FALSE\nIn the code chunk, use install.packages(\"&lt;package_name&gt;\") to install tidyverse and rmarkdown. Remember to run the code chunk to execute the commands.\nLoad the two libraries you just installed into your environment with library(&lt;package_name&gt;) (no surrounding quotation marks). Add this to the same code chunk you created previously and execute it again.\n\nDon’t worry that the install.packages() commands have already been executed once, R is smart and checks if you already have those installed.\n\nRun sessionInfo() to list all the loaded packages.\n\nYou should see the following packages under “other attached packages”: rmarkdown, dplyr, purrr, readr, tidyr, tibble, ggplot, and tidyverse.\n\nSince this is your first assignment, we have already completed most of this question below. You still need to run the code chunk on your computer to confirm that the packages installed and to get the sessionInfo() output for your computer. You might receive warnings that functions from other packages are masked when you load tidyverse, but this is fine.\n\n\n\nIn 4-5 sentences, what are some of the topics/questions in ecology and evolutionary biology that you are interested in for the group project?",
    "crumbs": [
      "Assignments",
      "Assignment00: individual interest description"
    ]
  },
  {
    "objectID": "assignments/assignment-01.html",
    "href": "assignments/assignment-01.html",
    "title": "Assignment 01: base R, command line, & Git(Hub)",
    "section": "",
    "text": "1. Variable assignment (1 mark)",
    "crumbs": [
      "Assignments",
      "Assignment 01: base R, command line, & Git(Hub)"
    ]
  },
  {
    "objectID": "assignments/assignment-01.html#variable-assignment-1-mark",
    "href": "assignments/assignment-01.html#variable-assignment-1-mark",
    "title": "Assignment 01: base R, command line, & Git(Hub)",
    "section": "",
    "text": "Assign the value 5 to the variable/object a. Display a. (0.25 marks)\n\n\nAssign the result of 10/3 to the variable b. Display b. (0.25 marks)\n\n\nWrite a function that adds two numbers and returns their sum. Use it to assign the sum of a and b to result. Display result. (In practice, there is already a more sophisticated built-in function for this: result &lt;- sum(a, b)) (0.25 marks)\n\n\nWrite a function that multiplies two numbers and returns their product. Use it to assign the product of a and b to product. Display product. (In practice, there is already a more sophisticated built-in function for this: product &lt;- prod(a, b)) (0.25 marks)",
    "crumbs": [
      "Assignments",
      "Assignment 01: base R, command line, & Git(Hub)"
    ]
  },
  {
    "objectID": "assignments/assignment-01.html#vectors-2.5-marks",
    "href": "assignments/assignment-01.html#vectors-2.5-marks",
    "title": "Assignment 01: base R, command line, & Git(Hub)",
    "section": "2. Vectors (2.5 marks)",
    "text": "2. Vectors (2.5 marks)\n\nCreate a vector v with all integers 0-30, and a vector w with every third integer in the same range. (0.25 marks)\n\n\nWhat is the difference in lengths of the vectors v and w? (0.25 marks)\n\n\nCreate a new vector, v_square, with the square of elements at indices 3, 6, 7, 10, 15, 22, 23, 24, and 30 from the variable v. Hint: Use indexing rather than a for loop. (0.25 marks)\n\n\nCalculate the mean and median of the first five values from v_square. (0.25 marks)\n\n\nCreate a boolean vector v_bool, indicating which vector v elements are bigger than 20. How many values are over 20? Hint: In R, TRUE = 1, and FALSE = 0, so you can use simple arithmetic to find this out. (0.5 marks)\n\n\nWrite a function that calculates the median of the last two elements of any numeric vector. Test this function with the v and v_square vectors. (1 marks)",
    "crumbs": [
      "Assignments",
      "Assignment 01: base R, command line, & Git(Hub)"
    ]
  },
  {
    "objectID": "assignments/assignment-01.html#data-frames-1.5-marks",
    "href": "assignments/assignment-01.html#data-frames-1.5-marks",
    "title": "Assignment 01: base R, command line, & Git(Hub)",
    "section": "3. Data frames (1.5 marks)",
    "text": "3. Data frames (1.5 marks)\n\nThere are many built-in data frames in R, which you can find more details about online. What are the column names of the built-in dataframe beaver1? How many observations (rows) and variables (columns) are there? (0.5 marks)\n\n\nDisplay both the first 6 and last 6 rows of this data frame. Show how to do so with both indexing as well as specialized functions. (0.5 marks)\n\n\n# With indexing\n\n\n# With functions\n\n\nWhat is the min, mean, and max body temperature for beavers inside and outside of the retreat? Hint: You can use ? on the beaver1 data set to get more information about it. Remember that each column in a data frame is a vector and you can use the same functions. (0.5 marks)",
    "crumbs": [
      "Assignments",
      "Assignment 01: base R, command line, & Git(Hub)"
    ]
  },
  {
    "objectID": "assignments/assignment-01.html#command-line-1-mark",
    "href": "assignments/assignment-01.html#command-line-1-mark",
    "title": "Assignment 01: base R, command line, & Git(Hub)",
    "section": "4. Command line (1 mark)",
    "text": "4. Command line (1 mark)\nWhat commands would you run to create a directory named “mydir”, check mydir’s permissions, add write permissions for users, copy mydir to “mydir2” , and then remove mydir? Hint: try running each step in your terminal to check if the output is what you expect (1 mark)",
    "crumbs": [
      "Assignments",
      "Assignment 01: base R, command line, & Git(Hub)"
    ]
  },
  {
    "objectID": "assignments/assignment-01.html#understanding-command-line-software-1-mark",
    "href": "assignments/assignment-01.html#understanding-command-line-software-1-mark",
    "title": "Assignment 01: base R, command line, & Git(Hub)",
    "section": "5. Understanding command line software (1 mark)",
    "text": "5. Understanding command line software (1 mark)\nOpen the documentation for Earl Grey, a software programme that identifies transposable elements in genomes. There may be a lot of unfamilliar terms on this web page, but we’ll just be skimming this page to practice identifying the kind of information command line software might require and how to provide that.\n\nWhat three arguments (files/information) are required input for this software? (0.3 marks)\nWhat would you type into the command line to provide Earl Grey with those minimum command options? You don’t need to make up example text for this, just ensure you have the three - arguments written out. Hint: search for “minimum command options” on the GitHub page and use the next line (0.3 marks)\nBased on the visual overview of the software (or the “Example Outputs” section) on the GitHub page, what does this software output? (0.2 marks)\nWhat optional parameter (i.e. the - argument) does Earl Grey offer to display help information? Is this kind of help parameter common? Hint: you can search for “help documentation” in your lecture notes (0.2 marks)",
    "crumbs": [
      "Assignments",
      "Assignment 01: base R, command line, & Git(Hub)"
    ]
  },
  {
    "objectID": "assignments/assignment-01.html#github-1-mark",
    "href": "assignments/assignment-01.html#github-1-mark",
    "title": "Assignment 01: base R, command line, & Git(Hub)",
    "section": "6. GitHub (1 mark)",
    "text": "6. GitHub (1 mark)\n\nMake a Github account. You can do this at github.com. What is your GitHub username? (1 mark)",
    "crumbs": [
      "Assignments",
      "Assignment 01: base R, command line, & Git(Hub)"
    ]
  },
  {
    "objectID": "assignments/assignment-02.html",
    "href": "assignments/assignment-02.html",
    "title": "Assignment 02: data wrangling",
    "section": "",
    "text": "1. Read in and pre-process plant biomass data (1.5 mark)\nYou will apply your data wrangling skills on the yearly change in biomass of plants in the beautiful Abisko national park in northern Sweden. We have preprocessed this data and made it available as a csv-file via this link. You can find the original data and a short readme on figshare and dryad. The original study1 is available on an open access license. Reading through the readme on figshare, and the study abstract will increase your understanding for working with the data.",
    "crumbs": [
      "Assignments",
      "Assignment 02: data wrangling"
    ]
  },
  {
    "objectID": "assignments/assignment-02.html#read-in-and-pre-process-plant-biomass-data-1.5-mark",
    "href": "assignments/assignment-02.html#read-in-and-pre-process-plant-biomass-data-1.5-mark",
    "title": "Assignment 02: data wrangling",
    "section": "",
    "text": "Read the data directly from the provided URL, into a variable called plant_biomass and display the first six rows. (0.25 mark)\nConvert the Latin column names into their common English names: lingonberry, bilberry, bog bilberry, dwarf birch, crowberry, and wavy hair grass. After this, display all column names. (0.25 marks) Hint: Search online to find out which Latin and English names pair up.\nThis is a wide data frame (species make up the column names). A long format is easier to analyze, so pivot the species names into one column (species) and the measurement values into another column (biomass). Assign it to the variable plant_long. (0.5 marks)\nRecreate the wide data frame (species names as columns again) by pivoting it from your plant_long data frame. (0.5 marks)",
    "crumbs": [
      "Assignments",
      "Assignment 02: data wrangling"
    ]
  },
  {
    "objectID": "assignments/assignment-02.html#wrangling-plant-biomass-with-dplyr-3.5-marks",
    "href": "assignments/assignment-02.html#wrangling-plant-biomass-with-dplyr-3.5-marks",
    "title": "Assignment 02: data wrangling",
    "section": "2. Wrangling plant biomass with dplyr (3.5 marks)",
    "text": "2. Wrangling plant biomass with dplyr (3.5 marks)\nNow that our data is in a tidy format, we can start exploring it!\n\nWhat is the average biomass in g/m2 for all observations in the study? (0.25 marks)\nHow does the average biomass compare between the grazed control sites and those that were protected from herbivores? (0.5 marks)\nDisplay a table of the average plant biomass for each year. (0.25 marks)\nWhat is the mean plant biomass per year for the grazedcontrol and rodentexclosure groups? Present the answer in a table that has these variables as column headers (use pivoting). (0.75 marks)\nCheck whether there is an equal number of observations per site. (0.25 marks)\nHow many biomass measurements were 0? Which species had the most measurements of 0 biomass? (0.5 marks)\nCreate a new column that represents the square of the biomass. Display the three largest squared_biomass observations in descending order. Only include the columns year, squared_biomass and species and only observations between the years 2003 and 2008 from the forest habitat. (1 mark)\n\nHint: Break this down into single criteria and add one at a time. It is possible to obtain the desired result with five operations (1 mark)",
    "crumbs": [
      "Assignments",
      "Assignment 02: data wrangling"
    ]
  },
  {
    "objectID": "assignments/assignment-02.html#read-in-and-wrangle-mammal-size-data-3-marks",
    "href": "assignments/assignment-02.html#read-in-and-wrangle-mammal-size-data-3-marks",
    "title": "Assignment 02: data wrangling",
    "section": "3. Read in and wrangle mammal size data (3 marks)",
    "text": "3. Read in and wrangle mammal size data (3 marks)\nDownload the “SSDinMammals.csv” file from Quercus, the course website, or Dryad. The original study2 is quite interesting!\n\nDownload the file to your computer and read it into a variable called mammal_sizes and provide a glimpse into the data. (0.25 mark)\nPull out the columns for “Scientific_Name”, “massM’, and”massF”. Then pivot the mass columns into long format, using “sex” and “mass” as the new column names. To clean up the text values for the “sex” column, you can pass names_pattern = \"mass(.)\" as an argument in your call to the pivot function. Save this data frame as as mammal_mass. Please glimpse mammal_mass or print the first six rows. (0.25 marks)\nPull out the columns for “Scientific_Name”, “lengthM’, and”lengthF”. Then, as in the last question, pivot the length columns into long format, using “sex” and “length” as the new column names. To clean up the text values for the “sex” column, you can pass names_pattern = \"length(.)\" as an argument in your call to the pivot function. Save this data frame as as mammal_length. Please glimpse mammal_length or print the first six rows. (0.25 marks)\nWere any species included more than once in mammal_mass and mammal_length? If so, please remove any rows that have duplicated sex and species combinations and save the result in two new data frames, called mass_nodup and length_nodup. Please do not include any columns besides “Scientific_Name”, “sex”, “mass”, and “length” in your final data set. Report the number of rows in your final data sets and glimpse/head your final data sets. Hint: for each named species and sex combination, how many rows exist in the data frame? The n() function will be useful here. (1 mark)\nThere is a tidyverse function called full_join() that enables the joining of data sets by a set of shared variables. Please read its documentation. Use full_join() to combine the deduplicated long mass and length data into one data frame called mammal_long. Please glimpse mammal_long or print the first six rows. Hint: the two shared columns to join by have the same name in each data set. (0.5 marks)\nHow many NA values are present in the length column? For the sex-species combos with and without recorded length values, what is the mean mass, grouped by sex? (0.75 marks)",
    "crumbs": [
      "Assignments",
      "Assignment 02: data wrangling"
    ]
  },
  {
    "objectID": "assignments/assignment-02.html#footnotes",
    "href": "assignments/assignment-02.html#footnotes",
    "title": "Assignment 02: data wrangling",
    "section": "",
    "text": "Olofsson J, te Beest M, Ericson L (2013) Complex biotic interactions drive long-term vegetation dynamics in a subarctic ecosystem. Philosophical Transactions of the Royal Society B 368(1624): 20120486. https://dx.doi.org/10.1098/rstb.2012.0486↩︎\nTombak, K.J., Hex, S.B.S.W. & Rubenstein, D.I. New estimates indicate that males are not larger than females in most mammal species. Nat Commun 15, 1872 (2024). https://doi.org/10.1038/s41467-024-45739-5↩︎",
    "crumbs": [
      "Assignments",
      "Assignment 02: data wrangling"
    ]
  },
  {
    "objectID": "assignments/assignment-03.html",
    "href": "assignments/assignment-03.html",
    "title": "Assignment 03: data visualisation and exploration",
    "section": "",
    "text": "1. Visualising plant biomass (3.5 marks)\nTo start this assignment, you will once again be looking at the yearly change in biomass of plants in the beautiful Abisko national park in northern Sweden. We have pre-processed this data and made it available as a csv-file via this link. You can find the original data and a short abstract on dryad. The original study1 is available on an open access license.\nplant_biomass &lt;- \n  read_csv('https://uoftcoders.github.io/rcourse/data/plant-biomass-preprocess.csv',\n           show_col_types = FALSE) %&gt;%\n  rename(dwarf_birch = betula_nana, \n         wavy_hair_grass = deschampsia_flexuosa,\n         crowberry = empetrum_nigrum,\n         bilberry = vaccinium_myrtillus,\n         bog_bilberry = vaccinium_uliginosum,\n         lingonberry = vaccinium_vitis_idaea) %&gt;%\n  pivot_longer(cols = dwarf_birch:lingonberry, names_to = \"species\",\n               values_to = \"biomass\")\n\nprint(plant_biomass)\n\n# A tibble: 1,080 × 6\n    year  site habitat treatment       species         biomass\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt;             &lt;dbl&gt;\n 1  1998     1 Forest  grazedcontrol   dwarf_birch        0   \n 2  1998     1 Forest  grazedcontrol   wavy_hair_grass    7.47\n 3  1998     1 Forest  grazedcontrol   crowberry         22.6 \n 4  1998     1 Forest  grazedcontrol   bilberry         118.  \n 5  1998     1 Forest  grazedcontrol   bog_bilberry      11.9 \n 6  1998     1 Forest  grazedcontrol   lingonberry        8.46\n 7  1998     1 Forest  rodentexclosure dwarf_birch        4.74\n 8  1998     1 Forest  rodentexclosure wavy_hair_grass    3.32\n 9  1998     1 Forest  rodentexclosure crowberry         24.4 \n10  1998     1 Forest  rodentexclosure bilberry          57.0 \n# ℹ 1,070 more rows",
    "crumbs": [
      "Assignments",
      "Assignment 03: data visualisation and exploration"
    ]
  },
  {
    "objectID": "assignments/assignment-03.html#visualising-plant-biomass-3.5-marks",
    "href": "assignments/assignment-03.html#visualising-plant-biomass-3.5-marks",
    "title": "Assignment 03: data visualisation and exploration",
    "section": "",
    "text": "Compare the mean biomass for grazedcontrol with that of rodentexclosure graphically in a line plot. What could explain the big dip in biomass year 2012? (0.75 marks) Hint: The published study might be able to help with the second question\nCompare the mean yearly change in biomass for each species in a lineplot. (0.5 marks)\nWe’ve found that the biomass is higher in the sites with rodent exclosures (especially in recent years), and that the crowberry is the dominant species. Notice how the lines for rodentexclosure and crowberry are of similar shape. Coincidence? Let’s find out! Use a facetted line plot to explore whether all plant species are impacted equally by grazing. (0.75 mark)\nThe habitat could also be affecting the biomass of different species. Explore this in a line plot of the mean biomass over time. (0.75 marks)\nExplore the relationship between species, habitat, and biomass in a box plot. (0.5 marks)\nIt looks like both habitat and treatment have an effect on most of the species! Let’s dissect the data further by visualizing the effect of both the habitat and treatment on each species by faceting the plot accordingly. (0.75 mark)",
    "crumbs": [
      "Assignments",
      "Assignment 03: data visualisation and exploration"
    ]
  },
  {
    "objectID": "assignments/assignment-03.html#customising-plots-1-mark",
    "href": "assignments/assignment-03.html#customising-plots-1-mark",
    "title": "Assignment 03: data visualisation and exploration",
    "section": "2. Customising plots (1 mark)",
    "text": "2. Customising plots (1 mark)\n\nCreate a ggplot theme object that makes five modifications to plot appearance. Save it this way so you can use it in other plots (including outside of this course). You can use the vignettes at the bottom of ggplot’s theme documentation to find arguments you might like to modify. Some components you might like to alter: panel.background to change the colour of the plot body (remember, colour alters plot element outlines and fill alters the body of plot elements), panel.grid to alter the plot grid, strip.text for facet box text, and legend.title to change the size of the text of the legend’s title. (0.5 mark)\nApply this theme to your plot from 1f and add labels (a title, colour legend title, x axis lable, and y axis label). (0.5 marks)",
    "crumbs": [
      "Assignments",
      "Assignment 03: data visualisation and exploration"
    ]
  },
  {
    "objectID": "assignments/assignment-03.html#visualising-mammal-size-dimorphism-3-marks",
    "href": "assignments/assignment-03.html#visualising-mammal-size-dimorphism-3-marks",
    "title": "Assignment 03: data visualisation and exploration",
    "section": "3. Visualising mammal size dimorphism (3 marks)",
    "text": "3. Visualising mammal size dimorphism (3 marks)\nDownload the “SSDinMammals.csv” file from Quercus, the course website, or Data Dryad. The original study2 is quite interesting!\n\nmammal_sizes &lt;- read_csv(\"~/eeb313website/lectures/data/SSDinMammals.csv\",\n                         show_col_types = FALSE)\nprint(mammal_sizes)\n\n# A tibble: 691 × 18\n   Order      Family Species Scientific_Name massM SDmassM massF SDmassF lengthM\n   &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Afrosoric… Chrys… Hotten… Amblysomus hot…  80.6    1.24  66      8.64      NA\n 2 Afrosoric… Chrys… Namib … Eremitalpa gra…  28      6.7   23.1    3.6       NA\n 3 Afrosoric… Tenre… Lesser… Echinops telfa… 102.    19.3   99.9   17.8       NA\n 4 Afrosoric… Tenre… Large-… Geogale aurita    7.3    1.02   7      1.34      NA\n 5 Afrosoric… Tenre… Highla… Hemicentetes n… 111     15.6   98      6.56      NA\n 6 Afrosoric… Tenre… Lowlan… Hemicentetes s… 110.    16.7  108.    16.0       NA\n 7 Afrosoric… Tenre… Short-… Microgale brev…   9.4    1.52   9.7    1.16      NA\n 8 Afrosoric… Tenre… Cowan'… Microgale cowa…  12.4    1.71  12.7    1.83      NA\n 9 Afrosoric… Tenre… Dobson… Microgale dobs…  26.9    4.38  28.9    5.39      NA\n10 Afrosoric… Tenre… Drouha… Microgale drou…  10.1    1.26  12.5    1.79      NA\n# ℹ 681 more rows\n# ℹ 9 more variables: SDlengthM &lt;dbl&gt;, lengthF &lt;dbl&gt;, SDlengthF &lt;dbl&gt;,\n#   n_M &lt;dbl&gt;, n_F &lt;dbl&gt;, n_Mlength &lt;dbl&gt;, n_Flength &lt;dbl&gt;, Comments &lt;chr&gt;,\n#   Source &lt;chr&gt;\n\n\n\nMake a scatterplot with female mass on the x-axis and male mass on the y-axis. Plot a straight line with slope y = x (can use the abline geom) behind the mass data points. Make subplots according to Order, allowing free scaling of the axes. Apply the theme you made in question 2. (1 mark)\nCalculate the proportional mass difference ([massM - massF] / massM) for all the rows in the data set. Separating the data by Order, create a boxplot of these proportional mass differences. Plot a horizontal line behind the data, with a y-intercept that would reflect no mass difference between sexes. Apply your theme from question 2. Finally, ensure the Order text is legible. (1 mark)\nImprove your plot from 3b. Include points behind the boxplot that reflect the number of observations for each point (i.e. sizing by n_M or n_F). You should ensure these points are reasonably visible by using a variant of the point geom (look to the data exploration lecture’s section on boxplot augmentation) and specifying opacity. Visually differentiate the horizontal reference line from the data (using colour, for example). Label your axes and legend. (1 mark)",
    "crumbs": [
      "Assignments",
      "Assignment 03: data visualisation and exploration"
    ]
  },
  {
    "objectID": "assignments/assignment-03.html#footnotes",
    "href": "assignments/assignment-03.html#footnotes",
    "title": "Assignment 03: data visualisation and exploration",
    "section": "",
    "text": "Olofsson J, te Beest M, Ericson L (2013) Complex biotic interactions drive long-term vegetation dynamics in a subarctic ecosystem. Philosophical Transactions of the Royal Society B 368(1624): 20120486. https://dx.doi.org/10.1098/rstb.2012.0486↩︎\nTombak, K.J., Hex, S.B.S.W. & Rubenstein, D.I. New estimates indicate that males are not larger than females in most mammal species. Nat Commun 15, 1872 (2024). https://doi.org/10.1038/s41467-024-45739-5↩︎",
    "crumbs": [
      "Assignments",
      "Assignment 03: data visualisation and exploration"
    ]
  },
  {
    "objectID": "assignments/assignment-04.html",
    "href": "assignments/assignment-04.html",
    "title": "Assignment 04: inference!",
    "section": "",
    "text": "1. Conceptual understanding of probability, random variables, & likelihood (2 marks)\n\\(\\Pr(A) = 0.6, \\Pr(B) = 0, \\Pr(C) = \\Pr(D) = \\Pr(E) = \\Pr(F) = 0.1.\\)\nWhat is the probability of seeing A, B, C, or D in this case? Explain how you got your answer.\nWhat probability distribution would you use to model the random number that counts how many As occurred in the \\(N = 11\\) trials?\nAssuming that we could do keep doing the experiment, what probability distribution would you use to model the random number that counts the number of trials until the 1st C?\n\\[f(x|\\lambda) = \\lambda e^{-\\lambda x}.\\]\nWrite down the likelihood function for the data \\(x_1 = 0.2, x_2 = 1\\), using the density function above. Plug in \\(\\lambda = 1\\) and \\(\\lambda = 2\\) into your expression. Of these values for the rate parameter, which is more likely to explain the data?",
    "crumbs": [
      "Assignments",
      "Assignment 04: inference!"
    ]
  },
  {
    "objectID": "assignments/assignment-04.html#conceptual-understanding-of-probability-random-variables-likelihood-2-marks",
    "href": "assignments/assignment-04.html#conceptual-understanding-of-probability-random-variables-likelihood-2-marks",
    "title": "Assignment 04: inference!",
    "section": "",
    "text": "Suppose you do a random experiment with six outcomes: A, B, C, D, E, F. There are \\(2^6 = 64\\) events (i.e., combinations of outcomes, including the one where nothing happens) that can be formed from these outcomes. List 12 of these events below.\nSuppose the following probabilities measure how likely the six outcomes (A,B,C,D,E,F) are to occur:\n\n\n\n\nSuppose you did the experiment in (a) 11 times (independently, under identical conditions), where the outcomes occur with the probabilities in (b), and you observed the sequence ACCFCAAAAAC. That is, you saw 6 As, 4 Cs, and 1 F.\n\n\n\n\nSuppose we measure the times between earthquakes in Toronto and want to estimate the rate at which such events occur. We model the inter-arrival times using an Exponential distribution with rate \\(\\lambda\\). The time between the first earthquake and the second is \\(x_1 = 0.2\\) decades. The time between the second earthquake and the third is \\(x_2 = 1\\) decades. The probability density function for the Exponential is",
    "crumbs": [
      "Assignments",
      "Assignment 04: inference!"
    ]
  },
  {
    "objectID": "assignments/assignment-04.html#simulating-random-variables-1-marks",
    "href": "assignments/assignment-04.html#simulating-random-variables-1-marks",
    "title": "Assignment 04: inference!",
    "section": "2. Simulating random variables (1 marks)",
    "text": "2. Simulating random variables (1 marks)\n\nWhen working with random numbers, it is often a good idea to use the function set.seed(). This function ensures that the random numbers that are generated are reproducible, including on other machines. Please read the documentation for this function and this blog post to learn more.\n\n\nset.seed(313)\n\n\nSimulate \\(n = 1000\\) realizations (i.e., draws) of a Geometric random variable with success probability \\(p = 0.01\\). Put these realizations in a vector x. Make a histogram of these realizations, using the default number of bins.\nExtract the first element of x, i.e., the first realization of the Geometric(\\(p = 0.01\\)) distribution. How likely is it that this observation arose from a Geometric distribution with probability \\(p = 0.9\\)? How likely is it that the observation arose from a Geometric distribution with probability \\(p = 0.1\\)? Hint: evaluate the Geometric(p) probability mass function at these values of the parameter.\nWhat is the probability that each element of x arose due to a Geometric distribution with \\(p = 0.9\\)? Store these probabilities in a vector, apply log to each element of the vector, and print the sum the elements of the vector of log-transformed probabilities. This is the log-likelihood of the data — i.e., the realizations of the Geometric distribution you simulated in (a) — at \\(p=0.9\\).",
    "crumbs": [
      "Assignments",
      "Assignment 04: inference!"
    ]
  },
  {
    "objectID": "assignments/assignment-04.html#maximum-likelihood-for-binomial-probabilities-3-marks",
    "href": "assignments/assignment-04.html#maximum-likelihood-for-binomial-probabilities-3-marks",
    "title": "Assignment 04: inference!",
    "section": "3. Maximum likelihood for Binomial probabilities (3 marks)",
    "text": "3. Maximum likelihood for Binomial probabilities (3 marks)\nFor this problem, it will be useful to carefully review the likelihood calculation we did in the 2nd inference lecture.\n\nDownload the Farrell and Davies dataset which we analyzed in class from Quercus or the website. Read the data into R as disease_distance.\nUse the unique() function to determine all unique elements of the Host and ParaFamily columns. Then, use the expand.grid() function to generate a data frame with all combinations of hosts and parasite families. Store the data frame of host and parasite family combinations in an object called Combinations. Hint: look at the documentation for expand.grid().\nWrite a function that returns, for a host and parasite family (i.e., a specific row of the Combination data frame you have just made), the maximum likelihood estimate for the probability of death, given infection, of that host with members of the parasite family. Like we did in class, assume deaths are Binomial with a host-specific number of cases and probability of death, and those in different countries and years are independent. If there are no observations for a specific host and parasite family combination, your function should return NA; else, the function should return the value of \\(p\\) which maximizes the likelihood function formed from the data specific to the host and parasite family supplied as arguments to the function. You may also run into issues where the log-likelihood is negative infinity at all \\(p\\); in this case, the function should return NA.\n\nThe skeleton of the function is provided below. The arguments that specify the host species and the parasite family. The cases where an NA should be returned are addressed using nested if {} else {} statements.\n\n## binomial probability evaluate from class\n\nbinomial_evaluator &lt;- function(d, N, p){\n  return(dbinom(x = d, size = N, prob = p)) # log=T returns the log-likelihood of the observation\n}\n\n## skeleton of function\n\nProbDeathEstimator &lt;- function(host = \"Capra_hircus\", para_family = \"Bacillaceae\"){\n  \n  data &lt;- disease_distance |&gt; subset(ParaFamily == paste(para_family) & Host == paste(host))\n  # this subsets the data to the parasite family and host supplied to the function\n  \n  if (nrow(data) == 0){\n    \n    return(NA) # return NA if there are no observations for the specific host and parasite family combination\n    \n  } else{\n    \n    \n    \n    \n    # likelihood calculation (based on data after subsetting to the specific host and family) should go here...\n    \n    \n    \n    \n    if (all(LogLik == -Inf)){\n      \n      ## NOTE: may need to change \"LogLik\" depending on how likelihoods for different values of p are stored/named...\n      ## this naming is consistent with what was done in class\n      \n      \n      \n      \n      return(NA) # if the log-likelihood is -Inf everywhere, return NA\n      \n      \n    } else{\n      \n      \n      \n      return() # return the maximum likelihood estimator for prob of death, given function\n      \n      \n      \n      \n    }\n    \n  }\n  \n}\n\n\nTest the function on the host and parasite family from class. Does it return the same maximum likelihood estimate for \\(p\\)? If not, the function needs to be de-bugged. If so, excellent – you are probably in a good position to finish the problem!\nLoop over all rows of the Combinations data frame you made in (b). For each row, apply the function you made in (d) to the host and parasite family in that row. Store the values returned by the function, for each row, in a vector ProbabilitiesDeath. At the end of the loop, this vector should have length equal to the number of rows in Combinations (i.e., one estimate for host-parasite family pair).\nView data.frame(p = ProbabilitiesDeath, Combinations). Each row includes a host, parasite family, and the estimate probability of death for the host when infected with a member of the parasite family. Using this data frame, visualize the distribution of estimates using a histogram. Then, visualize the distribution for each host (i.e., faceted by host).\n\nWhat do you notice about the distribution(s) of death probabilities?",
    "crumbs": [
      "Assignments",
      "Assignment 04: inference!"
    ]
  },
  {
    "objectID": "assignments/assignment-04.html#more-on-likelihood-confidence-intervals-and-hypothesis-testing-2-marks",
    "href": "assignments/assignment-04.html#more-on-likelihood-confidence-intervals-and-hypothesis-testing-2-marks",
    "title": "Assignment 04: inference!",
    "section": "4. More on likelihood, confidence intervals, and hypothesis testing (2 marks)",
    "text": "4. More on likelihood, confidence intervals, and hypothesis testing (2 marks)\n\nRead the lecture notes for hypothesis testing and confidence interval construction again. They can be found here.\nConsider the following data, and suppose we model the number of Bos taurus deaths due to Dermatophilus congolensis as Poisson(\\(\\lambda\\)). Assume that observations from different years and countries are independent. (This is similar to what we did in class but notice that we have changed from what distribution we use to model the data generative process. The rate parameter \\(\\lambda\\) cannot be interpreted as a probability of death. Instead, \\(\\lambda\\) is the expected number of individuals that die of the disease.)\n\n\ndataNEW &lt;- disease_distance |&gt; \n  subset(Parasite == \"Dermatophilus_congolensis\" & Host == \"Bos_taurus\")\n\nWrite code that returns a plot of the log-likelihood of these data as a function of \\(\\lambda\\). The values of \\(\\lambda\\) on the \\(x\\) axis should range from 0 to 100 (in increments of 0.01).\n\nWhat is the most likely value of \\(\\lambda\\) to explain the data? That is, based on what you saw in the (b), what value of \\(\\lambda\\) is where the log-likelihood reaches its maximum value?\nSuppose we have prior knowledge which indicates the expected number of deaths of this host with this parasite is around 5. We want to test \\(H_0: \\lambda = 5\\) vs \\(H_1: \\lambda \\neq 5\\) at significance level \\(\\alpha = 0.05\\).\n\nCalculate the log-likelihood ratio test statistic for \\(\\lambda_0 = 5\\). Then, using the value you calculated and the fact the LR test statistic has a approximate chi-square distribution with one degree of freedom, determine the probability of observing a test statistic more extreme than the one we saw (i.e., the \\(p\\)-value).\nReport the value of your test statistic, \\(p\\) value, and if you reject the null hypothesis at level \\(\\alpha = 0.05\\).\n\nConstruct a \\(99\\%\\) confidence interval for \\(\\lambda\\) using the log-likelihood ratio test statistic. Based on this confidence interval, would you reject the hypothesis \\(\\lambda = 3.7\\) at level \\(\\alpha = 0.01\\)?",
    "crumbs": [
      "Assignments",
      "Assignment 04: inference!"
    ]
  },
  {
    "objectID": "assignments/assignment-05.html",
    "href": "assignments/assignment-05.html",
    "title": "Assignment 05: linear models, model selection, and multivariate statistics",
    "section": "",
    "text": "1. Conceptual understanding of linear models\nHint: look up the probability density function for the Normal distribution.\nLM_simulator &lt;- function(beta0, beta1, beta2, sigma, n = 100){\n  x1 &lt;- runif(n = n, max = 1000, min = 10)\n  x2 &lt;- rnorm(n = n, mean = 10000, sd = 1000)\n  y &lt;- rnorm(n = n, mean = beta0 + beta1*x1 + beta2*x2, sd = sigma*sqrt(x1))\n  return(cbind(x1, x2, y))\n}\n\ndata &lt;- LM_simulator(beta0 = 0, beta1 = 0.5, beta2 = 0, sigma = 1)\ndata |&gt; ggplot(aes(x = x2, y = y)) + geom_point()",
    "crumbs": [
      "Assignments",
      "Assignment 05: linear models, model selection, and multivariate statistics"
    ]
  },
  {
    "objectID": "assignments/assignment-05.html#conceptual-understanding-of-linear-models",
    "href": "assignments/assignment-05.html#conceptual-understanding-of-linear-models",
    "title": "Assignment 05: linear models, model selection, and multivariate statistics",
    "section": "",
    "text": "Write down the likelihood function for data that arise from the multiple linear regression model \\(Y_i \\sim \\text{Normal}(\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}, \\sigma^2 x_{1i}).\\) Notice that the variance in the observations is proportional to the 1st explanatory variable.\n\n\n\nThe following function simulates data from the model in (a). Explain what each line does.\n\n\n\nWrite a function that evaluates the log-likelihood function which you wrote down in (a). Evaluate the log-likelihood of \\(\\beta_0 = 0, \\beta_1 = 0.5, \\beta_2 = 0, \\sigma = 1\\) using the data simulated in (b). Do the same thing, except using \\(\\beta_0 = 0.1, \\beta_1 = 0.5, \\beta_2 = 0, \\sigma = 1\\).\nUsing the data simulated in (b), regress y on x1 and x2 but not their interaction. Is there an effect of x and how similar is it to the effect specified in the previous code chunk?\nAre the assumptions of of the multiple regression framework (normality, constant error variance, independence of observations) satisfied? If not, explain how you know.",
    "crumbs": [
      "Assignments",
      "Assignment 05: linear models, model selection, and multivariate statistics"
    ]
  },
  {
    "objectID": "assignments/assignment-05.html#implementation-of-generalized-linear-models",
    "href": "assignments/assignment-05.html#implementation-of-generalized-linear-models",
    "title": "Assignment 05: linear models, model selection, and multivariate statistics",
    "section": "2. Implementation of generalized linear models",
    "text": "2. Implementation of generalized linear models\n\nDownload the “SSDinMammals.csv” file from Quercus, the course website, or Dryad. Read it into R as SSDdata.\n\n\nSSDdata &lt;- read_csv(\"~/eeb313website/lectures/data/SSDinMammals.csv\")\n\nThe following chunk creates a new data frame, SSDdataNew which includes a column that indicates if the mean mass of males exceeds that of females for the species. If an entry in this column equals one, the mean male mass exceeds the mean female mass.\n\nSSDdata |&gt; mutate(AreMalesLarger = \n                    case_when(massM &gt; massF ~ 1, massM &lt;= massF ~ 0)) |&gt;\n  group_by(Order) |&gt; mutate(n = n()) |&gt; ungroup() -&gt; SSDdataNew\n\n\nUsing SSDdataNew, fit a generalized linear model to test if number of observations predicts the likelihood that males are larger than females. That is, regress the binary variable indicating if males are larger than females on the number \\(n\\) of observations for an Order. Report the regression coefficient for \\(n\\), the value of the test statistic, and the associated \\(p\\) value.\nInterpret the regression coefficient from (b) – what does it mean? Think about any transformations that may have taken place!\nVisualize the fitted model.\nWhat would you conclude based on b-d about sexual size dimorphism in Orders that have many vs few species? Explain your answer.\nTo determine if the variation in the size of males predicts if males are larger than females, add SDmassM as an explanatory variable to the model you fit in b. Report the regression coefficient that is estimated, as well as the \\(p\\)-value and value of the test statistic. Interpret the coefficient associated to SDmassM – what does it mean?",
    "crumbs": [
      "Assignments",
      "Assignment 05: linear models, model selection, and multivariate statistics"
    ]
  },
  {
    "objectID": "assignments/assignment-05.html#linear-mixed-models-group-by-group-analyses-some-model-selection",
    "href": "assignments/assignment-05.html#linear-mixed-models-group-by-group-analyses-some-model-selection",
    "title": "Assignment 05: linear models, model selection, and multivariate statistics",
    "section": "3. Linear mixed models, group-by-group analyses, some model selection",
    "text": "3. Linear mixed models, group-by-group analyses, some model selection\n\nSuppose you are an respiratory syncytial virus (RSV) researcher and wish to understand how the age of chimpanzees treated with a prospective vaccine affects the concentration of RSV antibodies that are found in the blood at weeks \\(1,2,3,\\dots,10\\) after vaccination. You predict that antibody concentrations will be highest for the individuals which were young when they were vaccinated. You plan to conduct an experiment where \\(n = 50\\) chimpanzees of various ages were vaccinated, and antibody titres are measured for each individual (say, 3 times!) at every week post-vaccination.\n\nYou plan to fit the following linear mixed model:\n\\[\\text{antibody titre} \\sim \\text{age}\\text{+(1+age|individual)}+\\text{(1|week)}.\\]\nThe model is written in the syntax used to specify a model using lme4. What are the fixed and random effects? Explain why it makes sense to include the random effects which are specified.\n\nThe following function simulates data that arise from the LMM specified in the (a).\n\n\nsimulator &lt;- function(age_effect = -5, week_sd = 0.5, individual_sd = 0.2){\n  \n  age &lt;- runif(n = 50, min = 6, max = 10) # get ages for all individuals\n  data &lt;- NULL\n  \n  for (weeks in 1:10){ # loop over weeks in which each individual is measured 3x\n    \n    for (individual in 1:50){ # loop over individuals\n      \n      intercept_individual &lt;- 200*exp(-weeks/100) + rnorm(n = 1, mean = 0, sd = individual_sd)\n      # determine random effect (intercept) for each individual\n      # intercept is 200*exp(-weeks/100), which is declining with weeks\n      # for each individual, baseline antibody concentrations are randomly perturbed from intercept\n      \n      intercept_week &lt;- rnorm(1, mean = 0, sd = week_sd)\n      # determine random effect (intercept) for each week\n      \n      age_effect_individual &lt;- age_effect + rnorm(n = 1, mean = 0, sd = 1)\n      # determine random effect (of age on antibody) for each individual\n      # mean is age_effect, but there is variation around this effect size\n      \n      antibody_titre &lt;- rnorm(n = 3, \n                              mean = intercept_individual + \n                                intercept_week +\n                                age_effect_individual*age[individual],\n                              sd = 1)\n      # draw antibodies 3x for each individual in each week\n      # intercept is random and depends on individual, week\n      # effect of age changes mean antibody titre, but randomly based on the individual\n      \n      data &lt;- rbind.data.frame(data,\n                               cbind(antibody = antibody_titre, \n                                     week = weeks,\n                                     individual = individual,\n                                     age = age[individual]\n                                     )\n                               )\n      # adds row-wise to data frame with all response, covariates\n      \n    }\n  }\n  \n  return(data)\n}\n\nThink about what EACH LINE of this function does and how it relates to the model specified in part (a). Comments have been added to help you understand how the data are simulated. If you have questions, talk to Mete, Zoe, Jessie, or Gavia!\nSimulate data from this model using the default parameters. Plot antibody concentrations against age when vaccinated. Does there appear to be a trend?\n\nHow would you modify simulator() so that the standard deviation in the response (antibody titre) for an individual in a given week is a parameter, \\(\\gamma\\) times the age of the individual? Explain.\nUsing the data which you simulated, fit the model in (a). Report estimates for the fixed effects, including the standard errors, and visualize the values predicted under the fitted model with only the fixed effects (i.e., ignore all random effects). Hint: use the predict() function.\nSplit the simulated data up by week. For the data associated to each week, regress antibody concentration on age at vaccination. Make a plot of the estimated effect of age on antibody concentration vs week. Include the confidence intervals for the effect of age in your plot.\n\nHint: use a for loop or the tidyverse function do().\n\nReport the regression coefficient for age in week 7 (and the associated confidence interval, value of the test statistic, \\(p\\)-value). Explain what the value of the regression coefficient means in plain English. Does the confidence interval for the parameter overlap the true value of the effect, which is equal to -5?\nWhat are the advantages and disadvantages of fitting the full LMM (with individual and week as random effects) or performing a group-by-group analysis? What about an individual-by-individual analysis or an analysis where antibody titres are regressed on age for each individual-week combination – would fitting models at those levels in the model make sense? Explain.\nHow could you modify the model from (a) to have random intercepts dependent on the variables individual and week? What about both random slopes and intercepts dependent on individual and week? Fit these models to the simulated data and compare them to original model, which you fit in (d). Which model best describes the data? Look at the output for the best-fit model – what does it tell us about the variance associated with the random effects?",
    "crumbs": [
      "Assignments",
      "Assignment 05: linear models, model selection, and multivariate statistics"
    ]
  },
  {
    "objectID": "assignments/assignment-05.html#interpretation-of-a-pca",
    "href": "assignments/assignment-05.html#interpretation-of-a-pca",
    "title": "Assignment 05: linear models, model selection, and multivariate statistics",
    "section": "4. Interpretation of a PCA",
    "text": "4. Interpretation of a PCA\nIn Koutsidi et al. 2020, the authors use traits to define the life-history strategies of species in the Mediterranean Sea. Traits related to resource use were used to determine if species with different life-history traits occupy different niches and if these traits are related to habitat use.\nHint: read the abstract, figure 1, figure 2, section 2.4, and section 3.1 to answer these questions.\n\nIn the analysis quantifying ecological niches and niche overlap, how many dimensions define this trait space? What is the one sentence definition of a MCA according to the authors?\nGo to Figure 2:\n\n\nWhat traits are PC1 based on? What traits are PC2 based on?\nCharacterize the traits of species H. huso? What traits does D. passtinaca have?",
    "crumbs": [
      "Assignments",
      "Assignment 05: linear models, model selection, and multivariate statistics"
    ]
  },
  {
    "objectID": "assignments/challenge-assignment.html",
    "href": "assignments/challenge-assignment.html",
    "title": "Challenge assignment",
    "section": "",
    "text": "1. Estimation of the population mutation rate [3.5 marks]\nThe Wright-Fisher model describes how, in a haploid population of constant size \\(N\\), the frequency of two alleles (\\(A_1\\) and \\(A_2\\)) at a locus change due to random sampling of gametes and mutation. The rules of the WF model are very simple — the population size is constant, generations are discrete and non-overlapping, and children choose their parents (regardless of type) at random (i.e., uniformly among the parents). This means that the number of individuals that carry the \\(A_1\\) allele in generation \\(n+1\\) is Binomial(\\(N\\),\\(p_{n}\\)) where \\(p_n\\) is the frequency of \\(A_1\\)s in the previous generation. The model is widely used in population genetics, and can be used to make inferences about the population mutation rate \\(\\mu = 2 N u\\) from present-day allele frequency data, where \\(u\\) is the per-individual per-generation rate at which mutations are introduced in the population.1 The population mutation rate measures the number of mutants that enter the population per generation, which means it has units of individuals per time. In this problem, you will use maximum likelihood to make inferences about this parameter!\nTo learn more about the Wright-Fisher model (including how to simulate realizations), read these notes.\n1a. Read in the “WF_inference_data.csv” dataset, which can be found on the course website. The observations are frequencies of the \\(A_1\\) allele in 52 replicate populations. For the purpose of this exercise, we will treat the allele frequencies as independent. This is reasonable if there is no gene flow between populations sampled.\n1b. It can be shown that, when the WF model has been run out for a long enough time and with a large enough population size, the allele frequencies follow a Dirichlet distribution:\n\\[f(p|\\mu) = \\frac{\\Gamma(2\\mu)}{(\\Gamma(\\mu))^2} (p(1-p))^\\mu,\\]\nwhere \\(\\Gamma(\\cdot)\\) is the Gamma function (a generalization of the factorial). Write a function eval_dirichlet() that evaluates the probability density at a set (vector) of allele frequencies and value of \\(\\mu\\). To evaluate the Gamma function at a scalar value, you can use the base R function gamma().\nTest your function on the vector of allele frequencies p = c(0.81,0.9) and mutation rate mu = 0.0001. [0.5 mark]\n1c. Assuming the allele frequency data are independent and all follow the Dirichlet distributed, the log-likelihood function for your data is\n\\[\\sum_{i=1}^{52} \\ln f(p_i|\\mu),\\]\nwhere \\(p_i\\) is the allele frequency in the \\(i\\)th population and \\(\\mu\\) is a specific value of the mutation rate. The function you wrote in eval_dirichlet function evaluates \\(f(p|\\mu)\\) for allele frequencies that are supplied as a vector in its first argument and a specific value of the mutation rate which is specified in its second.\nCalling the function you wrote in (b), write a function that calculates the log-likelihood of the allele frequency data from the 52 populations. The function should have an argument corresponding to the value of the population mutation rate \\(\\mu\\). [0.5 mark]\n1d. Using the function from (c), evaluate the log-likelihood across a range of values for the parameter. A suitable range to use is from 10 to 30. Plot the likelihood as a function of \\(\\mu\\) over this range. What is the maximum likelihood estimator for \\(\\mu\\)? [1.5 mark]\n1e. Build a \\(95\\%\\) confidence interval for \\(\\mu\\) by determining what values of \\(\\mu\\) are such that the log-likelihood is within 1.92 units of its maximum value, i.e., the value assumed when the estimator for \\(\\mu\\) is equal to its MLE. (The 1.92 quantity is the cutoff based on the approximate \\(\\chi^2\\) distribution of the likelihood ratio test statistic, which we have previously used to build confidence intervals and perform hypothesis tests.) [0.75 marks]\n1f. Based on the confidence interval constructed in the previous question, test the hypotheses \\(H_0 \\colon \\mu = 10\\) vs \\(H_1 \\colon \\mu \\neq 10\\) at significance level \\(\\alpha = 0.05\\). [0.25 marks]",
    "crumbs": [
      "Assignments",
      "Challenge assignment"
    ]
  },
  {
    "objectID": "assignments/challenge-assignment.html#inference-on-scaled-size-differences-5.5-marks",
    "href": "assignments/challenge-assignment.html#inference-on-scaled-size-differences-5.5-marks",
    "title": "Challenge assignment",
    "section": "2. Inference on scaled size differences [5.5 marks]",
    "text": "2. Inference on scaled size differences [5.5 marks]\n2a. Load the “SSDinMammals.csv” dataset into R.\n2b. Mutate the dataframe to include a column, called PropMassDiff, which is male mass minus female mass divided male mass. (In other words, if \\(M\\) is male mass and \\(F\\) female mass, PropMassDiff is \\((M-F)/M\\).)\nVisualize the distribution of relative size differences using a rug plot. (You may need to look up the geom to do this…) There should be no y axis – we will add one later. [0.25 marks]\nDo you think, based on the histogram, that males are larger than females?\nAssume that proportional mass differences are independent and Normally distributed with an unknown mean and variance. The goal of this question will be to determine the most likely mean and variance for the scaled differences (a measure of the relative difference in mass between males and females), based on the likelihood function. Conceptually, this is similar to (a) — but requires looping over two parameters (the mean and variance of the Normal distribution), rather than one to determine where the log-likelihood achieves a maximum. The following questions walk you through how to maximize the log-likelihood with respect to both parameters.\n2c. Generate a data frame/grid of means and variance combinations for which you will calculate the log-likelihood. Use mean values that range from -1 to 1 in increments of 0.005 and variance values from 0.01 to 0.1 in increments of 0.001. [0.25 marks]\n2d. Write a function which evalutes the log-likelihood of observing the proportional size difference data, given we have assumed the observations are Normal and independent, for specific values of the Normal distribution (i.e., the mean and the variance). Because the mean and variance are the parameters we want to estimate, your function should have two arguments – the mean and variance of the Normal – and should return the associated log-likelihood. Note: dnorm() has the standard deviation as an input, not variance. To get the standard deviation from the variance, take the square root. [0.5 marks]\n2e. Loop over rows of the grid of values you generated in (c). Store the log-likelihood values in a vector and combine this vector with the data frame of parameter (i.e., mean and variance) combinations. [0.75 marks]\n2f. What is the most likely combination of the mean and variance for the proportional sizes, assuming normality and independence of observations? [0.25 marks]\n2g. On top of the rug plot you made in (b), plot the distribution associated to the maximum likelihood estimator which you calculated in (f). To do this, evaluate the probability density for the Normal at the mean and variance which you have just estimated and overlay the values of the density on the rug plot. [1.25 marks]\n2h. Based on the estimates for the mean and variance in the distribution of proportional size differences that you obtained in (f), do you think males are larger than females on average? Explain what property/properties of the previous plot led you to your conclusion. [0.25 marks]\n2i. Group dataframe you created in (b) by Family and, for each group, calculate the maximum proportional size differences among all species. Store these data in a column called PropMassDiffMax. (This column represents the greatest relative deviation in size between males and females for species in the Family.) Then visualize the distribution of maximum proportional size differences using a histogram. [0.5 marks]\n2j. A common way extreme value statistics are modeled is using the Gumbel distribution. Assume that the maximum proportional size differences among families are independent and Gumbel distributed.\nFind the maximum likelihood estimator for the two parameters of the Gumbel distribution, given these data. Report the MLEs for the location and scale parameters of the Gumbel. [1.5 marks]",
    "crumbs": [
      "Assignments",
      "Challenge assignment"
    ]
  },
  {
    "objectID": "assignments/challenge-assignment.html#a-mathematical-model-of-mrnaprotein-production-3-marks",
    "href": "assignments/challenge-assignment.html#a-mathematical-model-of-mrnaprotein-production-3-marks",
    "title": "Challenge assignment",
    "section": "3. A mathematical model of mRNA/protein production [3 marks]",
    "text": "3. A mathematical model of mRNA/protein production [3 marks]\nLet \\(M\\) be the number of mRNAs in a cell and \\(P\\) be the number of proteins that are made from that mRNA. Suppose mRNAs are made at rate \\(\\mu\\), degraded at rate \\(\\gamma\\), and translated into protein at rate \\(\\beta\\). Protein molecules degrade at rate \\(\\nu\\). A model for the number of mRNA and protein molecules is as follows:\n\\[\\frac{d M}{d t} = \\mu - \\gamma M - \\beta M\\] \\[\\frac{d P}{d t} = \\beta M - \\nu P\\]\n3a. Numerically solve this system of equations from time \\(t=0\\) to time \\(t=100\\) in increments of 0.1. Use the following parameter values: \\(\\mu = 25, \\gamma = 0.1, \\beta = 0.2, \\nu = 0.01, M(0) = 50, P(0) = 0\\).\nPlot \\(M(t)\\) from \\(t=0\\) to \\(t=100\\). [1.5 mark]\n3b Write a function to determine the solution to the system in (a) at time \\(t=100\\) for a given value of \\(\\nu\\), and with all other parameters and initial conditions fixed at the values provided in (a). Plot the solution at time \\(t=100\\) for values of \\(\\nu\\) (i.e., protein degradation rate) between 0 and 1 in increments of 0.01. \\(\\nu\\) should be on the x axis and the solution at time 100 on the y. Hint: to check if your answer is correct, see if it matches your intuition about how protein concentrations should change as the rate of degradation increases. [1.25 marks]\n3c. Interpret the results from (a) and (b). How do mRNA concentrations behave over time and how does (qualitatively) the steady-state concentration of protein depend on the degradation rate \\(\\nu\\). [0.25 marks]",
    "crumbs": [
      "Assignments",
      "Challenge assignment"
    ]
  },
  {
    "objectID": "assignments/challenge-assignment.html#take-a-break-1-mark",
    "href": "assignments/challenge-assignment.html#take-a-break-1-mark",
    "title": "Challenge assignment",
    "section": "4. Take a break [1 mark]",
    "text": "4. Take a break [1 mark]\nExecute the following code chunk:\n\nlibrary(praise)\nreplicate(100, praise())",
    "crumbs": [
      "Assignments",
      "Challenge assignment"
    ]
  },
  {
    "objectID": "assignments/challenge-assignment.html#glms-power-analysis-and-fitting-7-marks",
    "href": "assignments/challenge-assignment.html#glms-power-analysis-and-fitting-7-marks",
    "title": "Challenge assignment",
    "section": "5. GLMs: power analysis and fitting [7 marks]",
    "text": "5. GLMs: power analysis and fitting [7 marks]\nThe horseshoe crab Limulus polyphemus has two male reproductive morphs; the smaller males have a a special appendage known, which are used to attach to female crabs. When female crabs dig a nest and lay eggs on the beach, the attached male can then fertilize the eggs. Alternatively, “satellite” males can crowd around nesting pairs and then fertilize eggs laid. In this question, you will\n\ndo a power analysis to determine the effect size, under a Poisson regression model2, is determined to be significant at least 99% of the time\nfit a Poisson regression model to satellite crab count data to determine if female traits are associated with more satellite crabs\ninterpret the Poisson regression coefficients (be sure to make note of the units!)\nperform model selection!\n\nThe data are retrieved below. Variables include female color, spine condition, carapace width (cm), mass (kg), and number of satellite males.\n\nsatellites_data &lt;- read_csv(\"https://eeb313.github.io/lectures/data/satellites.csv\")\n\nRows: 173 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): color, spine\ndbl (3): width.cm, nsatellites, mass.kg\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(satellites_data)\n\n# A tibble: 6 × 5\n  color        spine    width.cm nsatellites mass.kg\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n1 medium       both.bad     28.3           8    3.05\n2 dark-medium  both.bad     22.5           0    1.55\n3 light-medium good         26             9    2.3 \n4 dark-medium  both.bad     24.8           0    2.1 \n5 dark-medium  both.bad     26             4    2.6 \n6 medium       both.bad     23.8           0    2.1 \n\n\nBefore we analyze the data, we will determine the effect size we can reliably estimate from the data at hand at a given significance level. This is typically done before collecting data, but for the purposes of demonstration we will have you do it right before diving into fitting a GLM to the data.\n4a. Define statistical power in the context of regression (i.e., where the null hypothesis is that an effect size is zero and the alternative hypothesis is that it is non-zero) in your own words. [0.25 marks]\n4b. The following function draws observations from a Poisson regression model \\(Y \\sim \\text{Poisson}(\\lambda)\\), where \\(\\log \\lambda = \\beta_1 x\\). The parameter \\(\\lambda\\) is the expected number of satellite males, and \\(\\beta_1\\) is the size of the effect of increasing female carapace width \\(x\\) on \\(\\log \\lambda\\). The function has one scalar argument, beta. (This means beta = 3 is a reasonable input but beta = c(3,4) is not.) The function returns a data frame of trait values x in the first column and associated number of satellite crabs y in the second.\n\nn &lt;- 173\n\ndata_generator &lt;- function(beta){\n  x &lt;- sample(size = n, seq(20, 35, by = 0.001), replace = T)\n  obs &lt;- c()\n  \n  for (i in 1:n){\n    obs[i] &lt;- rpois(1, exp(beta*x[i]))\n  }\n  \n  return(data.frame(x, y = obs))\n}\n\nUse this function to simulate draws from the statistical model when \\(\\beta=0.0001,0.001,0.01,0.1\\). Bind these observations into a data frame with a column that specifies the effect size associated to a given observation. Display the head of this data frame, and make a scatter plot of \\(y\\) against \\(x\\), i.e., the number of satellite crabs against carapace width, faceted by effect size. Allow the \\(y\\) axis on each plot to be free. [1 mark]\n4c. Write a function to determine, for a given effect size, the power of a Poisson regression of number of satellite males on carapace width. To do this, you will have to\n\nwrite a for loop to simulate sims = 1000 draws of the Poisson regression model via data_generator()\nfit a Poisson regression of number of satellite males on carapace width using each simulated dataset\nextract the \\(p\\)-values associated to each model\ncalculate outside of the for loop what proportion of the \\(p\\)-values which are \\(&lt; 0.05\\)\n\nThe proportion of \\(p\\) values which are \\(&lt; 0.05\\) is your estimate of power at the effect size, sample size, and significance level specified. This is because it gives, for a large number of draws from the Poisson regression model at a given effect and for \\(x\\) distributed according to our expectations for carapace width, the fraction of fitted models for which the effect was determined to be \\(\\neq 0\\). [1.75 marks]\n4d. Using the function from the previous question, estimate the power of the Poisson regression model for \\(\\beta = 0,0.025,0.05,0.075,0.1\\). Display the effect sizes and associated power estimates in a table. At what effect size is the power for a Poisson regression with \\(n = 173\\) observations &gt;99%? [0.75 marks]\n4e. Using the satellites data, fit a Poisson regression of the number of satellite males on carapace width. Report the regression coefficients, associated \\(p\\) values, and AIC. [0.5 marks]\n4f. Recall the description of the Poisson regression model in question 4b. How should we interpret the regression coefficient associated to carapace width? Hint: think about transformations applied to the response. [0.5 marks]\n4g. Plot the data and the fitted model from 4e. The regression line should be curvilinear on the data scale. [1 mark]\n4h. Using the satellites data, fit the following Poisson regressions:\n\nthe number of satellite males on female mass\nthe number of satellite males on carapace width and female mass\nthe number of satellite males on carapace width, female mass, and their interaction\n\nPerform model selection: report which of these models (including the one from 4e) is the best fit to the data. Report the model that is the best fit to the data and its AIC. Are there any models with AICs that are close? What does this mean? [1.25 mark]",
    "crumbs": [
      "Assignments",
      "Challenge assignment"
    ]
  },
  {
    "objectID": "assignments/challenge-assignment.html#footnotes",
    "href": "assignments/challenge-assignment.html#footnotes",
    "title": "Challenge assignment",
    "section": "",
    "text": "To make inference under the model tractable, it is actually an approximation to the discrete WF model is used, in which it is assumed the \\(N\\) is large and the per-individual mutation rate is small.↩︎\nThis is a special kind of generalized linear model! The Poisson distribution models count data, like the number of satellite males at a beach.↩︎",
    "crumbs": [
      "Assignments",
      "Challenge assignment"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Project description",
    "section": "",
    "text": "Option 1: Hypothesis-driven project\nGroups will formulate their own hypotheses based on their interests within ecology and evolution. Groups will test predictions borne out of their hypotheses with reproducible and quantitative analysis techniques (e.g., ANOVA). If your group has an idea for statistical analyses that are beyond the scope of the course, please let us know. We are happy to support any groups who want to learn new tools, but expect that these groups are ready to learn how these tools work on their own; we hope to equip you with enough understanding to learn new things independently. Finally, the work must be original – while we may be repurposing data, we will not be simply redoing analyses. Keep in mind also that any work you do as part of this course may not be submitted for credit in another course (such as a fourth-year research project) and vice versa. While you may not submit your work for this course for credit in another course, you are welcome to publish or present your work in an academic setting. (In fact, if you do so, please let us know!)\nA note about community/citizen science websites: since the data is community-controlled, it may not always be research quality. There may be incorrect species IDs, inaccurate geolocations or time of observations, or discrepancies in protocols. When working with community science data, make sure that the data is cleaned and wrangled so that it is reliable. Quality control is a good first step when working with data, as simple errors can exist in any dataset.",
    "crumbs": [
      "Project",
      "Project description"
    ]
  },
  {
    "objectID": "projects.html#option-1-hypothesis-driven-project",
    "href": "projects.html#option-1-hypothesis-driven-project",
    "title": "Project description",
    "section": "",
    "text": "What is a hypotheses? What is a prediction?\nA hypothesis is a testable and falsifiable statement that offers a possible explanation of a phenomenon based on background knowledge, preliminary observations, or logic.\nE.g., Primary productivity is an important driver of mammal species richness.\nA prediction is based on a hypothesis. It is meant to describe what will happen in a specific situation, such as during an experiment, if the hypothesis is correct.\nE.g., If primary productivity is an important driver of mammal species richness, then more mammalian species would be found in sites with more plant biomass (proxy for primary productivity) compared with sites with less plant growth.",
    "crumbs": [
      "Project",
      "Project description"
    ]
  },
  {
    "objectID": "projects.html#option-2-modeling",
    "href": "projects.html#option-2-modeling",
    "title": "Project description",
    "section": "Option 2: Modeling",
    "text": "Option 2: Modeling\nGroups will develop a mathematical model to answer a question in ecology and/or evolution they find interesting. There are many reasons to develop models: they help clarify assumptions, generate predictions, nullify hypotheses, provide mechanistic explanations for observed data, and help us know what kinds of data to look for. New models almost always build on existing and well-studied ones (e.g., the Lotka-Volterra model). The fact models are simplifying representations of the real world is by design! The goal of building a model is to identify the key features that make a process interesting, represent the process mathematically (and, in doing so, clarify what assumptions are being made!), characterize the behaviour of the model, and from this characterization draw conclusions about how the process being modelled works. Characterization of a model can involve mathematical analysis, simulation, and confrontation with data.\nThe key steps in this project are to 1) identify an interesting question in ecology or evolution, 2) develop (and likely revise) a model to address that question, 3) characterize the behaviour of the model, and 4) draw biological conclusions from the model and its characterization.\nIf you are interested in modeling, let Mete and Zoë know as soon possible!",
    "crumbs": [
      "Project",
      "Project description"
    ]
  },
  {
    "objectID": "projects.html#option-3-simulation-study",
    "href": "projects.html#option-3-simulation-study",
    "title": "Project description",
    "section": "Option 3: Simulation study",
    "text": "Option 3: Simulation study\nSimilar to Option 1, groups that do a simulation study will formulate hypotheses and use reproducible and quantitative analysis techniques to test predictions borne out of those hypotheses. The difference is that students will simulate their own data, instead of using an existing dataset. One reason to do a simulation study is to see what kind of data would be needed to test a hypothesis in the field, e.g., how much data would be needed to find a significant association between response and predictor variables.\nIf you are interested in doing a simulation study, let Mete and Zoë know as soon possible!",
    "crumbs": [
      "Project",
      "Project description"
    ]
  },
  {
    "objectID": "projects.html#individual-interest-description",
    "href": "projects.html#individual-interest-description",
    "title": "Project description",
    "section": "Individual interest description",
    "text": "Individual interest description\nDue Sept 12th, worth 1% of final grade\nTo make sure you pair up with group mates who share common interests, we ask that you write a short (4-5 sentences) description of your interests in ecology and evolution. Please do so in RStudio by creating an .Rmd file and knitting it into a pdf.\nHere are some discussion questions to help you:\n\nWhat is a scientific paper or popular science article you read (or a podcast you listened to) recently that you found interesting (how microbial communities differ among environments, frequency of herbicide resistance alleles in weed populations, bird species richness in regions that have experienced climatic shifts, understanding the relationship between longevity and traits like body size in mice…)?\nWhat is your favourite EEB course so far? Why did you like it?\nThinking about EEB professors, was there anyone whose work you are particularly interested in?\nBrowse through some recent issues of broad scope EEB journals such as Trends in Ecology and Evolution and Annual Review of Ecology, Evolution, and Systematics. Any articles catching your eyes?\nCheck out this paper. Any of those questions spark your interest?",
    "crumbs": [
      "Project",
      "Project description"
    ]
  },
  {
    "objectID": "projects.html#project-proposal",
    "href": "projects.html#project-proposal",
    "title": "Project description",
    "section": "Project proposal",
    "text": "Project proposal\nDue Oct 17rd, worth 3% of final grade\nGood research takes time! The purpose of the proposal is to get your group started on this process early on so that you will have sufficient time to do your project justice. This will also serve as official documentation of your project development process. Your projects will likely evolve over time, and there can be many reasons for this. For instance, as you explore your data, you might be inspired to ask different questions, or you may need to refine your hypotheses due to limitations in the data.\nInclude the following information in your proposal:\n\nOption 1: your hypotheses and predictions (point form or short paragraph) and data source (short paragraph). Include a citation, a brief description of how the data was collection, and which section of the dataset you plan to use in your analysis (e.g., which columns).\nOption 2: a question you want to answer using a mathematical model (short paragraph describing the problem and the value modeling may add). Be sure to include a description of the variables that you may want to track and the kind of model you envision using.\nOption 3: same as 1, except with a description of how to simulate the data.",
    "crumbs": [
      "Project",
      "Project description"
    ]
  },
  {
    "objectID": "projects.html#mid-project-update",
    "href": "projects.html#mid-project-update",
    "title": "Project description",
    "section": "Mid-project update",
    "text": "Mid-project update\nDue Nov 21st, worth 6% of final grade\nThe purpose of the mid-project update is to ensure you are on track with your projects. By now, you should have completed your exploratory data analyses, modelling, or simulation. You should have also solidified your hypotheses, predictions, and analyses plan (i.e., the Methods section of your final report!).\nInclude the following information in your mid-project report:\n\nOptions 1 and 3:\n\n\nYour hypotheses and predictions (point form or short paragraph). If these differ from the ones in your proposal, explain clearly the rationale for the change.\nA detailed description of your data (a paragraph), including how the data was collected or simulated, along with any manipulation(s) you performed to get your data ready for the analysis.\nYour analysis plan (a paragraph): describe the statistical test(s) that you will use to test each prediction, including how you will validate the assumptions of each test.\n\n\nOption 2:\n\n\nA detailed description of the question you want to answer, any previous work (modelling and otherwise), the model you have built to answer this question, and your modelling assumptions.\nDetailed descriptions of the model analysis and biological interpretations of the results so far.\nYour analysis plan (a paragraph): describe additional analysis that you will do and any assumptions you would like to relax.",
    "crumbs": [
      "Project",
      "Project description"
    ]
  },
  {
    "objectID": "projects.html#presentation",
    "href": "projects.html#presentation",
    "title": "Project description",
    "section": "Presentation",
    "text": "Presentation\nDue Dec 3rd, worth 10% of final grade\nThe presentations will be held on the last day of class during regular class hours (Dec 3rd, 2-4 pm). Each presentation will be 10 minutes long, followed by 2 minutes of questions from the audience. If you cannot make it to class, please get in touch with us to make alternative arrangements no later than Nov 28th.",
    "crumbs": [
      "Project",
      "Project description"
    ]
  },
  {
    "objectID": "projects.html#report",
    "href": "projects.html#report",
    "title": "Project description",
    "section": "Report",
    "text": "Report\nDue Dec 10th, worth 20% of final grade\nThis report will be styled as a journal article, with these sections:\n\nAbstract\nIntroduction\nMethods (including “Data Description” and “Data Analysis” subsections)\nResults\nDiscussion\nReferences\nSupplementary material consisting of data and code required to reproduce analysis\n\nFor your sake (and ours), we are enforcing a two page limit (single spaced, excluding figures, tables, code, references, and appendices). Please use a standard font, size 12, with regular margins. One goal of this assignment is to write clearly and concisely – it is often clarifying to put your analyses in as few words as possible.\nFor the report, you are expected to:\n\nPut your research questions in the context of existing research and literature.\nHave clear and explicit objectives, hypotheses, and/or predictions.\nAdequately describe and properly cite the data source(s) you will analyze. If your project involves modeling, describe other modeling work that is relevant.\nDescribe your analysis in sufficient detail for others to understand.\nDiscuss the interpretation of your results and their implications.\n\nThe data and code associated with your report is expected to be entirely reproducible. Your supplementary files must include the following:\n\nA description of what every column/row in your submitted data file.\nA well-annotated R script or R notebook file. We must be able to run your code once you submit the project. This lesson on best practices for writing R code is a good starting place. Also check out this coding style guide and these simple rules on how to write code that is easy to read.\n\nHermann et al. 2016 is a great example of what we expect your code to look like. Refer to their supplementary materials for examples of how to describe your data set and how to annotate your code.",
    "crumbs": [
      "Project",
      "Project description"
    ]
  },
  {
    "objectID": "projects.html#individual-interest-description-1",
    "href": "projects.html#individual-interest-description-1",
    "title": "Project description",
    "section": "Individual interest description",
    "text": "Individual interest description\n1 marks total\nThis part of the project will be graded based on completion. That said, it will help determine your group-mates. Make sure to spend some time on it and reflect on what questions in EEB you would like to work on.",
    "crumbs": [
      "Project",
      "Project description"
    ]
  },
  {
    "objectID": "projects.html#project-proposal-1",
    "href": "projects.html#project-proposal-1",
    "title": "Project description",
    "section": "Project proposal",
    "text": "Project proposal\n3 marks total\nOption 1: Two marks for your hypotheses and associated predictions, and one mark for a description of your data source(s). Students are expected to demonstrate effort in formulating hypotheses and predictions, and identifying a suitable dataset.\nOption 2: 1.5 marks each for 1) a clear description of the question or problem in ecology or evolution you would like to address using a model, and 2) a description of the kind of model you envision using, including what variables to track.\nOption 3: One mark for your hypotheses and associated predictions, and two marks for describing the appropriate analyses.\nThese components will be graded mostly on completion. The purpose of this assignment is to ensure you start early and are heading towards the right track.",
    "crumbs": [
      "Project",
      "Project description"
    ]
  },
  {
    "objectID": "projects.html#mid-project-update-1",
    "href": "projects.html#mid-project-update-1",
    "title": "Project description",
    "section": "Mid-project update",
    "text": "Mid-project update\n6 marks total\nOptions 1 and 3: Three marks are given to clearly stating hypotheses and predictions. In the case that these are different from those in the proposal, the rationale for refinement needs to be clearly explained.\nEach of the following criteria are scored out of 3: 3 == excellent, 2 == good, 1 == acceptable, but needs improvement.\n\nData description\n\nThe data source(s) are sufficiently described, specifically, where was the obtained and how it was originally collected.\nThe data is sufficient described, including any initial observations from your exploratory data analyses.\nThe suitability of the data is justified.\nAny manipulations done to the data are thoroughly explained and well-justified.\n\nData analysis plan\n\nClearly lay out the statistical test(s) you will use to test each prediction.\nState how you will validating assumptions associated with each statistical test.\n\n\nOption 2: Each of the following criteria are scored out of 3: 3 == excellent, 2 == good, 1 == acceptable, but needs improvement.\n\nDescription of question, previous work, the model, modeling assumptions, and any predictions you have ahead of the analysis\n\nThe question you want to address and previous work in that direction (modeling or otherwise) is described in detail.\nThe relationship between the question/problem and modeling approach is clear and well-justified.\nModeling assumptions and choices (including limitations) are clear and well-motivated.\nPredictions for how the model will behave, what it might have to say about the question/problem, etc. are inclued and well thought out.\n\nAnalysis and analysis plan\n\nThe details of all analysis (mathematical or computational) are explained clearly.\nThe biological interpretations of results so far are clearly presented and their validity/applicability is discussed.\nClearly lay out plans for remaining analysis (e.g., relaxing model assumptions) and justify why they are reasonable.",
    "crumbs": [
      "Project",
      "Project description"
    ]
  },
  {
    "objectID": "projects.html#the-presentation",
    "href": "projects.html#the-presentation",
    "title": "Project description",
    "section": "The presentation",
    "text": "The presentation\n10 marks total\nEach of the following criteria are scored out of 3: 3 == excellent, 2 == adequate, 1 == needs improvement.\n\nContent – background and methods\n\nThe context for the study, along with hypotheses and predictions, are clearly set up.\nData source(s), manipulations, and statistical tests used are succinctly and adequately described.\nIf modeling, the relationship between the question/problem addressed and modeling approach is well-explained, and previous work (modeling or otherwise) is discussed.\n\nContent – results and conclusions\n\nResults are accurately described and interpreted, with particular attention to how they related to the hypotheses and predictions the group set out to test.\nThe conclusion to the study is succinct and clear.\n\nDelivery\n\nAll students participated in presenting the information.\nAll students spoke clearly and without jargon.\nThe presentation is well organized and ideas flowed naturally from one to the next.\nThe presentation is well rehearsed and is an appropriate length.\nFigures are easy to read (e.g., axis labels are big enough to read and are informative) and are explained thoroughly (e.g., x and y axis and what each data point is).\n\n\nThe final 1 mark will be assigned to the question period, and students will be assessed on whether they are able to answer questions thoughtfully.",
    "crumbs": [
      "Project",
      "Project description"
    ]
  },
  {
    "objectID": "projects.html#the-report",
    "href": "projects.html#the-report",
    "title": "Project description",
    "section": "The report",
    "text": "The report\n20 marks total\nEach of the following criteria are scored out of 4: 4 == excellent, 3 == good, 2 == acceptable, 1 == needs improvement.\n\nContent and concepts\n\nAuthors demonstrate a full understanding of the existing literature on the topic, and these concepts are critically integrated into their own insights.\nOptions 1 and 3: Hypotheses and predictions are clearly defined, and rational for choosing/simulating this data is justified.\nOption 2: The question, modeling approach, and relevant work are thoughtfully explained; the rationale for using the model (and its assumptions) is justified.\n\nCommunication\n\nWriting is succinct, clear, logical, and free of grammatical and spelling errors.\n\nAnalysis: see below.\nResults\n\nResults are accurately and sufficiently described.\nConclusions are supported by evidence.\nFigures and tables are clearly presented and are informative.\n\nCoding style and reproducibility\n\nData and code are well-organized and well-documented.\nThe analysis is easily reproducible.\nAll team members have pushed to a common GitHub repo.\n\n\nNote: marks for the 3rd criterion (Analysis) depend on if groups did a modeling or data-driven project:\nOptions 1 and 3: Statistical analysis\n\nStatistical tests chosen or modeling choices made are appropriate.\nAssumptions for each statistical test is validated.\nLimitations in the data and analysis are discussed.\n\nOption 2: Analysis of model\n\nCharacterization of the model is appropriate and explained in detail.\nImportantly, biological conclusions explained in detail and in terms of the processes described (or not described) by the model.\nLimitations of modeling assumptions are discussed, and extensions are proposed.\n\nPlease note that we are only going to be marking the two pages of your report. Please do not go over the page limit (with the exception of tables, figures, references, and appendices).",
    "crumbs": [
      "Project",
      "Project description"
    ]
  },
  {
    "objectID": "projects.html#issues-working-in-groups",
    "href": "projects.html#issues-working-in-groups",
    "title": "Project description",
    "section": "Issues working in groups",
    "text": "Issues working in groups\nIf you are having trouble working with your group, e.g., because you feel like the work is not being equitably divided, please let us know as soon as possible. We will work with you and your group to identify a solution that works for everyone. Do not wait until the last minute to let us know that your group has been having trouble – at that point, there is little we can do to fix the situation. Moreover, we expect group members to communicate with each other and to try to work out their concerns before we become involved.",
    "crumbs": [
      "Project",
      "Project description"
    ]
  },
  {
    "objectID": "projects_databases.html",
    "href": "projects_databases.html",
    "title": "Some open-access databases",
    "section": "",
    "text": "Below are some resources that would be a good to look at if you are in search of data for the term project, or in search of a question in ecology or evolution:\n\nGenBank: annotated collection of all publicly available DNA/protein sequences. It is possible to download sequences manually, but command line tools can help to automate the process.\nPanTHERIA: database of ecology, life history, and geography of all extant and recently extinct mammal species. Includes body size, lifespan, litter size, and other trait data at the species level.\nThe Open Traits Network has information about trait datasets for other taxa – including spiders, nematodes, amphibians, and birds!\nGene Expression Omnibus: repository of gene expression, methylation, and annotated genomic data which are (like GenBank) most readily accessible using command line tools.\nContinuous Plankton Recorder Survey: data (going back to 1958!) on northern hemisphere plankton species, including the location (latitude, longitude) and date of sampling.\nRed-backed salamander abundance: abundance of red-backed salamanders from 4 sites in the Bruce Peninsula from 2004 to 2017.\nNorth American Bird Breeding Survey: repository containing information regarding the number of birds at multiple sites in North America. Many datasets of varying size that need to be linked together.\nMalaria Atlas Project: publicly available and up-to-date malaria prevalence and distribution data. Vector distribution, bednet coverage, etc. data also available.\nGISAID hosts much of the available sequence data for rapidly-evolving RNA viruses such as influenza, SARS-CoV-2, monkeypox, and RSV as well as relevant meta-data (e.g., host species).\nThe Verena Institute has developed a large collection of datasets on the interaction between pathogens, especially viruses, and their hosts. These datasets include information about surveillance/sampling, as well as more biological details.\n\nLet Mete and Zoe know if data from any of these resources interests you, or if you would like to discuss where data to answer questions you find interesting might be found…",
    "crumbs": [
      "Project",
      "Some open-access databases"
    ]
  },
  {
    "objectID": "review-for-challenge.html",
    "href": "review-for-challenge.html",
    "title": "Review for challenge assignment",
    "section": "",
    "text": "1. Maximum likelihood!\n1a. Load the “SSDinMammals.csv” dataset into R.\n1b. Visualize the distribution of male over female masses. What would it mean if the male over female mass assumed a value &gt;1? Explain in words.\n1c. Assume that observations of mass divided by female mass are distributed according to a Gamma distribution with shape parameter \\(\\alpha\\) and scale parameter \\(\\beta\\). Write down a function Gamma_eval which, for specific values of these parameters, evaluates the log-likelihood of observing the data.\n1d. Using maximum likelihood, determine the most likely values of the shape and scale parameters to have given rise to the data (i.e., all male over female masses). To do this, you must loop over combinations of the shape and scale parameters and determine the log-likelihood for each combination. Use values of the shape parameter between 1 and 3 in increments of 0.05 and of the scale parameter between 0.001 and 0.5 in increments of 0.001.\n1e. Plot the best-fitting Gamma distribution (i.e., the distribution associated to the maximum likelihood estimates for the shape and scale parameters you determined in the previous question) over the data.\n1f. Find a \\(95\\%\\) confidence interval for the shape parameter. You can do this by setting the scale parameter equal to its maximum likelihood estimate and then identifying the set of values for the shape parameter for which the log-likelihood is within 1.92 units of its maximum.\n1g. What is the mean and variance of the Gamma distribution which best fits the data, under the assumptions we have made? Using the best-fitting distribution, determine the probability of observing males that are 2x larger than females (i.e., the ratio of masses is &gt;2). Then do the same thing but for the probability of observing males that are larger than females (i.e., the ratio is &gt; 1).\n1h. Based on 1g, would you conclude males are significantly larger than females?",
    "crumbs": [
      "Miscellaneous notes",
      "Review for challenge assignment"
    ]
  },
  {
    "objectID": "review-for-challenge.html#multiple-linear-regression",
    "href": "review-for-challenge.html#multiple-linear-regression",
    "title": "Review for challenge assignment",
    "section": "2. Multiple linear regression",
    "text": "2. Multiple linear regression\n2a. Load the “influenza_cities.csv” dataset into R. The data are from this paper on seasonal influenza dynamics in cities across the US. Each row corresponds a ZIP code. Information about the columns can be found here.\n2b. Regress mean epidemic intensity on the log population size. Plot the regression atop the data, interpret the regression coefficient, and check if model assumptions are satisfied.\n2c. Regress mean epidemic intensity on the log population size, daytime crowding (described in the paper as the expected census block-level population size experienced by a randomly selected individual within a city), and mean specific humidity. Use this model to predict the mean epidemic intensity in a city of population size 713,252 with a daytime crowding of 714.6 and an average specific humidity of 0.010333. Compare the value of mean epidemic potential in this hypothetical city to the median among all cities in the dataset.\n2d. Write a function to simulate \\(n=1000\\) observations from the model \\(Y_i \\sim \\text{Normal}(\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{1i}^2 + \\beta_3 x_{1i}^3, \\sigma^2/x_{1i})\\). To generate values of the response, first sample values of \\(x_1\\) from 0.1 to 10 in increments of 0.1.\n2e. Simulate data that arise from the model with \\(\\beta_0 = 100\\), \\(\\beta_1 = 1\\), \\(\\beta_2 = 0.2\\), \\(\\beta_3 = 0.03\\), and and \\(\\sigma = 10\\). Visualize the data. Then, using this data, regress y on x1. Are the assumptions of the standard regression framework satisfied? No need to use diagnostic plots, as you simulated the data!\n2f. Using the simulated data, regress y on x1, x1 squared, and x1 cubed. Hint: use the poly function.",
    "crumbs": [
      "Miscellaneous notes",
      "Review for challenge assignment"
    ]
  },
  {
    "objectID": "review-for-challenge.html#generalized-linear-models",
    "href": "review-for-challenge.html#generalized-linear-models",
    "title": "Review for challenge assignment",
    "section": "3. Generalized linear models",
    "text": "3. Generalized linear models\n3a. Load the “influenza_cities.csv” dataset into R.\n3b. Regress baseline transmission potential on log population size assuming that the response is GAMMA distributed. Interpret the effect of population size on baseline transmission potential. Hint: think about transformations of the response and the explanatory variable(s). The link function for the Gamma and other common GLMs can be found here.\n3c. Visualize the fitted model. The “line of best fit” should be curve-linear on the data scale. (Why?)\n3d. Ignoring values of mean epidemic potential which are zero, fit the following Gamma GLMs:\n\nmean epidemic potential on log population size\nmean epidemic potential on baseline transmission potential\nmean epidemic potential on mean specific humidity\nmean epidemic potential on log population size, mean specific humidity, and their interaction\nmean epidemic potential on baseline transmission potential, mean specific humidity, and their interaction\n\nNote: these are very similar to the models fitted and compared in the original study.\nCompare these models using AIC. Which are “best” and why?",
    "crumbs": [
      "Miscellaneous notes",
      "Review for challenge assignment"
    ]
  },
  {
    "objectID": "review-for-challenge.html#linear-mixed-models",
    "href": "review-for-challenge.html#linear-mixed-models",
    "title": "Review for challenge assignment",
    "section": "4. Linear mixed models",
    "text": "4. Linear mixed models\n4a. Load the “influenza_cities.csv” dataset into R. Can you fit a linear mixed model of mean epidemic potential on log population size with ZIP code as a random effect? Does it make sense to include baseline transmission potential as a random effect – why or why not?\n4b. Load the “SSDinMammals.csv” dataset into R. Fit a mixed model of log female mass on log male mass with Order as a random effect which affects the intercept but not the slope of the female mass-male mass relationship. Visualize your findings. Then do the same analysis except with Order as a random effect that affects the slope of the relationship but not the intercept.",
    "crumbs": [
      "Miscellaneous notes",
      "Review for challenge assignment"
    ]
  },
  {
    "objectID": "review-for-challenge.html#mathematical-models",
    "href": "review-for-challenge.html#mathematical-models",
    "title": "Review for challenge assignment",
    "section": "5. Mathematical models",
    "text": "5. Mathematical models\nApproximately \\(99.9\\%\\)$ of all of eukaryotes have the ability to reproduce sexually (i.e., are facultatively sexual). Why this is the case remains an important question in evolutionary genetics. Here we will use a mathematical model of one hypothesis for the ubiquity of sex across eukaryotes: differences in extinction rates between sexual and asexual linages.\nLet \\(S(t)\\) be the number of sexual species and \\(A(t)\\) be the number of asexual species at time \\(t\\). Both types of species spectate at per-capita rate \\(b\\), sexual species go extinct at rate \\(d\\), and asexual species go extinct at an elevated rate \\(d+\\delta\\). Sexual species give rise to asexual species a fraction \\(p\\) of the time, but asexual species never give rise to sexual species. Time is measured in thousands of years.\nUnder these assumptions, equations we could use to model the number of sexual and asexual species are\n\\[\\frac{\\text{d} S}{\\text{d} t} = b (1-p) S - d S.\\] \\[\\frac{\\text{d} A}{\\text{d} t} = b A + b p S - (d+\\delta) A\\]\n5a. Starting from ten sexual and ten asexual species, solve the model equations out to time \\(t=100\\) and plot the fraction of species that are sexual through time. Use the following parameter values: \\(b=1, d = 0.7, \\delta = 0.5, p = 0.1.\\) What fraction of species are sexual in the long run given these parameters?\n5b. Suppose \\(p=0.01\\) and \\(b = d = 1\\). Write a function to numerically solve the model equations for a specific value of \\(\\delta\\) and return the fraction of asexual species. Then loop over values of \\(\\delta\\) to determine the value of \\(\\delta\\) so that the long-run fraction of sexual species is \\(=0.999\\). In other words, determine the value of \\(\\delta\\) so that the fraction of sexual species is equal to what has been estimated empirically.\n5c. Explain the findings of 2b. For the chosen parameters, how much larger does the extinction rate of asexual lineages have to be to explain the fact \\(99.9\\%\\) of all eukaryotes are able to reproduce sexually?",
    "crumbs": [
      "Miscellaneous notes",
      "Review for challenge assignment"
    ]
  },
  {
    "objectID": "review-for-challenge.html#power-analysis",
    "href": "review-for-challenge.html#power-analysis",
    "title": "Review for challenge assignment",
    "section": "6. Power analysis",
    "text": "6. Power analysis\n6a. Re-read the notes on power analysis on the website VERY carefully. Make sure you understand how the data are simulated and then used to estimate power at a given sample size.\n6b. Define statistical power in your own words.\n6c. The below function simulates genome sizes (which range from 1000 to 10000 base pairs) and associated mutation rates (measured in units of number of mutations per individual per base pair) under a linear model motivated by the observation that larger genomes tend to be more robust to mutation (i.e., the per-base mutation rate is a decreasing function of genome size). In the model, the effect of increasing genome size by one unit on mutation rate is set equal to -0.0001. The function takes sample size as an input.\n\ndata_generator &lt;- function(sample_size = 100){\n  genome_size &lt;- sample(size = sample_size, x = seq(1000,10000,1), replace = T)\n  \n  mutation_rate &lt;- c()\n  \n  for (i in 1:length(genome_size)){\n    mutation_rate[i] &lt;- (100 -0.0001*genome_size[i] + rnorm(1, mean=0, sd = 10))*1e-9\n  }\n  \n  return(data.frame(genome_size = genome_size, mutation_rate = mutation_rate))\n}\n\nThe below code chunk plots genome size and mutation rate data generated under the linear model above, and preforms a regression of mutation rate on genome size using the simulated data.\n\ndata &lt;- data_generator()\n\ndata %&gt;% pivot_longer(! genome_size) %&gt;% \n  ggplot(aes(x = genome_size, y = value)) + geom_point() + \n  geom_smooth(method = \"lm\") + ylab(\"mutation rate\") + xlab(\"genome size\")\n\nsummary(lm(mutation_rate~genome_size, data))\n\nWrite a function to estimate, for a given sample size, the power at level \\(\\alpha = 0.01\\). That is, from a large number of simulations (generated using the data_generator() function), determine the fraction of simulations for which fitting a linear model (of mutation rate on genome size) the coefficient associated to genome size has a \\(p\\) value which is \\(&lt; \\alpha\\).\nConsider sample sizes of \\(150,160,\\dots,200\\). Use the function you have written to determine the sample size (of the ones tested) needed to detect a significant effect \\(&gt;75\\%\\) of the time.\n6d. Do the analysis in 6c, except vary the effect of genome size on mutation rate instead of the sample size. That is, fix the sample size at, say, \\(n=200\\) and determine the power associated to effect sizes of -0.0001, -0.0005, -0.00075, -0.001. The power should be smallest for effect sizes that are close to zero. (Why?)\nHint: you need to adjust the data_generator() function to have an effect size argument.\nAt what effect size (of the ones tested) is the power \\(&gt;75\\%\\)?",
    "crumbs": [
      "Miscellaneous notes",
      "Review for challenge assignment"
    ]
  }
]