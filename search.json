[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "",
    "text": "Welcome!\nWelcome to the online home of EEB 313, Winter 2026. We created this website as an easy-to-navigate resources where we can share all the course details, detailed lectures notes, homework assignments, and other resources. Most of the files are created as interactive R Markdown documents, which you can download and run yourself.\nEEB 313 was originally developed as a student-led course, and over the years many contributors have helped to develop the course material in its current form. See the About Us page for more details on our current and past teaching team/course developers.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Course overview",
    "text": "Course overview\nEEB313 covers foundational concepts in scientific computing and data analytics using the programming language R, with applications in ecology and evolutionary biology. Using interactive instructional sessions and group work, students will learn to program mathematical calculations and simple algorithms, to analyze and visualize complex datasets, to implement models to simulate biological population dynamics, and to document and disseminate their code. No prior programming experience is required.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus: EEB 313 Winter 2026",
    "section": "",
    "text": "Course overview\nThis course covers foundational concepts in scientific computing and data analytics using the programming language R, with applications in ecology and evolutionary biology. Using interactive instructional sessions and group work, students will learn to program mathematical calculations and simple algorithms, to analyze and visualize complex datasets, to implement models to simulate biological population dynamics, and to document and disseminate their code. No prior programming experience is required.\nPrerequisites: BIO220H1 and one of EEB225H1, STA288H1, or STA220H1",
    "crumbs": [
      "Syllabus: EEB 313 Winter 2026"
    ]
  },
  {
    "objectID": "syllabus.html#course-details",
    "href": "syllabus.html#course-details",
    "title": "Syllabus: EEB 313 Winter 2026",
    "section": "Course details",
    "text": "Course details\n\nCourse learning outcomes\n\nMaster the basic syntax, data types, and operations needed for basic scientific computing in R\nManipulate, visualize, and analyze complex biological data sets programmatically\nFit standard and customized statistical models to data\nSimulate simple stochastic and deterministic models of population dynamics\nImplement principles of reproducible computational research\nAcquire the confidence to approach scientific problems using computational methods\n\n\n\nTime\nMondays and Wednesdays 1-3pm in Carr Hall 325\nBoth weekly meetings are mandatory instructional sessions. Students should have a laptop computer capable of running the most recent versions of R and R Studio, or able to access POSIT/RStudio Cloud from a web browser, and should bring this to each class meeting.\nIn lieu of office hours, 2 members of the teaching team will be available after each class session to answer any questions students have.\n\n\nTeaching team\n\nInstructor\nProf. Alison Hill\nProf. Hill is a faculty member in the Department of Ecology & Evolutionary Biology. She runs a research group studying the dynamics and evolution of human infectious diseases within patients and across populations. Her team develops mathematical, statistical, and computational models to predict disease trajectories and help design interventions. Before moving to U of T, she was faculty at Johns Hopkins, and did her graduate and post-graduate training at Harvard. Coding is still her favourite part of her job, and she has used R - along with other programming languages - for many large open-source computational projects focusing on diseases such as COVID-19, HIV, RSV, and the opioid crisis.\n\n\nTeaching assistants\nJessie Wang\nJessie is a 4th year PhD student in the Frederickson lab at UTSG. She studies plant-microbe interactions using high- throughput experimentation in duckweeds. She fell in love with R during her time as an undergraduate and took EEB313 in 2020, simultaneously sharpening her coding skills while conducting research alone in the lab. Jessie loves to spend too much money on fancy coffee as she types away, making sure her code is well-annotated and her figures look beautiful. Outside of work, she enjoys caring for her many houseplants and aquariums, finding new delicious eats, and admiring other people’s pets.\nErik Curtis\nErik is a 2nd year PhD student interested in the epidemiology and population ecology of Pacific salmon, as well as the ecology of infectious diseases. In his PhD research, he is investigating the prevalence of co-infection in juvenile salmon. He’s also using eDNA metabarcoding to examine the coastal marine community concurrent with juvenile salmon migration and salmon farm activity. Prior to joining the MK lab, he studied at the University of Notre Dame, majoring in Biology and Math, where he examined the fate and transport of eDNA in experimental streams. \n\n\nContacts\nFor scientific/technical questions on the content of course material in lectures or on assignments, please email TAs Jessie (jae.wang@mail.utoronto.ca) and Erik (erik.curtis@mail.utoronto.ca). For questions on course policies or something the TAs were unable to answer, please email Prof. Hill (alison.hill@utoronto.ca). Send your email through Quercus (ideal), or if that’s not possible, include “EEB313” in the subject line. Responses may take a few days.\n\n\n\nLecture schedule\nThis schedule is tentative\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic (tentative)\n\n\n\n\n1\nJan 5\nIntro to course\n\n\n1\nJan 7\nIntro to R\n\n\n2\nJan 12\nData manipulation\n\n\n2\nJan 14\nData visualization\n\n\n3\nJan 19\nAdvanced data analysis\n\n\n4\nJan 21\nExploratory data analysis (Activity)\n\n\n4\nJan 26\nSNOW DAY\n\n\n4\nJan 28\nModel-informed data analysis (Activity)\n\n\n5\nFeb 2\nRandom variables and computational statistics\n\n\n5\nFeb 4\nProbability, likelihood, and statistical inference\n\n\n6\nFeb 9\nLinear Models 1\n\n\n6\nFeb 11\nLinear Models II\n\n\n\nFeb 16\nReading week\n\n\n\nFeb 18\nReading week\n\n\n7\nFeb 23\nMathematical models in ecology and evolution II\n\n\n7\nFeb 25\nProject work\n\n\n8\nMar 2\nMathematical models in ecology and evolution II\n\n\n8\nMar 5\nClustering, dimensionality reduction, and machine learning\n\n\n9\nMar 9\nProject work\n\n\n9\nMar 11\nProject work\n\n\n10\nMar 16\nReproducible research\n\n\n10\nMar 18\nProject work\n\n\n11\nMar 23\nProject work\n\n\n11\nMar 25\nProject work\n\n\n12\nMar 30\nGroup presentations\n\n\n12\nApr 1\nGroup presentations\n\n\n\n\n\nAssessment\n\nGrade breakdown summary\n\nProblem Sets  40%\nChallenge Assignment/Take-home exam ~ 20%\nGroup Project 30%\n\nPresentation 15%\nReport 15%\n\nOther 10%\n\nParticipation, surveys, progress reports, etc\n\n\n\n\nAssessment schedule\n\n\n\n\n\n\n\n\n\n\nAssignment\nType\nSubmitted on\nDue date\n%\n\n\n\n\nIntro survey\nIndividual\nQuercus\nJan 7 (flexible)\n1\n\n\nProblem Set 1\nIndividual\nQuercus\nJan 14\n8\n\n\nProblem Set 2\nIndividual\nQuercus\nJan 21\n8\n\n\nProblem Set 3\nIndividual\nQuercus\nJan 28\n8\n\n\nProblem Set 4\nIndividual\nQuercus\nFeb 13\n8\n\n\nProblem Set 5\nIndividual\nQuercus\nFeb 20\n8\n\n\nProject proposal\nGroup\nQuercus\nMar 5\n3\n\n\nChallenge assignment\nIndividual\nQuercus\nMar 11\n20\n\n\nMid-project update\nGroup\nGitHub\nMar 18\n6\n\n\nPresentation\nGroup\nIn-class\nMar 30 and Apr 1\n15\n\n\nFinal report\nGroup\nGitHub\nApr 8\n15\n\n\n\nThere are 100 marks in total. Your final course mark will be the sum of your assignment scores, which will be translated to a letter grade according to the official grading scale of the Faculty of Arts and Science.\nAssignments will be distributed and submitted in the R Markdown format via Quercus. Assignments will typically be handed out on Wed after class and are due at 11:59 PM on the following Wed. All students will be given 2 “free” late days that they can distribute across assignments if an extension is needed. Otherwise, late assignments will face a penalty of 50% per day.\nThe Challenge Assignment is equivalent to a take home exam. The format will be the same as the other assignments, but this assignment is designed challenge you to go a little beyond what was taught in class. It will be distributed on 9:00 AM on Mar 6, and it will be due 11:59 PM on Mar 11. Students should work on their own and submit their own original work. No extensions will be granted on this assignment except under the same extra-ordinary circumstances akin to those under which an exam might be deferred. We only expect you to do your best!\nThe project will be conducted as a group and groups will receive a single grade (except in exceptional circumstances). No late assignments will be accepted for any component of the group project.\nAll submissions to Quercus/GitHub must be submitted as PDFs (i.e., knitted).\n\n\n\nPre-requisites and preparation",
    "crumbs": [
      "Syllabus: EEB 313 Winter 2026"
    ]
  },
  {
    "objectID": "syllabus.html#resources",
    "href": "syllabus.html#resources",
    "title": "Syllabus: EEB 313 Winter 2026",
    "section": "Resources",
    "text": "Resources\n\nCourse websites\n\nQuercus https://q.utoronto.ca/courses/419604 (Assignments, announcements)\nhttps://eeb313.github.io/ (Detailed course info, lecture notes after class, links to code)\n\n\n\nR resources\n\nInstallation:\n\nInstall R: https://cran.rstudio.com\nInstall R Studio: https://posit.co/download/rstudio-desktop\nRegister for a free R Studio Cloud (“POSIT”) account: https://posit.cloud/plans/Links to an external site.freeLinks to an external site.\nRegister for a Github account: https://github.com/Links to an external site.signupLinks to an external site.\n\nThe EEB R Manual : https://rman.eeb.utoronto.ca/\nMathematics review: In Otto & Day, “A biologist’s guide to mathematical modeling in ecology & evolution”, Appendix 1 (basic math rules) and 2 (calculus). Available online via U of T libraries\n\n\n\nImproving your writing skills\nEffective communication is crucial in science. The University of Toronto provides services to help you improve your writing, from general advices on effective writing to writing centers and writing courses. The Faculty of Arts & Science also offers an English Language Learning (ELL) program, which provides free individualized instruction in English skills. Take advantage of these!\n\n\nFAS student engagement programs\nThere are a few programs on campus aimed at increasing student engagement with their coursework and keeping them socially connected. Recognized Study Groups are voluntary, peer-led study groups of up to 8 students enrolled in the same course. Meet to Complete are online drop-in study sessions for A&S undergrads. These are worth checking out if you are interested in participating in a study group.",
    "crumbs": [
      "Syllabus: EEB 313 Winter 2026"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus: EEB 313 Winter 2026",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance\nStudents are expected to attend and participate in all classes. If you are experiencing symptoms or suspect you have a communicable disease but decide to come to class, please practice hand hygiene and wear a face mask. \n\n\nAcademic Integrity\nYou should be aware of the University of Toronto Code of Behaviour on Academic Matters; all suspected cases of academic dishonesty will be investigated following procedures outlined in there. Also see How Not to Plagiarize. Notably, it is NOT appropriate to use large sections from internet sources, and inserting a few words here and there does not make it an original piece of writing. Be careful in using internet sources – most online material are not reviewed and there are many errors out there. Make sure you read material from many sources (published, peer-reviewed, trusted internet sources) and that you write an original text using this information. Always cite your sources. In case of doubt about plagiarism, talk to your instructors and TAs. Please make sure that what you submit for the final project does not overlap with what you submit for other classes, such as the 4th-year research project. If you have questions or concerns about what constitutes appropriate academic behaviour or appropriate research and citation methods, please reach out to me.\n\n\nOn the use of generative AI\nWe recognize that there are emerging generative artificial intelligence tools that can not only help with syntax and errors, but can write code de novo given text prompts. While these tools can be extremely useful when used responsibly by experienced coders, we believe it is critical to understand the foundational principles and syntax of programming by generating your own code, from scratch. Thus, we do not permit students to submit work generated by chat-bot programs, and will investigate suspicions of such use according to existing procedures for academic dishonesty. Students should be prepared to discuss, justify, and recreate any of their submitted work if prompted by course instructors at any time. Towards the end of the class, we will discuss the promise and pitfalls of such tools and experiment with their use for applications discussed in the course.\n\n\nOnline Communication\nAll communication regarding the course should be done through Quercus or using your mail.utoronto.ca email address. Please post questions that may be relevant to other students in the Discussions section of the course website, instead of asking the instructor by email\n\n\nAccessibility needs\nIf you require accommodations for a disability, or have any accessibility concerns about the course or course materials, please notify the course instructor, or contact Accessibility Services, as soon as possible regarding accommodations.\n\n\nDiversity and inclusion statement\nAs students, you all have something unique and special to offer to science. It is our intent that students from all backgrounds and perspectives be well served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that students bring to this class be recognized as a resource, strength, and benefit.\nDiversity can refer to multiple ways that we identify ourselves, including but not limited to race, national origin, language, cultural heritage, physical ability, neurodiversity, age, sexual orientation, gender identity, religion, and socio-economic class. Each of these varied, and often intersecting, identities, along with many others not mentioned here, shape the perspectives we bring to this class, to this department, and to the greater EEB community. We will work to promote diversity, equity, and inclusion not only because diversity fuels excellence and innovation, but because we want to pursue justice.\nWe expect that everybody in this class will respect each other, and demonstrate diligence in understanding how other people’s perspectives, behaviors, and worldviews may be different from their own. Racist, sexist, colonialist, homophobic, transphobic, and other abusive and discriminatory behavior and language will not be tolerated in this class and will result in disciplinary action, such as removal from class session or revocation of group working privileges. Please consult the University of Toronto Code of Student Conduct for details on unacceptable conduct and possible sanctions.\nPlease let us know if something said or done in this class, by either a member of the teaching team or other students, is particularly troubling or causes discomfort or offense. While our intention may not be to cause discomfort or offense, the impact of what happens throughout the course is not to be ignored and is something that we consider to be very important and deserving of attention. If and when this occurs, there are several ways to alleviate some of the discomfort or hurt you may experience:\n\nDiscuss the situation privately with a member of the teaching team. We are always open to listening to students’ experiences, and want to work with students to find acceptable ways to process and address the issue.\nNotify us of the issue through another source such as a trusted faculty member or a peer. If for any reason you do not feel comfortable discussing the issue directly with us, we encourage you to seek out another, more comfortable avenue to address the issue.\nContact the Anti-Racism and Cultural Diversity Office to report an incident and receive complaint resolution support, which may include consultations and referrals.\n\nWe acknowledge our imperfections while we also fully commit to the work, inside and outside of our classrooms, of building and sustaining a community that increasingly embraces these core values. Your suggestions and feedback are encouraged and appreciated. Please let us know ways to improve the effectiveness of the course for you personally or for other students or student groups.\n\n\nWellness statement\nWe on the teaching team value your health and wellness. In order to succeed in this class, in university, and beyond, you must balance your work with rest, exercise, and attention to your mental and physical health. Working until exhaustion is NOT a badge of honor. If you are finding it difficult to balance your health and well-being with your work in this class, please do not hesitate to let us know. We are happy to help connect you with resources and services on campus and also to make accommodations to our course plan as needed. Our inboxes are always open, and we are also available for virtual chats by appointment. You have our support, and we believe in you.",
    "crumbs": [
      "Syllabus: EEB 313 Winter 2026"
    ]
  },
  {
    "objectID": "about-us.html",
    "href": "about-us.html",
    "title": "Instructors",
    "section": "",
    "text": "Instructor\nProf. Alison Hill (alison.hill@utoronto.ca)\nProf. Hill is a faculty member in the Department of Ecology & Evolutionary Biology. She runs a research group studying the dynamics and evolution of human infectious diseases within patients and across populations. Her team develops mathematical, statistical, and computational models to predict disease trajectories and help design interventions. Before moving to U of T, she was faculty at Johns Hopkins, and did her graduate and post-graduate training at Harvard. Coding is still her favourite part of her job, and she has used R - along with other programming languages - for many large open-source computational projects focusing on diseases such as COVID-19, HIV, RSV, and the opioid crisis.",
    "crumbs": [
      "Instructors"
    ]
  },
  {
    "objectID": "about-us.html#teaching-assistants",
    "href": "about-us.html#teaching-assistants",
    "title": "Instructors",
    "section": "Teaching assistants",
    "text": "Teaching assistants\nJessie Wang (jae.wang@mail.utoronto.ca)\nJessie is a 4th year PhD student in the Frederickson lab at UTSG. She studies plant-microbe interactions using high- throughput experimentation in duckweeds. She fell in love with R during her time as an undergraduate and took EEB313 in 2020, simultaneously sharpening her coding skills while conducting research alone in the lab. Jessie loves to spend too much money on fancy coffee as she types away, making sure her code is well-annotated and her figures look beautiful. Outside of work, she enjoys caring for her many houseplants and aquariums, finding new delicious eats, and admiring other people’s pets.\nErik Curtis (erik.curtis@mail.utoronto.ca\nErik is a 2nd year PhD student interested in the epidemiology and population ecology of Pacific salmon, as well as the ecology of infectious diseases. In his PhD research, he is investigating the prevalence of co-infection in juvenile salmon. He’s also using eDNA metabarcoding to examine the coastal marine community concurrent with juvenile salmon migration and salmon farm activity. Prior to joining the MK lab, he studied at the University of Notre Dame, majoring in Biology and Math, where he examined the fate and transport of eDNA in experimental streams.",
    "crumbs": [
      "Instructors"
    ]
  },
  {
    "objectID": "about-us.html#prior-year-course-instructors",
    "href": "about-us.html#prior-year-course-instructors",
    "title": "Instructors",
    "section": "Prior year course instructors",
    "text": "Prior year course instructors\nMete Yuksel (2023 and 2024)\nZoe Humphries (2024)\nVicki Zhang (2022 and 2023)\nEmma Walker (2021 and 2022)\nTia Harrison (2020 and 2021)\nAmber Gig Hoi (2020)\nAhmed Hasan (2018 and 2019)\nSara Mahallati (2018)\nJames Santangelo (2018)\nMadeleine Bonsma-Fisher (2017, 2018)\nLindsay Coome (2017, 2018)\nJoel Ostblom (2017, 2018)\nLuke Johnston (2017)\nLina Tran (2017)\nElliott Sales de Andrade (2017)",
    "crumbs": [
      "Instructors"
    ]
  },
  {
    "objectID": "about-us.html#other-source-material",
    "href": "about-us.html#other-source-material",
    "title": "Instructors",
    "section": "Other source material",
    "text": "Other source material\nData Analysis and Visualization in R for Ecologists, Data Carpentry, https://datacarpentry.github.io/R-ecology-lesson/\nBrian Seok, François Michonneau, Tobias Busch, Katrin Leinweber, Maneesha Sane, njlyon0, Ed Bennett, Hugo Tavares, Mike Mahoney, Paula Nieto, Susan Washko, Terry Loecke, Wasila Dahdul, xli677, Abhijna Parigi, Aleksander Jankowski, Allison Shay Theobold, Analytics Enlightened LLC, Anna K. Moeller, … vmzhang. (2024). datacarpentry/R-ecology-lesson: Data Carpentry: Data Analysis and Visualization in R for Ecologists 2024-07 (v2024.07). Zenodo https://zenodo.org/records/12684301\nChristie Bahlai, Reproducible Quantitative Methods, https://cbahlai.github.io/rqm-template/",
    "crumbs": [
      "Instructors"
    ]
  },
  {
    "objectID": "about-us.html#publications",
    "href": "about-us.html#publications",
    "title": "Instructors",
    "section": "Publications",
    "text": "Publications\nJohnston LW, Bonsma-Fisher M, Ostblom J, Hasan AR, Santangelo JS, Coome L, Tran L, Andrade ES, Mahallati S. A graduate student-led participatory live-coding quantitative methods course in R: Experiences on initiating, developing, and teaching. Journal of Open Source Education. 2019;2: 49. doi:10.21105/jose.00049",
    "crumbs": [
      "Instructors"
    ]
  },
  {
    "objectID": "lectures/lec01-downloading-r.html",
    "href": "lectures/lec01-downloading-r.html",
    "title": "1  Downloading and installing R",
    "section": "",
    "text": "1.1 Introduction\nThis course uses R, a computing environment that combines numerical analysis tools for linear algebra, a wide range of scientific computing algorithms, functions for classical and modern statistical analysis; and functions for graphics and data visualization. It is based on the programming language S, developed by John Chambers in the 1970s. Today, R is most popular among statisticians, data scientists, biologists, and public health researchers, but is used broadly across many fields.\nWe will use the graphical user interface (GUI) to R, a software called RStudio, throughout this course. Although the GUI makes many tasks easier, it is not necessary to use it when running R. Both methods will be described below.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Downloading and installing R</span>"
    ]
  },
  {
    "objectID": "lectures/lec01-downloading-r.html#installing-r",
    "href": "lectures/lec01-downloading-r.html#installing-r",
    "title": "1  Downloading and installing R",
    "section": "1.2 Installing R",
    "text": "1.2 Installing R\nDownload R, a free software environment for statistical computing and graphics from CRAN, the Comprehensive R Archive Network. Use the links specific to your operating system at the top of the page (i.e., the `precompiled binary distribution’).\n\nFor Mac users: Select the .pkg file for the latest R version. Mac users with an Apple Silicon chip (e.g., M1 or M2) should install the “arm64” version of R, while Mac users with an Intel chip should install the regular (64-bit) version of R. You can check your laptop’s hardware specifications by clicking the Apple icon (top left corner) \\&gt; About This Mac. Once the .pkg file is downloaded to your computer, double click it and follow the prompts to install R\nFor Windows users: Select the base file and download. Run the .exe file that was just downloaded and follow the instructions on screen to install the downloaded software locally on your computer.\nFor Linux users: Follow the links and instructions provided for your distribution\n\nIf you previously downloaded R for another class or purpose, check what version you have. For maximum compatibility with the code provided for the class, ensure that the R version is 4.5.2 (the latest). You can check your R version by opening the R program (or RStudio if you already have it), and at the prompt (indicated by the &gt; ) typing version. If you have an out-of-date version, you have to download a new one following the instructions above (there’s no separate update process).\n![The R terminal](figures/r_console.jpg){width=50%}\nNo laptop? No problem! If you don’t have a laptop computer that allows you to install software (e.g., a tablet or Chromebook), or if your laptop is older and difficult to run resource-heavy programs, you can instead access R and RStudio from your web browser and do you your computations in the cloud using the cloud-based version of RStudio, called POSIT Cloud. Sign up for a free account here. We’ll add you to our EEB313 workspace so you’ll get more cloud credits than a regular free account.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Downloading and installing R</span>"
    ]
  },
  {
    "objectID": "lectures/lec01-downloading-r.html#installing-rstudio",
    "href": "lectures/lec01-downloading-r.html#installing-rstudio",
    "title": "1  Downloading and installing R",
    "section": "1.3 Installing RStudio",
    "text": "1.3 Installing RStudio\nDownload and install RStudio by choosing your specific operating system, then following the instructions to download the software package and install it for your operating system.\nNote: If your have previously installed RStudio but newer versions have been released, it will automatically notify you and provide the link to update it. There is no connection between versions of R and RStudio - you have to update them independently. If you are updating your version of R after opening R Studio, make sure R Studio is restarted and then verify it recognizes the most recent version of R. On Macs, this is usually automatic, but on Windows, it is possible to have multiple versions of R installed, so if you didn’t delete an older version in the process of installing the new one, you might have to tell RStudio which version of R to use.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Downloading and installing R</span>"
    ]
  },
  {
    "objectID": "lectures/lec01-downloading-r.html#navigating-rstudio",
    "href": "lectures/lec01-downloading-r.html#navigating-rstudio",
    "title": "1  Downloading and installing R",
    "section": "1.4 Navigating RStudio",
    "text": "1.4 Navigating RStudio\nRStudio includes the R console, where you can directly type R commands, but also many other convenient functionalities, which makes it easier to get started and to work with R.\nDetails on the different panels you see in RStudio, with pictures, are provided in the EEB R Manual https://rman.eeb.utoronto.ca/getting-stated/navigating-rstudio/. To summarize, the panels are\n\nTop left: The text editor panel. This is where we can write scripts, i.e. putting several commands of code together and saving them as a text document so that they are accessible for later and so that we can execute them all at once by running the script instead of typing them in one by one. You might not see this panel if you’re opening RStudio for the first time. If so, go to File &gt; RScript to open an new R script and give it a name (like my_test.R)\nBottom left: The console is another space we can input code, only now the code is executed immediately and doesn’t get saved at the end.\nTop right: The environment panel, which shows us all the files, functions, and objects we currently loaded into R. To view more details about any of these objects, click on them to visually inspect them.\nBottom right: The files-plots-help panel. This panel shows the files in the current directory (the folder we are working out of), any plots we make later, and also documentation for various packages and functions. Here, the documentation is formatted in a way that is easier to read and also provides links to the related sections.\n\nTo change the appearance of your RStudio, navigate to Tools &gt; Global Options &gt; Appearance. You can change the font and size, and the editor theme. The default is “Textmate”, but if you like dark mode, a good option is “Tomorrow Night Bright”. You can also change how your panels are organized.\nThere are lots of resources providing tips and tricks for navigating RStudio; for example, check out https://www.dataquest.io/blog/rstudio-tips-tricks-shortcuts/",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Downloading and installing R</span>"
    ]
  },
  {
    "objectID": "lectures/lec01-downloading-r.html#coding-in-r",
    "href": "lectures/lec01-downloading-r.html#coding-in-r",
    "title": "1  Downloading and installing R",
    "section": "1.5 Coding in R",
    "text": "1.5 Coding in R\nWithin RStudio, there are a few ways we can run code in R.\nFor short commands that we aren’t likely to want to repeat, we can simply type then into the console, and the answer will be returned immediately, e.g.\n\n&gt; 1+2\n\nWe can similarly to a series of calculations where the current one depends on the prior. However, this method becomes inconvenient for all but the simplest calculations, since it’s more difficult to repeat a series of calculations in an automated way. For this reason, it is more common to write commands in scripts. Scripts are simply series of lines of code saved in a text file.\nTo create a new script, go to File &gt; New File &gt; RScript, give it a name (like my_test.R), and save it. Once you have written the code you want, there are a few ways you can actually get it to run\n\nusing your cursor to highlight all the code text, and then hitting the icon with the green forward arrow and the word `Run’ in the top right of the scripts panel\ntyping &gt;source('mytest.R') at the prompt in the RStudio console\n\nBoth methods result in the same code being run, although in the latter method, the result of each calculation will not be printed off to the console. You need to enclose the statement you wanted displayed in the print() command, i.e. print(2+3)\nComments! Running any code with a # at the beginning of the line results in the line being read as a comment. This means that the calculation which is specified in the line is not processed and the output not returned. Comments are a useful way to keep track of what line(s) of code do, multiple versions of the same code, etc. Usually we use them to make notes about what the code following the comment is doing and why. Its the most basic way of documenting code for others who may use it (as well as our future selves). However, comments are very limited, since they can’t include any text formatting, so for most of this course we’ll instead be using a different type of R script document called a notebook, which allows us to use the R Markdown format, for much nicer documentation and presentation of our results.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Downloading and installing R</span>"
    ]
  },
  {
    "objectID": "lectures/lec01-downloading-r.html#installing-r-packages",
    "href": "lectures/lec01-downloading-r.html#installing-r-packages",
    "title": "1  Downloading and installing R",
    "section": "Installing R packages",
    "text": "Installing R packages\nR packages are basically bundles of functions that perform related tasks. There are many some that will be come with a base install of R since they are considered critical for using R, such as c(), mean(), +, -, etc.\nThere is an official repository for R-packages beyond the base packages called CRAN (Comprehensive R Archive Network). CRAN has thousands of packages, and all these cannot be installed by default, because then base R installation would be huge and most people would only be using a fraction of everything installed on their machine. It would be like if you downloaded the Firefox or Chrome browser and you would get all extensions and add-ons installed by default, or as if your phone came with every app ever made for it already installed when you bought it: quite impractical.\nInstead, individual users tend to download and install only the packages they really need for their work. While this may be efficient in terms of the space in memory taken up by the app, it can lead to some challenges when sharing code, if you’re not careful to ensure that the code is documented to specify the required packages, and to autmoatically download and install then if they’re missing.\n\n1.5.1 TinyTex\nThere is one package we have to install first before we can create PDF reports, which will be necessary for assignments and the project (the default is to create HTML reports, which can be opened in any web browser, and are useful to make some sorts of interactive visuals, but are not as easy to share and view as PDFs). Copy and paste into the console (where the \\&gt; symbol is) the two lines of code below to install a package called tinytex.\n\ninstall.packages(\"tinytex\") \ntinytex::install_tinytex()\n\n\n\n1.5.2 Tidyverse\ntidyverse1 is a large collection of packages with similar functions, similar to the way Microsoft Word is part of Microsoft Office. tidyverse, as its name may suggest, contains many packages that makes data cleaning and exploring more intuitive and effective. It is basically an entire philosophy on how to handle data and has a massive following.\nThe two tidyverse packages we will be using the most frequently in this course is dplyr and ggplot2. dplyr is great for data wrangling (Lecture 3) and ggplot2 makes killer plots (Lecture 4).\n\nCopy and paste the below code into your console.\n\n\ninstall.packages(c(\"tidyverse\", \"data.table\"), dependencies = TRUE)\n\nDuring installation, if you ever get the below message, click “No”.\n\nIf you get the message “Do you want to install from sources the packages which need compilation? (Yes/no/cancel)” in the Console, type “Yes” and press enter.\n\nCheck that the tidyverse package has been installed correctly. To do this, go to the bottom right pane and click the tab for “Packages”. If you can search for and find the below packages, then they have been installed! They do not need to be checked off. Alternatively, go to the Console and type library(tidyverse) to verify that the package is installed. An error along the lines “there is no package called tidyverse” will be returned if the package is not installed.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Downloading and installing R</span>"
    ]
  },
  {
    "objectID": "lectures/lec01-downloading-r.html#r-notebooks-and-r-markdown",
    "href": "lectures/lec01-downloading-r.html#r-notebooks-and-r-markdown",
    "title": "1  Downloading and installing R",
    "section": "1.6 R Notebooks and R Markdown",
    "text": "1.6 R Notebooks and R Markdown\n\n1.6.1 Creating R notebooks\nIn the RStudio interface, we will be writing code in a format called the R Notebook. As the name entails, this interface works like a notebook for code, as it allows us to save notes about what the code is doing, the code itself, and any output we get, such as plots and tables, all together in the same document.\nIn RStudio you can create a new R Markdown notebook by going to File &gt; R Markdown. It will appear in the same panel as scripts. The file name should end in .Rmd. - Read the guidelines provided in example text in that notebook.\nWhen we are in the notebook, the text we write is normal plain text, just as if we would be writing it in a text document. If we want to execute some R code, we need to insert a code chunk.\nYou insert a code chunk by either clicking the “Insert” button (icon with a green +C in the top right) or pressing Command + Option + i (on Mac/Linux, or Ctrl + Alt + i on Windows) simultaneously. You could also type out the surrounding backticks, but this would take longer. To run a code chunk, you press the green arrow, or Ctrl/Command + Shift + Enter.\n\n1+2\n\n[1] 3\n\n\nAs you can see, the output appears right under the code block.\nThis is a great way to perform explore your data, since you can do your analysis and write comments and conclusions right under it all in the same document. A powerful feature of this workflow is that there is no extra time needed for code documentation and note-taking, since you’re doing your analyses and taking notes at the same time. This makes it great for both taking notes at lectures and to have as a reference when you return to your code in the future.\n\n\n1.6.2 R Markdown\nThe text format we are using in the R Notebook is called R Markdown. This format allows us to combine R code with the Markdown text format, which enables the use of certain characters to specify headings, bullet points, quotations and even citations. A simple example of how to write in Markdown is to use a single asterisk or underscore to emphasize text (*emphasis*) and two asterisks or underscores to strongly emphasize text (**strong emphasis**). When we convert our R Markdown text to other file formats, these will show up as italics and bold typeface, respectively. If you have used WhatsApp, you might already be familiar with this style of writing. In case you haven’t seen it before, you have just learned something about WhatsApp in your quantitative methods class…\nTo learn more about R Markdown, check out this reference. More helpful commands are also provided in the [EEB R Manual](https://rman.eeb.utoronto.ca/basic-r/rmarkdown/)\n\n\n1.6.3 Saving data and generating reports\nTo save our notes, code, and graphs, all we have to do is to save the R Markdown file, and the we can open it in RStudio next time again. However, if we want someone else to look at this, we can’t always just send them the R Notebook file, because they might not have RStudio installed. Another great feature of R Notebooks is that it is really easy to export them to HTML, Microsoft Word, or PDF documents with figures and professional typesetting. There are actually many academic papers that are written entirely in this format and it is great for assignments and reports. (You might even use it to communicate with your collaborators!) Since R Notebook files convert to HTML, it is also easy to publish simple and good-looking websites in it, in which code chunks are embedded nicely within the text.\nLet’s try to create a document in R.\nFirst, let’s set up the YAML block. This is found at the top of your document, and it is where you specify the title of your document, what kind of output you want, etc.\n\n---\ntitle: \"Your title here\"\nauthor: \"Your name here\"\ndate: \"Insert date\"\noutput:\n  pdf_document: default\n---\n\nIf you are interested in playing with other YAML options, check out this guide.\nNext, let’s type code to perform the calculation we did above:\n\n1+2\n\n[1] 3\n\n\nTo create the output document, we say that we “knit” our R Markdown file into, e.g., a PDF. Simply press the Knit button here and the new document will be created. The first time you do this, you might be asked to install some R packages if it’s the first time you’ve done this - go ahead an let them install.\nAs you can see in the knitted document, the title showed up as we would expect, and lines with pound sign(s) in front of them were converted into headers. Most importantly, we can see both the code and its output! Plots are generated directly in the report without us having to cut and paste images! If we change something in the code, we don’t have to find the new images and paste it in again, the correct one will appear right in your code.\nWhen you quit, R will ask you if you want to save the workspace (that is, all of the variables you have defined in this session); in general, you should say “no” to avoid clutter and unintentional confusion of results from different sessions. Note: When you say “yes” to saving your workspace, it is saved in a hidden file named .RData. By default, when you open a new R session in the same directory, this workspace is loaded and a message informing you so is printed: [Previously saved workspace restored]. It is often best practice to turn this feature off completely.\n\n\n1.6.4 A quick note on variables and memory in R notebooks\nWhen you run code in a notebook by simply interacting with the text in the notebook text file (ie via clicking the green `run’ arrow or typing Command+Shift+Enter, any variables or other objects you create will be available in your RStudio projects environment (i.e. viewable in the Environment pane and accessible via the console). However, when you run a document by knitting, it is actually running a separate, private, session of R, and so the output cannot be accessed later. If you want to examine any output produced in a notebook that’s knitted, make sure the notebook itself contains commands to print the objects so they can be seen in the kitted form.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Downloading and installing R</span>"
    ]
  },
  {
    "objectID": "lectures/lec01-downloading-r.html#footnotes",
    "href": "lectures/lec01-downloading-r.html#footnotes",
    "title": "1  Downloading and installing R",
    "section": "",
    "text": "This course is focused on tidyverse functions, because that seems to be the trend these days. Although all of our teaching material is written in tidy lingo, it is mostly for the sake of consistency. In all honesty, tidy is pretty great, but some functions are more intuitive in base, so most people code in a mix of the two. If you learned base R elsewhere and prefer to use those functions instead, by all means, go ahead. The correct code is code that does what you want it to do.↩︎",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Downloading and installing R</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html",
    "href": "lectures/lec02-basic-r.html",
    "title": "2  Introduction to R: Assignment, vectors, functions, strings, and loops",
    "section": "",
    "text": "2.1 Lesson Preamble\nNote: Components of this lecture (and some others in the course!) were originally created by voluntary contributions to Data Carpentry and have been modified and expanded over the years to align with the aims of EEB313. Data Carpentry is an organization focused on data literacy, with the objective of teaching skills to researchers to enable them to retrieve, view, manipulate, analyze, and store their and other’s data in an open and reproducible way in order to extract knowledge from data.\nThe above paragraph is made explicit since it is one of the core features of working with an open language like R. Many smart people willingly and actively share their material publicly, so that others can modify and build off of the material themselves. See the About Us page for more information on all the contributors to EEB 313.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: Assignment, vectors, functions, strings, and loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#lesson-preamble",
    "href": "lectures/lec02-basic-r.html#lesson-preamble",
    "title": "2  Introduction to R: Assignment, vectors, functions, strings, and loops",
    "section": "",
    "text": "2.1.1 Learning Objectives\n\nDo simple arithmetic operations in R using values and objects.\nCall functions and use arguments to change their default options.\nDefine our own functions\nCreate, inspect, and manipulate vectors\nCreate for-loops\nUse if-else statements\nDefine the following terms as they relate to R: variable, environment, function, arguments\nUse comments within code blocks",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: Assignment, vectors, functions, strings, and loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#getting-started-in-rstudio",
    "href": "lectures/lec02-basic-r.html#getting-started-in-rstudio",
    "title": "2  Introduction to R: Assignment, vectors, functions, strings, and loops",
    "section": "2.2 Getting started in RStudio",
    "text": "2.2 Getting started in RStudio\nBefore starting this lecture, make sure you’ve completed all the steps to download and install R and RStudio (or set up a POSIT Cloud account if you will access R online instead of on your laptop), and get familiar with the layout of RStudio, including two of the ways we can enter commands: in the console, and in an R script file.\nFor this lecture, we’ll start off just entering commands at the console, and then move on to writing longer multi-line code in an R script file.\nFor later lectures and assignments, we’ll go over and start to use the R Markdown notebook format. These lecture notes are actually made using one of these notebooks, so whenever we want to show some code, it will be in a grey box - and we actually executed that code to make these notes, so we know all our examples work as expected!",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: Assignment, vectors, functions, strings, and loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#simple-calculations-and-creating-objects-in-r",
    "href": "lectures/lec02-basic-r.html#simple-calculations-and-creating-objects-in-r",
    "title": "2  Introduction to R: Assignment, vectors, functions, strings, and loops",
    "section": "2.3 Simple calculations and creating objects in R",
    "text": "2.3 Simple calculations and creating objects in R\n\n2.3.1 Using R as a calculator\nAs we saw in our first class, you can get output from R simply by typing math in the console:\n\n3 + 5\n\n[1] 8\n\n\n\n12 / 7\n\n[1] 1.714286\n\n\nMost arithmetic operations can be done in R using intuitive notation, and the regular rules of order-of-operations and use of brackets apply, e.g.\n\n(1+2)^2 + (6/3)\n\n[1] 11\n\n\nIn addition to typical arithmetic operations like +, -, *, /, ^, R can use relational operators and will return TRUE or FALSE statements\n\n5 &lt;= 10\n\n[1] TRUE\n\n\n\n2 == 3\n\n[1] FALSE\n\n\n\n\n2.3.2 Creating objects\nHowever, to do useful and interesting things, we need to assign values to objects.\n\nx &lt;- 3\n\nThis command creates a new variable (a type of object) in R’s memory, and assigns it a value of 3. (The symbol &lt;- does this assignment. You can also use the = sign that other programming langauges use, but it can sometimes have slightly different behaviour, so R users almost always recommend using the &lt;- symbol instead). If you look in the Environment panel in the top right of RStudio, you should see x listed along with its value.\nWe can now perform calculations using x; the symbol will be replaced by the assigned value in every calculation.\n\nx + 5\n\n[1] 8\n\n\nYou can name an object in R almost anything you want:\n\njoel &lt;- 3\njoel + 5\n\n[1] 8\n\n\n\n2.3.2.1 Challenge\nSo far, we have created two variables, joel and x. What is the sum of these variables?\n\n\n2.3.2.2 Some tips on naming objects\n\nObjects can be given any name: x, current_temperature, thing, or subject_id.\nYou want your object names to be explicit and not too long.\nObject names cannot start with a number: x2 is valid, but 2x is not.\nR is also case sensitive: joel is different from Joel.\nAvoid using the names of existing functions (e.g. mean, df). You can check whether the name is already in use by using tab completion (when you type something in R and pause, it will make auto-complete suggestions if what you’re typing is already a function in R (and provide snippets of related documentation)).\nGenerally good to use underscores (_) to separate words in variable and function names\n\nIt is also recommended to use nouns for variable names, and verbs for function names. It’s important to be consistent in the styling of your code (where you put spaces, how you name variables, etc.). Using a consistent coding style1 makes your code clearer to read for your future self and your collaborators. RStudio will format code for you if you highlight a section of code and press Ctrl/Cmd + Shift + a.\n\n\n\n2.3.3 Preforming calculations on objects\nWhen assigning a value to an object, R does not print anything. You can force R to print the value by using parentheses or by typing the object name:\n\nweight_kg &lt;- 55    # doesn't print anything\n(weight_kg &lt;- 55)  # but putting parentheses around the call prints the value of `weight_kg`\n\n[1] 55\n\nweight_kg          # and so does typing the name of the object\n\n[1] 55\n\n\nThe variable weight_kg is stored in the computer’s memory where R can access it, and we can start doing arithmetic with it efficiently. For instance, we may want to convert this weight into pounds (weight in pounds is 2.2 times the weight in kg):\n\n2.2 * weight_kg\n\n[1] 121\n\n\nWe can also change a variable’s value by assigning it a new one:\n\nweight_kg &lt;- 57.5\n2.2 * weight_kg\n\n[1] 126.5\n\n\nThis means that assigning a value to one variable does not change the values of other variables. For example, let’s store the animal’s weight in pounds in a new variable, weight_lb:\n\nweight_lb &lt;- 2.2 * weight_kg # Actually, 1 kg = 2.204623 lbs\n\nand then change weight_kg to 100.\n\nweight_kg &lt;- 100\n\n\n2.3.3.1 Challenge\n\nWhat do you think is the current content of the object weight_lb? 126.5 or 220?\n\n\nweight_lb\n\n\nWhat are the values after each statement in the following?\n\n\nmass &lt;- 47.5\nage  &lt;- 122\nmass &lt;- mass * 2.0      # mass?\nage  &lt;- age - 20        # age?\nmass_index &lt;- mass/age  # mass_index?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: Assignment, vectors, functions, strings, and loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#functions-and-their-arguments",
    "href": "lectures/lec02-basic-r.html#functions-and-their-arguments",
    "title": "2  Introduction to R: Assignment, vectors, functions, strings, and loops",
    "section": "2.4 Functions and their arguments",
    "text": "2.4 Functions and their arguments\n\n2.4.1 Understanding functions\nFunctions can be thought of as recipes. You give a few ingredients as input to a function, and it will generate an output based on these ingredients. Just as with baking, both the ingredients and the actual recipe will influence what comes out of the recipe in the end: will it be a cake or a loaf of bread? In R, the inputs to a function are not called ingredients, but rather arguments, and the output is called the return value of the function. A function does not technically have to return a value, but often does so. Functions are used to automate more complicated sets of commands and many of them are already predefined in R. A typical example would be the function sqrt(). The input (the argument) must be a number, and the return value (in fact, the output) is the square root of that number. Executing a function (‘running it’) is called calling the function. An example of a function call is:\n\nsqrt(9)\n\n[1] 3\n\n\nWhich is the same as assigning the value to a variable and then passing that variable to the function:\n\na &lt;- 9\nb &lt;- sqrt(a)\nb\n\n[1] 3\n\n\nHere, the value of a is given to the sqrt() function, the sqrt() function calculates the square root, and returns the value which is then assigned to variable b. This function is very simple, because it takes just one argument.\nThe return ‘value’ of a function need not be numerical (like that of sqrt()), and it also does not need to be a single item: it can be a set of things, or even a dataset, as we will see later on.\nArguments can be anything, not only numbers or filenames, but also other objects. Exactly what each argument means differs per function, and must be looked up in the documentation (see below). Some functions take arguments which may either be specified by the user, or, if left out, take on a default value: these are called options. Options are typically used to alter the way the function operates, such as whether it ignores ‘bad values’, or what symbol to use in a plot. However, if you want something specific, you can specify a value of your choice which will be used instead of the default.\nR has many many built in functions, especially for common tasks we need to do in math and science! It’s one of the reasons it’s such a popular programming language.\nLet’s see some examples:\n\nexp(1) # calculate the exponential (Euler's number e)\n\n[1] 2.718282\n\n\n\nlog10(100) # take the log base 10\n\n[1] 2\n\n\n\nabs(-5) # get the absolute value\n\n[1] 5\n\n\n\nsign(-5) # get the sign of a number - positive, negative, or zero\n\n[1] -1\n\n\n\nsin(pi) # calcuate the sine of a number. R also has built in variables like pi\n\n[1] 1.224647e-16\n\n\n\npi\n\n[1] 3.141593\n\n\nSo far we’ve used functions that take only a single input argument. However, many R functions can take multiple arguments. Let’s try one: round().\n\nround(3.14159)\n\n[1] 3\n\n\nHere, we’ve called round() with just one argument, 3.14159, and it has returned the value 3. That’s because the default is to round to the nearest whole number, or integer. If we want more digits we can pass a second input argument to specify how many decimals we want to round to\n\nround(3.14159, 2)\n\n[1] 3.14\n\n\nArguments to functions in R always correspond to names of parameters the function uses in it’s calculation, and sometimes it’s helpful to explicitly specify them when we pass our values in\n\nround(x = 3.14159, digits = 2)\n\n[1] 3.14\n\n\nHere the parameter for the argument we want to round is named x, and the parameter specifying how many places after the decimal place to keep is called digits. Knowing this nomenclature is not essential for doing your own data analysis, but it will be very helpful when you are reading through help documents online and in RStudio.\nIf you provide the names for both the arguments, we can switch their order:\n\nround(digits = 2, x = 3.14159)\n\n[1] 3.14\n\n\n… which means we don’t have to worry about remembering the order if the input parameters as long as we remember their names!\nIt’s good practice to put the non-optional arguments (like the number you’re rounding) first in your function call, and to specify the names of all optional arguments. If you don’t, someone reading your code might have to look up the definition of a function with unfamiliar arguments to understand what you’re doing.\n\n\n2.4.2 Help with defined R functions\nThere a few ways to get help on functions in R.\n\nYou can type the function name directly into the Help document browser (by default, in the bottom right of RStudio, near the Files and Plots windows). For example, if you search for sqrt, you’ll land on the documentation page which describes the mathematics underlying the function, and the required (or optional) input arguments. As you can see, sqrt() takes only one argument, x, which needs to be a numerical vector. Don’t worry too much about the fact that it says vector here; we will talk more about that later. Briefly, a numerical vector is one or more numbers. In R, every number is a vector, so you don’t have to do anything special to create a vector. More on vectors later. Note that some function shave their own pages where other pages are for a set of similar functions.\nUse a question mark in front of the name of the function, which brings up the Help browser page\n\n\n?sqrt\n\n\nUse tab-completion:\n\nFor example, to access help about sqrt, type s and press Tab.\n\ns&lt;tab&gt;q\n\nYou can see that R gives you suggestions of what functions and variables are available that start with the letter s, and thanks to RStudio they are formatted in this nice list. There are many suggestions here, so let’s be a bit more specific and append a q, to find what we want. If we press enter or tab again, R will insert the selected option.\nYou can see that R inserts a pair of parentheses together with the name of the function. This is how the function syntax looks for R and many other programming languages, and it means that within these parentheses, we will specify all the arguments (the ingredients) that we want to pass to this function.\nIf we press tab again, R will helpfully display all the available parameters for this function that we can pass an argument to. The word parameter is used to describe the name that the argument can be passed to. More on that later.\n\nsqrt(&lt;tab&gt;\n\nThere are many things in this list, but only one of them is marked in purple. Purple here means that this list item is a parameter we can use for the function, while yellow means that it is a variable that we defined earlier.2\nIf you try this with the round function, you should see a list of the arguments and if you hover over any one, a description of that parameter\n\nround(&lt;tab&gt;\n\n(Screenshot of tab-complete suggestions)\n\nIf you don’t already know the name or beginning of the name of the function you want to use, RStudio’s built in help documentation search function is not always very helpful, and then a Google search is usually better!\n\n\n2.4.3 Writing functions\nIn this class, you will be working a lot with functions, especially those that someone else has already written. When you type sum, c(), or mean(), you are using a function that has been made previously and built into R. To remove some of the magic around these functions, we will go through how to make a basic function of our own. Let’s start with a simple example where we add two numbers together:\n\nadd_two_numbers &lt;- function(num1, num2) {\n    return(num1 + num2)\n}\nadd_two_numbers(4, 5)\n\n[1] 9\n\n\nAs you can see, running this function on two numbers returns their sum. We could also assign to a variable in the function and return the function.\n\nadd_two_numbers &lt;- function(num1, num2) {\n    my_sum &lt;- num1 + num2\n    return(my_sum)\n}\nadd_two_numbers(4, 5)\n\n[1] 9\n\n\nWhen you define your own function and run that section of code, the function becomes accessible in your environment, in the same way variables you define do (you should be able to see it in the Environment window in RStudio).\nHowever, variables defined only within a function are not accessible outside that function:\n\nmy_sum\n\nError:\n! object 'my_sum' not found\n\n\nFunctions can include calculations of any complexity within them. For example, we can write a function that calculates a person’s BMI from their measured weight (in lbs) and height (in inches) in a multistep process:\n\nget_bmi &lt;- function(weight_lbs,height_inches){\n  \n  weight_kg &lt;- weight_lbs/2.2 # convert weight from lbs to kg\n  height_cm &lt;- height_inches * 2.54 # convert height from inches to cm\n  height_m &lt;- height_cm/100 # convert height from cm to m\n  bmi &lt;- weight_kg/(height_m^2)  # get BMI\n  \n  return(bmi)\n}\n\nweight_lbs &lt;- 150\nheight_inches &lt;- 72\n\nget_bmi(weight_lbs, height_inches)\n\n[1] 20.38619\n\n\nIt’s good practice to give the parameters of a function intuitive names and to document the steps with comments as much as possible, so that if you or someone else goes to use your function later, you’ll remember what it does and how.\nCheck and see that now that you’ve created your own function, it shows up when you use tab-complete with information on the input parameters!\n\n2.4.3.1 Challenge\nCan you write a function that calculates the mean of 3 numbers?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: Assignment, vectors, functions, strings, and loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#vectors-and-data-types",
    "href": "lectures/lec02-basic-r.html#vectors-and-data-types",
    "title": "2  Introduction to R: Assignment, vectors, functions, strings, and loops",
    "section": "2.5 Vectors and data types",
    "text": "2.5 Vectors and data types\n\n2.5.1 Creating vectors\nA vector is the most common and basic data type in R, and is pretty much the workhorse of R. A vector is composed by a series of values, which can be either numbers or characters. We can assign a series of values to a vector using the c() function, which stands for “concatenate (combine/connect one after another) values into a vector” For example we can create a vector of animal weights and assign it to a new object weight_g:\n\nweight_g &lt;- c(50, 60, 65, 82) # Concatenate/Combine values into a vector\nweight_g\n\n[1] 50 60 65 82\n\n\nYou can also use built-in commands in R to create simple types of numeric vectors, for example:\nThe semi-colon : creates a vector of numbers increasing by 1 each time\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nThe seq function creates a sequence of numbers increasing by a fixed amount\n\nseq(0, 30) # This is the same as just `0:30`\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n[26] 25 26 27 28 29 30\n\nseq(0, 30, 3) # Every third number\n\n [1]  0  3  6  9 12 15 18 21 24 27 30\n\nseq(from = 0, to = 20, by = 2.5) # from 0 to 20 with step size 2.5\n\n[1]  0.0  2.5  5.0  7.5 10.0 12.5 15.0 17.5 20.0\n\n\nThe rep function creates a vector with the same value repeated a specified number of times\n\nrep(0,7)\n\n[1] 0 0 0 0 0 0 0\n\n\nA vector can also contain characters (i.e., letters) or strings (i.e., words/phrases):\n\nanimals &lt;- c('mouse', 'rat', 'dog')\nanimals\n\n[1] \"mouse\" \"rat\"   \"dog\"  \n\n\nThe quotes around “mouse”, “rat”, etc. are essential here and can be either single or double quotes. Without the quotes R will assume there are objects called mouse, rat and dog. As these objects don’t exist in R’s memory, there will be an error message.\nThere are many functions that allow you to inspect the content of a vector. length() tells you how many elements are in a particular vector:\n\nlength(weight_g)\n\n[1] 4\n\nlength(animals)\n\n[1] 3\n\n\nAn important feature of a vector is that all of the elements are the same type of data. The function class() indicates the class (the type of element) of an object:\n\nclass(weight_g)\n\n[1] \"numeric\"\n\nclass(animals)\n\n[1] \"character\"\n\n\nThe function str() provides an overview of the structure of an object and its elements. It is a useful function when working with large and complex objects:\n\nstr(weight_g)\n\n num [1:4] 50 60 65 82\n\nstr(animals)\n\n chr [1:3] \"mouse\" \"rat\" \"dog\"\n\n\nYou can use the c() function to add other elements to your vector:\n\nweight_g &lt;- c(weight_g, 90) # add to the end of the vector\nweight_g &lt;- c(30, weight_g) # add to the beginning of the vector\nweight_g\n\n[1] 30 50 60 65 82 90\n\n\nIn the first line, we take the original vector weight_g, add the value 90 to the end of it, and save the result back into weight_g. Then we add the value 30 to the beginning, again saving the result back into weight_g.\nWe can do this over and over again to grow a vector, or assemble a dataset. As we program, this may be useful to add results that we are collecting or calculating.\nAn atomic vector is the simplest R data type and it is a linear vector of a single type, e.g. all numbers. Above, we saw 2 of the 6 main atomic vector types that R uses: \"character\" and \"numeric\" (or \"double\"). These are the basic building blocks that all R objects are built from.\nVectors are one of the many data structures that R uses. Other important ones are lists (list), matrices (matrix), data frames (data.frame), factors (factor) and arrays (array). In this class, we will focus on data frames, which is most commonly used one for data analyses.\n\n2.5.1.1 Challenge\nWe’ve seen that atomic vectors can be of type character, numeric (or double), integer, and logical. But what happens if we try to mix these types in a single vector? Find out by using class to test these examples.\n\nnum_char &lt;- c(1, 2, 3, 'a')\nnum_logical &lt;- c(1, 2, 3, TRUE)\nchar_logical &lt;- c('a', 'b', 'c', TRUE)\ntricky &lt;- c(1, 2, 3, '4')\n\nThis happens because vectors can be of only one data type. Instead of throwing an error and saying that you are trying to mix different types in the same vector, R tries to convert (coerce) the content of this vector to find a “common denominator”. A logical can be turn into 1 or 0, and a number can be turned into a string/character representation. It would be difficult to do it the other way around: would 5 be TRUE or FALSE? What number would ‘t’ be?\nIn R, we call converting objects from one class into another class coercion. These conversions happen according to a hierarchy, whereby some types get preferentially coerced into other types. Can you draw a diagram that represents the hierarchy of how these data types are coerced?\nThis can be important to watch for in data sets that you import.\n\n\n\n2.5.2 Subsetting vectors\nIf we want to extract one or several values from a vector, we must provide one or several indices in square brackets. For instance:\n\nanimals &lt;- c(\"mouse\", \"rat\", \"dog\", \"cat\")\nanimals[2]\n\n[1] \"rat\"\n\nanimals[c(3, 2)]\n\n[1] \"dog\" \"rat\"\n\n\nWe can also repeat the indices to create an object with more elements than the original one:\n\nmore_animals &lt;- animals[c(1, 2, 3, 2, 1, 4)]\nmore_animals\n\n[1] \"mouse\" \"rat\"   \"dog\"   \"rat\"   \"mouse\" \"cat\"  \n\n\nR indices start at 1. Programming languages like Fortran, MATLAB, Julia, and R start counting at 1, because that’s what human beings typically do. Languages in the C family (including C++, Java, Perl, and Python) count from 0 because that was historically simpler for computers and can allow for more elegant code in some situations.\n\n\n2.5.3 Conditional subsetting\nAnother common way of subsetting is by using a logical vector. TRUE will select the element with the same index, while FALSE will not:\n\nweight_g &lt;- c(21, 34, 39, 54, 55)\nweight_g[c(TRUE, FALSE, TRUE, TRUE, FALSE)]\n\n[1] 21 39 54\n\n\nTypically, these logical vectors are not typed by hand, but are the output of other functions or logical tests. For instance, if you wanted to select only the values above 50:\n\nweight_g &gt; 50    # will return logicals with TRUE for the indices that meet the condition\n\n[1] FALSE FALSE FALSE  TRUE  TRUE\n\n## so we can use this to select only the values above 50\nweight_g[weight_g &gt; 50]\n\n[1] 54 55\n\n\nWe will consider conditions in more detail in the next few lectures.\n\n\n2.5.4 Strings and character vectors\nJust a small note about character vectors, also called strings. There are built-in packages for subsetting them that we’ll learn about later. They can be particularly relevant for ecological/genomics because important data can be nested in complicated strings of text (ex: extracting only the observations that occurred in wet habitats from a column of habitat descriptions or only genes with functions related to drought tolerance).\n\nstring1 &lt;- \"This is a string\" # you can include spaces between your quotes\nstring2 &lt;- c(string1, \"so is this\") # concatenate with another string\nstring2[2] # can access the second string via subsetting\n\n[1] \"so is this\"\n\n# Playing a bit with declaring variables\n\"You can include 'quotes' in a string\"\n\n[1] \"You can include 'quotes' in a string\"\n\nstring3 &lt;- 'You can include \"quotes\" in a string'\nstring3\n\n[1] \"You can include \\\"quotes\\\" in a string\"\n\n\"You can include \\\"matching quotes\\\" if you 'escape' them with a backslash (\\\\)\"\n\n[1] \"You can include \\\"matching quotes\\\" if you 'escape' them with a backslash (\\\\)\"\n\n\nYou can combine two separate strings into a single string using the paste function\n\npaste(\"Julius\",\"Caesar\")\n\n[1] \"Julius Caesar\"\n\n\n\npaste(\"pan\",\"cake\",sep=\"\")\n\n[1] \"pancake\"\n\n\nWhich can also be used to convert a vector of strings back into a single string\n\nemperor &lt;- c(\"Julius\",\"Caesar\")\npaste(emperor, collapse = \" \")\n\n[1] \"Julius Caesar\"",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: Assignment, vectors, functions, strings, and loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#loops-and-ifelse-statements",
    "href": "lectures/lec02-basic-r.html#loops-and-ifelse-statements",
    "title": "2  Introduction to R: Assignment, vectors, functions, strings, and loops",
    "section": "2.6 Loops and if/else statements",
    "text": "2.6 Loops and if/else statements\nLoops, specifically for-loops, are essential to programming. They allow us to follow a formula to do a repeated task in a systematic way. You can think of a for-loop as: “for each number contained in a list/vector, perform this operation” and the syntax basically says the same thing:\n\nv &lt;- c(2, 4, 6)\nfor (num in v) {\n    print(num)\n}\n\n[1] 2\n[1] 4\n[1] 6\n\n\nInstead of printing out every number to the console, we could also add numbers cumulatively, to calculate the sum of all the numbers in the vector:\n\n# To increment `w` each time, we must first create the variable,\n# which we do by setting `w &lt;- 0`, referred to as initializing.\n# This also ensures that `w` is zero at the start of the loop and\n# doesn't retain the value from last time we ran this code.\nw &lt;- 0\nfor (num in v) {\n    w &lt;- w + num\n}\nw\n\n[1] 12\n\n\nIf we put what we just did inside a function, we have essentially recreated the sum function in R.\n\nmy_sum &lt;- function(input_vector) {\n    vector_sum &lt;- 0\n    for (num in input_vector){\n        vector_sum &lt;- vector_sum + num\n    }\n    return(vector_sum)\n}\n\nmy_sum(v)\n\n[1] 12\n\n\nAlthough this gives us the same output as the built-in function sum, the built-in function has many more optimizations so it is much faster than our function.\nIn general, in R, it is always faster to try to find a way of doing things without writing a loop yourself, if possible. When you are reading about R, you might see suggestions that you should try to vectorize your code to make it faster. What people are referring to, is that you should not write for loops in R and instead use the ready-made functions that are much more efficient in working with vectors and essentially performs operations on entire vector at once instead of one number at a time. Conceptually, loops operate on one element at a time while vectorized code operates on all elements of a vector at once. However, sometimes there is no vectorized way of going about what you want to do (or at least, no easy, obvious, or quick-to-code vectorized way!).\nFor some calculations we can’t easily avoid for-loops. For example, let’s generate the first 25 values of the Fibonacci sequence\n\nfib_vec &lt;- rep(0,25)\nfib_vec[c(1,2)] &lt;- c(0,1)\n\nfor (i in 3:25){\n  fib_vec[i] &lt;- fib_vec[i-2]+fib_vec[i-1]\n}\n\nfib_vec\n\n [1]     0     1     1     2     3     5     8    13    21    34    55    89\n[13]   144   233   377   610   987  1597  2584  4181  6765 10946 17711 28657\n[25] 46368\n\n\nR also provides two other types of loops that can be helpful for repetitive or recursive tasks: while-loops (which continue iterating while a statement is true) and repeat loops (which continue repeating until a certain break condition is met).\nAnother very useful programming structure we can use in R, which is often combined with for-loops, is if/else statements. We can use this approach to only do an operation on an object if a certain condition is met, and do an alternative operation if the condition is not met.\nFor example, we could loop through a vector of ages and classify each individual as either an adult or a child\n\nages &lt;- c(67,13,45,3,7,34,90,8)\nage_cat &lt;-rep(\"\",length(ages)) # set up an empty character vector\n\nfor (i in 1:length(ages)){\n  \n  if (ages[i] &lt; 18){\n    age_cat[i] &lt;- \"child\"\n  }else{\n    age_cat[i] &lt;- \"adult\"\n  }\n  \n}\n\nprint(age_cat)\n\n[1] \"adult\" \"child\" \"adult\" \"child\" \"child\" \"adult\" \"adult\" \"child\"\n\n\nFor simple if/else statements, R provides a shorter notation\n\nages &lt;- c(67,13,45,3,7,34,90,8)\nage_cat &lt;-rep(\"\",length(ages)) # set up an empty character vector\n\nfor (i in 1:length(ages)){\n  \n  age_cat[i] &lt;- ifelse(ages[i] &lt; 18,\"child\",\"adult\")\n  \n}\n\nprint(age_cat)\n\n[1] \"adult\" \"child\" \"adult\" \"child\" \"child\" \"adult\" \"adult\" \"child\"\n\n\nFor more complex criteria, multiple “else” statements can be used:\n\nages &lt;- c(67,13,45,3,7,34,90,8)\nage_cat &lt;-rep(\"\",length(ages)) # set up an empty character vector\n\nfor (i in 1:length(ages)){\n  \n  if (ages[i] &lt; 18){\n    age_cat[i] &lt;- \"child\"\n  }else if (ages[i] &lt; 65){\n    age_cat[i] &lt;- \"adult\"\n  }else{\n    age_cat[i] &lt;- \"senior\"\n  }\n  \n}\n\nprint(age_cat)\n\n[1] \"senior\" \"child\"  \"adult\"  \"child\"  \"child\"  \"adult\"  \"senior\" \"child\" \n\n\nNote that this is an example of a computation that could be sped up by vectorization:\n\nages &lt;- c(67,13,45,3,7,34,90,8)\nage_cat &lt;-rep(\"\",length(ages))\n\nage_cat[ages&lt;18] &lt;-\"child\"\nage_cat[ages&gt;18 & ages&lt;65] &lt;-\"adult\"\nage_cat[ages&gt;65] &lt;-\"senior\"\nage_cat\n\n[1] \"senior\" \"child\"  \"adult\"  \"child\"  \"child\"  \"adult\"  \"senior\" \"child\" \n\n# OR\n# age_cat &lt;- ifelse(ages &lt; 18, \"child\",ifelse(ages&gt;65, \"senior\", \"adult\"))",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: Assignment, vectors, functions, strings, and loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec02-basic-r.html#footnotes",
    "href": "lectures/lec02-basic-r.html#footnotes",
    "title": "2  Introduction to R: Assignment, vectors, functions, strings, and loops",
    "section": "",
    "text": "Refer to the tidy style guide for which style to adhere to.↩︎\nThere are a few other symbols as well, all of which can be viewed at the end of this post about RStudio code completion.↩︎",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R: Assignment, vectors, functions, strings, and loops</span>"
    ]
  },
  {
    "objectID": "lectures/lec03-data-wrangling.html",
    "href": "lectures/lec03-data-wrangling.html",
    "title": "3  Manipulating and analyzing data",
    "section": "",
    "text": "3.1 Lesson preamble",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating and analyzing data</span>"
    ]
  },
  {
    "objectID": "lectures/lec03-data-wrangling.html#lesson-preamble",
    "href": "lectures/lec03-data-wrangling.html#lesson-preamble",
    "title": "3  Manipulating and analyzing data",
    "section": "",
    "text": "3.1.1 Learning objectives\n\nLoad external data from a .csv file into a data frame in R using read_csv\nDescribe what a data frame is\nLearn multiple ways to “index” (ie., the values from) data frames\nUnderstand the purpose of the dplyr package.\nUse dplyr functions to summarize the contents of data frames (e.g., str, head, tail, summary, dim, ncol, nrow, colnames, rownames)\nLearn to use data wrangling commands select, filter,and mutate from the dplyr package\nCreate multi-step data-manipulation processes using pipes (%&gt;%) to chains steps together",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating and analyzing data</span>"
    ]
  },
  {
    "objectID": "lectures/lec03-data-wrangling.html#set-up",
    "href": "lectures/lec03-data-wrangling.html#set-up",
    "title": "3  Manipulating and analyzing data",
    "section": "3.2 Set up",
    "text": "3.2 Set up\nTo do this lecture, we’ll need to be able to create and knit R Markdown documents, and analyze data using functions contained in the tidyverse package. If you haven’t done either of these tasks before, navigate to the course notes for Lecture 1 and follow the instructions there first. Then, open a new document (File &gt; New File &gt; R Markdown) to run commands during this lecture.\nWhen we make Markdown documents, we want them to be shareable and reproducible, so it’s good practice to include code to install and load any packages the code requires, even if we’ve already done it ourselves. While packages only need to be installed once (they’ll still be there even if RStudio is closed and reopened again or if R is accessed outside of RStudio), someone we share our code with might have never used them before. Or, if you move from using your code on one computer to another, or one computer to the POSIT Cloud, you’ll need to re-install.\nWe can add code install new packages using the function install.packages(). We’ll pass eval=FALSE to knitr at the top of our code chunk to make sure that the chunk won’t be evaluated when we knit the document, since we don’t really need to install it each time, and this option can be changed by another user. You can find other possible options to pass that can be helpful for formatting your output document.\n\ninstall.packages('tidyverse')\n\nThe two tidyverse packages we will be using the most frequently in this course is dplyr and ggplot2. dplyr is great for data wrangling (Lecture 3) and ggplot2 makes killer plots (Lecture 4).\nTo use functions in the dplyr package, type dplyr:: and then the function name.\n\ndplyr::glimpse(cars) # `glimpse` is similar to `str`\n\nRows: 50\nColumns: 2\n$ speed &lt;dbl&gt; 4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13…\n$ dist  &lt;dbl&gt; 2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34…\n\n# cars is a built-in data set\n\nSince we will be using this package a lot, it would be a little annoying to have to type dplyr:: every time. We can bypass this step by loading the package into our current environment. Think of this is “opening” the package for your work session.\n\n# We could also do `library(dplyr)`, but we need the rest of the\n# tidyverse packages later, so we might as well import the entire collection.\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nglimpse(cars)\n\nRows: 50\nColumns: 2\n$ speed &lt;dbl&gt; 4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13…\n$ dist  &lt;dbl&gt; 2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34…\n\n\nThis needs to be done once for every new R session, and so it is common practice to keep a list of all the packages used at the top of your script or notebook for convenience and load all of it at start up.\nYou might notice a lot of red text printed in the console! What are these warning signs and checks?\nAll the warning signs indicate are the version of R that they were built under. They can frequently be ignored unless your version of R is so old that the packages can no longer be run on R! Note that packages are frequently updated, and functions may become deprecated.\nNext, the warning shows you all the packages that were successfully installed.\nFinally, there are usually some conflicts reported. All this means is that there are multiple functions with the same name that may do different things. R prioritizes functions from certain packages over others. So, in this case, the filter() function from dplyr will take precedent over the filter() function from the stats package. If you want to use the latter, use double colons :: to indicate that you are calling a function from a certain package:",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating and analyzing data</span>"
    ]
  },
  {
    "objectID": "lectures/lec03-data-wrangling.html#dataset-background",
    "href": "lectures/lec03-data-wrangling.html#dataset-background",
    "title": "3  Manipulating and analyzing data",
    "section": "3.3 Dataset background",
    "text": "3.3 Dataset background\nToday, we will be working with real data from a longitudinal study of the species abundance in the Chihuahuan desert ecosystem near Portal, Arizona, USA. This study includes observations of plants, ants, and rodents from 1977 - 2002, and has been used in over 100 publications.\nMore information is available in the abstract of this paper from 2009, in this pre-print, and on the Portal Project’s website. There are several datasets available related to this study, and we will be working with datasets that have been preprocessed by the Data Carpentry to facilitate teaching. These are made available online as The Portal Project Teaching Database, both at the Data Carpentry website, and on Figshare. Figshare is a great place to publish data, code, figures, and more openly to make them available for other researchers and to communicate findings that are not part of a longer paper.\n\n\n\n\n\nThe Portal Project is a long-term ecological study in the a desert ecosystem in Arizona, which includes frequent sampling over 30+ years and controlled manipulations of the ecosystem\n\n\n\n\n\n3.3.1 Presentation of the survey data\nWe are studying the species and weight of animals caught in plots in our study area. The dataset is stored as a comma separated value (CSV) file. Each row holds information for a single animal, and the columns represent:\n\n\n\nColumn\nDescription\n\n\n\n\nrecord_id\nunique id for the observation\n\n\nmonth\nmonth of observation\n\n\nday\nday of observation\n\n\nyear\nyear of observation\n\n\nplot_id\nID of a particular plot\n\n\nspecies_id\n2-letter code\n\n\nsex\nsex of animal (“M”, “F”)\n\n\nhindfoot_length\nlength of the hindfoot in mm\n\n\nweight\nweight of the animal in grams\n\n\ngenus\ngenus of animal\n\n\nspecies\nspecies of animal\n\n\ntaxa\ne.g. rodent, reptile, bird, rabbit\n\n\nplot_type\ntype of plot\n\n\n\nTo read the data into R, we are going to use a function called read_csv. This function is contained in an R-package called readr. R-packages are a bit like browser extensions; they are not essential, but can provide nifty functionality. We will go through R-packages in general and which ones are good for data analyses. One useful option that read_csv includes, is the ability to read a CSV file directly from a URL, without downloading it in a separate step:\n\nlibrary(readr)\n\n\nsurveys &lt;- readr::read_csv('https://ndownloader.figshare.com/files/2292169')\n\nHowever, it is often a good idea to download the data first, so you have a copy stored locally on your computer in case you want to do some offline analyses, or the online version of the file changes or the file is taken down. You can either download the data manually or from within R:\n\ndownload.file(\"https://ndownloader.figshare.com/files/2292169\",\n              \"data/portal_data.csv\") # Saves to current directory with this name\n\nThe data is read in by specifying its local path.\n\nsurveys &lt;- readr::read_csv('data/portal_data.csv')\n\nRows: 34786 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): species_id, sex, genus, species, taxa, plot_type\ndbl (7): record_id, month, day, year, plot_id, hindfoot_length, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis statement produces some output regarding which data type it found in each column. If we want to check this in more detail, we can print the variable’s value: surveys.\n\nsurveys\n\n# A tibble: 34,786 × 13\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2        72     8    19  1977       2 NL         M                  31     NA\n 3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n 4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n 5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n 6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n 7       435    12    10  1977       2 NL         &lt;NA&gt;               NA     NA\n 8       506     1     8  1978       2 NL         &lt;NA&gt;               NA     NA\n 9       588     2    18  1978       2 NL         M                  NA    218\n10       661     3    11  1978       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 34,776 more rows\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\nThis displays a nice tabular view of the data, which also includes pagination when there are many rows and we can click the arrow to view all the columns. Technically, this object is actually a tibble rather than a data frame, as indicated in the output. The reason for this is that read_csv automatically converts the data into to a tibble when loading it. Since a tibble is just a data frame with some convenient extra functionality, we will use these words interchangeably from now on.\nIf we just want to glance at how the data frame looks, it is sufficient to display only the top (the first 6 lines) using the function head():\n\nhead(surveys)\n\n# A tibble: 6 × 13\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1         1     7    16  1977       2 NL         M                  32     NA\n2        72     8    19  1977       2 NL         M                  31     NA\n3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating and analyzing data</span>"
    ]
  },
  {
    "objectID": "lectures/lec03-data-wrangling.html#what-are-data-frames",
    "href": "lectures/lec03-data-wrangling.html#what-are-data-frames",
    "title": "3  Manipulating and analyzing data",
    "section": "3.4 What are data frames?",
    "text": "3.4 What are data frames?\nData frames are the de facto data structure for most tabular data, and what we use for statistics and plotting. A data frame can be created by hand, but most commonly they are generated by the function read_csv(); in other words, when importing spreadsheets from your hard drive (or the web).\nA data frame is a representation of data in the format of a table where the columns are vectors that all have the same length. Because the columns are vectors, they all contain the same type of data as we discussed in last class (e.g., characters, integers, factors). We can see this when inspecting the structure of a data frame with the function str():\n\nstr(surveys)\n\nspc_tbl_ [34,786 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ record_id      : num [1:34786] 1 72 224 266 349 363 435 506 588 661 ...\n $ month          : num [1:34786] 7 8 9 10 11 11 12 1 2 3 ...\n $ day            : num [1:34786] 16 19 13 16 12 12 10 8 18 11 ...\n $ year           : num [1:34786] 1977 1977 1977 1977 1977 ...\n $ plot_id        : num [1:34786] 2 2 2 2 2 2 2 2 2 2 ...\n $ species_id     : chr [1:34786] \"NL\" \"NL\" \"NL\" \"NL\" ...\n $ sex            : chr [1:34786] \"M\" \"M\" NA NA ...\n $ hindfoot_length: num [1:34786] 32 31 NA NA NA NA NA NA NA NA ...\n $ weight         : num [1:34786] NA NA NA NA NA NA NA NA 218 NA ...\n $ genus          : chr [1:34786] \"Neotoma\" \"Neotoma\" \"Neotoma\" \"Neotoma\" ...\n $ species        : chr [1:34786] \"albigula\" \"albigula\" \"albigula\" \"albigula\" ...\n $ taxa           : chr [1:34786] \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ...\n $ plot_type      : chr [1:34786] \"Control\" \"Control\" \"Control\" \"Control\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   record_id = col_double(),\n  ..   month = col_double(),\n  ..   day = col_double(),\n  ..   year = col_double(),\n  ..   plot_id = col_double(),\n  ..   species_id = col_character(),\n  ..   sex = col_character(),\n  ..   hindfoot_length = col_double(),\n  ..   weight = col_double(),\n  ..   genus = col_character(),\n  ..   species = col_character(),\n  ..   taxa = col_character(),\n  ..   plot_type = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nInteger refers to a whole number, such as 1, 2, 3, 4, etc. Numbers with decimals, 1.0, 2.4, 3.333, are referred to as floats. Factors are used to represent categorical data. Factors can be ordered or unordered, and understanding them is necessary for statistical analysis and for plotting. Factors are stored as integers, and have labels (text) associated with these unique integers. While factors look (and often behave) like character vectors, they are actually integers under the hood, and you need to be careful when treating them like strings.\n\n3.4.1 Inspecting data.frame objects\nWe already saw how the functions head() and str() can be useful to check the content and the structure of a data frame. Here is a non-exhaustive list of functions to get a sense of the content/structure of the data. Let’s try them out!\n\nSize:\n\ndim(surveys) - returns a vector with the number of rows in the first element and the number of columns as the second element (the dimensions of the object)\nnrow(surveys) - returns the number of rows\nncol(surveys) - returns the number of columns\n\nContent:\n\nhead(surveys) - shows the first 6 rows\ntail(surveys) - shows the last 6 rows\n\nNames:\n\nnames(surveys) - returns the column names (synonym of colnames() for data.frame objects)\nrownames(surveys) - returns the row names\n\nSummary:\n\nstr(surveys) - structure of the object and information about the class, length, and content of each column\nsummary(surveys) - summary statistics for each column\n\n\nNote: most of these functions are “generic”, they can be used on other types of objects besides data.frame.\n\n3.4.1.1 Challenge\nBased on the output of str(surveys), can you answer the following questions?\n\nWhat is the class of the object surveys?\nHow many rows and how many columns are in this object?\nWhat range of years are included in this dataset?\nWhat is the mean hindfoot length of all observed animals?\n\n\n\n\n3.4.2 Indexing and subsetting data frames\nOur survey data frame has rows and columns (it has 2 dimensions). If we want to extract some specific data from it, we need to specify the “coordinates” we want from it. Row numbers come first, followed by column numbers. When indexing, base R data frames return a different format depending on how we index the data (i.e. either a vector or a data frame), but with enhanced data frames, tibbles, the returned object is almost always a data frame.\n\nsurveys[1, 1]   # first element in the first column of the data frame\n\n# A tibble: 1 × 1\n  record_id\n      &lt;dbl&gt;\n1         1\n\nsurveys[1, 6]   # first element in the 6th column\n\n# A tibble: 1 × 1\n  species_id\n  &lt;chr&gt;     \n1 NL        \n\nsurveys[, 1]    # first column in the data frame\n\n# A tibble: 34,786 × 1\n   record_id\n       &lt;dbl&gt;\n 1         1\n 2        72\n 3       224\n 4       266\n 5       349\n 6       363\n 7       435\n 8       506\n 9       588\n10       661\n# ℹ 34,776 more rows\n\nsurveys[1]      # first column in the data frame\n\n# A tibble: 34,786 × 1\n   record_id\n       &lt;dbl&gt;\n 1         1\n 2        72\n 3       224\n 4       266\n 5       349\n 6       363\n 7       435\n 8       506\n 9       588\n10       661\n# ℹ 34,776 more rows\n\nsurveys[1:3, 7] # first three elements in the 7th column\n\n# A tibble: 3 × 1\n  sex  \n  &lt;chr&gt;\n1 M    \n2 M    \n3 &lt;NA&gt; \n\nsurveys[3, ]    # the 3rd element for all columns\n\n# A tibble: 1 × 13\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\nsurveys[1:6, ]  # equivalent to head(surveys)\n\n# A tibble: 6 × 13\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1         1     7    16  1977       2 NL         M                  32     NA\n2        72     8    19  1977       2 NL         M                  31     NA\n3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\n: is a special operator that creates numeric vectors of integers in increasing or decreasing order; test 1:10 and 10:1 for instance. This works similarly to seq, which we looked at earlier in class:\n\n0:10\n\n [1]  0  1  2  3  4  5  6  7  8  9 10\n\nseq(0, 10)\n\n [1]  0  1  2  3  4  5  6  7  8  9 10\n\n# We can test if all elements are the same\n0:10 == seq(0,10)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\nall(0:10 == seq(0,10))\n\n[1] TRUE\n\n\nYou can also exclude certain parts of a data frame using the “-” sign:\n\nsurveys[,-1]    # All columns, except the first\n\n# A tibble: 34,786 × 12\n   month   day  year plot_id species_id sex   hindfoot_length weight genus  \n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  \n 1     7    16  1977       2 NL         M                  32     NA Neotoma\n 2     8    19  1977       2 NL         M                  31     NA Neotoma\n 3     9    13  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 4    10    16  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 5    11    12  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 6    11    12  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 7    12    10  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 8     1     8  1978       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 9     2    18  1978       2 NL         M                  NA    218 Neotoma\n10     3    11  1978       2 NL         &lt;NA&gt;               NA     NA Neotoma\n# ℹ 34,776 more rows\n# ℹ 3 more variables: species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\nsurveys[-c(7:34786),] # Equivalent to head(surveys)\n\n# A tibble: 6 × 13\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1         1     7    16  1977       2 NL         M                  32     NA\n2        72     8    19  1977       2 NL         M                  31     NA\n3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\nAs well as using numeric values to subset a data.frame (or matrix), columns can be called by name, using one of the four following notations: \n\nsurveys[\"species_id\"]       # Result is a data.frame\n\n# A tibble: 34,786 × 1\n   species_id\n   &lt;chr&gt;     \n 1 NL        \n 2 NL        \n 3 NL        \n 4 NL        \n 5 NL        \n 6 NL        \n 7 NL        \n 8 NL        \n 9 NL        \n10 NL        \n# ℹ 34,776 more rows\n\nsurveys[, \"species_id\"]     # Result is a data.frame\n\n# A tibble: 34,786 × 1\n   species_id\n   &lt;chr&gt;     \n 1 NL        \n 2 NL        \n 3 NL        \n 4 NL        \n 5 NL        \n 6 NL        \n 7 NL        \n 8 NL        \n 9 NL        \n10 NL        \n# ℹ 34,776 more rows\n\n\nFor our purposes, these notations are equivalent. RStudio knows about the columns in your data frame, so you can take advantage of the autocompletion feature to get the full and correct column name.\nAnother syntax that is often used to specify column names is $. In this case, the returned object is actually a vector. We will not go into detail about this, but since it is such common usage, it is good to be aware of this.\n\n# We use `head()` since the output from vectors are not automatically cut off\n# and we don't want to clutter the screen with all the `species_id` values\nhead(surveys$species_id)          # Result is a vector\n\n[1] \"NL\" \"NL\" \"NL\" \"NL\" \"NL\" \"NL\"\n\n\n\n3.4.2.1 Challenge\n\nCreate a data.frame (surveys_200) containing only the observations from row 200 of the surveys dataset.\nNotice how nrow() gave you the number of rows in a data.frame?\n\nUse that number to pull out just that last row in the data frame.\nCompare that with what you see as the last row using tail() to make sure it’s meeting expectations.\nPull out that last row using nrow() instead of the row number.\nCreate a new data frame object (surveys_last) from that last row.\n\nUse nrow() to extract the row that is in the middle of the data frame. Store the content of this row in an object named surveys_middle.\nCombine nrow() with the - notation above to reproduce the behavior of head(surveys) keeping just the first through 6th rows of the surveys dataset.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating and analyzing data</span>"
    ]
  },
  {
    "objectID": "lectures/lec03-data-wrangling.html#data-wrangling-with-dplyr",
    "href": "lectures/lec03-data-wrangling.html#data-wrangling-with-dplyr",
    "title": "3  Manipulating and analyzing data",
    "section": "3.5 Data wrangling with dplyr",
    "text": "3.5 Data wrangling with dplyr\nWrangling here is used in the sense of maneuvering, managing, controlling, and turning your data upside down and inside out to look at it from different angles in order to understand it. The package dplyr provides easy tools for the most common data manipulation tasks. It is built to work directly with data frames, with many common tasks optimized by being written in a compiled language (C++), this means that many operations run much faster than similar tools in R. An additional feature is the ability to work directly with data stored in an external database, such as SQL-databases. The ability to work with databases is great because you are able to work with much bigger datasets (100s of GB) than your computer could normally handle. We will not talk in detail about this in class, but there are great resources online to learn more (e.g. this lecture from Data Carpentry).\n\n3.5.1 Selecting columns and filtering rows\nWe’re going to learn some of the most common dplyr functions: select(), filter(), mutate(), group_by(), and summarise(). To select columns of a data frame, use select(). The first argument to this function is the data frame (surveys), and the subsequent arguments are the columns to keep. Note that we don’t need quotation marks around the column names here like with did with base R. You do still need quotation marks around strings, though!\n\n\n# A tibble: 34,786 × 4\n   plot_id species_id weight  year\n     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1       2 NL             NA  1977\n 2       2 NL             NA  1977\n 3       2 NL             NA  1977\n 4       2 NL             NA  1977\n 5       2 NL             NA  1977\n 6       2 NL             NA  1977\n 7       2 NL             NA  1977\n 8       2 NL             NA  1978\n 9       2 NL            218  1978\n10       2 NL             NA  1978\n# ℹ 34,776 more rows\n\n\nTo choose rows based on a specific criteria, use filter():\n\n\n# A tibble: 1,180 × 13\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1     22314     6     7  1995       2 NL         M                  34     NA\n 2     22728     9    23  1995       2 NL         F                  32    165\n 3     22899    10    28  1995       2 NL         F                  32    171\n 4     23032    12     2  1995       2 NL         F                  33     NA\n 5     22003     1    11  1995       2 DM         M                  37     41\n 6     22042     2     4  1995       2 DM         F                  36     45\n 7     22044     2     4  1995       2 DM         M                  37     46\n 8     22105     3     4  1995       2 DM         F                  37     49\n 9     22109     3     4  1995       2 DM         M                  37     46\n10     22168     4     1  1995       2 DM         M                  36     48\n# ℹ 1,170 more rows\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\n\n3.5.1.1 An aside on conditionals\nNote that to check for equality, R requires two equal signs (==). This is to prevent confusion with object assignment, since otherwise year = 1995 might be interpreted as ‘set the year parameter to 1995’, which is not what filter does!\nBasic conditionals in R are broadly similar to how they’re already expressed mathematically:\n\n2 &lt; 3\n\n[1] TRUE\n\n5 &gt; 9\n\n[1] FALSE\n\n\nHowever, there are a few idiosyncrasies to be mindful of for other conditionals:\n\n2 != 3 # not equal\n\n[1] TRUE\n\n2 &lt;= 3 # less than or equal to\n\n[1] TRUE\n\n5 &gt;= 9 # greater than or equal to\n\n[1] FALSE\n\n\nFinally, the %in% operator is used to check for membership:\n\n2 %in% c(2, 3, 4) # check whether 2 in c(2, 3, 4)\n\n[1] TRUE\n\n\nAll of the above conditionals are compatible with filter, with the key difference being that filter expects column names as part of conditional statements instead of individual numbers.\n\n\n\n3.5.2 Chaining functions together using pipes\nBut what if you wanted to select and filter at the same time? There are three ways to do this: use intermediate steps, nested functions, or pipes. With intermediate steps, you essentially create a temporary data frame and use that as input to the next function. This can clutter up your workspace with lots of objects:\n\n\n# A tibble: 1,180 × 4\n   plot_id species_id weight  year\n     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1       2 NL             NA  1995\n 2       2 NL            165  1995\n 3       2 NL            171  1995\n 4       2 NL             NA  1995\n 5       2 DM             41  1995\n 6       2 DM             45  1995\n 7       2 DM             46  1995\n 8       2 DM             49  1995\n 9       2 DM             46  1995\n10       2 DM             48  1995\n# ℹ 1,170 more rows\n\n\nYou can also nest functions (i.e. one function inside of another). This is handy, but can be difficult to read if too many functions are nested as things are evaluated from the inside out. Readability can be mildly improved by enabling “rainbow parentheses” (open settings &gt; Code &gt; Display and check rainbow parentheses), but it’s still basically impossible to document and effectively convey your work with this method.\n\nfilter(select(surveys, plot_id, species_id, weight, year), year == 1995)\n\n# A tibble: 1,180 × 4\n   plot_id species_id weight  year\n     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1       2 NL             NA  1995\n 2       2 NL            165  1995\n 3       2 NL            171  1995\n 4       2 NL             NA  1995\n 5       2 DM             41  1995\n 6       2 DM             45  1995\n 7       2 DM             46  1995\n 8       2 DM             49  1995\n 9       2 DM             46  1995\n10       2 DM             48  1995\n# ℹ 1,170 more rows\n\n\nThe last option, pipes, are a fairly recent addition to R. Pipes let you take the output of one function and send it directly to the next, which is useful when you need to do many things to the same dataset. Pipes in R look like %&gt;% and are made available via the magrittr package that also is included in the tidyverse. If you use RStudio, you can type the pipe with Ctrl/Cmd + Shift + M.\n\nsurveys %&gt;% \n    select(., plot_id, species_id, weight, year) %&gt;% \n    filter(., year == 1995)\n\n# A tibble: 1,180 × 4\n   plot_id species_id weight  year\n     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1       2 NL             NA  1995\n 2       2 NL            165  1995\n 3       2 NL            171  1995\n 4       2 NL             NA  1995\n 5       2 DM             41  1995\n 6       2 DM             45  1995\n 7       2 DM             46  1995\n 8       2 DM             49  1995\n 9       2 DM             46  1995\n10       2 DM             48  1995\n# ℹ 1,170 more rows\n\n\nThe . refers to the object that is passed from the previous line. In this example, the data frame surveys is passed to the . in the select() statement. Then, the modified data frame which is the result of the select() operation, is passed to the . in the filter() statement. Put more simply: whatever was the result from the line above the current line, will be used in the current line.\nSince it gets a bit tedious to write out all the dots, dplyr allows for them to be omitted. By default, the pipe will pass its input to the first argument of the right hand side function; in dplyr, the first argument is always a data frame. The chunk below gives the same output as the one above:\n\nsurveys %&gt;% \n    select(plot_id, species_id, weight, year) %&gt;% \n    filter(year == 1995)\n\n# A tibble: 1,180 × 4\n   plot_id species_id weight  year\n     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1       2 NL             NA  1995\n 2       2 NL            165  1995\n 3       2 NL            171  1995\n 4       2 NL             NA  1995\n 5       2 DM             41  1995\n 6       2 DM             45  1995\n 7       2 DM             46  1995\n 8       2 DM             49  1995\n 9       2 DM             46  1995\n10       2 DM             48  1995\n# ℹ 1,170 more rows\n\n\nAnother example:\n\nsurveys %&gt;%\n  filter(weight &lt; 5) %&gt;%\n  select(species_id, sex, weight)\n\n# A tibble: 17 × 3\n   species_id sex   weight\n   &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt;\n 1 PF         F          4\n 2 PF         F          4\n 3 PF         M          4\n 4 RM         F          4\n 5 RM         M          4\n 6 PF         &lt;NA&gt;       4\n 7 PP         M          4\n 8 RM         M          4\n 9 RM         M          4\n10 RM         M          4\n11 PF         M          4\n12 PF         F          4\n13 RM         M          4\n14 RM         M          4\n15 RM         F          4\n16 RM         M          4\n17 RM         M          4\n\n\nIn the above code, we use the pipe to send the surveys dataset first through filter() to keep rows where weight is less than 5, then through select() to keep only the species_id, sex, and weight columns. Since %&gt;% takes the object on its left and passes it as the first argument to the function on its right, we don’t need to explicitly include it as an argument to the filter() and select() functions anymore.\nIf this runs off your screen and you just want to see the first few rows, you can use a pipe to view the head() of the data. (Pipes work with non-dplyr functions, too, as long as either the dplyr or magrittr package is loaded).\n\nsurveys %&gt;%\n  filter(weight &lt; 5) %&gt;%\n  select(species_id, sex, weight) %&gt;% \n  head()\n\n# A tibble: 6 × 3\n  species_id sex   weight\n  &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt;\n1 PF         F          4\n2 PF         F          4\n3 PF         M          4\n4 RM         F          4\n5 RM         M          4\n6 PF         &lt;NA&gt;       4\n\n\nIf we wanted to create a new object with this smaller version of the data, we could do so by assigning it a new name:\n\nsurveys_sml &lt;- surveys %&gt;%\n  filter(weight &lt; 5) %&gt;%\n  select(species_id, sex, weight)\n\nsurveys_sml\n\n# A tibble: 17 × 3\n   species_id sex   weight\n   &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt;\n 1 PF         F          4\n 2 PF         F          4\n 3 PF         M          4\n 4 RM         F          4\n 5 RM         M          4\n 6 PF         &lt;NA&gt;       4\n 7 PP         M          4\n 8 RM         M          4\n 9 RM         M          4\n10 RM         M          4\n11 PF         M          4\n12 PF         F          4\n13 RM         M          4\n14 RM         M          4\n15 RM         F          4\n16 RM         M          4\n17 RM         M          4\n\n\nNote that the final data frame is the leftmost part of this expression.\nA single expression can also be used to filter for several criteria, either matching all criteria (&) or any criteria (|):\n\nsurveys %&gt;% \n    filter(taxa == 'Rodent' & sex == 'F') %&gt;% \n    select(sex, taxa)\n\n# A tibble: 15,690 × 2\n   sex   taxa  \n   &lt;chr&gt; &lt;chr&gt; \n 1 F     Rodent\n 2 F     Rodent\n 3 F     Rodent\n 4 F     Rodent\n 5 F     Rodent\n 6 F     Rodent\n 7 F     Rodent\n 8 F     Rodent\n 9 F     Rodent\n10 F     Rodent\n# ℹ 15,680 more rows\n\n\n\nsurveys %&gt;% \n    filter(species == 'clarki' | species == 'leucophrys') %&gt;% \n    select(species, taxa)\n\n# A tibble: 3 × 2\n  species    taxa   \n  &lt;chr&gt;      &lt;chr&gt;  \n1 leucophrys Bird   \n2 clarki     Reptile\n3 leucophrys Bird   \n\n\n\n3.5.2.1 Challenge\nUsing pipes, subset the survey data to include individuals collected before 1995 and retain only the columns year, sex, and weight.\n\n\n\n3.5.3 Creating new columns with mutate\nFrequently, you’ll want to create new columns based on the values in existing columns. For instance, you might want to do unit conversions, or find the ratio of values in two columns. For this we’ll use mutate().\nTo create a new column of weight in kg:\n\nsurveys %&gt;%\n    mutate(weight_kg = weight / 1000)\n\n# A tibble: 34,786 × 14\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2        72     8    19  1977       2 NL         M                  31     NA\n 3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n 4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n 5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n 6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n 7       435    12    10  1977       2 NL         &lt;NA&gt;               NA     NA\n 8       506     1     8  1978       2 NL         &lt;NA&gt;               NA     NA\n 9       588     2    18  1978       2 NL         M                  NA    218\n10       661     3    11  1978       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 34,776 more rows\n# ℹ 5 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;,\n#   weight_kg &lt;dbl&gt;\n\n\nYou can also create a second new column based on the first new column within the same call of mutate():\n\nsurveys %&gt;%\n    mutate(weight_kg = weight / 1000,\n           weight_kg2 = weight_kg * 2)\n\n# A tibble: 34,786 × 15\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2        72     8    19  1977       2 NL         M                  31     NA\n 3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n 4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n 5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n 6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n 7       435    12    10  1977       2 NL         &lt;NA&gt;               NA     NA\n 8       506     1     8  1978       2 NL         &lt;NA&gt;               NA     NA\n 9       588     2    18  1978       2 NL         M                  NA    218\n10       661     3    11  1978       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 34,776 more rows\n# ℹ 6 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;,\n#   weight_kg &lt;dbl&gt;, weight_kg2 &lt;dbl&gt;\n\n\nThe first few rows of the output are full of NAs, so if we wanted to remove those we could insert a filter() in the chain:\n\nsurveys %&gt;%\n    filter(!is.na(weight)) %&gt;%\n    mutate(weight_kg = weight / 1000)\n\n# A tibble: 32,283 × 14\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1       588     2    18  1978       2 NL         M                  NA    218\n 2       845     5     6  1978       2 NL         M                  32    204\n 3       990     6     9  1978       2 NL         M                  NA    200\n 4      1164     8     5  1978       2 NL         M                  34    199\n 5      1261     9     4  1978       2 NL         M                  32    197\n 6      1453    11     5  1978       2 NL         M                  NA    218\n 7      1756     4    29  1979       2 NL         M                  33    166\n 8      1818     5    30  1979       2 NL         M                  32    184\n 9      1882     7     4  1979       2 NL         M                  32    206\n10      2133    10    25  1979       2 NL         F                  33    274\n# ℹ 32,273 more rows\n# ℹ 5 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;,\n#   weight_kg &lt;dbl&gt;\n\n\nis.na() is a function that determines whether something is an NA. The ! symbol negates the result, so we’re asking for everything that is not an NA.\n\n3.5.3.1 Challenge\nCreate a new data frame from the surveys data that meets the following criteria: contains only the species_id column and a new column called hindfoot_half containing values that are half the hindfoot_length values. In this hindfoot_half column, there are no NAs and all values are less than 30.\nHint: think about how the commands should be ordered to produce this data frame!",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating and analyzing data</span>"
    ]
  },
  {
    "objectID": "lectures/lec04-data-visualization.html",
    "href": "lectures/lec04-data-visualization.html",
    "title": "4  Summarizing and visualizating data",
    "section": "",
    "text": "4.1 Lesson preamble\nLast lecture we learned about the impressive data collected in the long-term Portal desert ecology study and had a chance to look through some of the variables, but since there were so many data points (over 30,000!) and so many variables collected, it was hard to get a sense for what was really going on. There’s lot more analysis we can run, but usually it helps to first actually see what the data actually looks like. Luckily, our favourite package-of-packages tidyverse has us covered – it comes with a wonderful package for generating graphics called ggplot2!\nWe can load tidyverse into our current R session:\nlibrary(tidyverse)\nNow we’ll reload the data that we previously downloaded (and saved locally):\nsurveys &lt;- readr::read_csv('data/portal_data.csv')\n\nRows: 34786 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): species_id, sex, genus, species, taxa, plot_type\ndbl (7): record_id, month, day, year, plot_id, hindfoot_length, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n(If you weren’t able to successfully save it last time, you can instead read it in directly from the website):\nsurveys &lt;- readr::read_csv(\"https://ndownloader.figshare.com/files/2292169\")\nAnd let’s remind ourselves of the data structure by printing off a few of the entries\nsurveys[seq(1,nrow(surveys),1000),] # pick out every 1000th entry\n\n# A tibble: 35 × 13\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2      5282     1    25  1982       2 DS         M                  52     82\n 3     13321     8    26  1987       2 AB         &lt;NA&gt;               NA     NA\n 4     16167     6     4  1989       3 SH         F                  29     60\n 5       121     8    21  1977      15 NL         &lt;NA&gt;               NA     NA\n 6     35176    11    10  2002      15 PB         F                  26     36\n 7     18921     8     7  1991      17 PF         F                  16      8\n 8     32062     5    26  2001      17 PB         M                  27     29\n 9     25542     4    12  1997      12 DM         F                  35     42\n10     21937    12     4  1994      12 DO         M                  38     47\n# ℹ 25 more rows\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Summarizing and visualizating data</span>"
    ]
  },
  {
    "objectID": "lectures/lec04-data-visualization.html#lesson-preamble",
    "href": "lectures/lec04-data-visualization.html#lesson-preamble",
    "title": "4  Summarizing and visualizating data",
    "section": "",
    "text": "4.1.1 Learning Objectives\n\nUse ggplot to create simple scatterplots\nVary x, y, and colour variables in scatterplots to understand trends in data\nUse group_by, summarize, tally, and arrange to split a data frame into groups of observations, apply a summary statistics for each group, and then combine the results\nUse summarize_all to calculate a summary statistic for an entire data frame\nRemove missing data using is.na or na.omit\nCombine data manipulation with data visualization using split-apply-combine techniques, along with the pipe (%&gt;%) and layering (+) operators",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Summarizing and visualizating data</span>"
    ]
  },
  {
    "objectID": "lectures/lec04-data-visualization.html#plotting-with-ggplot2",
    "href": "lectures/lec04-data-visualization.html#plotting-with-ggplot2",
    "title": "4  Summarizing and visualizating data",
    "section": "4.2 Plotting with ggplot2",
    "text": "4.2 Plotting with ggplot2\nggplot2 is a plotting package that makes it simple to create complex plots from data frames. The name ggplot2 comes from its inspiration, the book A Grammar of Graphics, and the main goal is to allow coders to distill complex data structure and express their desired graphical outcome in a concise manner instead of telling the computer every detail about what should happen. For example, you would say “colour my data by species” instead of “go through this data frame and plot any observations of species1 in blue, any observations of species2 in red, etc”. Thanks to this functional way of interfacing with data, only minimal changes are required if the underlying data change or if you want to try a different type of visualization. Publication-quality plots can be created with minimal amounts of adjustment and tweaking.\nggplot2 graphics are built step by step by adding new elements, or layers. Adding layers in this fashion allows for extensive flexibility and customization of plots. To build a ggplot, we need to:\n1. Use the ggplot() function and bind the plot to a specific data frame using the data argument\n\nggplot(data = surveys)\n\n\n\n\n\n\n\n\nRemember, if the arguments are provided in the right order then the names of the arguments can be omitted.\n\nggplot(surveys)\n\n\n\n\n\n\n\n# You can also use the %&gt;% operator to pass the data to ggplot\nsurveys %&gt;% \n  ggplot()\n\n\n\n\n\n\n\n\n2. Define aesthetics (aes), by selecting the columns to be plotted and the presentation variables (ex: point size, shape, colour, etc.)\n\nggplot(surveys, aes(x = weight, y = hindfoot_length))\n\n\n\n\n\n\n\n\n3. Add geoms – geometrical objects as a graphical representation of the data in the plot (points, lines, bars). ggplot2 offers many different geoms. We will use a few common ones today, including: * geom_point() for scatter plots, dot plots, etc. * geom_line() for trend lines, time-series, etc. * geom_histogram() for histograms\nTo add a geom to the plot use + operator. Because we have two continuous variables (weight and hindfoot_length), let’s use geom_point() first:\n\nggplot(surveys, aes(x = weight, y = hindfoot_length)) +\n  geom_point()\n\nWarning: Removed 4048 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNote: Notice that triangle-! warning sign above the plot? ggplot is telling you that it wasn’t able to plot all of your data. Typically this means that there are NAs in the data, or that some data points lie outside of the bounds of the axes. Can you figure what it is in this instance?\nThe + in the ggplot2 package is particularly useful because it allows you to modify existing ggplot objects. This means you can easily set up plot “templates” and conveniently explore different types of plots. The above plot can be generated with code like this:\n\n# Assign plot to a variable\nsurveys_plot &lt;- ggplot(surveys, aes(x = weight, y = hindfoot_length))\n\n# Draw the plot\nsurveys_plot + geom_point()\n\nWarning: Removed 4048 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThree notes:\n\nAnything you put in the top ggplot() call can be seen/used by any geom layers that you add, including the x and y axis variables you set up in aes(). These are essentially universal plot settings.\nYou can specify aesthetics for a geom independently of the aesthetics defined by ggplot(), which is particularly handy when you’re layering data from different data frames\nThe + sign used to add layers must be placed at the end of each line containing a layer. If it’s used at the start of line, ggplot2 will not add the new layer and R will return an error message.\n\n\n4.2.1 Building plots iteratively\nBuilding plots with ggplot is typically an iterative process. Start simply. We will define the dataset to use, lay the axes, and choose one geom, as we just did:\n\nggplot(surveys, aes(x = weight, y = hindfoot_length)) +\n    geom_point()\n\nWarning: Removed 4048 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThen, we start modifying this plot to extract more information from it. For instance, we can add the argument for transparency (alpha) to reduce overplotting:\n\nggplot(data = surveys, aes(x = weight, y = hindfoot_length)) +\n    geom_point(alpha = 0.2)\n\nWarning: Removed 4048 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nBased on the hindfoot length and the weights, there appears to be 4 clusters in this data. Potentially, one of the categorical variables we have in the data could explain this pattern. Colouring the data points according to a categorical variable is an easy way to find out if there seems to be correlation. Let’s try colouring this points according to plot_type. As a reminder, this variable keeps track of whether the plot was subjected to one of the environmental manipulations, and in this teaching dataset, plot_type refers only to the types of rodents that were excluded from the plot).\nFirst, let’s check how many different types of plots there are - if there’s too many types, we’ll need a lot of colours, and our plot could get messy fast! We can use the unique function, which works on any vector (including dataframe columns) to list all the unique values\n\nunique(surveys$plot_type)\n\n[1] \"Control\"                   \"Long-term Krat Exclosure\" \n[3] \"Short-term Krat Exclosure\" \"Rodent Exclosure\"         \n[5] \"Spectab exclosure\"        \n\n\nWe can then could up how many unique values there are. 5 seems like a reasonable number!\n\nlength(unique(surveys$plot_type))\n\n[1] 5\n\n\n\nggplot(surveys, aes(x = weight, y = hindfoot_length, colour = plot_type)) +\n    geom_point(alpha = 0.2)\n\nWarning: Removed 4048 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIt seems like the type of plot the animal was captured on correlates well with some of these clusters, but there are still many that are quite mixed. Let’s try to do better!\n\n4.2.1.1 Challenge\n\nCome up with a hypothesis to explain why the plot type seems to be related to the height-weight clusters\nTry colouring your plot based on 1 or 2 of the other dataset variables. In each case, try to state what is the specific hypothesis you have in mind when choosing the variable",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Summarizing and visualizating data</span>"
    ]
  },
  {
    "objectID": "lectures/lec04-data-visualization.html#split-apply-combine",
    "href": "lectures/lec04-data-visualization.html#split-apply-combine",
    "title": "4  Summarizing and visualizating data",
    "section": "4.3 Split-apply-combine",
    "text": "4.3 Split-apply-combine\nBefore we dig further into trying to explain these clusters, we need to learn a few more techniques for aggregating and analyzing data.\nMany data analysis tasks can be approached using the split-apply-combine paradigm: split the data into groups, apply some analysis to each group, and then combine the results.\n\n4.3.1 Summarizing data by group with simple statistics\ndplyr facilitates this workflow through the use of group_by() to split data and summarize(), which collapses each group into a single-row summary of that group. The arguments to group_by() are the column names that contain the categorical variables for which you want to calculate the summary statistics. Let’s view the mean weight by sex.\n\nsurveys %&gt;%\n    group_by(sex) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n# A tibble: 3 × 2\n  sex   mean_weight\n  &lt;chr&gt;       &lt;dbl&gt;\n1 F              NA\n2 M              NA\n3 &lt;NA&gt;           NA\n\n\nThe mean weights become NA since there are individual observations that are NA. Let’s remove those observations.\n\nsurveys %&gt;%\n    filter(!is.na(weight)) %&gt;%\n    group_by(sex) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n# A tibble: 3 × 2\n  sex   mean_weight\n  &lt;chr&gt;       &lt;dbl&gt;\n1 F            42.2\n2 M            43.0\n3 &lt;NA&gt;         64.7\n\n\nThere is one row here that is neither male nor female, these are observations where the animal escaped before the sex could not be determined. Let’s remove those as well.\n\nsurveys %&gt;%\n    filter(!is.na(weight) & !is.na(sex)) %&gt;%\n    group_by(sex) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n# A tibble: 2 × 2\n  sex   mean_weight\n  &lt;chr&gt;       &lt;dbl&gt;\n1 F            42.2\n2 M            43.0\n\n\nYou can also group by multiple columns:\n\nsurveys %&gt;%\n    filter(!is.na(weight) & !is.na(sex)) %&gt;%\n    group_by(genus, sex) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n`summarise()` has grouped output by 'genus'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 20 × 3\n# Groups:   genus [10]\n   genus           sex   mean_weight\n   &lt;chr&gt;           &lt;chr&gt;       &lt;dbl&gt;\n 1 Baiomys         F            9.16\n 2 Baiomys         M            7.36\n 3 Chaetodipus     F           23.8 \n 4 Chaetodipus     M           24.7 \n 5 Dipodomys       F           55.2 \n 6 Dipodomys       M           56.2 \n 7 Neotoma         F          154.  \n 8 Neotoma         M          166.  \n 9 Onychomys       F           26.8 \n10 Onychomys       M           26.2 \n11 Perognathus     F            8.57\n12 Perognathus     M            8.20\n13 Peromyscus      F           22.5 \n14 Peromyscus      M           20.6 \n15 Reithrodontomys F           11.2 \n16 Reithrodontomys M           10.2 \n17 Sigmodon        F           71.7 \n18 Sigmodon        M           61.3 \n19 Spermophilus    F           57   \n20 Spermophilus    M          130   \n\n\nSince we will use the same filtered and grouped data frame in multiple code chunks below, we could assign this subset of the data to a new variable and use this variable in the subsequent code chunks instead of typing out the functions each time.\n\nfiltered_surveys &lt;- surveys %&gt;%\n    filter(!is.na(weight) & !is.na(sex)) %&gt;%\n    group_by(genus, sex)\n\nIf you want to display more data, you can use the print() function at the end of your chain with the argument n specifying the number of rows to display.\n\nfiltered_surveys %&gt;%\n    summarize(mean_weight = mean(weight)) %&gt;%\n    print(n = 15) # Will change the knitted output, not the notebook\n\n`summarise()` has grouped output by 'genus'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 20 × 3\n# Groups:   genus [10]\n   genus           sex   mean_weight\n   &lt;chr&gt;           &lt;chr&gt;       &lt;dbl&gt;\n 1 Baiomys         F            9.16\n 2 Baiomys         M            7.36\n 3 Chaetodipus     F           23.8 \n 4 Chaetodipus     M           24.7 \n 5 Dipodomys       F           55.2 \n 6 Dipodomys       M           56.2 \n 7 Neotoma         F          154.  \n 8 Neotoma         M          166.  \n 9 Onychomys       F           26.8 \n10 Onychomys       M           26.2 \n11 Perognathus     F            8.57\n12 Perognathus     M            8.20\n13 Peromyscus      F           22.5 \n14 Peromyscus      M           20.6 \n15 Reithrodontomys F           11.2 \n# ℹ 5 more rows\n\n\nOnce the data are grouped, you can also summarize multiple variables at the same time. For instance, we could add a column indicating the minimum weight for each species for each sex:\n\nfiltered_surveys %&gt;%\n    summarize(mean_weight = mean(weight),\n              min_weight = min(weight))\n\n`summarise()` has grouped output by 'genus'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 20 × 4\n# Groups:   genus [10]\n   genus           sex   mean_weight min_weight\n   &lt;chr&gt;           &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n 1 Baiomys         F            9.16          6\n 2 Baiomys         M            7.36          6\n 3 Chaetodipus     F           23.8           5\n 4 Chaetodipus     M           24.7           4\n 5 Dipodomys       F           55.2          10\n 6 Dipodomys       M           56.2          12\n 7 Neotoma         F          154.           32\n 8 Neotoma         M          166.           30\n 9 Onychomys       F           26.8           5\n10 Onychomys       M           26.2           9\n11 Perognathus     F            8.57          4\n12 Perognathus     M            8.20          4\n13 Peromyscus      F           22.5           8\n14 Peromyscus      M           20.6           7\n15 Reithrodontomys F           11.2           4\n16 Reithrodontomys M           10.2           4\n17 Sigmodon        F           71.7          15\n18 Sigmodon        M           61.3          16\n19 Spermophilus    F           57            57\n20 Spermophilus    M          130           130\n\n\nIf we don’t want to make any groups and want to summarize all the columns, we can instead use summarize_all\n\nfiltered_surveys %&gt;%\n    summarize_all(mean)\n\nWarning: There were 80 warnings in `summarise()`.\nThe first warning was:\nℹ In argument: `species_id = (function (x, ...) ...`.\nℹ In group 1: `genus = \"Baiomys\"` `sex = \"F\"`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\nℹ Run `dplyr::last_dplyr_warnings()` to see the 79 remaining warnings.\n\n\n# A tibble: 20 × 13\n# Groups:   genus [10]\n   genus    sex   record_id month   day  year plot_id species_id hindfoot_length\n   &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;\n 1 Baiomys  F        18658.  6.39  13.4 1991.    10.9         NA            13.2\n 2 Baiomys  M        18386.  8.21  11.6 1990.    10.2         NA            12.6\n 3 Chaetod… F        28223.  7.20  16.2 1998.    11.7         NA            NA  \n 4 Chaetod… M        28352.  6.95  16.3 1998.    11.1         NA            NA  \n 5 Dipodom… F        14278.  6.37  16.1 1988.    10.5         NA            NA  \n 6 Dipodom… M        15251.  6.25  16.1 1989.    10.2         NA            NA  \n 7 Neotoma  F        14624.  6.89  16.8 1988.    12.6         NA            NA  \n 8 Neotoma  M        13741.  6.97  17.0 1987.    11.1         NA            NA  \n 9 Onychom… F        17170.  6.94  16.4 1990.    10.7         NA            NA  \n10 Onychom… M        17313.  7.07  16.3 1990.    10.5         NA            NA  \n11 Perogna… F        18100.  6.90  16.6 1991.    13.5         NA            NA  \n12 Perogna… M        16955.  6.27  17.1 1990.    12.9         NA            NA  \n13 Peromys… F        17913.  6.34  16.4 1991.    13.6         NA            NA  \n14 Peromys… M        17187.  6.40  17.2 1990.    13.4         NA            NA  \n15 Reithro… F        16300.  5.74  14.8 1989.    13.5         NA            NA  \n16 Reithro… M        16633.  5.33  15.2 1990.    13.9         NA            NA  \n17 Sigmodon F        18456.  6.04  12.6 1991.    10.9         NA            NA  \n18 Sigmodon M        21166.  6.15  14.4 1993.    11.9         NA            NA  \n19 Spermop… F         4875  10     24   1981     20           NA            NA  \n20 Spermop… M         1002   6      9   1978     14           NA            NA  \n# ℹ 4 more variables: weight &lt;dbl&gt;, species &lt;dbl&gt;, taxa &lt;dbl&gt;, plot_type &lt;dbl&gt;\n\n\nThis gives errors since some columns are strings, not numbers, we get a warning. A summary statistic that works for all columns is to count the number of distinct values that variable can take:\n\nsurveys %&gt;%\n    summarize_all(n_distinct)\n\n# A tibble: 1 × 13\n  record_id month   day  year plot_id species_id   sex hindfoot_length weight\n      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt;           &lt;int&gt;  &lt;int&gt;\n1     34786    12    31    26      24         48     3              57    256\n# ℹ 4 more variables: genus &lt;int&gt;, species &lt;int&gt;, taxa &lt;int&gt;, plot_type &lt;int&gt;\n\n\n\n4.3.1.1 Challenge\n\nUse group_by() and summarize() to find the mean, min, and max hindfoot length for each species.\nWhat was the heaviest animal measured in each year? Return the columns year, genus, species, and weight.\n\n\n\n\n4.3.2 Using tally to summarize categorical data\nWhen working with data, it is also common to want to know the number of observations found for each factor or combination of factors. For this, dplyr provides tally(). For example, if we want to group by taxa and find the number of observations for each taxa, we would do:\n\nsurveys %&gt;%\n    group_by(taxa) %&gt;%\n    tally()\n\n# A tibble: 4 × 2\n  taxa        n\n  &lt;chr&gt;   &lt;int&gt;\n1 Bird      450\n2 Rabbit     75\n3 Reptile    14\n4 Rodent  34247\n\n\nWe can also use tally() when grouping on multiple variables:\n\nsurveys %&gt;%\n    group_by(taxa, sex) %&gt;%\n    tally()\n\n# A tibble: 6 × 3\n# Groups:   taxa [4]\n  taxa    sex       n\n  &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;\n1 Bird    &lt;NA&gt;    450\n2 Rabbit  &lt;NA&gt;     75\n3 Reptile &lt;NA&gt;     14\n4 Rodent  F     15690\n5 Rodent  M     17348\n6 Rodent  &lt;NA&gt;   1209\n\n\nHere, tally() is the action applied to the groups created by group_by() and counts the total number of records for each category.\nIf there are many groups, tally() is not that useful on its own. For example, when we want to view the five most abundant species among the observations:\n\nsurveys %&gt;%\n    group_by(species) %&gt;%\n    tally()\n\n# A tibble: 40 × 2\n   species             n\n   &lt;chr&gt;           &lt;int&gt;\n 1 albigula         1252\n 2 audubonii          75\n 3 baileyi          2891\n 4 bilineata         303\n 5 brunneicapillus    50\n 6 chlorurus          39\n 7 clarki              1\n 8 eremicus         1299\n 9 flavus           1597\n10 fulvescens         75\n# ℹ 30 more rows\n\n\nSince there are 40 rows in this output, we would like to order the table to display the most abundant species first. In dplyr, we say that we want to arrange() the data.\n\nsurveys %&gt;%\n    group_by(species) %&gt;%\n    tally() %&gt;%\n    arrange(n)\n\n# A tibble: 40 × 2\n   species          n\n   &lt;chr&gt;        &lt;int&gt;\n 1 clarki           1\n 2 scutalatus       1\n 3 tereticaudus     1\n 4 tigris           1\n 5 uniparens        1\n 6 viridis          1\n 7 leucophrys       2\n 8 savannarum       2\n 9 fuscus           5\n10 undulatus        5\n# ℹ 30 more rows\n\n\nStill not that useful. Since we are interested in the most abundant species, we want to display those with the highest count first, in other words, we want to arrange the column n in descending order:\n\nsurveys %&gt;%\n    group_by(species) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n)) %&gt;%\n    head(5)\n\n# A tibble: 5 × 2\n  species          n\n  &lt;chr&gt;        &lt;int&gt;\n1 merriami     10596\n2 penicillatus  3123\n3 ordii         3027\n4 baileyi       2891\n5 megalotis     2609\n\n\nIf we want to include more attributes about these species, we can include these in the call to group_by():\n\nsurveys %&gt;%\n    group_by(species, taxa, genus) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n)) %&gt;%\n    head(5)\n\n# A tibble: 5 × 4\n# Groups:   species, taxa [5]\n  species      taxa   genus               n\n  &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;           &lt;int&gt;\n1 merriami     Rodent Dipodomys       10596\n2 penicillatus Rodent Chaetodipus      3123\n3 ordii        Rodent Dipodomys        3027\n4 baileyi      Rodent Chaetodipus      2891\n5 megalotis    Rodent Reithrodontomys  2609\n\n\nBe careful not to include anything that would split the group into subgroups, such as sex, year etc.\n\n4.3.2.1 Challenge\n\nHow many individuals were caught in each plot_type surveyed?\nYou saw above how to count the number of individuals of each sex using a combination of group_by() and tally(). How could you get the same result using group_by() and summarize()? Hint: see ?n.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Summarizing and visualizating data</span>"
    ]
  },
  {
    "objectID": "lectures/lec04-data-visualization.html#split-apply-combine-plot",
    "href": "lectures/lec04-data-visualization.html#split-apply-combine-plot",
    "title": "4  Summarizing and visualizating data",
    "section": "4.4 Split-apply-combine… plot!",
    "text": "4.4 Split-apply-combine… plot!\nNow we’ll come back to trying to understand our weight-vs-hindfoot length data, but armed with more tools! We’ll see how combining just a handful of tools from the dplyr and ggplot packages, we can create a powerful data exploration workflow.\nGlancing at our prior plot, we can get a clues to which explanatory variable to look at next. The plot above suggests that there might be 4 clusters, so a variable with 4 values is a good guess for what could explain the observed pattern in the scatter plot. Let’s figure out which variable that is.\n\nsurveys %&gt;%\n    summarize_all(n_distinct) \n\n# A tibble: 1 × 13\n  record_id month   day  year plot_id species_id   sex hindfoot_length weight\n      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt;           &lt;int&gt;  &lt;int&gt;\n1     34786    12    31    26      24         48     3              57    256\n# ℹ 4 more variables: genus &lt;int&gt;, species &lt;int&gt;, taxa &lt;int&gt;, plot_type &lt;int&gt;\n\n# `n_distinct` is a function that counts unique values in a set of vectors\n\nRemember that there are still NA values here, that’s why there are 3 unique sexes although only male and female were coded in our original data set. There are four taxa, so maybe that could be a good candidate to explain the clusters? Let’s check it out!\n\nsurveys %&gt;%\n    distinct(taxa)\n\n# A tibble: 4 × 1\n  taxa   \n  &lt;chr&gt;  \n1 Rodent \n2 Rabbit \n3 Bird   \n4 Reptile\n\n# alternatively, we could have ran unique(surveys$sex)\n\nIt seems reasonable that these taxa contain animals different enough to have diverse weights and length of their feet. Lets use this categorical variable to colour the scatter plot.\n\nggplot(surveys, aes(x = weight, y = hindfoot_length, colour = taxa)) +\n    geom_point(alpha = 0.2)\n\nWarning: Removed 4048 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nOnly rodents? That was unexpected… Let’s check what’s going on.\n\nsurveys %&gt;%\n    group_by(taxa) %&gt;%\n    tally()\n\n# A tibble: 4 × 2\n  taxa        n\n  &lt;chr&gt;   &lt;int&gt;\n1 Bird      450\n2 Rabbit     75\n3 Reptile    14\n4 Rodent  34247\n\n\nDefinitely mostly rodents in our data set…. Let’s check again after making sure we only take observations where the hindfoot length was actually measured (ie, not NA)\n\nsurveys %&gt;%\n    filter( !is.na(hindfoot_length) ) %&gt;% # control by removing `!`\n    group_by(taxa) %&gt;%\n    tally()\n\n# A tibble: 1 × 2\n  taxa       n\n  &lt;chr&gt;  &lt;int&gt;\n1 Rodent 31438\n\n\n…and it turns out that only rodents have had their hindfeet measured! Rats.\nLet’s remove all records of animals without hindfoot measurements, including rodents. We’ll also remove any observations that did not include weights.\n\nsurveys_hf_wt &lt;- surveys %&gt;%\n    filter(!is.na(hindfoot_length) & !is.na(weight))\n\nsurveys_hf_wt %&gt;%\n    summarize_all(n_distinct)\n\n# A tibble: 1 × 13\n  record_id month   day  year plot_id species_id   sex hindfoot_length weight\n      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt;           &lt;int&gt;  &lt;int&gt;\n1     30738    12    31    26      24         24     3              55    252\n# ℹ 4 more variables: genus &lt;int&gt;, species &lt;int&gt;, taxa &lt;int&gt;, plot_type &lt;int&gt;\n\n\nMaybe the genus of the animals can explain what we are seeing.\n\nggplot(surveys, aes(x = weight, y = hindfoot_length, colour = genus)) +\n    geom_point(alpha = 0.2)\n\nWarning: Removed 4048 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNow this looks good! There is a clear separation between different genera but also significant spread within genus. For example, in the weight of the green Neotoma observations. There are also two clearly separate clusters that are both coloured in olive green (Dipodomys). Maybe separating the observations into different species would be better?\n\nggplot(surveys_hf_wt, aes(x = weight, y = hindfoot_length, colour = species)) +\n    geom_point(alpha = 0.2)\n\n\n\n\n\n\n\n\nGreat! Together with the genus plot, this definitely seems to explain most of the variation we see in the hindfoot length and weight measurements. It is still a bit messy as it appears like we have around five clusters of data points but there are 21 species in the legend.\n\nsurveys %&gt;%\n    filter(!is.na(hindfoot_length) & !is.na(weight)) %&gt;%\n    group_by(species) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n))\n\n# A tibble: 21 × 2\n   species          n\n   &lt;chr&gt;        &lt;int&gt;\n 1 merriami      9739\n 2 penicillatus  2978\n 3 baileyi       2808\n 4 ordii         2793\n 5 megalotis     2429\n 6 torridus      2085\n 7 spectabilis   2026\n 8 flavus        1471\n 9 eremicus      1200\n10 albigula      1046\n# ℹ 11 more rows\n\n\nThere is a big drop from 838 to 159, let’s include only those with more than 800 observations.\n\nsurveys_abun_species &lt;- surveys %&gt;%\n    filter(!is.na(hindfoot_length) & !is.na(weight)) %&gt;%\n    group_by(species) %&gt;%\n    mutate(n = n()) %&gt;% # add count value to each row\n    filter(n &gt; 800) %&gt;%\n    select(-n)\n\nsurveys_abun_species %&gt;%\n  # Remember, print limits lines displayed when knitted\n  print(10)\n\n# A\n#   tibble:\n#   30,320\n#   × 13\n# Groups:  \n#   species\n#   [12]\n# ℹ 30,310\n#   more\n#   rows\n# ℹ 13\n#   more\n#   variables:\n#   record_id &lt;dbl&gt;, …\n\n\nStill has almost 31k observations, so only ~3k observations were removed.\n\nsurveys_abun_species %&gt;%\n  ggplot(aes(x = weight, y = hindfoot_length, colour = species)) +\n  geom_point(alpha = 0.2)\n\n\n\n\n\n\n\n\nThe plot is now cleaner; there are fewer species and so fewer colours and the clusters are more distinct. But one final thing. Maybe you find it hard to keep track of the rodents, and how they’re related, from their species name only. Suppose we want our legend to report both the genus AND the species. We can do this by creating a new variable that is the genus+species name combined into a single string, using the function paste\n\nsurveys_abun_species %&gt;%\n  mutate(genus_species = paste(genus,species)) %&gt;%\n  ggplot(aes(x = weight, y = hindfoot_length, colour = genus_species)) +\n  geom_point(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n4.4.0.1 Challenge\nCreate a scatter plot of hindfoot_length against species with the weight data displayed using colours. If you’re unsure of which variable to put on which axis, Y variables are generally “against” X variables. Also, continuous variables are generally plotted on the Y axis.\nDo you notice any potential issues with this plot given the sheer number of observations we know exist in the data?\n(This is further illustrating the iterative nature of constructing plots)\nParts of this lesson material were taken and modified from Data Carpentry under their CC-BY copyright license. See their lesson page for the original source.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Summarizing and visualizating data</span>"
    ]
  },
  {
    "objectID": "lectures/lec05-advanced-data-viz.html",
    "href": "lectures/lec05-advanced-data-viz.html",
    "title": "5  Advanced data manipulation & visualization",
    "section": "",
    "text": "5.1 Lesson preamble",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Advanced data manipulation & visualization</span>"
    ]
  },
  {
    "objectID": "lectures/lec05-advanced-data-viz.html#lesson-preamble",
    "href": "lectures/lec05-advanced-data-viz.html#lesson-preamble",
    "title": "5  Advanced data manipulation & visualization",
    "section": "",
    "text": "5.1.1 Learning Objectives\n\nLearn to make histograms, line plots, and reference lines using ggplot (geom_histogram, geom_line, geom_abline)\nUnderstand and apply faceting in ggplot (using facet_wrap and facet_grid)\nLearn to switch between long and wide format data using pivot_longer and pivot_wider\nFormat plots to be more readable using ggplot’s theme options and labeling (labs)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Advanced data manipulation & visualization</span>"
    ]
  },
  {
    "objectID": "lectures/lec05-advanced-data-viz.html#review",
    "href": "lectures/lec05-advanced-data-viz.html#review",
    "title": "5  Advanced data manipulation & visualization",
    "section": "5.2 Review",
    "text": "5.2 Review\nWe’ll be continuing to work for one more lecture on the rich dataset from the Portal Project, which is a 30+ year study of a Chihuahuan desert ecosystem near Portal, Arizona, USA. The dataset we’re working with contains observations of animals found at the study site, which was subdivided into plots which underwent controlled environmental manipulation in some years.\n\n\n\n\n\nThe Portal Project is a long-term ecological study in the a desert ecosystem in Arizona, which includes frequent sampling over 30+ years and controlled manipulations of the ecosystem\n\n\n\n\nWe’ll make sure the tidyverse package is loaded and then read the data in again\nNote: make sure to clear your environment before starting a new class\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n# If you didn't download and save the data locally last time\n# download.file(\"https://ndownloader.figshare.com/files/2292169\", \"data/portal_data.csv\")\n\nsurveys &lt;- readr::read_csv('data/portal_data.csv')\n\nRows: 34786 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): species_id, sex, genus, species, taxa, plot_type\ndbl (7): record_id, month, day, year, plot_id, hindfoot_length, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nsurveys\n\n# A tibble: 34,786 × 13\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2        72     8    19  1977       2 NL         M                  31     NA\n 3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n 4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n 5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n 6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n 7       435    12    10  1977       2 NL         &lt;NA&gt;               NA     NA\n 8       506     1     8  1978       2 NL         &lt;NA&gt;               NA     NA\n 9       588     2    18  1978       2 NL         M                  NA    218\n10       661     3    11  1978       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 34,776 more rows\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\nBefore we go further, we’re just going to add one new column to surveys : the full Latin name (Genus species) in a new column called species_latin\n\nsurveys &lt;- surveys %&gt;% \n  mutate(species_latin = paste(genus,species)) # new column for full Latin name\n\nUsually we don’t recommend altering the original data, but instead suggest saving edited forms under a new name, but because this is simply adding a new column (which is just a restatement of existing information), we’ll break that rule for now!\nFor each animal captured, the weight and hindfoot length were measured, and we previously investigated the relationship between the two variables (after removing missing data and filtering for species that weren’t very rare) using a scatterplot and found strong clustering by species\n\nsurveys_abun_species &lt;- surveys %&gt;% \n  filter(!is.na(hindfoot_length) & !is.na(weight)) %&gt;% # remove rows with NA in either\n  group_by(species) %&gt;%\n  mutate(n = n()) %&gt;% # add count value (total observations of each species) to each row\n  filter(n &gt; 800) %&gt;% # filter for species observed &gt;800 times total\n  select(-n) # remove the n column, don't need it anyomre\n\nsurveys_abun_species %&gt;%\n  ggplot(aes(x = weight, y = hindfoot_length, colour = species_latin)) +\n  geom_point(alpha = 0.2)\n\n\n\n\n\n\n\n\nLooking at the same variables a different way,\n\nsurveys_abun_species %&gt;%\n  ggplot(aes(x = species_latin, y = hindfoot_length, colour = weight)) +\n  geom_jitter(size = 0.1, height = 0, width = 0.4) + \n  theme(axis.text.x = element_text(angle = 45,vjust=1, hjust=1))",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Advanced data manipulation & visualization</span>"
    ]
  },
  {
    "objectID": "lectures/lec05-advanced-data-viz.html#visualizing-data-with-other-plot-types",
    "href": "lectures/lec05-advanced-data-viz.html#visualizing-data-with-other-plot-types",
    "title": "5  Advanced data manipulation & visualization",
    "section": "5.3 Visualizing data with other plot types",
    "text": "5.3 Visualizing data with other plot types\nIn this section, we’ll explore a few more types of plots we can make in ggplot.\nFirst, let’s go back to the originally-imported data and just count the total number of times each species was observed. We need to group the data and count records within each group:\n\nsurveys %&gt;% \n  group_by(species_latin) %&gt;%\n  tally()%&gt;%\n  arrange(desc(n)) # Adding arrange just to compare with histogram\n\n# A tibble: 48 × 2\n   species_latin                 n\n   &lt;chr&gt;                     &lt;int&gt;\n 1 Dipodomys merriami        10596\n 2 Chaetodipus penicillatus   3123\n 3 Dipodomys ordii            3027\n 4 Chaetodipus baileyi        2891\n 5 Reithrodontomys megalotis  2609\n 6 Dipodomys spectabilis      2504\n 7 Onychomys torridus         2249\n 8 Perognathus flavus         1597\n 9 Peromyscus eremicus        1299\n10 Neotoma albigula           1252\n# ℹ 38 more rows\n\n\nBefore doing any complex analysis, it’s important to understand how much data there is. Are there species for which very few animals were ever observed in the study site? The best way to look at this sort of count data is with a histogram.\nTo create a histogram, we can assign this table to a variable, and then pass that variable to ggplot().\n\nspecies_counts &lt;- surveys %&gt;% \n  group_by(species_latin) %&gt;%\n  tally()\n\nggplot(species_counts, aes(x = n)) +\n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nRemember that a histogram plots the number of observations based on a variable, so you only need to specify the x-axis in the ggplot() call.\nA histogram’s bin size can really change what you might understand about the data. The histogram geom has a bins argument that allows you to specify the number of bins and a binwidth argument that allows you to specify the size of the bins.\nIf we reduce the number of bins, the plot looks cleaner, but we are missing details on most of the species, which are all falling into the lowest bin.\n\nggplot(species_counts, aes(x = n)) +\n    geom_histogram(bins=10)\n\n\n\n\n\n\n\n\nAlternatively we can specify how wide each bin should be using binwidth - let’s try 100, and move the rectangular bars so that instead of being centered at the median value within them (default), they span from the lowest to highest value included in the bin\n\nggplot(species_counts, aes(x = n)) +\n    geom_histogram(binwidth=100, boundary = 0)\n\n\n\n\n\n\n\n\nWe can understand the distribution even better by defining custom bin widths, which allows us to get more resolution for the rarely-observed species while not having too many empty bins for the few very frequently-observed species\n\nggplot(species_counts, aes(x = n)) +\n    geom_histogram(breaks = c(seq(0,1000,100),seq(2000,11000,1000)), boundary = 0)\n\n\n\n\n\n\n\n\nFrom this latest version, we can see there’s a huge number of species (over 30!) that were observed less than 100 times over the entire 4 decade study period. These are the ones we might want to filter out going forward, since there’s probably too little data to to interpret trends. Using the value of 800 observations as a cut off for filtering (what we did previously) seems reasonable!\nNow, since this was a longitudinal study, let’s calculate the number of counts per year for each species. First, we need to group the data and count records within each group:\n\nyearly_counts &lt;- surveys %&gt;%\n    group_by(year, species_latin) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n)) # Adding arrange just to compare with histogram\n\nyearly_counts\n\n# A tibble: 509 × 3\n# Groups:   year [26]\n    year species_latin           n\n   &lt;dbl&gt; &lt;chr&gt;               &lt;int&gt;\n 1  2002 Chaetodipus baileyi   892\n 2  1985 Dipodomys merriami    667\n 3  1982 Dipodomys merriami    609\n 4  1997 Dipodomys merriami    576\n 5  1981 Dipodomys merriami    559\n 6  2000 Chaetodipus baileyi   555\n 7  2001 Chaetodipus baileyi   538\n 8  1983 Dipodomys merriami    528\n 9  1998 Dipodomys merriami    503\n10  1980 Dipodomys merriami    493\n# ℹ 499 more rows\n\n\nNow, let’s again do a histogram\n\nggplot(yearly_counts, aes(x = n)) +\n    geom_histogram(boundary=0)\n\n\n\n\n\n\n\n\nNote that creating an intermediate variable for the processed data ( yearly_counts ) is be preferable for time consuming calculations, because you would not want to do that operation every time you change the plot aesthetics. However, if it is not a time consuming calculation, or you would like the flexibility of changing the data summary and the plotting options in the same code chunk, you can pipe the output of your split-apply-combine operation to the plotting command:\n\nsurveys %&gt;%\n    group_by(year, species_latin) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = n)) +\n        geom_histogram(boundary = 1)\n\n\n\n\n\n\n\n\nWe can perform a quick check that the plot corresponds to the table by colouring the histogram by species. However, we first want to get rid of all those rarely observed species that we know will crowd our plot\n\nyearly_counts_abun &lt;- yearly_counts %&gt;%\n  group_by(species_latin) %&gt;%\n  mutate(n_tot_species=sum(n)) %&gt;% # new column containing total observations per species\n  filter(n_tot_species&gt;800) %&gt;%\n  select(-n_tot_species)\n\nggplot(yearly_counts_abun, aes(x = n, fill = species_latin)) + \n        geom_histogram(boundary = 0)\n\n\n\n\n\n\n\n# We are using \"fill\" here instead of \"colour\"\n\nNote: Here we are using fill to assign colours to species rather than colour. In general colour refers to the outline of points/bars or whatever it is you are plotting and fill refers to the colour that goes inside the point or bar. If you are confused, try switching out fill for colour to see what looks best!\nNow let’s explore how the number of each species varies over time. Longitudinal data can be visualized as a line plot with years on the x axis and counts on the y axis:\n\nyearly_counts_abun  %&gt;%\n    ggplot(aes(x = year, y = n)) +\n        geom_line()\n\n\n\n\n\n\n\n\nUnfortunately, this does not work because we plotted data for all the species together as one line. We need to tell ggplot to draw a line for each species by modifying the aesthetic function to include group = species:\n\nyearly_counts_abun  %&gt;%\n    ggplot(aes(x = year, y = n, group = species_latin)) +\n        geom_line()\n\n\n\n\n\n\n\n\nWe will be able to distinguish species in the plot if we add colours (using colour also automatically groups the data):\n\nannual_plot &lt;- yearly_counts_abun  %&gt;%\n    ggplot(aes(x = year, y = n, color = species_latin)) + # you can say color or colour\n        geom_line()\n\nannual_plot\n\n\n\n\n\n\n\n\nThere seem to be large fluctuations over time in the levels of many of these species! We can calculate an average\n\navg_species &lt;- yearly_counts_abun %&gt;%\n  group_by(species_latin) %&gt;%\n  summarize(avg_yearly_count = mean(n))\n\navg_species\n\n# A tibble: 12 × 2\n   species_latin             avg_yearly_count\n   &lt;chr&gt;                                &lt;dbl&gt;\n 1 Chaetodipus baileyi                  361. \n 2 Chaetodipus penicillatus             120. \n 3 Dipodomys merriami                   408. \n 4 Dipodomys ordii                      116. \n 5 Dipodomys spectabilis                119. \n 6 Neotoma albigula                      48.2\n 7 Onychomys leucogaster                 45.7\n 8 Onychomys torridus                    86.5\n 9 Perognathus flavus                    66.5\n10 Peromyscus eremicus                   50.0\n11 Peromyscus maniculatus                45.0\n12 Reithrodontomys megalotis            100. \n\n\n\nannual_plot + \n  geom_abline(data=avg_species, aes(slope = 0, intercept = avg_yearly_count, colour = species_latin), linetype = \"dashed\")",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Advanced data manipulation & visualization</span>"
    ]
  },
  {
    "objectID": "lectures/lec05-advanced-data-viz.html#creating-multi-panel-plots-faceting",
    "href": "lectures/lec05-advanced-data-viz.html#creating-multi-panel-plots-faceting",
    "title": "5  Advanced data manipulation & visualization",
    "section": "5.4 Creating multi-panel plots (faceting)",
    "text": "5.4 Creating multi-panel plots (faceting)\nggplot has a special technique called faceting that allows the user to split one plot into multiple subplots based on a variable included in the dataset. This allows us to examine the trends associated with each grouping variable more closely. We will use it to make a time series plot for each species:\n\nyearly_counts_abun %&gt;%\n    ggplot(aes(x = year, y = n)) + \n        geom_line() +\n        facet_wrap(~species_latin)\n\n\n\n\n\n\n\n\nThe facet_wrap function creates a new subplot for each value of the variable passed to it (~name). You can control the number of rows and columns as well as lots of other options.\nNow we would like to split the line in each plot by the sex of each individual measured. To do that we need to make counts in the data frame after grouping by year, species_latin, and sex, so we are going to redo our earlier filtering, preserving the breakdown by sex:\n\nyearly_counts_by_sex_abun &lt;- surveys %&gt;%\n    group_by(year, species_latin, sex) %&gt;%\n    tally() %&gt;% \n    group_by(species_latin) %&gt;%\n    mutate(n_tot_species=sum(n)) %&gt;% # new column containing total observations per species\n    filter(n_tot_species&gt;800) %&gt;%\n    select(-n_tot_species)\n\nyearly_counts_by_sex_abun\n\n# A tibble: 687 × 4\n# Groups:   species_latin [12]\n    year species_latin            sex       n\n   &lt;dbl&gt; &lt;chr&gt;                    &lt;chr&gt; &lt;int&gt;\n 1  1977 Chaetodipus penicillatus F         4\n 2  1977 Chaetodipus penicillatus M         3\n 3  1977 Dipodomys merriami       F       104\n 4  1977 Dipodomys merriami       M       152\n 5  1977 Dipodomys merriami       &lt;NA&gt;      8\n 6  1977 Dipodomys ordii          F        10\n 7  1977 Dipodomys ordii          M         2\n 8  1977 Dipodomys spectabilis    F        58\n 9  1977 Dipodomys spectabilis    M        34\n10  1977 Dipodomys spectabilis    &lt;NA&gt;      6\n# ℹ 677 more rows\n\n\nWe can reflect this grouping by sex in the faceted plot by splitting further with colour (within a single plot).\n\nyearly_counts_by_sex_abun %&gt;%\n    ggplot(aes(x = year, y = n, colour = sex)) +\n        geom_line() +\n        facet_wrap(~species_latin)\n\n\n\n\n\n\n\n\nThere are several observations where sex was not recorded. Let’s filter out those values.\n\nyearly_counts_by_sex_abun &lt;- yearly_counts_by_sex_abun %&gt;% \n  filter(!is.na(sex))\n\nyearly_counts_by_sex_abun %&gt;%\n    ggplot(aes(x = year, y = n, color = sex)) +\n        geom_line() +\n        facet_wrap(~species_latin)\n\n\n\n\n\n\n\n\nIt is possible to specify exactly which colors1 to use and to change the thickness of the lines to make the them easier to distinguish.\n\nyearly_counts_by_sex_abun %&gt;%\n    ggplot(aes(x = year, y = n, colour = sex)) +\n        geom_line(size = 1) +\n        scale_colour_manual(values = c(\"black\", \"orange\")) +\n        facet_wrap(~species_latin) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nNot sure what colours would look good on your plot? The R Community got you covered! Check out these awesome color palettes where nice-looking color combos come predefined. We especially recommend the viridis color palettes. These palettes are not only pretty, they are specifically designed to be easier to read by those with colourblindness.\nLastly, let’s change the x labels so that they don’t overlap, split the subplot titles into multiple lines if they’re too long, and remove the grey background to increase contrast with the lines. To customize the non-data components of the plot, we will pass some theme statements2 to ggplot.\n\nyearly_counts_by_sex_abun %&gt;%\n  ggplot(aes(x = year, y = n, color = sex)) +\n  geom_line(size = 1) +\n  scale_colour_viridis_d() +\n  facet_wrap(~species_latin, labeller = label_wrap_gen(width = 14)) +\n  theme_classic() +\n  theme(text = element_text(size = 10),\n        axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n\nThere are other popular theme options, such as theme_bw().\nOur plot looks pretty polished now! It would be difficult to share with other, however, given the lack of information provided on the Y axis. Let’s add some meaningful axis labels.\n\nyearly_counts_by_sex_abun %&gt;%\n  ggplot(aes(x = year, y = n, color = sex)) +\n  geom_line(size = 1) +\n  scale_colour_viridis_d() +\n  facet_wrap(~species_latin, labeller = label_wrap_gen(width = 14)) +\n  theme_classic() +\n  theme(text = element_text(size = 10),\n        axis.text.x = element_text(angle = 30, hjust = 1)) +\n  labs(title = \"Species abundance over time\",\n       x = \"Year\",\n       y = \"Number observed\",\n       colour = \"Sex\")\n\n\n\n\n\n\n\n\nThere is a related sub-plotting function called facet_grid, which can instead take two variables, where one will vary along the rows of the grid and another along the columns. Here, we’ll make a set of plots to examine how different genuses of animals (genus) respond to the different experimental manipulations applied to the plots (plot_type).\n\nsurveys %&gt;% \n  filter(!is.na(sex)) %&gt;%\n  group_by(year, genus, plot_type, sex) %&gt;%\n  tally() %&gt;%\n  ggplot(aes(x = year, y = n, color = sex)) +\n  geom_line(size = 1) +\n  facet_grid(genus ~ plot_type, scales = \"free\",labeller = label_wrap_gen(width = 10))  +\n  theme(text = element_text(size = 8),\n        axis.text.x = element_text(angle = 30, hjust = 1)) +\n  labs(title = \"Rodent abundance over time\",\n       x = \"Year\",\n       y = \"Number observed\",\n       colour = \"Sex\")\n\n\n\n\n\n\n\n\nIt might be hard to see the plot (it’ll be cramped!) if you’re just viewing the Markdown notebook in the RStudio editor. You can click on the pop-out window icon in the top right corner of the plot to open a new window and expand it (the text will adjust accordingly).\n\n5.4.1 Challenge\nUse the filtered data frame (surveys_abun_species) for part 2.\n1. Remember the histogram coloured according to each species? Starting from that code, how could we separate each species into its own subplot?\n2.a. Create a plot that shows the average weight over years. Which year was the average weight of all animals the highest?\n2.b. Iterate on the plot so it shows differences among species of their average weight over time. Is the yearly trend the same for all species?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Advanced data manipulation & visualization</span>"
    ]
  },
  {
    "objectID": "lectures/lec05-advanced-data-viz.html#reshaping-with-pivot_wider-and-pivot_longer",
    "href": "lectures/lec05-advanced-data-viz.html#reshaping-with-pivot_wider-and-pivot_longer",
    "title": "5  Advanced data manipulation & visualization",
    "section": "5.5 Reshaping with pivot_wider and pivot_longer",
    "text": "5.5 Reshaping with pivot_wider and pivot_longer\n\n5.5.1 Defining wide vs long data\nThe survey data presented here is almost in what we call a long format – every observation of every individual is its own row. This is an ideal format for data with a rich set of information per observation. It makes it difficult, however, to look at the relationships between measurements across plots/trials. For example, what is the relationship between mean weights of different genera across all plots?\nTo answer that question, we want each plot to have its own row, with each measurements in its own column. This is called a wide data format. For the surveys data as we have it right now, this is going to be one heck of a wide data frame! However, if we were to summarize data within plots and species, we can reduce the dataset and begin to look for some relationships we’d want to examine. We need to create a new table where each row is the values for a particular variable associated with each plot. In practical terms, this means the values in genus would become the names of column variables and the cells would contain the values of the mean weight observed on each plot by genus.\nWe can use the functions called pivot_wider() and pivot_longer() (these are newer replacements for spread() and gather(), which were the older functions). These can feel tricky to think through, but do not feel alone in this! Many others have squinted at their data, unsure exactly how to reshape it, so there are many guides and cheatsheets available to help!\n\n\n5.5.2 Summary of long vs wide formats\nLong format:\n\nevery column is a variable\n\nfirst column(s) repeat\n\nevery row is an observation\n\nWide format:\n\neach row is a measured thing\neach column is an independent observation\n\nfirst column does not repeat\n\n\n\n\n5.5.3 Long to Wide with pivot_wider\nLet’s start by using dplyr to create a data frame with the mean body weight of each genus by plot.\n\nsurveys_gw &lt;- surveys %&gt;%\n    filter(!is.na(weight)) %&gt;%\n    group_by(genus, plot_id) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n`summarise()` has grouped output by 'genus'. You can override using the\n`.groups` argument.\n\nsurveys_gw\n\n# A tibble: 196 × 3\n# Groups:   genus [10]\n   genus       plot_id mean_weight\n   &lt;chr&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n 1 Baiomys           1        7   \n 2 Baiomys           2        6   \n 3 Baiomys           3        8.61\n 4 Baiomys           5        7.75\n 5 Baiomys          18        9.5 \n 6 Baiomys          19        9.53\n 7 Baiomys          20        6   \n 8 Baiomys          21        6.67\n 9 Chaetodipus       1       22.2 \n10 Chaetodipus       2       25.1 \n# ℹ 186 more rows\n\n\nNow, to make this long data wide, we use pivot_wider() from tidyr to spread out the different taxa into columns. pivot_wider() takes 3 arguments: the data , the names_from column variable that will eventually become the column names, and the values_from column variable that will fill in the values. We’ll use a pipe so we don’t need to explicitly supply the data argument.\n\nsurveys_gw_wide &lt;- surveys_gw %&gt;% \n  pivot_wider(names_from = genus, values_from = mean_weight)\n\nsurveys_gw_wide\n\n# A tibble: 24 × 11\n   plot_id Baiomys Chaetodipus Dipodomys Neotoma Onychomys Perognathus\n     &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1       1    7           22.2      60.2    156.      27.7        9.62\n 2       2    6           25.1      55.7    169.      26.9        6.95\n 3       3    8.61        24.6      52.0    158.      26.0        7.51\n 4       5    7.75        18.0      51.1    190.      27.0        8.66\n 5      18    9.5         26.8      61.4    149.      26.6        8.62\n 6      19    9.53        26.4      43.3    120       23.8        8.09\n 7      20    6           25.1      65.9    155.      25.2        8.14\n 8      21    6.67        28.2      42.7    138.      24.6        9.19\n 9       4   NA           23.0      57.5    164.      28.1        7.82\n10       6   NA           24.9      58.6    180.      25.9        7.81\n# ℹ 14 more rows\n# ℹ 4 more variables: Peromyscus &lt;dbl&gt;, Reithrodontomys &lt;dbl&gt;, Sigmodon &lt;dbl&gt;,\n#   Spermophilus &lt;dbl&gt;\n\n\nNow we can go back to our original question: what is the relationship between mean weights of different genera across all plots? We can easily see the weights for each genus in each plot! Notice that some genera have NA values. That’s because some genera were not recorded in that plot.\n\n\n5.5.4 Wide to long with pivot_longer\nWhat if we had the opposite problem, and wanted to go from a wide to long format? For that, we can use pivot_longer() to gather a set of columns into one key-value pair. To go backwards from surveys_gw_wide, we should exclude plot_id.\npivot_longer() takes 4 arguments: the data, the names_to column variable that comes from the column names, the values_to column with the values, and cols which specifies which columns we want to keep or drop. Again, we will pipe from the dataset so we don’t have to specify the data argument:\n\nsurveys_gw_long &lt;- surveys_gw_wide %&gt;% \n  pivot_longer(names_to = \"genus\", values_to = \"mean_weight\", cols = -plot_id)\n\nsurveys_gw_long\n\n# A tibble: 240 × 3\n   plot_id genus           mean_weight\n     &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt;\n 1       1 Baiomys                7   \n 2       1 Chaetodipus           22.2 \n 3       1 Dipodomys             60.2 \n 4       1 Neotoma              156.  \n 5       1 Onychomys             27.7 \n 6       1 Perognathus            9.62\n 7       1 Peromyscus            22.2 \n 8       1 Reithrodontomys       11.4 \n 9       1 Sigmodon              NA   \n10       1 Spermophilus          NA   \n# ℹ 230 more rows\n\n\nIf the columns are directly adjacent as they are here, we don’t even need to list the all out: we can just use the : operator, as before.\n\nsurveys_gw_wide %&gt;% \n  pivot_longer(names_to = \"genus\", values_to = \"mean_weight\", cols = Baiomys:Spermophilus)\n\n# A tibble: 240 × 3\n   plot_id genus           mean_weight\n     &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt;\n 1       1 Baiomys                7   \n 2       1 Chaetodipus           22.2 \n 3       1 Dipodomys             60.2 \n 4       1 Neotoma              156.  \n 5       1 Onychomys             27.7 \n 6       1 Perognathus            9.62\n 7       1 Peromyscus            22.2 \n 8       1 Reithrodontomys       11.4 \n 9       1 Sigmodon              NA   \n10       1 Spermophilus          NA   \n# ℹ 230 more rows\n\n\nNote that now the NA genera are included in the long format.\n\n\n5.5.5 Challenge\nStarting with the surveys_gw_wide dataset, how would you display a new dataset that gathers the mean weight of all the genera (excluding NAs) except for the genus Perognathus?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Advanced data manipulation & visualization</span>"
    ]
  },
  {
    "objectID": "lectures/lec05-advanced-data-viz.html#footnotes",
    "href": "lectures/lec05-advanced-data-viz.html#footnotes",
    "title": "5  Advanced data manipulation & visualization",
    "section": "",
    "text": "There are so many colors to chose from in R. Check out the R Color doc to find something that brings you joy.↩︎\nThe amount of control over various plot elements in ggplot is truly astonishing. Check out the complete list of themes here. Have fun!↩︎",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Advanced data manipulation & visualization</span>"
    ]
  },
  {
    "objectID": "lectures/lec06-self-directed-analysis-without-answers.html",
    "href": "lectures/lec06-self-directed-analysis-without-answers.html",
    "title": "6  Self-directed data exploration: Disease mortality across species with Farrell & Davies, PNAS, 2019",
    "section": "",
    "text": "6.1 Introduction\nYou can download this .Rmd file here. Example answers are here.\nThe goal of this class is to review concepts from the first second of the course, and to become comfortable writing your own code to wrangle and explore data. Since the 2nd second half of the course requires familiarity with the basics of R, this is a good point to assess if there are programming principles or specific concepts which you need to brush up on.\nHere is how the class will work: you will split up into groups for each of the questions below and work on the questions collaboratively. The questions will guide you through an exploratory analysis of a nice dataset generated by Farrell and Davies. In their paper, Farrell and Davies seek to test the hypothesis that disease-induced mortality (or virulence of a pathogen) increases with the degree to which an infected host species is diverged from other mammalian hosts. Your answers will NOT be marked, but we are happy to check your answers and to provide guidance if you become stuck. We have also provided some tips and tricks regarding best practices for troubleshooting your code, and encourage you to try to solve the problems by applying some of those practices.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Self-directed data exploration: Disease mortality across species with Farrell & Davies, PNAS, 2019</span>"
    ]
  },
  {
    "objectID": "lectures/lec06-self-directed-analysis-without-answers.html#learning-preamble",
    "href": "lectures/lec06-self-directed-analysis-without-answers.html#learning-preamble",
    "title": "6  Self-directed data exploration: Disease mortality across species with Farrell & Davies, PNAS, 2019",
    "section": "6.2 Learning preamble",
    "text": "6.2 Learning preamble\n\n6.2.1 Learning objectives\n\nBecome more confortable writing code on your own\nReview concepts from the first half of the course, including\n\nReading datasets into R\nNavigating between directories, including absolute and relative file paths\nSummarizing information about the structure of data\nUsing ggplot to visualize data\nCommon uses of dplyr functions: mutate, select, subset, summarize, group_by, pivot_longer, etc.\nExporting plots\n\nBecome more comfortable with troubleshooting code\nDiscuss project ideas with your class-mates!",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Self-directed data exploration: Disease mortality across species with Farrell & Davies, PNAS, 2019</span>"
    ]
  },
  {
    "objectID": "lectures/lec06-self-directed-analysis-without-answers.html#best-practices-for-troubleshooting-code",
    "href": "lectures/lec06-self-directed-analysis-without-answers.html#best-practices-for-troubleshooting-code",
    "title": "6  Self-directed data exploration: Disease mortality across species with Farrell & Davies, PNAS, 2019",
    "section": "6.3 Best practices for troubleshooting code",
    "text": "6.3 Best practices for troubleshooting code\n\n6.3.1 Tips\nIf you have an error message, look for the FIRST occurrence of an error in the output and Google that line.\nTips for Googling:\n\nInclude the name of the function that failed in your query\n\nIf you’re not sure what failed, look for function names in that first error line (names with () after them) or the line above it\n\nRemove any variable names or values from your query\nIf you’re given line numbers, use them to double-check your script at that line location for syntax mistakes\n\nIf you got the error after running a whole code chunk, you can also try running your code line by line (with cmd+enter on Mac or ctrl+enter on PC) to pinpoint the source of the problem\n\n\nIf you’re not getting output you expect, test what you’re trying to do with toy data (make a tiny data frame or vector) and/or use your actual data but perform the operations one at a time.\nGenerally, ensure you understand any warning messages that printed by your code. You may decide to change your script to eliminate the message, but make that decision carefully. This could be affecting analyses you’re performing later in your script. Google is your friend here as well.\nIt is absolutely disheartening to spend a lot of time trying to fix something. We’re not here to dismiss that feeling. It’ll take a while to get good at recognizing what keywords will be important for your Google searches & being able to fully understand the Stack Overflow posts. Troubleshooting and debugging are very valuable skills, though, and it’s worth the effort!\n\n\n6.3.2 Quick Examples\n\nlibrary(tdyverse)\n\noutput: Error in library(tdyverse) : there is no package called ‘tdyverse’\nWhat would you search?\n\nx = 7\n\nif (x &lt; 5) {\n  print(\"x is less than 5\")\nelse {\n  print(\"x is greater than 5\")\n}\n  \nrm(x)\n\noutput: Error: unexpected 'else' in:\"  print(\"x is less than 5\") else\"\nWhat would you search?\n\nmean(iris$Species)\n\noutput: Warning: argument is not numeric or logical: returning NA\nWhat would you search?\n\n# Function name: mean\n# message: argument is not numeric or logical: returning NA",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Self-directed data exploration: Disease mortality across species with Farrell & Davies, PNAS, 2019</span>"
    ]
  },
  {
    "objectID": "lectures/lec06-self-directed-analysis-without-answers.html#the-dataset",
    "href": "lectures/lec06-self-directed-analysis-without-answers.html#the-dataset",
    "title": "6  Self-directed data exploration: Disease mortality across species with Farrell & Davies, PNAS, 2019",
    "section": "6.4 The dataset",
    "text": "6.4 The dataset\nInformation regarding the columns of the dataset are below. Each row corresponds to a particular host-parasite combination, and includes information about the order of the host, type of parasite, number of documented host species per parasite, the number of reported cases and deaths in the focal host, if transmission of the parasite is vector-borne, the mean phylogenetic distance of the focal host species from all species known to host the parasite, and information regarding the times and locations of case and fatality count data.\n\nYear – Year of report\nCountry – Country name\nDisease – OIE reported disease name\nParasite – Parasite Latin binomial\nHost – Latin binomial of host reported by World Organisation for Animal Health\nCases – Number of reported cases\nDeaths – Number of reported deaths due to disease\nDestroyed – Number of animals reported to have been destroyed\nSlaughtered – Number of animals reported to have been slaughtered\nHostOrder – Taxonomic order of host species\nSR – The number of documented host species per parasite, also referred to as “host species richness”\nEvoIso – Host evolutionary isolation (in millions of years); see Figure 1 and SI 1.3 for details.\nTaxonomic Range – The largest taxonomic range among documented host species (1-6 representing parasites restricted to a single species, genus, family, order, class, or multiple classes, respectively)\nParaType – Gross parasite taxonomic group (arthropod, bacteria, fungi, helminth, protozoa, virus)\nParaFamily – Parasite taxonomic family\nVector – Binary variable (0/1) indicating whether parasite can be transmitted by an arthropod vector\nEnviroRestingStage – Binary variable (0/1) indicating whether parasite has a resting stage capable of persisting for long periods of time in the environment (typically months to years)\nAvianReservoir –Binary variable (0/1) indicating whether parasite has been documented to use avian species as reservoir hosts\nReproduction – Binary variable (0/1) indicating whether parasite can be vertically transmitted as a function of reproduction (either vertically transmitted, sexually transmitted, or passed from mother to offspring via ingestion of milk or colostrum\nWOK_citations – number of Web of Knowledge citations per parasite\nWOK_citations_noHuman – number of Web of Knowledge citations per parasite excluding those that reference humans\nlatitude – latitude of country in degrees\ngdp – Gross Domestic Product of country (current US$); WDI code “NY.GDP.MKTP.CD”\ngdp_pcap – Gross Domestic Product of country per capita (current US$); WDI code “NY.GDP.PCAP.CD”",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Self-directed data exploration: Disease mortality across species with Farrell & Davies, PNAS, 2019</span>"
    ]
  },
  {
    "objectID": "lectures/lec06-self-directed-analysis-without-answers.html#exercise-1-45-mins",
    "href": "lectures/lec06-self-directed-analysis-without-answers.html#exercise-1-45-mins",
    "title": "6  Self-directed data exploration: Disease mortality across species with Farrell & Davies, PNAS, 2019",
    "section": "6.5 Exercise 1 (~45 mins)",
    "text": "6.5 Exercise 1 (~45 mins)\na. Use download.file() to download the disease_distance.csv dataset from the EEB313 website. Save the file to your computer and call it disease_distance.csv.\nb. Clear your environment and console.\nc. Read the disease_distance.csv dataset into R in two ways. Name the data object disease_data.\n\nChange your working directory to the location where the file is saved using setwd(), and provide read_csv() with the relative path to the data.\nProvide the absolute file path to the Farrell & Davies data when you call read_csv().\n\nd. View aspects of the data in the console.\n\nRun str() on the data and interpret the output.\nPrint the first 100 rows of the data. Print the last 100 rows.\nAssign to an object new_df a data frame with 1000 randomly selected rows. Do this using the tidy function slice_sample(). Hint: use the documentation for the slice() function found here.\nAssign to an object new_df2 a data frame with every 10th row of the data. Do this using base R. Hint: use seq() to create a vector whose entries are 10, 20, 30, … and use this vector to extract the rows of interest.\nSelect columns Reproduction, Host, Cases, Country, and Deaths from the new_df2 data frame you just made.\nRun class() on the following columns: Reproduction, Deaths, and Country. Which of these columns can be effectively treated — or, in other words, coerced — into a logical vector (i.e., a vector whose entries are either TRUE or FALSE)?\n\nFrom here on out, use the FULL data frame (i.e., disease_data, where all columns and rows present). To ensure that you do not call new_df, run new_df &lt;- NULL. Try to think of a few reasons why, in writing a program for a research project, one would write over an intermediate data frame or file.\ne. Remove any rows with missing data using the tidy function drop_na(). Documentation can be found here. Are there any missing rows? Why or why not?\nf. A calculation and some coercion!\n\nUse the function mutate to estimate the case fatality rate by dividing number of individuals that die of a disease by the number of confirmed cases. Return a data frame with one column, corresponding to the case fatality rates that were just calculated.\nCoerce the case fatality rate data column into a numeric vector using as.numeric(). Use the function class to check what type of variable the case fatality rate is before and after this coercion. What function might you use to coerce something into a character vector? (R is nice in that the naming conventions are very natural…when you get the hang of them!)\nWhat do you notice about the distribution of case fatality rates? (hint: make a plot!)\nTry to identify conditions under which the case fatality rate would be a problematic measure of parasite virulence?\n\ng. Use tidyverse to count the number of observations associated to each parasite. Do the same thing for each host, and then for each host-country combination.\nh. Compute mean, median, max, and min of Cases for each host in each country. Then, filter the resulting data frame so that rows where the mean number is &gt;5000 are kept. What countries are such that the mean number is &gt;5000? Think about why some countries may have fewer reported cases (key word: reported).",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Self-directed data exploration: Disease mortality across species with Farrell & Davies, PNAS, 2019</span>"
    ]
  },
  {
    "objectID": "lectures/lec06-self-directed-analysis-without-answers.html#exercise-2-45-min",
    "href": "lectures/lec06-self-directed-analysis-without-answers.html#exercise-2-45-min",
    "title": "6  Self-directed data exploration: Disease mortality across species with Farrell & Davies, PNAS, 2019",
    "section": "6.6 Exercise 2 (~ 45 min)",
    "text": "6.6 Exercise 2 (~ 45 min)\na. Histograms, histograms, histograms!\n\nMake a histogram of the number of cases, and another for the number of deaths. What do you notice?\nMake histograms of the numbers of the log-transformed cases and deaths. What do you notice? By log transformed, we mean the logarithm (base 10) of the variable. Are there any warnings which appear and, if so, why?\nUse the function unique() to determine the unique host species represented in the dataset. How many are there?\nMake a histogram of the log-transformed number of deaths, faceted by host. Play with the parameter bins. The default for this parameter is bins = 30. What happens if you set bins = 1 and bins = 100?\nMake a histogram of the number of deaths over the number of cases by host order and parasite type. Hint: Use facet_grid() instead\nInterpret the histograms you just made. Do you notice anything interesting?\n\nb. Boxplots (and a bunch of other geoms…)\n\nExplore the documentation for ggplot “geoms”, which can be found here here.\nMake a boxplot with host on the x axis and the log-transformed number of deaths on the y axis. Call this plot p.\nExplore the structure of the plot object p using the global environment and, after this, the str() function?\nTry changing the theme. Classics include bw(), classic(), light(), and void(). Which ones do you like? (More on ggplot themes can be found here.) Also, check out the GitHub repo for the newly-developed Barbie and Oppenheimer themes here! You can use these themes by first installing the package ThemePark.\nUse Google to determine what to add to the ggplot so the text on the x axis is rotated by 90 degrees (i.e., is readable).\nExport the plot as a .png and then as a .pdf (look up the function ggsave)\n\nc. Visualize the data to see if there is relationship between the case fatality rate (the number of deaths over the number of cases) for a host and evolutionary isolation from the other hosts?\nBased on your visualization of the data, what you predict the relationship between these variables is? How would you test your understand and/or visualize the relationship in a different way? If constructing a statistical model to determine if there is relationship, are there variables in the dataset that you think would be important to control for?\nAfter thinking about these questions, read the abstract and significance statement for the paper from which this dataset is from. Discuss with your group what what exploratory analysis of data can and cannot do, based on the information in the paper and what you noticed about your visualization.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Self-directed data exploration: Disease mortality across species with Farrell & Davies, PNAS, 2019</span>"
    ]
  },
  {
    "objectID": "lectures/lec06-self-directed-analysis-without-answers.html#bonus-exercise-15-min",
    "href": "lectures/lec06-self-directed-analysis-without-answers.html#bonus-exercise-15-min",
    "title": "6  Self-directed data exploration: Disease mortality across species with Farrell & Davies, PNAS, 2019",
    "section": "6.7 Bonus Exercise (15 min)",
    "text": "6.7 Bonus Exercise (15 min)\na. Check out this documentation for the package ggridges.\nb. Install ggridges and make sure to load it using library() or require().\nc. Play around with ggridges! Using the disease_distance dataset which you have been exploring this lecture, make a ridgeline plot with a continuous variable on the x axis and discrete variable on the y. If you like, try to customize it (e.g., with fun colors)!\nd. Run the following code chunk :)\n\ninstall.packages(\"praise\")\nlibrary(praise)\nfor (i in 1:10) { print(praise()) }",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Self-directed data exploration: Disease mortality across species with Farrell & Davies, PNAS, 2019</span>"
    ]
  },
  {
    "objectID": "lectures/lec07-random-variables.html",
    "href": "lectures/lec07-random-variables.html",
    "title": "7  Random variables and computational statistics",
    "section": "",
    "text": "7.1 Lesson preamble\nlibrary(tidyverse)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random variables and computational statistics</span>"
    ]
  },
  {
    "objectID": "lectures/lec07-random-variables.html#lesson-preamble",
    "href": "lectures/lec07-random-variables.html#lesson-preamble",
    "title": "7  Random variables and computational statistics",
    "section": "",
    "text": "7.1.1 Learning Objectives\n\nUnderstand how the concept of random samples forms the foundation of statistical inference\nDraw random numbers using sample, rbinom, and other distributions\nCalculate statistics on random samples\nConduct simple hypothesis tests\nDo simple maximum likelihood estimation",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random variables and computational statistics</span>"
    ]
  },
  {
    "objectID": "lectures/lec07-random-variables.html#introduction-probability-random-variables-sampling-and-statistical-inference",
    "href": "lectures/lec07-random-variables.html#introduction-probability-random-variables-sampling-and-statistical-inference",
    "title": "7  Random variables and computational statistics",
    "section": "7.2 Introduction: Probability, random variables, sampling, and statistical inference",
    "text": "7.2 Introduction: Probability, random variables, sampling, and statistical inference\nIn this lecture we’re going to introduce the concept of random variables and techniques for simulating random processes in R. This will serve as a foundation for two future topics we’ll cover in this class:\n\nSimulating stochastic mathematical models: Many important biological processes that we want to understand and predict are random, and we must include the probabilistic nature of their outcomes in order to accurately describe them. Some examples include the processes of mutation and recombination during cell division, the changes in the frequency of genes in a population, and the fluctuations over time in the size of small populations. We will get to mathematical models later in the course\nConducting statistical inference: “Statistical inference” means taking observed data and trying to estimating an underlying trend, infer the properties of an entire population from a small sample, or more formally, estimating the parameters of a model that best describe the process that generated the data. We are doing statistical inference whenever we report the slope of a regression line or the prevalence of a trait in a population. All statistical inference starts by thinking about the (random) process through which the observed sample data were generated from the larger underlying population. This is the topic we will move directly into during this lecture and continue with for the next few classes.\n\nYou have learned the basic concepts of probability, random variables, and statistical inference in earlier courses. However, you most likely studied the probability of outcomes in thought experiments, and used mathematical formulas that have been derived to give approximate values for inference. In this class, we hope to help you gain a little more intuition for random processes and the philosophy behind statistical inference by using computational methods to generate and analyze random samples.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random variables and computational statistics</span>"
    ]
  },
  {
    "objectID": "lectures/lec07-random-variables.html#sampling-random-variables",
    "href": "lectures/lec07-random-variables.html#sampling-random-variables",
    "title": "7  Random variables and computational statistics",
    "section": "7.3 Sampling random variables",
    "text": "7.3 Sampling random variables\nLet’s think about the simplest possible experiment we can think of where the outcome is random: the flip of a coin. For now, we’ll assume we’re talking about a perfectly unweighted coin - one that is identical on both sides and so has equal probability of landing on either heads or tails.\nWe can use the built-in sample function in R to simulate random coin flips. We give sample the possible outcomes, the probability of each, and the number of simulations we want it to run for us:\n\nsample(x = c(\"heads\", \"tails\"), prob = c(0.5, 0.5), size = 1)\n\n[1] \"tails\"\n\n\nNote that in cases where we want the probability of each outcome to be the same, we can omit the prob argument.\nLet’s see what happens when we repeat this multiple times:\n\nprint(sample(c(\"heads\", \"tails\"), size = 1))\n\n[1] \"heads\"\n\nprint(sample(c(\"heads\", \"tails\"), size = 1))\n\n[1] \"tails\"\n\nprint(sample(c(\"heads\", \"tails\"), size = 1))\n\n[1] \"tails\"\n\n\nUnlike any other calculation we’ve every done in R, every time we run this we can get something different! Just like a real-life coin flip.\nSuppose we wanted to simulate the outcome of flipping a coin twice: we can simply update the size argument.\n\nsample(c(\"heads\", \"tails\"), size = 2, replace = TRUE)\n\n[1] \"tails\" \"heads\"\n\n\nNote that we’ve set replace = TRUE, which means that when the first sample is drawn from the set of options (“heads”, “tails”), it is not “removed” from the options available when the second number is chosen. There are other situations when we might be taking a random sample from a set and want replace = FALSE. For example, if we’re randomly choosing the batting order for a baseball team or if we’re choosing in which order to visit a set of locations to collect data.\n\n7.3.1 Challenge\nUsing the sample function, simulate the following:\n\nrolling a standard six-sided dice 10 times\nchoose a random order in which to visit the countries of Thailand, Cambodia, and Vietnam on your next trip\nthe distribution of males and females in a sample of size 10, when the sex ratio in the population is known to be 1.5 males: 1 female\n\n\nsample(x = 1:6, size = 10, replace = TRUE)\n\n [1] 6 5 5 4 1 5 5 5 3 1\n\n\n\nsample(x = c(\"Thailand\",\"Cambodia\", \"Vietnam\"), size = 3, replace = FALSE)\n\n[1] \"Thailand\" \"Vietnam\"  \"Cambodia\"\n\n\n\ntable(sample(x = c(\"males\",\"females\"), prob = c(0.6,0.4), size = 10, replace = TRUE))\n\n\nfemales   males \n      3       7",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random variables and computational statistics</span>"
    ]
  },
  {
    "objectID": "lectures/lec07-random-variables.html#drawing-repeated-samples-from-probability-distributions",
    "href": "lectures/lec07-random-variables.html#drawing-repeated-samples-from-probability-distributions",
    "title": "7  Random variables and computational statistics",
    "section": "7.4 Drawing repeated samples from probability distributions",
    "text": "7.4 Drawing repeated samples from probability distributions\nNow let’s think about if we wanted to simulate even more coin flips … like 10, or 100, or 1000! We can certainly do this with the same method:\n\ncoin_flip &lt;- sample(c(\"heads\", \"tails\"), size = 100, replace = TRUE)\nprint(coin_flip)\n\n  [1] \"heads\" \"heads\" \"heads\" \"tails\" \"tails\" \"heads\" \"tails\" \"tails\" \"tails\"\n [10] \"heads\" \"tails\" \"tails\" \"tails\" \"tails\" \"tails\" \"tails\" \"tails\" \"heads\"\n [19] \"tails\" \"tails\" \"heads\" \"heads\" \"heads\" \"tails\" \"heads\" \"tails\" \"heads\"\n [28] \"heads\" \"tails\" \"heads\" \"tails\" \"heads\" \"heads\" \"tails\" \"heads\" \"tails\"\n [37] \"tails\" \"tails\" \"heads\" \"tails\" \"tails\" \"heads\" \"heads\" \"tails\" \"heads\"\n [46] \"tails\" \"heads\" \"tails\" \"heads\" \"tails\" \"tails\" \"heads\" \"heads\" \"heads\"\n [55] \"tails\" \"heads\" \"heads\" \"heads\" \"heads\" \"heads\" \"tails\" \"heads\" \"heads\"\n [64] \"tails\" \"tails\" \"tails\" \"heads\" \"tails\" \"heads\" \"tails\" \"heads\" \"heads\"\n [73] \"heads\" \"heads\" \"tails\" \"tails\" \"tails\" \"tails\" \"heads\" \"tails\" \"heads\"\n [82] \"heads\" \"tails\" \"heads\" \"tails\" \"heads\" \"tails\" \"heads\" \"heads\" \"heads\"\n [91] \"heads\" \"heads\" \"heads\" \"heads\" \"tails\" \"heads\" \"heads\" \"tails\" \"heads\"\n[100] \"tails\"\n\n\n\ntable(coin_flip)\n\ncoin_flip\nheads tails \n   53    47 \n\n\nAnd we can calculate some statistics based on these outcomes, like “what proportion of times did we get heads?”\n\nsum(coin_flip == \"heads\")/length(coin_flip)\n\n[1] 0.53\n\n\nHowever, this is not really a very efficient or generalization way of sampling binary outcomes. We probably don’t actually care about the entire ordered sequence of {“heads”, “tails”} outcomes. Most of the time we’re probably only interested in the number of heads vs the number of tails (i.e., the proportion of one vs the other). There is actually a way we can sample this result directly!\nTo do so, we need a quick refresher on the formal names statisticians give to the probability distributions that result from some of the most common random experiments that scientists like to think about. A probability distribution is simply the formal mathematical description of the set of possible outcomes of a random process and each of their probabilities. A coin flip is an example of a Bernoulli trial - any random experiment that asks a yes/no question or has any other binary outcome. The result of such an experiment is said to follow a Bernoulli distribution. It’s a bit more general than the coin flip, as the probability of a “yes” outcome or a “success” (e.g., a heads) doesn’t need to be 0.5 but can be any number between 0 and 1.\nThe outcome of multiple Bernoulli trials (i.e., repeated coin flips) can be described by a binomial distribution, which gives the probability of getting a certain number of “successes” (i.e., heads) after doing a certain number of iterations (i.e., coin flips). So, to generate a random number describing the number of heads we land after flipping a coin 100 times, we can use the function rbinom() (the random number generator for the binomial distribution):\n\nrbinom(n = 1, size = 100, prob = 0.5)\n\n[1] 46\n\n# n: how many times to repeat all the trials\n# size: how many trials to do \n# prob: chance of success in each trial\n\nRun this cell a few more times, and see that you get a different answer each time.\nNow, let’s set up a loop to repeat this 10-flip experiment 100 times, and make a plot to see how much the outcomes vary\n\nbinom_samples &lt;- rbinom(n = 100, size = 10, prob = 0.5)\nbinom_samples\n\n  [1] 5 6 3 6 6 5 6 5 4 6 4 5 6 3 4 5 6 3 6 7 1 3 6 7 6 3 8 3 7 3 5 5 4 8 5 7 5\n [38] 3 4 5 6 5 4 8 5 6 4 6 7 4 5 6 4 3 6 8 4 3 4 4 6 7 5 3 7 4 4 3 3 5 6 2 6 5\n [75] 5 1 6 5 3 5 8 5 4 3 2 5 5 6 6 6 5 7 6 5 3 5 8 6 3 5\n\n\n\nmean(binom_samples)\n\n[1] 4.95\n\n\n\ndata.frame(value = binom_samples) %&gt;% ggplot(aes(x = value)) + \n  geom_histogram(binwidth = 1, center = 0, color = \"black\") + \n  geom_vline(aes(xintercept = 5),linetype = \"dashed\", color = \"red\") + \n  scale_x_continuous(breaks = 0:11, limits = c(-0.5,10.5)) + \n  labs(x = \"Number of heads\") + \n  theme_classic()\n\n\n\n\n\n\n\n\n\n7.4.1 Challenge question\n\nRegenerate this histogram but assuming 1000 trials were done. What do you notice about the change in the graph?\nImagine you flipped a coin 10 times and 8 of these times you got heads. Your friend insists this result means that the coin you are using must be biased towards heads or that you are controlling the flip to preferentially land on heads. What would you say in response to this? How could you quantify this argument? (Hint: How can you quantify exactly how likely an outcome this extreme would be with an unweighted coin?)\n\n\nbinom_samples_more &lt;- rbinom(n = 1000, size = 10, prob = 0.5)\ndata.frame(value = binom_samples_more) %&gt;% ggplot(aes(x = value)) + \n  geom_histogram(binwidth = 1, center = 0, color = \"black\") + \n  geom_vline(aes(xintercept = 5),linetype = \"dashed\", color = \"red\") + \n  scale_x_continuous(breaks = 0:11, limits = c(-0.5,10.5)) + \n  labs(x = \"Number of heads\") + \n  theme_classic()\n\n\n\n\n\n\n\n\n\nsum(binom_samples_more == 8)/length(binom_samples_more)\n\n[1] 0.048\n\n# In the above sample of 1000 repeats of 10 flips each, we see that about 5% of the time we get 8 heads. This is rare, but not THAT rare\n\n\nprop.table(table(binom_samples_more))\n\nbinom_samples_more\n    0     1     2     3     4     5     6     7     8     9 \n0.001 0.007 0.047 0.110 0.214 0.246 0.200 0.121 0.048 0.006 \n\n# In the above sample of 1000 repeats of 10 flips each, we see that about 6% of the time we get 8 or more heads. This is rare, but not THAT rare",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random variables and computational statistics</span>"
    ]
  },
  {
    "objectID": "lectures/lec07-random-variables.html#probability-distributions",
    "href": "lectures/lec07-random-variables.html#probability-distributions",
    "title": "7  Random variables and computational statistics",
    "section": "7.5 Probability distributions",
    "text": "7.5 Probability distributions\nLet’s make a plot to explicitly see how our distribution of outcomes changes as we do more and more trials:\n(make facet plot with different trial sizes and a few iterations of each?)\n\nbinom_samples_10 &lt;- rbinom(n = 10, size = 10, prob = 0.5)\nbinom_samples_100 &lt;- rbinom(n = 100, size = 10, prob = 0.5)\nbinom_samples_1000 &lt;- rbinom(n = 1000, size = 10, prob = 0.5)\nbinom_samples_10000 &lt;- rbinom(n = 10000, size = 10, prob = 0.5)\n\nbinom.df &lt;- data.frame(value = binom_samples_10, sample_size = 10) %&gt;% \n  rbind(data.frame(value = binom_samples_100, sample_size = 100)) %&gt;%\n  rbind(data.frame(value = binom_samples_1000, sample_size = 1000)) %&gt;%\n  rbind(data.frame(value = binom_samples_10000, sample_size = 10000)) \n\nplot.binom &lt;- binom.df %&gt;% \n  ggplot(aes(x = value)) + \n  geom_histogram(binwidth = 1, center = 0, color = \"black\",aes(y = after_stat(density))) + \n  geom_vline(aes(xintercept = 5),linetype = \"dashed\", color = \"red\") + \n  scale_x_continuous(breaks = 0:11, limits = c(-0.5,10.5)) + \n  labs(x = \"Number of heads\") + \n  theme_classic() + \n  facet_wrap(~sample_size)\n\nplot.binom\n\n\n\n\n\n\n\n\nWe see that as we do more and more trials, the distribution of our outcomes becomes more predictable, smoother, and more clearly centered around the mean value. In fact, in this case, it actually converges to a well-characterized shape that can be described with a simple mathematical formula, and is know as the binomial distribution.\nThe binomial distribution is an example of a probability mass function, which exists for every random process that generates a set of discrete values, and formally describes the probability of getting each possible outcome value \\(X\\), :\n\\[\\Pr(X=k) = p_k \\hspace{12pt} (\\text{e.g. } p_k = 1/6 \\text{ for all } k \\text{ when a die is fair})\\]\nThe probability mass function for the binomial distribution is\n\\[\\Pr(X=x) = f(x|N,p) = {N \\choose x} (1-p)^{N-x} p^x.\\]\nwhere \\(N\\) is the number of trials, \\(p\\) is the probability of success in each trial, and \\(x\\) is the observed number of successes.\nProbability mass functions must satisfy a few criteria:\n\nthe probability of each outcome must be between 0 and 1\nthe probability of all outcomes must sum to 1\n\nNote that for any random/stochastic process, the PMF always exists, though it might not always have a nice mathematical representation. For random processes that generate continuous random variables (e.g., choosing any number between 0 and 1, or sampling from a normal distribution), the name for the probability distribution is instead the probability density function (“PDF”). We’ll get to examples of continuous random variables later.\nHere we can plot it along with our simulated distributions, by using the built-in R function for the binomial probability distribution (dbinom) :\n\nbinom.pmf &lt;- data.frame(x = 0:10, freq = dbinom(x = 0:10, size = 10, prob = 0.5))\n\nplot.binom + \n  geom_point(data = binom.pmf, aes(x=x, y = freq), color = \"blue\")\n\n\n\n\n\n\n\n\nThe probability distribution is useful because it allows us to directly calculate the long-term probability of a given outcome, without simulation. For example, if we were to keep repeating our experiment of flipping a coin 10 times and counting up the heads, what would be the proportion of times we landed 8 heads? Using the built-in function for the probability distribution for the binomial, dbinom :\n\ndbinom(x = 8, size = 10, prob = 0.5)\n\n[1] 0.04394531\n\n\nwhich is very similar to the value of 0.049 we estimated before from 1000 simulations!",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random variables and computational statistics</span>"
    ]
  },
  {
    "objectID": "lectures/lec07-random-variables.html#hypothesis-testing",
    "href": "lectures/lec07-random-variables.html#hypothesis-testing",
    "title": "7  Random variables and computational statistics",
    "section": "7.6 Hypothesis testing",
    "text": "7.6 Hypothesis testing\nWith the simple tools we have now and using the binomial distribution, we can already show how we can do formal statistical inference just using simulations.\nSuppose we want to conduct an experiment to test if our coin is unweighted (vs biased towards one side or the other). Formally, we want to test the null hypothesis that the probability of heads is 50% (vs an alternative hypothesis that the coin is biased towards heads). To do this, we conduct an experiment where we flip our coin 100 times. Let’s imagine that the result of this experiment is that we get 66 heads. This is our data.\nTesting the null hypothesis means we have to ask, “How likely would I be to observe an outcome like my data if the null hypothesis were true?”, or more specifically, “How likely would I be to observe 66 heads out of 100 coin flips for an unweighted coin”?. If our observed data is very unlikely under the null hypothesis, then we would reject it, and say it doesn’t explain the data well.\nBefore we calculate anything, we should decide what we mean by “very unlikely”. Less than 10% of the time? Less than 1%? Less than 0.00001%? This level of critical “unlikeliness” is called the significance level, and is usually referred to with the symbol \\(\\alpha\\). For (somewhat arbitrary) historical reasons, a value of \\(\\alpha\\) = 0.05 (5%) is typically used, meaning that we reject a null hypothesis if the likelihood of generating data like ours if that hypothesis were true would be less than 5%.\nNow that we have our cut-off level of “unlikeliness” defined, we have to calculate how likely our observed data is under the null hypothesis. This will be our p value! We can do this in two ways. Since we have a probability mass function for the random process that describes how this data was generated, we can simply calculate the probability\n\ndbinom(x = 66, size = 100, prob = 0.5)\n\n[1] 0.0004581053\n\n\nThis means that under the null hypothesis that the coin is unweighted, there’s about a 0.045% chance of getting exactly 66 heads out of 100 flips. However, note that for hypothesis testing we don’t typically want to look at just the probability of getting exactly our observed result. We want to look at getting a results at least as extreme as our observed result, since any such result would provide evidence against the null hypothesis.\nFor the binomial distribution, we can calculate the probability of getting a value of 66 or higher by using the cumulative distribution function for the binomial. The cumulative distribution functions tell us the probability of getting the value \\(X\\) or lower, so instead we have to find the probability of an outcome of 65 or lower and then subtract this from one to get the remainder:\n\n# pbinom(q = 66, size=100, prob=0.5) # probability of 66 or lower\n# pbinom(q = 65, size=100, prob=0.5) # probability of 65 or lower\n1- pbinom(q = 65, size=100, prob=0.5) # probability of over 65, ie prob of 66 or higher\n\n[1] 0.0008949652\n\n\nUnder the null hypothesis that the coin is unweighted, there’s about a 0.09% chance of getting at least 66 heads out of 100 flips, so p = 0.0009 for this test. Since we’ve already defined out significance level as 5%, we would say we have sufficient evidence to reject the null hypothesis that the coin is unweighted, and instead we can conclude that there is statistically-significant evidence that the coin is weighted.\nNote that even if we didn’t know the mathematical formula for the probability mass function, we wouldn’t be out of luck! As long as we can simulate the process, we can still estimate the p value. Let’s simulate our exact experiment, but under the null hypothesis, and lets do this many times. So, let’s flip an unweighted coin 100 times, record the results, and repeat this 100,000 times. Then let’s calculate what % of the time we got at least 66\n\nn_trials = 10^5\nbinom.null &lt;- rbinom(n = n_trials, size = 100, prob = 0.5)\nsum(binom.null &gt;=66)/n_trials\n\n[1] 0.00095\n\n\nNow we have again have p = 0.0009!\nThis process of simulating data under the null hypothesis - instead of using a known formula to calculate a test statistic - is extremely useful for when we consider data that is generated by more complex random processes or models. The downside is that it can be slow, as you often have to do a lot of trials to get an accurate estimate, especially when p is small. As a rule of thumb, you want to make sure that there were at least 10 occurrances of the events that form the numerator of the p value calculation (in this case, sets of flips that gave at least 66 heads).\n\n7.6.1 Challenge\nNow that we feel confident rejecting the null hypothesis of an unweighted coin, let’s say we wanted to test a different hypothesis - for example, that instead of choosing the unweighted coin, we picked one up that had a 70% probability of giving heads. Under this new hypothesis, what is the p value? (ie, the probability of observing at least 66 heads)\n\n1- pbinom(q = 65, size=100, prob=0.7) # probability of over 65, ie prob of 66 or higher\n\n[1] 0.8371417\n\n# We would say \"with our observed data (66/100 heads) we fail to reject the null hypothesis that our coin is weighted 70% towards heads, with a p value of 0.83 (vs a significance level of 0.05)\"",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random variables and computational statistics</span>"
    ]
  },
  {
    "objectID": "lectures/lec07-random-variables.html#maximum-likelihood-estimation",
    "href": "lectures/lec07-random-variables.html#maximum-likelihood-estimation",
    "title": "7  Random variables and computational statistics",
    "section": "7.7 Maximum likelihood estimation",
    "text": "7.7 Maximum likelihood estimation\nFinally, let’s say we wanted to go beyond simply testing a few different hypothesis, and would instead like to estimate exactly how biased our coin is. More precisely, if we define a coin by it’s propensity to give heads, and call this parameter w, can we estimate the w value that is most likely, given our observed data?\nWe can use a very similar idea! For any proposed coin weight, we can calculate how likely we would be to observe the data that we did. We can repeat this for all possible coin weights, and then report the one that gives the maximum likelihood of our observed data! Let’s try it\n\n7.7.1 Challenge\n\nWrite a function that takes in the observed number of heads (heads_observed) for a given number of coin flips (n_flips), along with a potential propensity for the coin to give heads (weight), and return now likely the observed data would be under that coin weight. Test it for a coin weight of 0.7\n\n\nlikelihood_vs_coin_weight &lt;- function(heads_observed, n_flips, weight){\n  dbinom(x = heads_observed, size = n_flips, prob = weight)\n}\nlikelihood_vs_coin_weight(63, 100, 0.7)\n\n[1] 0.02683446\n\n\n\nUse this function to consider a range of weights between 0 and 1 and generate a data frame with the columns weights and likelihood. and plot the likelihood of the data vs the true coin weight. What is the most likely coin weight?\n\n\nheads_observed &lt;- 63\nn_flips &lt;- 100\n\ntest_weights &lt;- seq(0,1,0.01)\n\nlik.df &lt;- data.frame(weight = test_weights, likelihood = likelihood_vs_coin_weight(heads_observed, n_flips, test_weights))\n\nlik.df %&gt;% \n  ggplot(aes(x = weight, y = likelihood)) + \n  geom_point() + \n  labs(x = \"Coin weight (prob. of heads)\")\n\n\n\n\n\n\n\n\n\n#max\nlik.df %&gt;% filter(likelihood == max(likelihood))\n\n  weight likelihood\n1   0.63 0.08240399\n\n\nWhen you complete this, you will have just done your first maximum likelihood estimation!\nLet’s review the idea of likelihood! We have a set of data points  \\(x_1,x_2,\\dots,x_n\\) that we assume were generated by a random process with probability distribution \\(f(x|\\theta)\\), where \\(\\theta\\) is one or more parameters of that distribution. Our goal is to to estimate \\(\\theta\\), i.e., to determine what value of the parameters were mostly likely to have given rise to the data. We do this by maximizing the likelihood function\n\\[L(x_1,\\dots,x_n|\\theta) = f(x_1|\\theta) \\cdots f(x_n|\\theta) = \\prod_{i=1}^n f(x_i|\\theta),\\]\nwhich is formed under the assumption \\(x_1,\\dots,x_n\\) are independent. Note that for our example with the coin flip, we just had a single data point (\\(x_1 = 66\\)), and the binomial distribution just has one unknown parameter, the coin weight \\(w\\).\nA couple things to notice:\n\nThe likelihood is formed by simply plugging in the data into the probability distribution function from which they jointly arose. (When the data are independent, the joint probability distribution function is the product of the individual distribution functions.)\nThis formulation of the likelihood is agnostic to whether or not the observations are discrete or continuous — regardless, the distribution of the data is denoted \\(f(\\cdot|\\theta)\\)!\nThe likelihood function is a function of the parameters \\(\\theta\\), but NOT of the data. This is because the data has already been collected and is, thus, fixed.\nAmazingly, the likelihood contains all of the information in the data about the parameters.\nViewed as a function of \\(\\theta\\), the likelihood tells us how likely each set of parameter values is to have given rise to the data, given the data. (The \\(|\\) symbol means given…)\n\nMost importantly, the value(s) of parameter(s) \\(\\theta\\) which jointly maximize \\(L\\) (i.e., have the highest likelihood of generating the observed data) is called the maximum likelihood estimator. It is often denoted \\(\\hat{\\theta}_{\\text{MLE}}\\), and is our best guess for the parameter(s) which generated the data.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random variables and computational statistics</span>"
    ]
  },
  {
    "objectID": "lectures/lec08-inference.html",
    "href": "lectures/lec08-inference.html",
    "title": "8  Probability, likelihood, and statistical inference",
    "section": "",
    "text": "8.1 Lesson preamble\nlibrary(tidyverse)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability, likelihood, and statistical inference</span>"
    ]
  },
  {
    "objectID": "lectures/lec08-inference.html#lesson-preamble",
    "href": "lectures/lec08-inference.html#lesson-preamble",
    "title": "8  Probability, likelihood, and statistical inference",
    "section": "",
    "text": "8.1.1 Learning Objectives\n\nDescribe and sample from a variety of probability distributions commonly encountered in the sciences\nUse computational methods to test hypotheses about population parameters and processes that gave rise to observed data\nUnderstand the concept of permutation tests and compare to standard statistical tests\nApply simple maximum likelihood estimation to real data",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability, likelihood, and statistical inference</span>"
    ]
  },
  {
    "objectID": "lectures/lec08-inference.html#common-probability-distributions",
    "href": "lectures/lec08-inference.html#common-probability-distributions",
    "title": "8  Probability, likelihood, and statistical inference",
    "section": "8.2 Common probability distributions",
    "text": "8.2 Common probability distributions\nThere are two major categories of random variables that have important differences in how they are treated in statistics:\n\n8.2.1 Discrete random variables\n.. can take on only a finite number of discrete values. The probability of any outcome \\(X\\) is described by the probability mass function:\n\\[\\Pr(X=k) = p_k \\hspace{12pt} (\\text{e.g. } p_k = 1/6 \\text{ for all } k \\text{ when a die is fair})\\]\nwhere the sum of all outcomes must be 1: \\[ \\sum_{k=1}^n \\Pr (k) =1 \\]\nExamples of discrete probability distributions include:\n\nThe Poisson distribution. This distribution models the number of occurrences of a rare event. It has one parameter: the rate \\(\\lambda\\) at which such events occur.\nThe Binomial distribution. This distribution is associated to a random variable which counts the number of “successes” or “failures” in \\(N\\) trials, where the success probability is \\(p\\).\nThe Hypergeometric distribution models the probability that, if we draw \\(n\\) objects from a population of size \\(N\\) where \\(K\\) of the objects have a specific attribute without replacement, that we see \\(k\\) objects in our sample with the attribute.\nThe Geometric distribution. If we do many trials, each of which are independent and have probability of “success” \\(p\\), a random variable that has a \\(\\text{Geometric}(p)\\) distribution counts the number of tries until a success occurs.\nThe Negative binomial distribution models the number of trials until \\(r\\) successes occur, if the success probability in each trial is \\(p\\).\nThe Multinomial distribution counts the number of events in each of \\(K\\) categories after \\(N\\) trials. It is a generalization of the binomial distribution to outcomes that can take on more than two values\n\nNote: when \\(N\\), \\(K\\) are large relative to \\(n\\) and \\(K/N\\) is not too close to 0 or 1, the Binomial and Hypergeometric distributions are similar and its often advantageous to use the former (because it has a nicer probability distribution function, fewer parameters).\n\n\n8.2.2 Continuous random variables\n… can take on any infinite number of values within a range. The likelihood outcomes \\(Y\\) is described by the probability density function. Because \\(Y\\) can take on infinitely many values, the probability of an exact value is zero. Instead, the value of the probability density function at a particular \\(y\\) value \\(f(y)\\) can be used to understand the relative likelihood of on \\(y\\) value vs another, and the probability of observing \\(Y\\) within a range \\(A\\) is the integral of the probability density function over that range.\n\\[\\Pr(a \\leq Y \\leq b) = \\int_a^b f(y) \\text{d} y.\\]\nIntegrated over the entire range of possible \\(Y\\) values the probability density function is one\n\\[\\int_{-\\infty}^\\infty f(y) \\text{d} y = 1\\]\nExamples of continuous probability distributions include:\n\nThe Uniform distribution models outcomes where all occur with equal probability.\nThe Normal distribution is often used to model continuous observations that are sums of a large number of small, random contributions (e.g, individual alleles to a trait).\nThe Log-normal distribution models continuous observations such that their natural logarithm follows a Normal distribution. In other words, if \\(Y \\sim \\text{Lognormal}(\\mu,\\sigma^2)\\), then \\(\\ln Y \\sim \\text{Normal}(\\mu,\\sigma^2)\\) where \\(\\mu\\) is the mean.\nThe Exponential distribution models the time in-between events, if the events occur at rate \\(\\lambda\\). What is means for events to occur at rate \\(\\lambda\\) is that, over a small interval of time \\(\\Delta t\\), an event occurs with probability approximately \\(\\lambda \\Delta t\\). The larger the \\(\\lambda\\), the more frequently the event occurs…\nThe Gamma distribution is commonly used to describe the waiting time to the \\(r\\)th event of a particular type. It is a flexible distribution which can be used in a number of contexts, especially when it is not clear what the right probabilistic model for observations may be. It is often used to model distributions that look approximately normal but are more right-skewed (long-tailed). In limiting cases, it approaches the normal distribution.\n\n\n\n8.2.3 Challenge\nFor each of the distributions above, think of random variables (e.g., the number of brown bears with body sizes \\(&gt;\\) 100 kg) with that distribution? Discuss with your neighbors.\n\n\n8.2.4 Sampling random numbers\nWe can generate random numbers in R from all of these probability distributions, and more!\nFor example, to simulate a large realizations of a uniform random variable using runif().\n\nN &lt;- 10000\nrealizations_N_uniform &lt;- runif(n = N, min = 0, max = 1)\n\ndata.frame(value = realizations_N_uniform) %&gt;% \n  ggplot(aes(x = value)) + \n  geom_histogram(aes(y = after_stat(density)))\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can evaluate the probability one realization arose from, say, the Uniform(0,100) distribution using dunif(). By evaluate, we mean return the probability density the observation would arise.1\n\ndunif(realizations_N_uniform[1], min = 0, max = 100)\n\n[1] 0.01\n\n\nNow let’s try a Poisson distribution\n\nN &lt;- 10000\nrealizations_N_poisson &lt;- rpois(n = N, lambda = 3)\n\ndata.frame(value = realizations_N_poisson) %&gt;% \n  ggplot(aes(x = value)) + \n  geom_histogram(aes(y = after_stat(density)))\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n8.2.5 Challenge\n\nDraw 10000 random numbers from the Gamma distribution with shape = 0.5 and scale =2\n\n\nN &lt;- 10000\nrealizations_N_gamma &lt;- rgamma(n = N, shape = 1/2, scale = 2)\n\ndata.frame(value = realizations_N_gamma) %&gt;% \n  ggplot(aes(x = value)) + \n  geom_histogram(aes(y = after_stat(density)))\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\nMake a plot of the probability density function of a Gamma distribution shape = 0.5 and scale =2 using dgamma. Add another curve with shape = 2 and scale = 1/2. (Hint: just like most R functions, ie log, sin, dgamma can take in vectors of x values and return vectors of probabilities)\n\n\nx_vec &lt;- seq(0,7,0.01)\ngamma_vec1 &lt;- dgamma(x = x_vec, shape = 1/2, scale = 2)\ngamma_vec2 &lt;- dgamma(x = x_vec, shape = 2, scale = 1/2)\n\ngamma_data &lt;- data.frame(x = x_vec, value = gamma_vec1, params = \"1\") %&gt;% \n  rbind(data.frame(x = x_vec, value = gamma_vec2, params = \"2\"))\n\nggplot(gamma_data, aes(x = x, y = value, color = params)) + \ngeom_line()",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability, likelihood, and statistical inference</span>"
    ]
  },
  {
    "objectID": "lectures/lec08-inference.html#maximum-likelihood-estimation-and-hypothesis-testing-with-real-data",
    "href": "lectures/lec08-inference.html#maximum-likelihood-estimation-and-hypothesis-testing-with-real-data",
    "title": "8  Probability, likelihood, and statistical inference",
    "section": "8.3 Maximum likelihood estimation and hypothesis testing with real data",
    "text": "8.3 Maximum likelihood estimation and hypothesis testing with real data\nOne of the main goals of statistics is to determine what processes gave rise to the data we observe, or, to compare among candidate processes. This determination is done by first specifying candidate mathematical formula that describe the distribution of a variable (e.g., \\(X \\sim Binom(N, p)\\)) or the relationships among the observed variables (e.g., \\(Y \\sim m X + b\\)) . These formula typically involve unknown parameters, and part of the evaluation process is estimating the values of these parameters that best describe the data. Maximum likelihood estimation is is the foundation for most statistical procedures used in the sciences.\n\n8.3.1 Review of the likelihood\nThe likelihood of observing any particular data point given a particular probability distribution (with parameters \\(\\theta\\)) is just equal to the values of the probability distribution function at this \\(X\\) value, e.g., \\(L(x|\\theta) = f(x|\\theta)\\). When we have multiple data points that could inform our estimate, the likelihood becomes\n\\[L(x_1,\\dots,x_n|\\theta) = f(x_1|\\theta) \\cdots f(x_n|\\theta) = \\prod_{i=1}^n f(x_i|\\theta),\\]\nwhich is formed under the assumption \\(x_1,\\dots,x_n\\) are independent and using the basic laws of probability (which state that the joint probability of observing the two independent events A and B together is just equal to the product of their probabilities).\nTo determine what parameters were mostly likely to have given rise to the data under the assumption \\(f\\) models the data generating process, we find what value of \\(\\theta\\) maximizes the likelihood.\nThe intuition for the method is quite simple: the value of a parameter (or, in the multi-parameter setting, the combination of values for the parameters) which gives rise to the highest probability of observing our data was most likely to have given rise to that data.\n\n\n8.3.2 Case study 1: Maximum likelihood estimation with disease mortality data\nNow we are going to test out applying this estimation technique to some real data! We’ll revisit the dataset from a a prior lecture - the Farrell & Davies 2019 dataset that measured cases and deaths for a set of infectious diseases that each infect a wide range of animal species.\nOur goal will be to estimate, for the host species Sus scrofa (the wild boar) and parasite family Coronavirinae (which includes SARS-CoV-1 and -2), the probability that a case results in a death. We will group the data, regardless of the year, sampling location, etc.\n\ndisease_distance &lt;- read_csv(\"data/disease_distance.csv\")\n\nRows: 4157 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (7): Country, Disease, Parasite, Host, HostOrder, ParaType, ParaFamily\ndbl (17): Year, Cases, Deaths, Destroyed, Slaughtered, SR, EvoIso, Taxonomic...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndata &lt;- disease_distance %&gt;% filter(ParaFamily == \"Coronavirinae\" & Host == \"Sus_scrofa\")\ndata\n\n# A tibble: 14 × 24\n    Year Country       Disease Parasite Host  Cases Deaths Destroyed Slaughtered\n   &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1  2006 Cayman Islan… Transm… Alphaco… Sus_…     8      0         0           0\n 2  2006 Russian Fede… Transm… Alphaco… Sus_…  5763    570         0           0\n 3  2007 Cyprus        Transm… Alphaco… Sus_…    10      0         0           0\n 4  2007 Russian Fede… Transm… Alphaco… Sus_…   673    585         0           0\n 5  2008 Mexico        Transm… Alphaco… Sus_…    10      0         0           0\n 6  2008 Russian Fede… Transm… Alphaco… Sus_…     6      6         0           0\n 7  2009 Cyprus        Transm… Alphaco… Sus_…   156      0         0           0\n 8  2009 Mexico        Transm… Alphaco… Sus_…     5      0         0           0\n 9  2010 Cyprus        Transm… Alphaco… Sus_…     1      0         0           0\n10  2011 Belarus       Transm… Alphaco… Sus_…    29     14         0           0\n11  2011 French Polyn… Transm… Alphaco… Sus_…   738    547         0           0\n12  2011 Germany       Transm… Alphaco… Sus_…     5      0         0           0\n13  2011 Mexico        Transm… Alphaco… Sus_…    20      0         0           0\n14  2011 Peru          Transm… Alphaco… Sus_…   136     45         0           0\n# ℹ 15 more variables: HostOrder &lt;chr&gt;, SR &lt;dbl&gt;, EvoIso &lt;dbl&gt;,\n#   Taxonomic_Range &lt;dbl&gt;, ParaType &lt;chr&gt;, ParaFamily &lt;chr&gt;, Vector &lt;dbl&gt;,\n#   EnviroRestingStage &lt;dbl&gt;, AvianReservoir &lt;dbl&gt;, Reproduction &lt;dbl&gt;,\n#   WOK_citations &lt;dbl&gt;, WOK_citations_noHuman &lt;dbl&gt;, latitude &lt;dbl&gt;,\n#   gdp &lt;dbl&gt;, gdp_pcap &lt;dbl&gt;\n\n\n\n8.3.2.1 Step 1: specify the likelihood (i.e., the distribution of the data)\nWhat distribution makes sense to model the number of Sus scrofa deaths due to viruses in the family Coronavirinae? The Binomial! One can think of the number of (confirmed) cases of infection with a member of Coronavirinae as the “number of trials” and probability death as a “success probability” (\\(p\\)). Written out, \\(\\text{deaths}_{ij} \\sim \\text{Binomial}(\\text{cases}_{ij}, p)\\) for each country-year \\(ij\\). We’re assuming that for each country + year, the number of deaths out of all the cases is is an independent draw from a binomial distribution with the same parameter \\(p\\).\n\n\n8.3.2.2 Step 2: evaluate the likelihood of individual observations\nSince we will need to evaluate the probability of all observations in order to estimate \\(p\\), the first thing to figure out to evaluate likelihood when we have only one observation (i.e., the first row of the previous data frame). The below function evaluate the probability there are exactly \\(d\\) deaths when there are \\(N\\) cases and a probability \\(p\\) that a boar dies due to infection (somewhere, at some time) with a member of the Coronavirinae.\n\n# let's figure out the likelihood of a single observation -- the first row\n\ncases &lt;- as.numeric(data[1,\"Cases\"])\ndeaths &lt;- as.numeric(data[1,\"Deaths\"])\n\nprint(dbinom(x = deaths, size = cases, p = 0.1))\n\n[1] 0.4304672\n\nprint(dbinom(x = deaths, size = cases, p = 0.5))\n\n[1] 0.00390625\n\nprint(dbinom(x = deaths, size = cases, p = 0.9))\n\n[1] 1e-08\n\n\nOf these values of \\(p\\), which is most likely to explain the observation in the first row?\n\n\n8.3.2.3 Step 3: evaluate the likelihood of all observations\nTo get a list of the likelihood at each observed combination of cases and deaths, we use the fact that dbinom can take vectors of parameters as inputs and return a vector of probabilities for reach corresponding element of the vectors\n\ndbinom(x = data$Deaths, size = data$Cases, p = 0.1)\n\n [1] 4.304672e-01 1.693764e-02 3.486784e-01 0.000000e+00 3.486784e-01\n [6] 1.000000e-06 7.274974e-08 5.904900e-01 9.000000e-01 1.596866e-07\n[11] 0.000000e+00 5.904900e-01 1.215767e-01 1.551125e-13\n\n\n\n\n8.3.2.4 Step 4: put the (log-)likelihoods together!\nWe use the prod function to take the product of all the values in the vector of probabilities:\n\nprod(dbinom(x = data$Deaths, size = data$Cases, p = 0.2))\n\n[1] 0\n\n\nUh oh! How can the probability of our data be zero when the probability of death in a case is 20%? Glancing at the data, it makes sense this could be an unlikely value, but it shouldn’t be zero.\nUnfortunately, numerical problems can arise when you take products of very small numbers (because there is a limit to how small of a number a computer can store in the regular format). For this reason, we often take the logarithm of the likelihood instead. Remember that the log of a value between zero and 1 will be a negative but large number. And, remember that the product of individual likelihoods (for each observation) = the sum of log-likelihoods\n\nsum(log(dbinom(x = data$Deaths, size = data$Cases, p = 0.2)))\n\n[1] -1502.624\n\n\n\n\n8.3.2.5 Step 5: identify the value of the parameter(s) most likely to explain the data\nSo far, we have only evaluated the log-likelihood (of one and all of observations) at specific values of the parameter of interest, \\(p\\). To identify what value of \\(p\\) is most likely to explain the data, we need to evaluate the log-likelihood across a RANGE of \\(p\\) values and identify when the function is maximized.\nWe can do this in a couple ways, but here is how using a for loop:\n\np_values_to_test &lt;- seq(0, 1, by = 0.001)\nLogLik &lt;- rep(0, length(p_values_to_test))\nindex &lt;- 1\n\nfor (p in p_values_to_test){\n  LogLik[index] &lt;- sum(log(dbinom(x = data$Deaths, size = data$Cases, p = p)))\n  index &lt;- index + 1\n}\n\nLogLik &lt;- data.frame(LogLik, p = p_values_to_test)\n\nLogLik %&gt;% ggplot(aes(x = p, y = LogLik)) + geom_line()\n\n\n\n\n\n\n\n\nValues of \\(p\\) where \\(\\ln L(p) - -\\infty\\) are very unlikely to explain the data. Since we are interested in the value of \\(p\\) which gives rise to the largest \\(\\ln L\\), and these values are infinitely negative, they are not candidates for our estimator of \\(p\\). As a result, they are suppressed in the plot.\nAnother thing to notice is the curvature of the likelihood function around the value where it assumes a maximum. This tells us something about how confident we should be about our “best guess” for the parameter, or the maximum likelihood estimate of that parameter. More on this later …\n\nestimate_p &lt;- LogLik[LogLik$LogLik == max(LogLik$LogLik),\"p\"]\nestimate_p\n\n[1] 0.234\n\n\n\n\n\n8.3.3 Case study 2: Comparing sex differences with permutations tests\nIn this example we are going to revisit a classic inference task that you have certainly seen in your introductory stats classes: asking if there is a significant difference between the mean of two populations. We’ll revisit our dataset from the Portal study (Lectures 3-5). We’re going to look for evidence of sex differences in weight. This study is useful because it measures the individual weights of every animals sampled.\n\n#surveys &lt;- readr::read_csv(\"https://ndownloader.figshare.com/files/2292169\")\nsurveys &lt;- readr::read_csv('data/portal_data.csv')\n\nRows: 34786 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): species_id, sex, genus, species, taxa, plot_type\ndbl (7): record_id, month, day, year, plot_id, hindfoot_length, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s look at which species might be a good candidate to choose for this analysis. We’ll start by filtering by species (keeping Genus information too), which suggests Dipodomys merriami (kangaroo rat) is most frequently observed\n\nsurveys %&gt;% group_by(genus, species) %&gt;% tally() %&gt;% arrange(desc(n))\n\n# A tibble: 48 × 3\n# Groups:   genus [26]\n   genus           species          n\n   &lt;chr&gt;           &lt;chr&gt;        &lt;int&gt;\n 1 Dipodomys       merriami     10596\n 2 Chaetodipus     penicillatus  3123\n 3 Dipodomys       ordii         3027\n 4 Chaetodipus     baileyi       2891\n 5 Reithrodontomys megalotis     2609\n 6 Dipodomys       spectabilis   2504\n 7 Onychomys       torridus      2249\n 8 Perognathus     flavus        1597\n 9 Peromyscus      eremicus      1299\n10 Neotoma         albigula      1252\n# ℹ 38 more rows\n\n\nAnd we can see at least some evidence of difference in weigh by sex here\n\nsurveys %&gt;% group_by(genus, species, sex) %&gt;% \n  filter(!is.na(sex) & !is.na(weight)) %&gt;% # remove rows with NA in either\n  mutate(n_tot_species_sex=n()) %&gt;% # new column containing total observations per species\n  filter(n_tot_species_sex&gt;50) %&gt;%\n  group_by(genus, species, sex) %&gt;%\n  summarize(mean_weight = mean(weight), na.rm = TRUE)\n\n`summarise()` has grouped output by 'genus', 'species'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 27 × 5\n# Groups:   genus, species [14]\n   genus       species      sex   mean_weight na.rm\n   &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt; &lt;lgl&gt;\n 1 Chaetodipus baileyi      F            30.2 TRUE \n 2 Chaetodipus baileyi      M            33.8 TRUE \n 3 Chaetodipus penicillatus F            17.2 TRUE \n 4 Chaetodipus penicillatus M            17.2 TRUE \n 5 Dipodomys   merriami     F            41.6 TRUE \n 6 Dipodomys   merriami     M            44.4 TRUE \n 7 Dipodomys   ordii        F            48.5 TRUE \n 8 Dipodomys   ordii        M            49.1 TRUE \n 9 Dipodomys   spectabilis  F           118.  TRUE \n10 Dipodomys   spectabilis  M           122.  TRUE \n# ℹ 17 more rows\n\n\nLet’s plot this!\n\nsurveys_kr &lt;- surveys %&gt;% \n  filter(genus == \"Dipodomys\", species == \"merriami\") %&gt;% \n  filter(!is.na(sex) & !is.na(weight)) # remove rows with NA in either\n\nsurveys_kr %&gt;% ggplot(aes(x=weight, fill = sex, y = after_stat(density))) + \n  geom_histogram() + \n  facet_grid(~sex)\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\ndim(surveys_kr)\n\n[1] 10248    13\n\n\nAnd take a look at the difference in average weight by sex\n\nsurveys_kr %&gt;% group_by(sex) %&gt;% \n  summarize(mean_weight = mean(weight), n = n())\n\n# A tibble: 2 × 3\n  sex   mean_weight     n\n  &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;\n1 F            41.6  4440\n2 M            44.4  5808\n\n\n\nweight_diff &lt;- mean(surveys_kr[surveys_kr$sex == \"M\",]$weight) - mean(surveys_kr[surveys_kr$sex == \"F\",]$weight)\nweight_diff\n\n[1] 2.743449\n\n\nNow we want to understand - is this difference more that we would expect by chance?\nWe can use the popular Student’s t-test that is a favourite of statistics 101 classes everywhere to test this:\n\nt.test(weight ~ sex, data = surveys_kr)\n\n\n    Welch Two Sample t-test\n\ndata:  weight by sex\nt = -20.539, df = 9478.7, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n -3.005279 -2.481619\nsample estimates:\nmean in group F mean in group M \n       41.60968        44.35313 \n\n\nHowever, there is another way we can do this, without having to remember anything about t-tests! Another way of thinking about what it means to test for a significant differences in weight by sex is to say we are testing the null hypothesis that weight is the same by sex. Under that hypothesis, if we shuffled the sex labels on each animal, the distribution of weights should roughly be the same. We want to see if our data is consistent with what we’d expect if the sex labels were shuffled. We might not see NO difference - some differences between groups can arise by chance when we have small samples sizes. But, we can simulate this null hypothesis many times to understand how much sex difference we expect to see just do to chance, and then compare it to what we actually see in our data.\nThis procedure is an example of a permutation test, which is a very powerful tool in statistics and increasingly used now that computers can very quickly simulate this sort of res-sampled data. The procedure we follow below is:\n\nTake the entire list of measured weights, and randomly shuffle their order\nAssign the first half of the measurements (roughly 5000/10000) to be male), and the second half to be female\nRe-calculate the difference in weights between male and female in this simulated data\nRepeat this many times (1000 here), recording the weight differences calculated each time\nExamine the distribution of weight differences from this simulated, permuted data. This represents the expected weight difference by sex under the null hypothesis of no difference.\nSee where in this distribution our observed weight difference falls. Calculate what % of the null distribution is more extreme than our observed value (e.g., 2*(1-quantile(observed, simulated)). This is the p value for our hypothesis test\n\n\nN_samples &lt;- 1000\n\ntest_weight_diff_samples &lt;- rep(0,N_samples)\n\nfor (i in 1:N_samples){\n  test_sample &lt;- sample(x = surveys_kr$weight) \n  test_sample_male &lt;- test_sample[1:5000]\n  test_sample_female &lt;- test_sample[5000:10000]\n  test_weight_diff &lt;- mean(test_sample_male) - mean(test_sample_female)\n  test_weight_diff_samples[i] &lt;- test_weight_diff\n}\n\n\nggplot(data = data.frame(weight_diffs = test_weight_diff_samples), aes(x = weight_diffs)) + geom_histogram() +\n  geom_vline(xintercept = weight_diff, color = \"red\")\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\nsum(test_weight_diff_samples &gt; weight_diff)/N_samples\n\n[1] 0\n\n#max p value 1/N_samples = 0.001",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability, likelihood, and statistical inference</span>"
    ]
  },
  {
    "objectID": "lectures/lec08-inference.html#footnotes",
    "href": "lectures/lec08-inference.html#footnotes",
    "title": "8  Probability, likelihood, and statistical inference",
    "section": "",
    "text": "A technical and somewhat confusing thing about probability: if a continuous random variable assumes the value \\(x\\), the probability density function at \\(x\\) can assume a value greater than 1, but the probability of drawing \\(x\\) exactly is zero. This means, if you throws a dart at a board, the probability it lands at a particular point is zero — even though the probability density associated to this experiment would likely assume a value \\(&gt;0\\). While this seems strange, and it is, there is a simple way to understand what’s going on. The density must be integrated over a small region around the outcome to get an honest-to-god probability. In other words, the probability of getting \\(x\\) exactly is zero, but the probability of getting a value from \\(x-\\delta\\) to \\(x+\\delta\\) is the integral of the probability density over this region.↩︎",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability, likelihood, and statistical inference</span>"
    ]
  },
  {
    "objectID": "lectures/lec09-linear-models.html",
    "href": "lectures/lec09-linear-models.html",
    "title": "9  Introduction to linear regression models",
    "section": "",
    "text": "9.1 Lesson preamble\nlibrary(tidyverse)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to linear regression models</span>"
    ]
  },
  {
    "objectID": "lectures/lec09-linear-models.html#lesson-preamble",
    "href": "lectures/lec09-linear-models.html#lesson-preamble",
    "title": "9  Introduction to linear regression models",
    "section": "",
    "text": "9.1.1 Learning Objectives\n\nPractice fitting simple linear regression models to data\nReview theoretical foundations of linear regression\nLearn how to interpret output of regressions fit using lm() in R\nExtract fitted model parameters and confidence intervals\nUnderstand hypothesis tests conducted during regression fits\nEvaluate if the assumptions behind linear regression hold\nUnderstand and practice fitting linear models with categorical variables, multiple variables, and variable transformations",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to linear regression models</span>"
    ]
  },
  {
    "objectID": "lectures/lec09-linear-models.html#motivating-example",
    "href": "lectures/lec09-linear-models.html#motivating-example",
    "title": "9  Introduction to linear regression models",
    "section": "9.2 Motivating example",
    "text": "9.2 Motivating example\nIn the early 2000s, researchers working at the Lion Center in the Department of Ecology & Evolutionary Biology at the University of Minnesota wanted to develop a reliable way to estimate the age of male African lions non-invasively and from a distance1. As world experts in lion conservation, they wanted to advise wildlife managers in the region on strategies to preserve lion populations - which are under threat from hunting - while recognizing that allowing some “trophy” hunting to continue might be inevitable. They hypothesized that by allowing hunting only of older males, , it may be possible to preserve healthy population sizes. While lion mane size and color had previously been used informally to estimate ages, their previous work showed it was unreliable.\nIn a 2004 Nature paper, they introduced a new, more reliable metric - the density of black coloration on the lion’s nose. They digitized high-resolution images of noses from 32 male lions in the Serengeti National Park whose ages were known, and examined the relationships. The data from this paper was extracted and shared publicly for use in the textbook “The Analysis of Biological Data” by Whitlock and Schulter 2 (two faculty in the Department of Zoology at University of British Columbia)\n\n\n\n\n\na, Identification photograph of a 3-yr-old Serengeti male. b, Excised photo of nose tip. c, GIS rendering of nose colouration\n\n\n\n\nLet’s load in the data (and save it for easier access later):\n\nlion &lt;- read_csv(url(\"https://whitlockschluter3e.zoology.ubc.ca/Data/chapter17/chap17e1LionNoses.csv\"))\n\nRows: 32 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): proportionBlack, ageInYears\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nwrite_csv(lion, \"data/lion.csv\")\n\nAnd take a look at what it contains. Since the dataset is small we can simply look through all the entries\n\nlion\n\n# A tibble: 32 × 2\n   proportionBlack ageInYears\n             &lt;dbl&gt;      &lt;dbl&gt;\n 1            0.21        1.1\n 2            0.14        1.5\n 3            0.11        1.9\n 4            0.13        2.2\n 5            0.12        2.6\n 6            0.13        3.2\n 7            0.12        3.2\n 8            0.18        2.9\n 9            0.23        2.4\n10            0.22        2.1\n# ℹ 22 more rows\n\n\nLet’s jump right to making a plot of the relationship between a lions age (ageInYears) and the proportion of the nose that is black (proportionBlack)\n\nggplot(lion, aes(proportionBlack, ageInYears)) + \n    geom_point(size = 3) + \n    labs(x = \"Proportion black\", y = \"Age (years)\")\n\n\n\n\n\n\n\n\nIt looks like there’s a clear trend in the data - more black noses are associated with older lions. Let’s do two things: add a curve of the best-fit linear regression line to the plot, and calculate the correlation coefficient between the two variables\n\nggplot(lion, aes(proportionBlack, ageInYears)) + \n    geom_point(size = 3) + \n    geom_smooth(method = \"lm\", se = TRUE) + \n    labs(x = \"Proportion black\", y = \"Age (years)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNote: While geom_smooth provides a quick way to add a regression line to an existing plot, we don’t recommend using that method to actually generate your model, as it’s hard to have full control over all the options and use the output to diagnose things.\n\ncor(lion)\n\n                proportionBlack ageInYears\nproportionBlack       1.0000000  0.7898272\nageInYears            0.7898272  1.0000000\n\n\nQualitatively, both the shape of the linear regression line and the magnitude of the correlation (0.80) suggest a strong relationship between nose coloration and age. However, we need do dig deeper here to really get something useful. Is a linear relationship really the best one to fit here? Does our data satisfy the assumptions needed to do this fit using ordinary linear regression? With this limited and noisy data, how sure can we be that this correlation is statistically significant? If we decided to use nose color to predict lion age, how accurate would that be?\nTo get at these issues, we want to formally estimate a model and examine the output carefully. We’ll do this using one of the simplest yet most powerful statistical tools - linear regression. Since you covered this topic already in your introductory statistics/biostatistics class, we’ll just review things here, and focus on how you can impliment these method in R. Let’s get started!",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to linear regression models</span>"
    ]
  },
  {
    "objectID": "lectures/lec09-linear-models.html#basics-of-linear-regression",
    "href": "lectures/lec09-linear-models.html#basics-of-linear-regression",
    "title": "9  Introduction to linear regression models",
    "section": "9.3 Basics of linear regression",
    "text": "9.3 Basics of linear regression\n\n9.3.1 Linear models: why we care\nLinear models are at the heart of statistical practice in the physical, life, and social sciences! Linear regression actually refers to a family of modeling approaches that attempt to learn how the mean and/or variance of a response variable \\(\\boldsymbol{y} = (y_1,\\dots,y_n)\\) depend on (linear) combinations of variables \\(\\boldsymbol{x}_i = (x_{i1},\\dots,x_{in})\\) called predictors. In this lecture and the subsequent ones, we will discuss various forms of the linear model and assumptions placed on the data to make estimation and inference of the relationships between variables tractable. Our goal will be to become familiar with how these models work – namely, how their parameters are inferred using maximum likelihood — and practice fitting them in R.\n\n\n9.3.2 The simple linear model\nThe simple linear models describes how a response \\(Y\\) depends on a contentious explanatory variable, \\(x\\), which is fixed. One way we write the assumption of the model is\n\\[\nY = \\beta_0 + \\beta_1 x + \\epsilon\n\\]\nThis is similar to the \\(Y = m x + b\\) notation you’ve seen since high school, but more generalizable and precise. We almost always use \\(\\beta\\) to describe regression coefficients, with subscripts to indicate if it’s a coefficient for an intercept (\\(\\beta_0\\)) or another variable (\\(\\beta_i\\)). The error (or noise) term \\(\\epsilon\\) makes it explicit that we don’t ever expect the \\(Y\\) data to be described exactly the regression model.\nYou likely have heard that linear regression chooses the “best fit” line by “minimizing the sum of the squared errors” - that is, the \\(\\epsilon\\) distances the line and the observed data, something like this:\n\n\n\n\n\nVisualization of the method of minimizing the sum of the squared errors for linear regression\n\n\n\n\nThis is correct! However, there is another, equivalent way to think about what regression is doing - that matches better with how we thought about finding the “best” parameters to describe random data in prior lectures.\nWe can also say that we imagine the \\(Y\\) data is drawn from a normal distribution, where the mean of the normal distribution depends on the value of the \\(x\\) data in a linear way ($ = _0 + _1 x$)\n\\[Y \\sim \\text{Normal}(\\beta_0 + \\beta_1 x, \\sigma^2).\\]\nThe notation “~” is used in statistics to mean that a random variable is drawn from a certain distribution. Below is a visual representation of how the data generative process for \\(Y\\) is modeled. Here, we are saying that at each \\(x\\), \\(Y\\) is Normally-distributed with a mean that depends on the explanatory variable and the model parameters ($ = _0 + _1 x$)), and fixed variance \\(\\sigma^2\\) (i.e., changing \\(x\\) does not change the variance in the observed response). The variance could be due to measurement error or other sources of variation in the variable \\(Y\\) that are not explained by the paired \\(x\\) value.\n\nIn turn, this specifies what the likelihood function is for the data \\((y_1,x_1),\\dots,(y_n,x_n)\\):\n\\[ L(\\beta_0,\\beta_1,\\sigma^2|\\boldsymbol{x}_i,\\boldsymbol{y}_i) = Normal(\\mu = \\beta_0 + \\beta_1 x,\\sigma^2) =\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-(y-\\mu)^2/2\\sigma^2}.\\]\nFitting a linear regression model is actually just a maximum likelihood problem! The result of fitting a regression model is a maximum likelihood estimation of the parameters \\(\\beta_0, \\beta_1\\) and \\(\\sigma\\) . For ordinary linear regression, it turns out that you can actually find the maximum likelihood values with pen and paper math (calculus!), instead of having to search over a range of values like we’ve been doing, but for more advanced models, numerical optimization procedures are used to efficiently search for the combination of parameters that leads to the maximum likelihood.\n\n\n9.3.3 Challenge\n\nWhat assumptions are we making about the data when we fit simple linear model? What must be true of the data? Discuss.\nUse the dnorm function to calculate the likelihood of observing a value Y = 10 paired with X = 5 when \\(\\beta_0 = 0.5\\) and \\(\\beta_0 = 2\\) and \\(\\sigma = 1\\)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to linear regression models</span>"
    ]
  },
  {
    "objectID": "lectures/lec09-linear-models.html#examining-regression-output-and-checking-assumptions",
    "href": "lectures/lec09-linear-models.html#examining-regression-output-and-checking-assumptions",
    "title": "9  Introduction to linear regression models",
    "section": "9.4 Examining regression output and checking assumptions",
    "text": "9.4 Examining regression output and checking assumptions\nLet’s dig into our regression fits more! First, let’s fit the simple linear regression model using the lm() function\n\nlionFitOutput &lt;- lm(ageInYears ~ proportionBlack, data = lion)\n\nWe can get a quick look at the output of using the summary() function on it\n\nsummary(lionFitOutput)\n\n\nCall:\nlm(formula = ageInYears ~ proportionBlack, data = lion)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5449 -1.1117 -0.5285  0.9635  4.3421 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       0.8790     0.5688   1.545    0.133    \nproportionBlack  10.6471     1.5095   7.053 7.68e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.669 on 30 degrees of freedom\nMultiple R-squared:  0.6238,    Adjusted R-squared:  0.6113 \nF-statistic: 49.75 on 1 and 30 DF,  p-value: 7.677e-08\n\n\nAfter a call to lm(), R returns a lot of information. Here is what is in the printed summary() output table above:\n\nDescriptive statistics for the “residuals” \\(\\varepsilon_i = y_i - \\widehat{\\beta_0} - \\widehat{\\beta_1} x_i\\), which tell us about how much variability there is in the data relative to the linear model specified and fitted.\nThe regression coefficients minimizing the likelihood of the data and \\(95\\%\\) confidence intervals for each. The CIs are expressed as standard errors, since the estimators have an approximate Normal distribution.\nA suite of test statistics! The \\(t\\) statistics and their \\(p\\) values are associated to the test \\(H_0: \\beta_i = 0\\) vs \\(H_1: \\beta_i \\neq 0\\). Significance codes specify the level \\(\\alpha\\) at which we have evidence to reject the null hypothesis for each coefficient.\nMeasures of goodness-of-fit: the multiple \\(R^2\\) and the adjusted \\(R^2\\). These explain the proportion of variance that are explained by the model. The latter measures the proportion of variance explained by the linear model upon adjusting for sample size and \\(\\#\\) of predictors.\n\nWe can access any of these quantities in R as variables. If we first assign a variable to the output, then use str() or View() to examine it’s structure, and then extract a particular variable. You can find a full description of the output here.\n\nlionFitSummary &lt;- summary(lionFitOutput)\nstr(lionFitSummary)\n\nList of 11\n $ call         : language lm(formula = ageInYears ~ proportionBlack, data = lion)\n $ terms        :Classes 'terms', 'formula'  language ageInYears ~ proportionBlack\n  .. ..- attr(*, \"variables\")= language list(ageInYears, proportionBlack)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"ageInYears\" \"proportionBlack\"\n  .. .. .. ..$ : chr \"proportionBlack\"\n  .. ..- attr(*, \"term.labels\")= chr \"proportionBlack\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(ageInYears, proportionBlack)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"ageInYears\" \"proportionBlack\"\n $ residuals    : Named num [1:32] -2.0149 -0.8696 -0.1502 -0.0631 0.4433 ...\n  ..- attr(*, \"names\")= chr [1:32] \"1\" \"2\" \"3\" \"4\" ...\n $ coefficients : num [1:2, 1:4] 0.879 10.647 0.569 1.51 1.545 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"(Intercept)\" \"proportionBlack\"\n  .. ..$ : chr [1:4] \"Estimate\" \"Std. Error\" \"t value\" \"Pr(&gt;|t|)\"\n $ aliased      : Named logi [1:2] FALSE FALSE\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"proportionBlack\"\n $ sigma        : num 1.67\n $ df           : int [1:3] 2 30 2\n $ r.squared    : num 0.624\n $ adj.r.squared: num 0.611\n $ fstatistic   : Named num [1:3] 49.8 1 30\n  ..- attr(*, \"names\")= chr [1:3] \"value\" \"numdf\" \"dendf\"\n $ cov.unscaled : num [1:2, 1:2] 0.116 -0.264 -0.264 0.818\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"(Intercept)\" \"proportionBlack\"\n  .. ..$ : chr [1:2] \"(Intercept)\" \"proportionBlack\"\n - attr(*, \"class\")= chr \"summary.lm\"\n\n\n\nprint(lionFitSummary$coefficients)\n\n                  Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept)      0.8790062  0.5688171 1.545323 1.327530e-01\nproportionBlack 10.6471194  1.5095005 7.053406 7.677005e-08\n\ncat(\"\\n\")\nprint(lionFitSummary$coefficients[\"proportionBlack\",\"Estimate\"])\n\n[1] 10.64712\n\n\nEven more information is contained in the non-summarized output, which you can preview using str(lionRegression) or View(lionRegression) or read more about here.\nLet’s interpret the output here!\n\n9.4.1 Challenge\n\nWhat is the best estimate for each parameter, including 95% confidence intervals? Can you write the equation of the best fit curve?\nIs there significant evidence that age varies with the proportion of the nose that is black? Describe the results of the hypothesis test in careful statistical language.\n\n\n\n9.4.2 Diagnostic plots\nWe can do a quick check to see if the assumptions of the regression model are valid, by calling the function plot() on the output of the model fitting:\n\nplot(lionFitOutput) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore on how to read diagnostic plots can be found here and here.\n\n\n9.4.3 Making predictions\nRemember that one of the goals of the study was to see if we could predict a lion’s age based on the nose coloration. How good is our model at this task?\nThere are lots of functions that can be applied to the output of lm(), and predict() is one of them we can use now (Note: for documentation, search predict.lm). For example, here is how to predict mean lion age corresponding to a value of 0.50 of proportion black in the nose.\n\nyhat &lt;- predict(lionFitOutput, data.frame(proportionBlack = 0.50), se.fit = TRUE, interval = \"prediction\", level = 0.95)\ndata.frame(yhat)\n\n   fit.fit  fit.lwr fit.upr    se.fit df residual.scale\n1 6.202566 2.698512 9.70662 0.3988321 30       1.668764\n\n\nIn the output, predicted age is indicated by fit , the standard error of the predicted mean age is se.fit, and the upper and lower 95% prediction intervals are indicated by fit.lwr and fit.upr.\nWe can repeat this prediction for all ages and visualize the uncertainty we should expect in our predictions.\n\nlion_predict &lt;- data.frame(lion, predict(lionFitOutput, interval = \"prediction\"))\n\nWarning in predict.lm(lionFitOutput, interval = \"prediction\"): predictions on current data refer to _future_ responses\n\nggplot(lion_predict, aes(proportionBlack, ageInYears)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill='prediction'), \n        fill = \"black\", alpha = 0.2) +\n    geom_line(aes(y = fit), color = \"black\") + \n    #geom_smooth(method = \"lm\", se = FALSE, col = \"blue\") +\n    geom_point(size = 3, color = \"red\") + \n    labs(x = \"Proportion black\", y = \"Age (years)\") \n\n\n\n\n\n\n\n\nNote that prediction interval is wider than the confidence bands because predicting the age of a new individual lion age from the proportion black in its nose has higher uncertainty than predicting the mean age of all lions having that proportion of black on their noses.\n\n\n9.4.4 Challenge\nThe authors simulations suggest that hunting only males 6 years of age and older would have minimal impact on the population sustainability. What is the range of nose colourations for which an age prediction of 6 years is within the 95% prediction interval. For what ages do these colourations correspond to the average levels?\n\n\n9.4.5 Transforming variables\nIf you suspect the raw data are not normally distributed, but transformed versions of the data are, you can replace \\(Y\\) with \\(f(Y)\\) where \\(f(\\cdot)\\) is the transformation and proceed with the analysis. The only thing to keep in mind is how to interpret the regression coefficients.\nIf \\(\\beta_1 = 0.1\\) when regression \\(f(Y)\\) on \\(x_1,\\dots,x_p\\), then that means _per unit change in \\(x_1\\), all else constant, \\(f(Y)\\) increases, on average, by unit 0.1. This does NOT mean that \\(Y\\), on the raw scale, increases by that amount.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to linear regression models</span>"
    ]
  },
  {
    "objectID": "lectures/lec09-linear-models.html#regressing-on-categorical-variables",
    "href": "lectures/lec09-linear-models.html#regressing-on-categorical-variables",
    "title": "9  Introduction to linear regression models",
    "section": "9.5 Regressing on categorical variables",
    "text": "9.5 Regressing on categorical variables\nIt turns out that we can use the exact same framework to regress \\(Y\\) on discrete, as well as continuous, predictor variables.\nTo see how this is done, and how to interpret the resulting regression coefficients, suppose a predictor has \\(K\\) levels. To estimate the effect of one of these levels on the response variable (say, of sampling individuals in a particular country), one of the levels of the discrete variable is treated as a “reference” and effects are estimated for all other levels. That is, we define the model in terms of a baseline and to interpret the regression coefficients relative to this baseline. This involves coding “dummy variables” for all but one the \\(K\\) values the predictor can take assume and estimating regression coefficients for each of these variables.\nThe regression coefficient for a “dummy variable” (associated to one of the values a categorical predictor) measures the expected change in the response, all else constant, if we were to change from the baseline to the level of interest.\nThis is most straightforward for binary outcomes, but works for outcomes with many levels as well.\n\n9.5.1 Example: In mammals, is there an effect of sex on body size?\nTo try this out, we’ll go back to out dataset on sexual dimorphism in mammals. After some data “cleaning” we did in Assignment 2, we’ll do a very crude test of the hypothesis that males and females differ in size. In particular, we will determine the quantitative effect of sex is on the size of a typical individual, regardless of their species, sampling effort, etc. Much more complicated models can be constructed, but it turns out these models give rise to qualitatively-similar conclusions.\n\nmammal_sizes &lt;- read_csv(\"data/SSDinMammals.csv\")\n\nRows: 691 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): Order, Family, Species, Scientific_Name, Comments, Source\ndbl (12): massM, SDmassM, massF, SDmassF, lengthM, SDlengthM, lengthF, SDlen...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(mammal_sizes)\n\n# A tibble: 6 × 18\n  Order       Family Species Scientific_Name massM SDmassM massF SDmassF lengthM\n  &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Afrosorici… Chrys… Hotten… Amblysomus hot…  80.6    1.24  66      8.64      NA\n2 Afrosorici… Chrys… Namib … Eremitalpa gra…  28      6.7   23.1    3.6       NA\n3 Afrosorici… Tenre… Lesser… Echinops telfa… 102.    19.3   99.9   17.8       NA\n4 Afrosorici… Tenre… Large-… Geogale aurita    7.3    1.02   7      1.34      NA\n5 Afrosorici… Tenre… Highla… Hemicentetes n… 111     15.6   98      6.56      NA\n6 Afrosorici… Tenre… Lowlan… Hemicentetes s… 110.    16.7  108.    16.0       NA\n# ℹ 9 more variables: SDlengthM &lt;dbl&gt;, lengthF &lt;dbl&gt;, SDlengthF &lt;dbl&gt;,\n#   n_M &lt;dbl&gt;, n_F &lt;dbl&gt;, n_Mlength &lt;dbl&gt;, n_Flength &lt;dbl&gt;, Comments &lt;chr&gt;,\n#   Source &lt;chr&gt;\n\n\nRemember that the unit of measurement here is a species. To pre-process the data, we’ll do a few steps\n\nFilter only for columns we care about - those describing the taxonomy of the species, and those tracking mean length, mean weight, and number of individuals of that species observed\nOnly includes species were at least 10 males and 10 females were observed\nGet rid of any duplicated data\nConvert the data from wide to long format, so that there is only one column each for length, weight, and n and another column for the sex (male or female)\n\n\nmammal_long &lt;- mammal_sizes %&gt;% \n  select(\"Order\",\"Family\",\"Species\",\"Scientific_Name\", \"massM\", \"massF\",\"lengthM\",\"lengthF\",\"n_M\",\"n_F\") %&gt;%\n  filter(n_M &gt; 10 & n_F &gt; 10)\n\nmammal_long_mass &lt;- mammal_long %&gt;%\n  select(-lengthM,-lengthF, -n_M, -n_F) %&gt;%\n  pivot_longer(c(massM, massF), names_to = \"sex\", values_to = \"mass\",\n               names_pattern = \"mass(.)\") %&gt;% \n  group_by(Scientific_Name, sex) %&gt;%\n  distinct(Scientific_Name, sex, .keep_all = TRUE)\n  \nmammal_long_length &lt;- mammal_long %&gt;% \n  select(-massM,-massF, -n_M, -n_F) %&gt;%\n  pivot_longer(c(lengthM, lengthF), names_to = \"sex\", values_to = \"length\",names_pattern = \"length(.)\") %&gt;% \n  group_by(Scientific_Name, sex) %&gt;%\n  distinct(Scientific_Name, sex, .keep_all = TRUE) \n  \nmammal_long_number &lt;- mammal_long %&gt;%  \n  select(-massM,-massF, -lengthM, -lengthF) %&gt;%\n  pivot_longer(c(n_M, n_F), names_to = \"sex\", values_to = \"number\",names_pattern = \"n_(.)\") %&gt;% \n  group_by(Scientific_Name, sex) %&gt;%\n  distinct(Scientific_Name, sex, .keep_all = TRUE)\n\nmammal_long &lt;- full_join(mammal_long_mass, mammal_long_length) %&gt;%\n  full_join(mammal_long_number)\n\nJoining with `by = join_by(Order, Family, Species, Scientific_Name, sex)`\nJoining with `by = join_by(Order, Family, Species, Scientific_Name, sex)`\n\nrm(mammal_long_mass,mammal_long_length,mammal_long_number) # clear, don't need\n\nhead(mammal_long)\n\n# A tibble: 6 × 8\n# Groups:   Scientific_Name, sex [6]\n  Order        Family          Species Scientific_Name sex    mass length number\n  &lt;chr&gt;        &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Afrosoricida Chrysochloridae Hotten… Amblysomus hot… M      80.6     NA    155\n2 Afrosoricida Chrysochloridae Hotten… Amblysomus hot… F      66       NA    148\n3 Afrosoricida Tenrecidae      Lesser… Echinops telfa… M     102.      NA     24\n4 Afrosoricida Tenrecidae      Lesser… Echinops telfa… F      99.9     NA     15\n5 Afrosoricida Tenrecidae      Large-… Geogale aurita  M       7.3     NA     17\n6 Afrosoricida Tenrecidae      Large-… Geogale aurita  F       7       NA     15\n\n\nNow let’s do the regression!\n\nmammal_size_sex_fit &lt;- lm(log(mass) ~ sex, data = mammal_long)\nsummary(mammal_size_sex_fit)\n\n\nCall:\nlm(formula = log(mass) ~ sex, data = mammal_long)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0918 -2.0342 -0.9029  1.9052 11.2435 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.11383    0.13805  37.042   &lt;2e-16 ***\nsexM         0.07658    0.19524   0.392    0.695    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.73 on 780 degrees of freedom\nMultiple R-squared:  0.0001972, Adjusted R-squared:  -0.001085 \nF-statistic: 0.1539 on 1 and 780 DF,  p-value: 0.695\n\n\nRemarkably, no effect! Males are not larger than females. The \\(p\\) value is not only \\(&gt;\\alpha = 0.05\\), the size of the estimated effect is quite small. This is pretty suprising, given the large number of papers that claim, despite the lack of evidence, that there are stark differences in size between male and female individuals.\nIt’s still important to visualize our effects, but in this case that’s a little more difficult that just plotting a line. Instead, let’s use a\n\nmammal_long %&gt;% ggplot(aes(x = sex, y = log(mass))) + \n  geom_violin() +\n  geom_jitter(alpha = 0.5, width = 0.1) + \n  geom_smooth(aes(group = 1), method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAlthough it seems weird to do a linear regression on a categorical x value, it turn out it’s totally legit, and is actually exactly equivalent to do a standard test for differences in means using the famous Student’s t-test.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to linear regression models</span>"
    ]
  },
  {
    "objectID": "lectures/lec09-linear-models.html#multivariable-regression",
    "href": "lectures/lec09-linear-models.html#multivariable-regression",
    "title": "9  Introduction to linear regression models",
    "section": "9.6 Multivariable regression",
    "text": "9.6 Multivariable regression\n\n9.6.1 Including multiple predictor variables\nIt is straightforward to extend the simple regression model to include multiple covariates/predictors. Suppose, for each realization of \\(Y\\), we have associated measurements \\(x_1,\\dots,x_p\\). We can model how \\(Y\\) changes with these predictors as follows:\n\\[Y \\sim \\text{Normal}(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p, \\sigma^2).\\] The likelihood that arises from data \\((y_i,x_{1i},\\dots,x_{pi})\\) where \\(i=1,\\dots,n\\) is\n\\[L(\\beta_0,\\dots,\\beta_p,\\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-(y_i-\\beta_0-\\beta_1 x_{1i}-\\cdots-\\beta_p x_{pi})^2/2\\sigma^2}.\\]\nAgain, assumptions of this model include\n\nThe data, i.e., observations of the response \\(Y\\), are independent (and normally) distributed for a given set of predictor values \\({x_i}\\)\nThe mean response is a linear function of the predictors.\nThe error variance \\(\\sigma^2\\) is constant and, thus, does not depend on the predictors.\nThe parameters \\(\\beta_0,\\dots,\\beta_p\\) (called regression coefficients or effect sizes) are non-random.\nThe predictors are known with certainty.\n\nMaximum likelihood estimation gives rise to point and interval estimates for \\(\\beta_1,\\dots,\\beta_p,\\sigma^2\\).3\nWe must be careful how we interpret the parameters of any statistic model after we fit the model to data. This is definitely true of the regression coefficients \\(\\beta_1,\\dots,\\beta_p\\). The estimates are not the same as the “true” values the parameters assume; they are our best guess of the “true” regression coefficients, given the (limited, imperfect, incomplete, noisy) data that we have. Moreover, \\(\\beta_j\\) must be understood as the amount the average value of the response variable changes when the predictor \\(x_j\\) increases by one unit, assuming all else is constant.\nThat is, \\(\\beta_j\\) is a measure of the sensitivity of \\(E(Y)\\) to \\(x_j\\) — how much does \\(Y\\) change, on average, if we increase \\(x_j\\) by one unit.\nFinally, linear models can accommodate non-linear interactions between explanatory variables. If \\(x_2 = x_1^2\\), one can estimate an effect for \\(x_2\\) (and do the same for all higher order terms for \\(x_1\\) and the other covariates). This effect sizes that are estimated have to be interpreted carefully but does not pose a difficulty to forming the likelihood nor maximizing it.\n\n\n9.6.2 Including interaction terms\nOther interactions can also be modeled. For example, suppose we were interested in the combined effects of changing salinity and temperature on the number of species in an ecosystem because we suspect that changing salinity has little effect on the effect on the number of species if temperature in high (e.g., there are no species left). Then, letting \\(x_1\\) be salinity and \\(x_2\\) temperature, their interaction would be included as a covariate \\(x_3 = x_1 x_2\\) and the associated effect estimated from the data.\nWhen there are interactions, coefficients are interpreted as follows. We can say that \\(\\beta_1\\) is the expected change in the response if we increase \\(x_1\\) by one unit and set all covariates with which it interacts equal to zero. The effect size for the interaction between \\(x_1\\) and \\(x_2\\) measures the change in the the expected response if \\(x_1\\) and \\(x_2\\) were to BOTH increase by one unit that is on top of or in addition to the change due each variable in isolation.\n\n\n9.6.3 Example: Does sexual dimorphism vary by order?\nFirst, let’s try to extend our examination of the impact of sex on body mass by trying to see if there is a different relationship depending on the Order the species belongs to. To do this, we’ll regress on both sex and Order, and their interaction.\nFirst, we’ll filter for Orders that have at least 10 species with both male and female data, since we expect it to be difficult to identify effects otherwise. Note that we don’t need to do this : regression analyses already takes into account uncertainty due to low samples and be very unlikely to reject a null hypothesis - but it doint so will simplify our output and interpretation\n\nmammal_long_order10 &lt;- mammal_long %&gt;%\n  group_by(Order,sex) %&gt;%\n  mutate(n_Order = n()) %&gt;%\n  filter(n_Order &gt; 10) %&gt;%\n  select(-n_Order)\n\nmammal_size_sex_order_fit &lt;- lm(log(mass) ~ sex+Order+sex*Order, data = mammal_long_order10)\nsummary(mammal_size_sex_order_fit)\n\n\nCall:\nlm(formula = log(mass) ~ sex + Order + sex * Order, data = mammal_long_order10)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.2033 -0.7920 -0.1814  0.7113  5.7114 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                3.14172    0.36683   8.565  &lt; 2e-16 ***\nsexM                      -0.01685    0.51877  -0.032  0.97410    \nOrderArtiodactyla          7.50428    0.44927  16.703  &lt; 2e-16 ***\nOrderCarnivora             5.11339    0.48730  10.493  &lt; 2e-16 ***\nOrderChiroptera           -0.32412    0.39163  -0.828  0.40815    \nOrderDidelphimorphia       1.56146    0.54184   2.882  0.00407 ** \nOrderDiprotodontia         4.73150    0.52947   8.936  &lt; 2e-16 ***\nOrderEulipotyphla          0.04496    0.47120   0.095  0.92401    \nOrderPrimates              4.29798    0.41263  10.416  &lt; 2e-16 ***\nOrderRodentia              1.12821    0.38529   2.928  0.00352 ** \nsexM:OrderArtiodactyla     0.20571    0.63536   0.324  0.74621    \nsexM:OrderCarnivora        0.31179    0.68915   0.452  0.65110    \nsexM:OrderChiroptera      -0.03183    0.55384  -0.057  0.95418    \nsexM:OrderDidelphimorphia  0.18088    0.76627   0.236  0.81346    \nsexM:OrderDiprotodontia    0.06423    0.74878   0.086  0.93167    \nsexM:OrderEulipotyphla     0.12244    0.66637   0.184  0.85427    \nsexM:OrderPrimates         0.18522    0.58354   0.317  0.75103    \nsexM:OrderRodentia         0.10076    0.54488   0.185  0.85335    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.323 on 716 degrees of freedom\nMultiple R-squared:  0.7693,    Adjusted R-squared:  0.7638 \nF-statistic: 140.4 on 17 and 716 DF,  p-value: &lt; 2.2e-16\n\n\nNotice that in the Order + Sex interaction model, there are significant effects of the Order on body mass but the effect of sex, even when order-specific, on mass is not significant. That is, the associated regression coefficients have confidence intervals overlapping zero, so that we fail to reject the null hypothesis \\(H_0: \\beta_j = 0\\).\n\n\n9.6.4 Example: Do body dimensions vary by sex or Order?\nIn addition to body mass, our data set also contains body length. Maybe something about the distribution of body mass - i.e., the relationship between body mass and length - differs by sex. Let’s regress body size on length - but first, let’s make a plot. We’ll use the same filtered data we created above\n\nmammal_long_order10 %&gt;%\n  ggplot(aes(x = length, y = mass)) + \n  #ggplot(aes(x = log(length), y = log(mass))) + \n  geom_point()\n\nWarning: Removed 400 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nLength and weight both vary over enormous scales (think about comparing a mouse to an elephant) - so this is the perfect situation to do a transformation. Let’s try log\n\nmammal_long_order10 %&gt;%\n  ggplot(aes(x = log10(length), y = log10(mass))) + \n  geom_point()\n\nWarning: Removed 400 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNow let’s fit a regression\n\nmass_vs_length_fit &lt;- lm(log10(mass) ~ log10(length), data = mammal_long_order10)\nsummary(mass_vs_length_fit)\n\n\nCall:\nlm(formula = log10(mass) ~ log10(length), data = mammal_long_order10)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0088 -0.2233 -0.0175  0.2498  1.6188 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -0.13627    0.05849   -2.33   0.0204 *  \nlog10(length)  1.86194    0.05197   35.83   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4505 on 332 degrees of freedom\n  (400 observations deleted due to missingness)\nMultiple R-squared:  0.7945,    Adjusted R-squared:  0.7939 \nF-statistic:  1283 on 1 and 332 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n9.6.5 Challenge\n\nHow would you interpret the coefficient here? Remember that the data has been log transformed\nList at least 2 ways this data violates the assumptions of simple linear regression with this current model fit\nExamine whether including sex or Order would explain this data better. Before fitting, use a visualization to motivate your choice, and after fitting, interpret the results",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to linear regression models</span>"
    ]
  },
  {
    "objectID": "lectures/lec09-linear-models.html#footnotes",
    "href": "lectures/lec09-linear-models.html#footnotes",
    "title": "9  Introduction to linear regression models",
    "section": "",
    "text": "Whitman, K., Starfield, A., Quadling, H. & Packer, C. Sustainable trophy hunting of African lions. Nature 428, 175–178 (2004). https://doi.org/10.1038/nature02395↩︎\nWhitlock, Michael, & Dolph Schluter. The analysis of biological data. Third Edition. Macmillen Publishers. (2020) [https://www.macmillanlearning.com/college/us/product/The-Analysis-of-Biological-Data/p/131922623X]↩︎\nIt is worth noting that what is often used in theory and practice is a matrix formulation of the model we have written down: \\[\\boldsymbol{y} = \\begin{bmatrix}\ny_{1} \\\\\ny_{2} \\\\\n\\vdots \\\\\ny_{n}\n\\end{bmatrix} = \\begin{bmatrix}\n1 & x_{11} & x_{21} & \\cdots & x_{p1} \\\\\n1 & x_{12} & x_{22} & \\cdots & x_{p2} \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n1 & x_{1n} & x_{2n} & \\cdots & x_{pn}\n\\end{bmatrix} \\begin{bmatrix}\n\\beta_{1} \\\\\n\\beta_{2} \\\\\n\\vdots \\\\\n\\beta_{p}\n\\end{bmatrix} + \\begin{bmatrix}\n\\varepsilon_{1} \\\\\n\\varepsilon_{2} \\\\\n\\vdots \\\\\n\\varepsilon_{n}\n\\end{bmatrix} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}.\\] Here, \\(\\boldsymbol{y} = (y_1,\\dots,y_n)'\\) is a vector of measurements for the response, \\(\\boldsymbol{x_i} = (x_{i1},\\dots,x_{in})'\\) is a vector of measurements for the \\(k\\)th predictor, and \\(\\boldsymbol{\\varepsilon} = (\\varepsilon_1,\\dots,\\varepsilon_n)'\\) is a vector of measurement errors. The \\('\\) symbol denotes transposition, the operation where the rows and columns of a vector or matrix are interchanged. In R, the function t() can be used to transpose a vector or matrix.↩︎",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to linear regression models</span>"
    ]
  },
  {
    "objectID": "assignments/assignment-01.html",
    "href": "assignments/assignment-01.html",
    "title": "Assignment 1: Basic R (8 marks)",
    "section": "",
    "text": "1. Using R Markdown notebooks (1 mark)\nDownload the .Rmd file here.\n*To submit this assignment, upload the full .Rmd document, including the original questions, your code, and the output. Submit your assignment as a knitted .pdf (Question 1 walks you through this part, and more details are in course notes).",
    "crumbs": [
      "Assignments",
      "Assignment 1: Basic R (8 marks)"
    ]
  },
  {
    "objectID": "assignments/assignment-01.html#using-r-markdown-notebooks-1-mark",
    "href": "assignments/assignment-01.html#using-r-markdown-notebooks-1-mark",
    "title": "Assignment 1: Basic R (8 marks)",
    "section": "",
    "text": "In RStudio, create a new R Markdown notebook. The file name should end in .Rmd.\nRead the guidelines provided in example text in that notebook\nUse the knit button to run the example notebook and generate a PDF file. You might be asked to install some R packages if it’s the first time you’ve done this - go ahead an let them install\nNext, download the assignment .Rmd file, and either a) open it in RStudio (ignoring the new file you created above) or b) copy the contents into example notebook you already have opened.\nInsert a code chunk below these bullets, write a simple mathematical expression to evaluate (e.g. 1+1), and then run the chunk (green arrow play button in top right of chunk)\nNow knit this notebook again, before proceeding with the rest of the assignment",
    "crumbs": [
      "Assignments",
      "Assignment 1: Basic R (8 marks)"
    ]
  },
  {
    "objectID": "assignments/assignment-01.html#variable-assignment-and-basic-math-1-mark",
    "href": "assignments/assignment-01.html#variable-assignment-and-basic-math-1-mark",
    "title": "Assignment 1: Basic R (8 marks)",
    "section": "2. Variable assignment and basic math (1 mark)",
    "text": "2. Variable assignment and basic math (1 mark)\n\nAssign the value 5 to the variable/object a. Display a. (0.25 marks)\n\n\nAssign the result of 10/3 to the variable b. Display b. (0.25 marks)\n\n\nWrite a function that uses Pythagorean theorem to calculate the length of the longest side (hypotenuse) of a right angle triangle given the lengths of the two other sides. Use it to assign the hypotenuse of a triangle with sides a and b to the variable hypot. Display hypot. (0.5 marks)",
    "crumbs": [
      "Assignments",
      "Assignment 1: Basic R (8 marks)"
    ]
  },
  {
    "objectID": "assignments/assignment-01.html#vectors-3-marks",
    "href": "assignments/assignment-01.html#vectors-3-marks",
    "title": "Assignment 1: Basic R (8 marks)",
    "section": "3. Vectors (3 marks)",
    "text": "3. Vectors (3 marks)\n\nCreate a vector v with all integers 0-30, and a vector w with every third integer in the same range. (0.25 marks)\n\n\nWhat is the difference in lengths of the vectors v and w? (0.25 marks)\n\n\nCreate a new vector, v_square, with the square of elements at indices 3, 6, 7, 10, 15, 22, 23, 24, and 30 from the variable v. Hint: Use indexing rather than a for loop. (0.25 marks)\n\n\nCalculate the mean and median of the first five values from v_square. (0.25 marks)\n\n\nCreate a boolean vector v_bool, indicating which vector v elements are bigger than 20. How many values are over 20? Hint: In R, TRUE = 1, and FALSE = 0, so you can use simple arithmetic to find this out. (0.5 marks)\n\n\nCreate a new vector of character strings v_bool_in_words, that contains the word big if the corresponding element of v is bigger than 20 and small if not (0.5 marks)\n\n\nWrite a function that calculates the median of the any values in a numeric vector greater than “n”. Test this function with the v and v_square vectors and n = 10. (1 marks)",
    "crumbs": [
      "Assignments",
      "Assignment 1: Basic R (8 marks)"
    ]
  },
  {
    "objectID": "assignments/assignment-01.html#loops-and-conditional-statements-3-marks",
    "href": "assignments/assignment-01.html#loops-and-conditional-statements-3-marks",
    "title": "Assignment 1: Basic R (8 marks)",
    "section": "4. Loops and conditional statements (3 marks)",
    "text": "4. Loops and conditional statements (3 marks)\nWe’re going to use some of the simple R commands we’ve used to create our first biological simulation! Imagine you are managing the cultivation of some type of organism (this could be anything from a population of cells growing in a petri dish, a crop of plants, or a colony of animals). We’ll call this organism the “crop” and assume the population is divided up into separate “plots”. The population in each plot grows over time, and when it reaches a specified size you will “harvest” it - removing most of the population - and then replant a small amount. We are going to track the population size of each plot over time.\n\nAssume you start with 10 plots with population sizes 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. Create a vector of length 10 that contains the size of each plot. (0.25 marks)\n\n\nIf the population size is greater than 5, harvest occurs. Using a single line of code and without defining any new variables, count how many plots will be harvested initially (0.25 marks)\n\n\nWrite a for-loop to go through the initial plots, find the ones that will be harvested, and set their population sizes to zero after harvest. Store the new population sizes in the vector v_post_harvest (0.5 mark)\n\n\nDo the same calculation as above, but use vector subsetting and boolean expressions to avoid having to do a for-loop (see lectures notes for reminders) (0.5 marks)\n\n\nEach year, the population size doubles due to growth. Update your for-loop to go through the initial plots and report their population sizes after 1 full year including harvesting and growth. Assume that after harvesting the population is “re-seeded” such that after a year it will have size 1. Make sure to make a not of any assumptions you are making. Store the new population sizes in the vector v_post_growth_harvest (1 mark)\n\n\nIs there a way to avoid using a for-loop for this last calculation? (0.5 marks)",
    "crumbs": [
      "Assignments",
      "Assignment 1: Basic R (8 marks)"
    ]
  },
  {
    "objectID": "assignments/assignment-02.html",
    "href": "assignments/assignment-02.html",
    "title": "Assignment 2: Manipulating and plotting data (8 marks)",
    "section": "",
    "text": "1. Analyze body temperature timeseries of beaver Castor canadensis (2.5 marks)\nDownload the .Rmd file here.\nTo submit this assignment, upload the full document, including the original questions, your code, and the output. Submit your assignment as a knitted .pdf. Please ensure the text on your .pdf does not continue past the end of the page.\na. There are many built-in data frames in R, which you can find more details about online. What are the column names of the built-in dataframe beaver1? How many observations (rows) and variables (columns) are there? (0.25 marks)\nHint: You don’t have to download, import, or `read’ them like external datasets - you can just work with them as if they are in your environment already. You can learn more about a built-in dataset by searching it in the Help panel (bottom left in RStudio) or by typing ? and it’s name in the command line. Make sure to load the tidyverse package before you use any readr, dplyr, or ggplot functions in your notebook.\nb. Display both the first 6 and last 6 rows of this data frame. Show how to do so with both indexing as well as specialized functions. (0.5 marks)\nc. Use the view() function to inspect the dataset visually in RStudio. Exactly how long was this beaver measured for? You can state this in words or numerically (0.25 points)\nd. What is the minimum, mean, and maximum body temperature for beavers inside and outside of the retreat? (0.5 marks)\nHint: An indicator variable is one that has a value of 1 when the criteria (i.e., “activity outside the retreat”) is true, and a value of zero when it is not.\ne. Make a plot of body temperature vs time of day, with points coloured differently for activity inside vs outside the retreat. (1 point)\nHint: You might want to create a new transformed time variable for this",
    "crumbs": [
      "Assignments",
      "Assignment 2: Manipulating and plotting data (8 marks)"
    ]
  },
  {
    "objectID": "assignments/assignment-02.html#analyze-mammal-size-data-5.5-marks",
    "href": "assignments/assignment-02.html#analyze-mammal-size-data-5.5-marks",
    "title": "Assignment 2: Manipulating and plotting data (8 marks)",
    "section": "2. Analyze mammal size data (5.5 marks)",
    "text": "2. Analyze mammal size data (5.5 marks)\nWe will be working with a data set compiled to understand sexual dimorphism in size across mammals. You can download the file from Dryad (note: you might have to do this manually, as the site can block R imports) or from the course website). The original study1 is quite interesting! The Dryad link includes a description of the study variables.\na. Read throught the original study abstract and introduction. In one sentence, what was the hypothesis the authors were trying to test with this study? (0.25 pts)\nb. Download the file to your computer and read it into a variable called mammal_sizes and provide a preview of data. (0.25 mark)\nHint: Make sure you tell read_csv the correct file path of your data based on where you put it. You can either specify the full directory path, or where it is relative to which directly R currently operating out of for this notebook, which you can get with the getwd() command and set with the setwd() command. (Or by navigating to the Files panel in RStudio, selecting the Settings menu (gear icon), and choosing “Set As Working Directory” or “Go To Working Directory”).\nc. Pull out the 4 columns containing information on the species name along with “massM”, “massF”, “n_M” and “n_F” and call this dataframe mammal_weights. Calculate the average weight by sex for each Order of mammal, including only species where at least 10 males and 10 females were measured, and only Orders that include at least 10 species (1 mark)\nd. Calculate the proportional mass difference ([massM - massF] / massM) for all the rows in the original dataset. Again including including only species where at least 10 males and 10 females were measured, and only Orders that include at least 10 species, calculate the average proportional difference in mass (1 mark).\ne. Repeat 2c and 2d but for length, and average by Family instead of by Order (1 mark)\nf. Make a scatter plot of the average female body mass versus the average male body mass for each species. Color the points by Order. Add in a solid line showing the case where male and female are equal size (1 mark)\nHints: You may need to transform the x and y data to make the plot easier to read, since the mass values span multiple orders of magnitude. Search the Help documentation for the geom_abline function to learn about adding lines to plots.\ng. Make a scatter plot of the proportional difference in mass between males and females of each species versus the Order, and color by Order as well (1 mark)\nNote: You can rotate x axis labels by adding the following command at the end of the set of ggplot commands : + theme(axis.text.x = element_text(angle = 45,vjust=1, hjust=1))",
    "crumbs": [
      "Assignments",
      "Assignment 2: Manipulating and plotting data (8 marks)"
    ]
  },
  {
    "objectID": "assignments/assignment-02.html#footnotes",
    "href": "assignments/assignment-02.html#footnotes",
    "title": "Assignment 2: Manipulating and plotting data (8 marks)",
    "section": "",
    "text": "Tombak, K.J., Hex, S.B.S.W. & Rubenstein, D.I. New estimates indicate that males are not larger than females in most mammal species. Nat Commun 15, 1872 (2024). https://doi.org/10.1038/s41467-024-45739-5↩︎",
    "crumbs": [
      "Assignments",
      "Assignment 2: Manipulating and plotting data (8 marks)"
    ]
  },
  {
    "objectID": "assignments/assignment-03.html",
    "href": "assignments/assignment-03.html",
    "title": "Assignment 3: Advanced data wrangling and visualization (8 marks)",
    "section": "",
    "text": "1. Read in and pre-process plant biomass data (1.75 marks)\nDownload the .Rmd file here.\nTo submit this assignment, upload the full document, including the original questions, your code, and the output. Submit your assignment as a knitted .pdf. Please ensure the text on your .pdf does not continue past the end of the page.\nReview: Before the assignment, review the data manipulation and plotting functions learned in class, such as read_csv, filter, select, mutate, arrange, head, group_by, summarize, tally, pivot_longer, pivot_wider, ggplot, geom_line, geom_point, labs, theme\nYou will apply your data wrangling skills on the yearly change in biomass of plants in the beautiful Abisko national park in northern Sweden. We have preprocessed this data and made it available as a csv file via this link. You can find the original data on Dryad, and the full-text of the original study1 in this pdf. Reading through the study abstract will increase your understanding for working with the data.\na. Import the data directly into R from the provided URL, assign it to a variable called plant_biomass, and display the first six rows. (0.25 mark)\nb. Convert the Latin column names into their common English names: lingonberry, bilberry, bog bilberry, dwarf birch, crowberry, and wavy hair grass. After this, display all column names. (0.25 marks)\nHint: Read the documentation on the dplyr function rename. Search online to find out which Latin and English names pair up.\nc. This is a wide data frame (species make up the column names). A long format is easier to analyze, so gather the species names into one column (species) and the measurement values into another column (biomass). Keep all other columns. Assign it to the variable plant_biomass_long, and show a preview of this new data frame (0.5 marks)\nd. Describe how the dimensions of the data frame have change between plant_biomass and plant_biomass_long . In this example, which format is more efficient in terms of the memory used to store the data? For the less efficient format, what is one potential benefit of this format that might make it worth the extra storage space required? (0.25 marks)\ne. Recreate the wide data frame (species names as columns again) by pivoting it from your plant_biomass_long data frame. Don’t overwrite your original plant_biomass variable! You don’t need to save this re-widened data frame, but if you do, give it a different name (0.5 marks)",
    "crumbs": [
      "Assignments",
      "Assignment 3: Advanced data wrangling and visualization (8 marks)"
    ]
  },
  {
    "objectID": "assignments/assignment-03.html#wrangling-plant-biomass-with-dplyr-3-marks",
    "href": "assignments/assignment-03.html#wrangling-plant-biomass-with-dplyr-3-marks",
    "title": "Assignment 3: Advanced data wrangling and visualization (8 marks)",
    "section": "2. Wrangling plant biomass with dplyr (3 marks)",
    "text": "2. Wrangling plant biomass with dplyr (3 marks)\nNow that our data is in a tidy format, we can start exploring it!\na. What is the average biomass in g/m2 for all observations in the study? (0.25 marks)\nb. How does the average biomass compare between the grazed control sites and those that were protected from herbivores? (0.25 marks)\nc. Display a table of the average plant biomass for each year. (0.25 marks)\nd. What is the mean plant biomass per year for the grazedcontrol and rodentexclosure groups? Present the answer in a table that has these variables as column headers (use pivoting). (0.75 marks)\ne. Check whether there is an equal number of observations per site. (0.25 marks)\nf. How many biomass measurements were 0? Which species had the most 0 biomass measurements? (0.5 marks)\ng. Create a new column that represents the square of the biomass. Display the three largest squared_biomass observations in descending order. Only include the columns year, squared_biomass and species and only observations between the years 2003 and 2008 from the forest habitat. (0.75 mark)\nHint: Break this down into single criteria and add one at a time. “Square” means taken to the power of 2. It does NOT mean square root (power of 1/2).",
    "crumbs": [
      "Assignments",
      "Assignment 3: Advanced data wrangling and visualization (8 marks)"
    ]
  },
  {
    "objectID": "assignments/assignment-03.html#visualising-plant-biomass-3.25-marks",
    "href": "assignments/assignment-03.html#visualising-plant-biomass-3.25-marks",
    "title": "Assignment 3: Advanced data wrangling and visualization (8 marks)",
    "section": "3. Visualising plant biomass (3.25 marks)",
    "text": "3. Visualising plant biomass (3.25 marks)\na. Compare the mean biomass over time for grazedcontrol with that of rodentexclosure graphically in a line plot. What could explain the big dip in biomass year 2012? (0.5 marks)\nHint: The published study might be able to help with the second question\nb. Compare the mean biomass for each species in a lineplot. (0.5 marks)\nc. We’ve found that the biomass is higher in the sites with rodent exclosures (especially in recent years), and that the crowberry is the dominant species. Notice how the lines for rodentexclosure and crowberry are of similar shape. Coincidence? Let’s find out! Use a facetted line plot to explore whether all plant species are impacted equally by grazing. (0.75 mark)\nd. The habitat could also be affecting the biomass of different species. Explore this in a faceted line plot of the mean biomass over time. (0.5 marks)\ne. Explore the relationship between species, habitat, and the distribution of biomass in a box plot. (0.5 marks)\nf. It looks like both habitat and treatment have an effect on most of the species! Let’s dissect the data further by visualizing the effect of both the habitat and treatment on each species by faceting either a box plot or line plot accordingly. (0.5 mark)",
    "crumbs": [
      "Assignments",
      "Assignment 3: Advanced data wrangling and visualization (8 marks)"
    ]
  },
  {
    "objectID": "assignments/assignment-03.html#footnotes",
    "href": "assignments/assignment-03.html#footnotes",
    "title": "Assignment 3: Advanced data wrangling and visualization (8 marks)",
    "section": "",
    "text": "Olofsson J, te Beest M, Ericson L (2013) Complex biotic interactions drive long-term vegetation dynamics in a subarctic ecosystem. Philosophical Transactions of the Royal Society B 368(1624): 20120486. https://dx.doi.org/10.1098/rstb.2012.0486↩︎",
    "crumbs": [
      "Assignments",
      "Assignment 3: Advanced data wrangling and visualization (8 marks)"
    ]
  },
  {
    "objectID": "assignments/assignment-04.html",
    "href": "assignments/assignment-04.html",
    "title": "Assignment 4: Random variables, likelihood, and inference",
    "section": "",
    "text": "1. Simulating genomic mutations (2.5 marks)\nDownload the .Rmd file here.\nTo submit this assignment, upload the full document, including the original questions, your code, and the output. Submit your assignment as a knitted .pdf. Please ensure the text on your .pdf does not continue past the end of the page.\na. Assume that as a strand of genomic DNA (or RNA, if we’re thinking about viruses!) is copied, mutations can be randomly introduced. There is a constant probability of mutation at each nucleotide (mutation_rate). What probability distribution best describes the distribution of the number of mutations you expect to to see in a given genome of fixed length? Why? (0.25 marks)\nAnswer:\nb. What distribution best describes the distance (measured in number of nucleotides) between each subsequent mutation? Why? (0.25 marks)\nAnswer:\nc. If the genome length is 1,000 and the mutation rate is 0.05, what is the average number of mutations you expect to see per genome? (Write code to answer this, creating variables genome_length and mutation_rate the input numbers, and assigning the answer to avg_num_mutations. (0.25)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nd. Draw a random number to represent the exact number of mutations observed in a particular genome, using the genome length and mutation rate above, and assign this to num_mutations. (0.25)\ne. To choose (randomly) the precise positions in the genome where mutations will occur, create a vector containing a number for each genomic position (genome_indices), and then sample a random subset of size num_mutations from this vector. Finally, sort them into ascending order, and call the final answer mutation_indices (0.5 marks)\nf. Calculate the number of nucleotides between each mutation using the diff function, and call this inter_mutation_distance (0.25 marks)\ng. Write a single code chunk to complete all of these steps together and plot a histogram of inter_mutation_distance. Increase the genome length to 10^6 for your final results. Adjust your histogram so that each single nucleotide extra distance is its own bin. Describe the shape of the distribution that you see (0.75 marks)",
    "crumbs": [
      "Assignments",
      "Assignment 4: Random variables, likelihood, and inference"
    ]
  },
  {
    "objectID": "assignments/assignment-04.html#generating-sampling-distributions-1.5-pts",
    "href": "assignments/assignment-04.html#generating-sampling-distributions-1.5-pts",
    "title": "Assignment 4: Random variables, likelihood, and inference",
    "section": "2. Generating sampling distributions (1.5 pts)",
    "text": "2. Generating sampling distributions (1.5 pts)\nWe’ll continue with our mutation rate example!\na. Imagine you are doing an experiment to estimate the mutation rate and understand the distribution of mutations across the genome. Sequencing and assembling the entire genome is costly and complex, so you’ll just sequencing and analyze shorter sections. Assume that with your shorter sequences, you are going to analyze the location of just 50 mutations (n_muts_sample). Assume that inter-mutation distances in your shorter sequences follow the same distribution as those you measured above in the full genome. Write code to draw a sample of this size from the previously-measured inter_mutation_distances and call this sub_genomic_sample (0.5 marks)\nb. You decide to generate and analyze 300 of these shorter sequences (n_samples). Write code (hint : use a for-loop) to generate each of these samples, calculate the average distance between mutations for each (avg_subgenomic_sample), and then plot a histogram of these means (0.5 marks)\nc. Describe the resulting distribution. Compare it’s shape to that obtained in 1g. Are you surprised by the comparison? What statistical principle is at play here? (0.5 marks)\nAnswer:",
    "crumbs": [
      "Assignments",
      "Assignment 4: Random variables, likelihood, and inference"
    ]
  },
  {
    "objectID": "assignments/assignment-04.html#probability-distributions-and-likelihood-1-mark",
    "href": "assignments/assignment-04.html#probability-distributions-and-likelihood-1-mark",
    "title": "Assignment 4: Random variables, likelihood, and inference",
    "section": "3. Probability distributions and likelihood (1 mark)",
    "text": "3. Probability distributions and likelihood (1 mark)\n\nWhen working with random numbers, it is often a good idea to use the function set.seed(). This function ensures that the random numbers that are generated are reproducible, including on other machines. Please read the documentation for this function and this blog post to learn more. (0 marks)\n\n\nSimulate \\(n = 1000\\) realizations (i.e., draws) of a Geometric random variable with success probability \\(p = 0.1\\). Put these realizations in a vector x. Make a histogram of these realizations, using the default number of bins (0.25 marks)\n\n\nExtract the first element of x, i.e., the first realization of the Geometric(\\(p = 0.1\\)) distribution. How likely is it that this observation arose from a Geometric distribution with probability \\(p = 0.9\\)? How likely is it that the observation arose from a Geometric distribution with probability \\(p = 0.5\\)? (0.25 marks)\n\n\nWhat is the probability that each element of x arose due to a Geometric distribution with \\(p = 0.5\\)? Store these probabilities in a vector, apply log to each element of the vector, and print the sum the elements of the vector of log-transformed probabilities. This is the log-likelihood of the data — i.e., the realizations of the Geometric distribution you simulated in (a) — at \\(p=0.5\\). (0.5 marks)",
    "crumbs": [
      "Assignments",
      "Assignment 4: Random variables, likelihood, and inference"
    ]
  },
  {
    "objectID": "assignments/assignment-04.html#maximum-likelihood-estimation-for-binomial-probabilities-3-marks",
    "href": "assignments/assignment-04.html#maximum-likelihood-estimation-for-binomial-probabilities-3-marks",
    "title": "Assignment 4: Random variables, likelihood, and inference",
    "section": "4. Maximum likelihood estimation for binomial probabilities (3 marks)",
    "text": "4. Maximum likelihood estimation for binomial probabilities (3 marks)\nFor this problem, it will be useful to carefully review the likelihood calculation we did in Lecture 8 to estimate disease mortality rate from reports of cases and deaths, using data from Farrell & Davies, PNAS, 2019 that measured cases and deaths for a set of infectious diseases that each infect a wide range of animal species.\n\nLoad the Farrell & Davies 2019 dataset into R and call it disease_distance (0 marks)\n\n\nUse the unique() function to determine all unique elements of the Host and ParaFamily columns. Then, use the expand.grid() function to generate a data frame with all combinations of hosts and parasite families. Store the data frame of host and parasite family combinations in an object called Combinations. Hint: look at the documentation for expand.grid(). (0.25 marks)\n\n\nWe are going to a function that takes in a host and parasite family (i.e., a specific row of the Combination data frame you have just made), and returns the maximum likelihood estimate for the probability of death, given infection, of that host being infected with members of the parasite family. In class, we did this just for the host species Sus scrofa (the wild boar) and parasite family Coronavirinae. Like we did in class, assume deaths are Binomial with a host-specific number of cases and probability of death, and those in different countries and years are independent. If there are no observations for a specific host and parasite family combination, your function should return NA; else, the function should return the value of \\(p\\) which maximizes the likelihood function formed from the data specific to the host and parasite family supplied as arguments to the function. You may also run into issues where the log-likelihood is negative infinity at all \\(p\\); in this case, the function should return NA.\n\nThe skeleton of the function is provided below. The input arguments specify the host species and the parasite family. The cases where an NA should be returned are addressed using nested if {} else {} statements.\nFill in the rest of the function! Test the function on the host and parasite family from class. Does it return the same maximum likelihood estimate for \\(p\\)? If not, the function needs to be de-bugged. If so, excellent – you are probably in a good position to finish the problem! (1.5 mark)\n\n# make sure remove eval = FALSE for submission\n\n## skeleton of function\n\nProbDeathEstimator &lt;- \n  function(host, para_family){\n  \n  data &lt;- disease_distance %&gt;% \n    filter(# FILL IN)\n\n  if (nrow(data) == 0) {\n    return(NA) # return NA if there are no observations for this host + para_family\n  } else \n    \n    p_values_to_test &lt;- # FILL IN # p values to search over\n    \n    # FILL IN: Generate a vector of log likelihood values for each p value\n    \n    LogLik &lt;- data.frame(LogLik, p = p_values_to_test)\n    \n    if (all(LogLik$LogLik == -Inf)){\n      return(NA)\n    } else {\n      # FILL IN - Estimate the maximum likelihood p value\n      return(# FILL IN ) # return the maximum likelihood estimator for prob of death, given function\n    }\n    \n}\n\n\n# make sure remove eval = FALSE for submission\nProbDeathEstimator(host = \"Sus_scrofa\", para_family = \"Coronavirinae\")\n\n\nLoop over all rows of the Combinations data frame you made in (b). For each row, apply the function you made in (c) to the host and parasite family in that row. Store the values returned by the function, for each row, in a vector ProbabilitiesDeath. At the end of the loop, this vector should have length equal to the number of rows in Combinations (i.e., one estimate for host-parasite family pair). Turn in into a dataframe data.frame(p = ProbabilitiesDeath, Combinations). Each row should have a host, parasite family, and the estimate probability of death for the host when infected with a member of the parasite family (0.75 mark)\n\n\nVisualize the distribution of all probability of death estimates using a histogram. Then, visualize the distribution for by host (i.e., faceted by host) and by parasite family. (0.5 marks)\n\nWhat do you notice about the distribution(s) of death probabilities?\nAnswer:",
    "crumbs": [
      "Assignments",
      "Assignment 4: Random variables, likelihood, and inference"
    ]
  },
  {
    "objectID": "lectures/outbreak-investigation.html",
    "href": "lectures/outbreak-investigation.html",
    "title": "Outbreak Investigation",
    "section": "",
    "text": "Exercise Introduction\nYou can download this .Rmd file here\nImagine it is early March 2020 in New York City, and COVID-19 is spreading rapidly. Policy makers want to make predictions about the future burden of disease in the city. The EEB313 Outbreak Task Force has been asked to advise the New York City Department of Health on the future trajectory of the epidemic and, the healthcare burden, and the impact of potential control measures. All we have at our disposal to do this is disease data through March 20, 2020 and our basic understanding of the transmission dynamics of infections:",
    "crumbs": [
      "Miscellaneous notes",
      "Outbreak Investigation"
    ]
  },
  {
    "objectID": "lectures/outbreak-investigation.html#data",
    "href": "lectures/outbreak-investigation.html#data",
    "title": "Outbreak Investigation",
    "section": "Data",
    "text": "Data\nThe available data we have are the number of people newly admitted to the hospital each day with diagnosed COVID-19 infection. Days are counted relative to Feb 15, 2020, when the first local hospitalization occurred.\n\nN_data &lt;- 8.33e6  # population size of NYC\n\nhosp_data &lt;- data.frame(\n  day = c(14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34),\n  hosp = c(1,1,2,7,2,14,8,8,18,37,60,78,106,152,177,210,338,385,494,631,738)\n)\n\nQuestion 1: Is there evidence that COVID-19 is spreading uncontrolled between people (i.e, exponential growth)? Make a plot of the disease trajectory so far, including axis labels, transformations, and anything else needed to make it easy to read. How quickly is the epidemic spreading? Can you quantify growth of the epidemic using either an exponential growth rate or a doubling time? See hints below:\n\nplot_tmax &lt;- 45\n\nplot_linear &lt;- ggplot(...# FILL IN!\n\nplot_log &lt;- plot_linear + \n  scale_y_log10() +\n  labs(y = \"Daily incidence (log10 scale)\")\n\nplot_grid(plot_linear, plot_log, labels = \"AUTO\")",
    "crumbs": [
      "Miscellaneous notes",
      "Outbreak Investigation"
    ]
  },
  {
    "objectID": "lectures/outbreak-investigation.html#fitting-exponential-growth",
    "href": "lectures/outbreak-investigation.html#fitting-exponential-growth",
    "title": "Outbreak Investigation",
    "section": "Fitting exponential growth",
    "text": "Fitting exponential growth\nMethod 1: Exponential growth rate from two points. Choose two time points along the curve and estimate exponential growth rate by calculating a simple rate of change between them. You can use the following code as a template\n\nind1 &lt;-  # Fill in - indices of the points you'll use in the calculation\nind2 &lt;- \n\nr &lt;-  # FILL IN\n\nt2 &lt;- log(2) / r # doubling time\n\nsprintf(\"exponential growth rate r = %.2f /day\", r)\nsprintf(\"doubling time t2 = %.1f days\", t2)\n\nhosp_data_w_model &lt;- rbind(hosp_data %&gt;% mutate(flag = \"data\"), hosp_data[c(ind1,ind2),] %&gt;% mutate(flag = \"model\"))\n\nggplot(hosp_data_w_model, aes(day, hosp, color = flag)) +\n  geom_point() +\n  geom_line() +\n  scale_y_log10() +\n  xlim(0, plot_tmax) +\n  scale_colour_manual(values = c(\"black\", \"red\")) +\n  labs(x = \"Time (days)\", y = \"Daily incidence (log10 scale)\", color = NULL)\n\nMethod 2 : Try another way to estimate the exponential growth rate using the whole curve. Just by trial and error (ie, without any formal fitting) try to find an exponential curve that matches the data reasonably well. Use the following code as a template\n\n# Guess parameters until model matches data\nt_zero &lt;-  # day of first hospitalization\nr_exp  &lt;-  # exponential growth rate (1/day)\n\nt_exp_model &lt;- seq(t_zero, plot_tmax, by = 1)\nhosp_exp_model &lt;- exp(r_exp * (t_exp_model - t_zero))\n\nexp_df &lt;- data.frame(day = t_exp_model, hosp = hosp_exp_model)\n\nggplot() +\n  geom_point(data = hosp_data, aes(day, hosp),color = \"black\") +\n  geom_line(data = exp_df, aes(day, hosp), color = \"red\") +\n  scale_y_log10() +\n  xlim(0, plot_tmax) +\n  labs(x = \"Time (days)\", y = \"Daily incidence (log10 scale)\")\n\nQuestion 2: Using the results of each of your fits above, how many people do we expect to be hospitalized by April 1st, 2020 without any additional control measures?\n\n#Assume April 1 corresponds to Day 45\n\n# Method 1\n\n\n# Method 2\n\nQuestion 3: What about by May 1st, 2020?\n\n#Assume May 1 corresponds to Day 75\n\n# Method 1\n\n\n# Method 2\n\nQuestion 4: Do these values seem realistic? (Hint: Try dividing them by the total population of NYC, encoded in variable N_data). What are the assumptions behind this projection method? When do you think they break down? How can we relax these assumptions?",
    "crumbs": [
      "Miscellaneous notes",
      "Outbreak Investigation"
    ]
  },
  {
    "objectID": "lectures/outbreak-investigation.html#model",
    "href": "lectures/outbreak-investigation.html#model",
    "title": "Outbreak Investigation",
    "section": "Model",
    "text": "Model\nWe will create a simple mechanistic mathematical model for COVID-19 transmission and clinical progression, calibrate this to the observed data, and use this to make improved projections about the future trajectory of the epidemic. This model is a type of “compartmental” model of disease dynamics (an expansion of the simple “SIR” model you may have heard of) and is encoded as differential equations which we will simulate here.\n\nknitr::include_graphics(\"figures/COVID_hosp_model.jpg\")\n\n\n\n\n\n\n\n\n\nEquations\n\\[\\begin{align}\n\\dot{S} &= -\\beta S I\\\\\n\\dot{E} &=\\beta S I - a E \\\\\n\\d, ot{I} &= a E - \\gamma_I I\\\\\n\\dot{H} &= p \\gamma_I I  - \\gamma_H H \\\\\n\\dot{R} & = (1-p)\\gamma_I I + \\gamma_H H\\\\\n\\end{align}\\]\n\n\nVariables\n\n\\(S\\): Susceptible individuals\n\\(E\\): Exposed individuals in the latent phase of infection - infected but not yet infectious\n\\(I\\): Infected (and infectious) individuals with mild infection\n\\(H\\): Infected invididuals with severe infection requiring hospitalization\n\\(R\\): individuals who a removed, either because they have recovered from disease and are now immune, or they have died\n\\(N=S+E+I + H +R\\): Total population size (constant)\n\n\n\nParameters\n\n\\(\\beta\\) rate at which infected individuals contact susceptibles and infect them\n\\(a\\) rate of progression from the exposed to infected class. \\(1/a\\) is the average duration of the latent period.\n\\(\\gamma_I\\) rate at which infected individuals progress to either recover from infection (and becoming immune), or developing severe disease. \\(1/\\gamma_I\\) is the average duration of infection\n\\(p\\) proportion of individuals that progress to severe disease (requiring hospitalization)\n\\(\\gamma_H\\) rate of progression from severe disease (to recovery + immunity, or to death). \\(1/\\gamma_H\\) is the average duration of hospitalization\n\nQuestion 5: Examine the model of COVID-19 above. Answer the following questions:\n\nWhat stages of infection does someone infected with SARS-CoV-2 pass through?\nAt what stage can someone transmit the infection to others?\nWhich parameters do you think are most important for determining how quickly the disease spreads?\nWhich parameters do you think are most important for determining how deadly the outbreak is?\nWhat sort of information/data would you need to estimate the values of the parameters 𝑎, 𝛾𝐼, 𝑝, or 𝛾𝐻 ?\n\n\n\nDefine differential equations\nQuestion 6: Fill in the rest of the code to simulate the differential equations for the model, based on the model definition above\n\nseir &lt;- function(t, y, parms) {\n  with(as.list(c(y, parms)), {\n    dy &lt;- rep(0, 7)\n\n    # Prevalence equations \n    dy[1] &lt;- -beta * I * S                 # S\n    dy[2] &lt;-  beta * I * S - a * E         # E\n    dy[3] &lt;-                 # I\n    dy[4] &lt;-  p * gI * I - gH * H          # H\n    dy[5] &lt;-      # R\n\n    # Extra equations to track cumulatives (ignore for now)\n    dy[6] &lt;- a * E         # cumulative infections (new infections entering I)\n    dy[7] &lt;- p * gI * I    # cumulative hospitalizations (flow into H)\n\n    list(dy)\n  })\n}",
    "crumbs": [
      "Miscellaneous notes",
      "Outbreak Investigation"
    ]
  },
  {
    "objectID": "lectures/outbreak-investigation.html#simulations",
    "href": "lectures/outbreak-investigation.html#simulations",
    "title": "Outbreak Investigation",
    "section": "Simulations",
    "text": "Simulations\nQuestion 7: We want to calibrate the model so that the hospitalizations predicted by the model (H compartment) roughly match those observed through March 20, 2020. You can do this visually by entering your best guess for the parameters below, then running the code and comparing the model prediction to the data. Keep tuning the parameters until you get a reasonable match, paying attention to what aspect of the curves each parameter is influencing. List the values of the parameters you used.\nHint: While you are welcome to ignore this, you might notice from the code that we have to do a few extra data manipulation steps before we can do that. The data represents the absolute number of individuals who are newly admitted to the hospital each day. However, in the model, the H compartment represents the fraction of the population that is currently in the hospital. In general people with COVID-19 stay in the hospital for multiple days to weeks, so this value includes individuals who were admitted over multiple days. To compare the model to data we need to first extract from the model the fraction of the population that newly enters the H compartment each day. We do this by making the model additionally track the cumulative number of people who have ever entered the H compartment, and then extract how much this value increases each day. We use the total population size to go from fractions of the population to total numbers.\n\nParameters\nFor the first set of parameters, try to start with reasonable values based on what you know about COVID-19 transmission\n\nLatentPeriod &lt;-   # latent period (days)\nDurInfect    &lt;-  # duration infectious (days)\nFracSevere   &lt;-   # fraction severe requiring hospitalization (0-1)\nDurHosp      &lt;-   # duration of hospitalization (days)\n\nFor the second set of parameters, you will have to change the values to try to match the model output to the existing data\n\nb            &lt;-   # probability per day of transmitting infection onwards\nInitialPrev  &lt;-   # fraction of the population infected on Feb 15 (time 0)\n\n\n\nTranslate parameters into model form\n\nN &lt;- N_data  # equations are scale-free; keep N here for readability\n\nbeta &lt;- b / N          # transmission parameter in this formulation\na    &lt;- 1 / LatentPeriod\np    &lt;- FracSevere\ngI   &lt;- 1 / DurInfect\ngH   &lt;- 1 / DurHosp\n\nsprintf(\"Beta = %4.2e\", beta)\nsprintf(\"Rate of progression to infectiousness (a) = %4.2f\", a)\nsprintf(\"Rate of recovery from infection (gamma_I) = %4.2f\", gI)\nsprintf(\"Rate of recovery from hospitalization (gamma_H) = %4.2f\", gH)\nsprintf(\"Basic reproduction number (R_0) = %4.2f\", b/a)\n\n\n\nSimulate without intervention\n\nRun and visualize model output\nNote that the output of the model is designed to be the number of individuals in the population that is in each state at each timepoint. We can transform this into a prevalence by dividing by the population size and changing to a %\n\ntmax &lt;- 120\ntvec &lt;- seq(0, tmax, by = 0.1)\n\nic &lt;- c(\n  S = N - InitialPrev * N,   # S0\n  E = InitialPrev * N,       # E0\n  I = 0,\n  H = 0,\n  R = 0,\n  CumInf  = 0,\n  CumHosp = 0\n)\n\nparms &lt;- c(beta = beta, a = a, gI = gI, p = p, gH = gH)\n\nsol &lt;- ode(y = ic, times = tvec, func = seir, parms = parms) %&gt;% as.data.frame()\n\nhead(sol)\n\nTransform this output into long form using pivot_longer, creating a new column called compartment that contains the stage of infection and another called count which contains the number of observations in this stage at this timepoint. Just keep the compartments between S and R (ie not the “cumulative” ones). Create a new column called prevalence_pct which is the percent of individuals in the population in that stage of infection. Show a preview of this data frame:\n\nsol_long &lt;- sol %&gt;% # FILL IN\n\nhead(sol_long)\n\nPlot the results over time (on both linear and log scale):\n\nplot_linear &lt;- ggplot( # FILL IN\n\nplot_log &lt;- plot_linear +\n  scale_y_log10(limits=c(0.001, 100)) + \n  labs(y = \"Prevalence (%) (log10)\", color = NULL)\n\nplot_grid(plot_linear, plot_log, labels = \"AUTO\")\n\n\n\nCompare model to data\nNow, we’ll do the final steps to be able to compare our model output to the observed data! Data is only reported at daily intervals, and is reported as “incidence” values (# of new cases) instead of “prevalence” values, so we’ll manipulate our model output to mimic that.\n\n# Calculate daily incidence from cumulative\n\n# Sample at (approximately) integer days\nsol_daily &lt;- sol %&gt;%\n  mutate(day = floor(time)) %&gt;%\n  group_by(day) %&gt;%\n  slice_min(order_by = abs(time - day), n = 1, with_ties = FALSE) %&gt;%\n  ungroup() %&gt;%\n  filter(day &gt;= 0, day &lt;= (tmax - 1)) %&gt;%\n  arrange(day)\n\ninc_daily &lt;- sol_daily %&gt;%\n  transmute(\n    day = day,\n    inc_inf  = c(NA, diff(CumInf)),\n    inc_hosp = c(NA, diff(CumHosp))\n  ) %&gt;%\n  filter(!is.na(inc_hosp))\n\n# Scale model to NYC population (in case N differs)\nmodel_hosp &lt;- inc_daily %&gt;%\n  mutate(hosp = (N_data / N) * inc_hosp)\n\nymax &lt;- max(c(model_hosp$hosp, hosp_data$hosp), na.rm = TRUE) * 1.2\n\np_inc_lin &lt;- ggplot() +\n  geom_line(data = model_hosp, aes(day, hosp), linewidth = 0.7) +\n  geom_point(data = hosp_data, aes(day, hosp)) +\n  coord_cartesian(xlim = c(0, plot_tmax), ylim = c(0, ymax)) +\n  labs(x = \"Time (days)\", y = \"Daily incidence\")\n\np_inc_log &lt;- ggplot() +\n  geom_line(data = model_hosp, aes(day, hosp), linewidth = 0.7) +\n  geom_point(data = hosp_data, aes(day, hosp)) +\n  scale_y_log10() +\n  coord_cartesian(xlim = c(0, plot_tmax), ylim = c(1, ymax)) +\n  labs(x = \"Time (days)\", y = \"Daily incidence (log10)\")\n\nplot_grid(p_inc_lin,p_inc_log, labels = \"AUTO\")\n\nRepeat the last few cells, changing the parameter values until you find something that is a reasonable match.\n\n\nEstimate time of first case\nQuestion 8: When do you estimate that the first case of COVID-19 occurred in NYC? Try this two ways\nMethod 1: Use your estimated value from the full model for the initial fraction infected at time zero (InitialPrev), along with your simple estimate for the exponential growth rate, and then back out how may days ago infections were exactly 1 using the formula for exponential growth\n\ntime_since_index &lt;- # FILL IN\nsprintf(\"Inferred days before Feb 15 2020 that first infection occurred = %.1f days\", time_since_index)\n\nMethod 2: Run the full model as above, but force the initial fraction of infected (InitialPrev) to be 1/N, and figure out what t_zero value you need to get a match with the data (note: since t_zero is relative to Feb 15, 2020, you’ll likely need it to be negative, which is fine!)\n\nt_zero &lt;-  # FILL IN : choose a negative value so that it occurs BEFORE Feb 15\n\nsprintf(\"Inferred days before Feb 15 2020 that first infection occurred = %.1f days\", -t_zero)\n\nInitialPrev0 &lt;- 1 / N_data  # one infected person at time 0 (approx)\n\ntmax &lt;- 120\ntvec2 &lt;- seq(t_zero, tmax, by = 0.1)\n\nic2 &lt;- c(\n  S = N - InitialPrev0 * N,\n  E = InitialPrev0 * N,\n  I = 0,\n  H = 0,\n  R = 0,\n  CumInf  = 0,\n  CumHosp = 0\n)\n\nsol2 &lt;- ode(y = ic2, times = tvec2, func = seir, parms = parms) %&gt;% as_tibble()\n\n# Repeat daily-incidence comparison\nsol2_daily &lt;- sol2 %&gt;%\n  mutate(day = floor(time)) %&gt;%\n  group_by(day) %&gt;%\n  slice_min(order_by = abs(time - day), n = 1, with_ties = FALSE) %&gt;%\n  ungroup() %&gt;%\n  filter(day &gt;= 0, day &lt;= (tmax - 1)) %&gt;%\n  arrange(day)\n\ninc2_daily &lt;- sol2_daily %&gt;%\n  transmute(\n    day = day,\n    inc_hosp = c(NA, diff(CumHosp))\n  ) %&gt;%\n  filter(!is.na(inc_hosp)) %&gt;%\n  mutate(hosp = (N_data / N) * inc_hosp)\n\nymax2 &lt;- max(c(inc2_daily$hosp, hosp_data$hosp), na.rm = TRUE) * 1.2\n\np_inc_lin &lt;- ggplot() +\n  geom_line(data = inc2_daily, aes(day, hosp), linewidth = 0.7) +\n  geom_point(data = hosp_data, aes(day, hosp)) +\n  coord_cartesian(ylim = c(0, ymax2)) +\n  labs(x = \"Time (days)\", y = \"Daily incidence\")\n\np_inc_log &lt;- ggplot() +\n  geom_line(data = inc2_daily, aes(day, hosp), linewidth = 0.7) +\n  geom_point(data = hosp_data, aes(day, hosp)) +\n  scale_y_log10() +\n  coord_cartesian(ylim = c(1, ymax2)) +\n  labs(x = \"Time (days)\", y = \"Daily incidence (log10)\")\n\nplot_grid(p_inc_lin,p_inc_log, labels = \"AUTO\")\n\n\n\nEvaluate hospital capacity\nQuestion 9: Will COVID-19 hospitalizations exceed the capacity of NYC hospitals to treat them? When do we expect this to occur?\nTo estimate hospital capacity, we use national statistics on the average number of hospital beds per capita (2.6 per 1000), and the average occupancy of those beds (due to other health concerns, which is 66% on average throughout the year but increased by 10% during the winter months due to influenza and other seasonal respiratory viruses). Note that we can now go back to using the prevalence of hospitalizations, since this is what matches with the measured used for hospital capacity\n\nAvailHospBeds &lt;- 2.6 * (1 - 0.66 * 1.1)  # available beds per 1000, based on total beds and occupancy\n\nh_cap &lt;- AvailHospBeds * (N_data / 1000)\n\nhprev &lt;- sol %&gt;%\n  transmute(time = time, hosp = (N_data / N) * H)\n\np_hosp_lin &lt;- ggplot(hprev, aes(time, hosp)) +\n  geom_line() +\n  geom_hline(yintercept = h_cap, linetype = \"dotted\") +\n  coord_cartesian(xlim = c(0, 120)) +\n  labs(x = \"Time (days)\", y = \"Number currently hospitalized\")\n\np_hosp_log &lt;- ggplot(hprev, aes(time, hosp)) +\n  geom_line() +\n  geom_hline(yintercept = h_cap, linetype = \"dotted\") +\n  scale_y_log10() +\n  coord_cartesian(xlim = c(0, 120), ylim = c(1, NA)) +\n  labs(x = \"Time (days)\", y = \"Number currently hospitalized (log10)\")\n\nplot_grid(p_hosp_lin,p_hosp_log, labels = \"AUTO\")\n\n\n\n\nEstimate the impact of interventions\nQuestion 10: What degree of social distancing is required to avoid infection levels growing to the point that hospitals no longer have the capacity to treat severe cases?\nHint: Assume that interventions are implemented starting on March 20, 2020, i.e. the day after the last point in the data we have available\n\nRun and visualize model output with an intervention\nTry different values of the intervention efficacy parameter (which describes the extent to which transmission is reduced) to see how much you need to prevent hospitalizations from exceeding the capacity of the healthcare system. The value you need could vary a lot depending on which values you chose earlier for the disease parameters\nFirst plot all the model output once you add an intervention:\n\nIntervEff &lt;-  # FILL IN: intervention efficacy (0-1) fractional reduction in; transmission (ie 0.1 corresponds to 10% reduction in transmission, or 90% transmission continuing)\n\nt_int     &lt;- 35   # day intervention implemented (March 20, 2020)\ntmax_int  &lt;- 100  # days to run post intervention\ntmax_all  &lt;- t_int + tmax_int\n\n# Pre-intervention run\nt_pre &lt;- seq(0, t_int, by = 0.1)\nic_pre &lt;- ic\nsol_pre &lt;- ode(y = ic_pre, times = t_pre, func = seir, parms = parms) %&gt;% as.data.frame()\n\n# Use last state as initial condition for post-intervention\ny_last &lt;- sol_pre %&gt;% slice_tail(n = 1) %&gt;% select(-time) %&gt;% as.numeric()\nnames(y_last) &lt;- names(ic)\n\n# Post-intervention parameters: reduce beta\nparms_post &lt;- parms\nparms_post[\"beta\"] &lt;- parms[\"beta\"] * (1 - IntervEff)\n\nt_post &lt;- seq(t_int, tmax_all, by = 0.1)\nsol_post &lt;- ode(y = y_last, times = t_post, func = seir, parms = parms_post) %&gt;% as.data.frame()\n\n# Combine (avoid duplicating t_int row)\nsol_all &lt;- bind_rows(sol_pre, sol_post %&gt;% filter(time &gt; t_int))\n\n# Prevalence plots with intervention\nsol_all_long &lt;- sol_all %&gt;%\n  select(time, S:R) %&gt;%\n  pivot_longer(-time, names_to = \"compartment\", values_to = \"count\") %&gt;%\n  mutate(prevalence_pct = 100 * count / N)\n\np_lin &lt;- ggplot(sol_all_long, aes(time, prevalence_pct, color = compartment)) +\n  geom_line() +\n  coord_cartesian(xlim = c(0, 120), ylim = c(0, 100)) +\n  labs(x = \"Time (days)\", y = \"Prevalence (%)\", color = NULL)\n\np_log &lt;- ggplot(sol_all_long, aes(time, prevalence_pct, color = compartment)) +\n  geom_line() +\n  scale_y_log10() +\n  coord_cartesian(xlim = c(0, 120), ylim = c(0.001, 100)) +\n  labs(x = \"Time (days)\", y = \"Prevalence (%) (log10)\", color = NULL)\n\nplot_grid(p_lin,p_log, labels = \"AUTO\")\n\nThen make a plot specifically to compare predicted hospitalizations to capacity\n\nhprev_pre  &lt;- sol_pre  %&gt;% transmute(time, hosp = (N_data / N) * H, scenario = \"no intervention\")\nhprev_post &lt;- sol_all  %&gt;% transmute(time, hosp = (N_data / N) * H, scenario = \"with intervention\")\n\np_int_lin &lt;- ggplot() +\n  geom_line(data = hprev_pre,  aes(time, hosp)) +\n  geom_line(data = hprev_post, aes(time, hosp), linetype = \"dashed\") +\n  geom_hline(yintercept = h_cap, linetype = \"dotted\") +\n  geom_vline(xintercept = t_int, linetype = \"dotted\") +\n  coord_cartesian(xlim = c(0, 120)) +\n  labs(x = \"Time (days)\", y = \"Number currently hospitalized\")\n\np_int_log &lt;- ggplot() +\n  geom_line(data = hprev_pre,  aes(time, hosp)) +\n  geom_line(data = hprev_post, aes(time, hosp), linetype = \"dashed\") +\n  geom_hline(yintercept = h_cap, linetype = \"dotted\") +\n  geom_vline(xintercept = t_int, linetype = \"dotted\") +\n  scale_y_log10() +\n  coord_cartesian(xlim = c(0, 120), ylim = c(1, NA)) +\n  labs(x = \"Time (days)\", y = \"Number currently hospitalized (log10)\")\n\nplot_grid(p_int_lin,p_int_log, labels = \"AUTO\")\n\nQuestion 11: Why do hospitalizations continue to increase for some time even after strong social distancing is implemented and infections are decreasing?",
    "crumbs": [
      "Miscellaneous notes",
      "Outbreak Investigation"
    ]
  },
  {
    "objectID": "lectures/outbreak-investigation-answers.html",
    "href": "lectures/outbreak-investigation-answers.html",
    "title": "Outbreak Investigation Answers",
    "section": "",
    "text": "Exercise Introduction\nImagine it is early March 2020 in New York City, and COVID-19 is spreading rapidly. Policy makers want to make predictions about the future burden of disease in the city. The EEB313 Outbreak Task Force has been asked to advise the New York City Department of Health on the future trajectory of the epidemic and, the healthcare burden, and the impact of potential control measures. All we have at our disposal to do this is disease data through March 20, 2020 and our basic understanding of the transmission dynamics of infections:",
    "crumbs": [
      "Miscellaneous notes",
      "Outbreak Investigation Answers"
    ]
  },
  {
    "objectID": "lectures/outbreak-investigation-answers.html#data",
    "href": "lectures/outbreak-investigation-answers.html#data",
    "title": "Outbreak Investigation Answers",
    "section": "Data",
    "text": "Data\nThe available data we have are the number of people newly admitted to the hospital each day with diagnosed COVID-19 infection. Days are counted relative to Feb 15, 2020, when the first local hospitalization occurred.\n\nN_data &lt;- 8.33e6  # population size of NYC\n\nhosp_data &lt;- data.frame(\n  day = c(14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34),\n  hosp = c(1,1,2,7,2,14,8,8,18,37,60,78,106,152,177,210,338,385,494,631,738)\n)\n\nQuestion 1: Is there evidence that COVID-19 is spreading uncontrolled between people (i.e, exponential growth)? Make a plot of the disease trajectory so far, including axis labels, transformations, and anything else needed to make it easy to read. How quickly is the epidemic spreading? Can you quantify growth of the epidemic using either an exponential growth rate or a doubling time? See hints below:\n\nplot_tmax &lt;- 45\n\nplot_linear &lt;- ggplot(hosp_data, aes(day, hosp)) +\n  geom_point() +\n  geom_line() +\n  xlim(0, plot_tmax) +\n  labs(x = \"Time (days since Feb 15 2020)\", y = \"Daily incidence\",\n       title = \"COVID-19 hospitalizations in NYC\")\n\nplot_log &lt;- plot_linear + \n  scale_y_log10() +\n  labs(y = \"Daily incidence (log10 scale)\")\n\nplot_grid(plot_linear, plot_log, labels = \"AUTO\")",
    "crumbs": [
      "Miscellaneous notes",
      "Outbreak Investigation Answers"
    ]
  },
  {
    "objectID": "lectures/outbreak-investigation-answers.html#fitting-exponential-growth",
    "href": "lectures/outbreak-investigation-answers.html#fitting-exponential-growth",
    "title": "Outbreak Investigation Answers",
    "section": "Fitting exponential growth",
    "text": "Fitting exponential growth\nMethod 1: Exponential growth rate from two points. Choose two time points along the curve and estimate exponential growth rate by calculating a simple rate of change between them. You can use the following code as a template\n\nind1 &lt;- 11  # (Python used 0-based indices; here we use 1-based)\nind2 &lt;- 18\n\nr &lt;- (log(hosp_data$hosp[ind2]) - log(hosp_data$hosp[ind1])) /\n  (hosp_data$day[ind2] - hosp_data$day[ind1])\n\nt2 &lt;- log(2) / r\n\nsprintf(\"exponential growth rate r = %.2f /day\", r)\n\n[1] \"exponential growth rate r = 0.27 /day\"\n\nsprintf(\"doubling time t2 = %.1f days\", t2)\n\n[1] \"doubling time t2 = 2.6 days\"\n\nhosp_data_w_model &lt;- rbind(hosp_data %&gt;% mutate(flag = \"data\"), hosp_data[c(ind1,ind2),] %&gt;% mutate(flag = \"model\"))\n\nggplot(hosp_data_w_model, aes(day, hosp, color = flag)) +\n  geom_point() +\n  geom_line() +\n  scale_y_log10() +\n  xlim(0, plot_tmax) +\n  scale_colour_manual(values = c(\"black\", \"red\")) +\n  labs(x = \"Time (days)\", y = \"Daily incidence (log10 scale)\", color = NULL)\n\n\n\n\n\n\n\n\nMethod 2 : Try another way to estimate the exponential growth rate using the whole curve. Just by trial and error (ie, without any formal fitting) try to find an exponential curve that matches the data reasonably well. Use the following code as a template\n\n# Guess parameters until model matches data\nt_zero &lt;- 11  # day of first hospitalization\nr_exp  &lt;- 0.3 # exponential growth rate (1/day)\n\nt_exp_model &lt;- seq(t_zero, plot_tmax, by = 1)\nhosp_exp_model &lt;- exp(r_exp * (t_exp_model - t_zero))\n\nexp_df &lt;- data.frame(day = t_exp_model, hosp = hosp_exp_model)\n\nggplot() +\n  geom_point(data = hosp_data, aes(day, hosp),color = \"black\") +\n  geom_line(data = exp_df, aes(day, hosp), color = \"red\") +\n  scale_y_log10() +\n  xlim(0, plot_tmax) +\n  labs(x = \"Time (days)\", y = \"Daily incidence (log10 scale)\")\n\n\n\n\n\n\n\n\nQuestion 2: Using the results of each of your fits above, how many people do we expect to be hospitalized by April 1st, 2020 without any additional control measures?\n\n#Assume April 1 corresponds to Day 45\n\n# Method 1\n\nprint(hosp_data$hosp[ind2]*exp(r*(45 - hosp_data$day[ind2])))\n\n[1] 15851.84\n\n# Method 2\nprint(exp(r_exp * (45 - t_zero)))\n\n[1] 26903.19\n\n\nQuestion 3: What about by May 1st, 2020?\n\n#Assume May 1 corresponds to Day 75\n\n# Method 1\n\nprint(hosp_data$hosp[ind2]*exp(r*(75 - hosp_data$day[ind2])))\n\n[1] 45706446\n\n# Method 2\nprint(exp(r_exp * (75 - t_zero)))\n\n[1] 217998775\n\n\nQuestion 4: Do these values seem realistic? (Hint: Try dividing them by the total population of NYC, encoded in variable N_data). What are the assumptions behind this projection method? When do you think they break down? How can we relax these assumptions?",
    "crumbs": [
      "Miscellaneous notes",
      "Outbreak Investigation Answers"
    ]
  },
  {
    "objectID": "lectures/outbreak-investigation-answers.html#model",
    "href": "lectures/outbreak-investigation-answers.html#model",
    "title": "Outbreak Investigation Answers",
    "section": "Model",
    "text": "Model\nWe will create a simple mechanistic mathematical model for COVID-19 transmission and clinical progression, calibrate this to the observed data, and use this to make improved projections about the future trajectory of the epidemic. This model is a type of “compartmental” model of disease dynamics (an expansion of the simple “SIR” model you may have heard of) and is encoded as differential equations which we will simulate here.\n\nknitr::include_graphics(\"figures/COVID_hosp_model.jpg\")\n\n\n\n\n\n\n\n\n\nEquations\n\\[\\begin{align}\n\\dot{S} &= -\\beta S I\\\\\n\\dot{E} &=\\beta S I - a E \\\\\n\\dot{I} &= a E - \\gamma_I I\\\\\n\\dot{H} &= p \\gamma_I I  - \\gamma_H H \\\\\n\\dot{R} & = (1-p)\\gamma_I I + \\gamma_H H\\\\\n\\end{align}\\]\n\n\nVariables\n\n\\(S\\): Susceptible individuals\n\\(E\\): Exposed individuals in the latent phase of infection - infected but not yet infectious\n\\(I\\): Infected (and infectious) individuals with mild infection\n\\(H\\): Infected invididuals with severe infection requiring hospitalization\n\\(R\\): individuals who a removed, either because they have recovered from disease and are now immune, or they have died\n\\(N=S+E+I + H +R\\): Total population size (constant)\n\n\n\nParameters\n\n\\(\\beta\\) rate at which infected individuals contact susceptibles and infect them\n\\(a\\) rate of progression from the exposed to infected class. \\(1/a\\) is the average duration of the latent period.\n\\(\\gamma_I\\) rate at which infected individuals progress to either recover from infection (and becoming immune), or developing severe disease. \\(1/\\gamma_I\\) is the average duration of infection\n\\(p\\) proportion of individuals that progress to severe disease (requiring hospitalization)\n\\(\\gamma_H\\) rate of progression from severe disease (to recovery + immunity, or to death). \\(1/\\gamma_H\\) is the average duration of hospitalization\n\nQuestion 5: Examine the model of COVID-19 above. Answer the following questions:\n\nWhat stages of infection does someone infected with SARS-CoV-2 pass through?\nAt what stage can someone transmit the infection to others?\nWhich parameters do you think are most important for determining how quickly the disease spreads?\nWhich parameters do you think are most important for determining how deadly the outbreak is?\nWhat sort of information/data would you need to estimate the values of the parameters 𝑎, 𝛾𝐼, 𝑝, or 𝛾𝐻 ?\n\n\n\nDefine differential equations\nQuestion 6: Fill in the rest of the code to simulate the differential equations for the model, based on the model definition above\n\nseir &lt;- function(t, y, parms) {\n  with(as.list(c(y, parms)), {\n    dy &lt;- rep(0, 7)\n\n    # Prevalence equations \n    dy[1] &lt;- -beta * I * S                 # S\n    dy[2] &lt;-  beta * I * S - a * E         # E\n    dy[3] &lt;-  a * E - gI * I               # I\n    dy[4] &lt;-  p * gI * I - gH * H          # H\n    dy[5] &lt;- (1 - p) * gI * I + gH * H     # R\n\n    # Extra equations to track cumulatives (ignore for now)\n    dy[6] &lt;- a * E         # cumulative infections (new infections entering I)\n    dy[7] &lt;- p * gI * I    # cumulative hospitalizations (flow into H)\n\n    list(dy)\n  })\n}",
    "crumbs": [
      "Miscellaneous notes",
      "Outbreak Investigation Answers"
    ]
  },
  {
    "objectID": "lectures/outbreak-investigation-answers.html#simulations",
    "href": "lectures/outbreak-investigation-answers.html#simulations",
    "title": "Outbreak Investigation Answers",
    "section": "Simulations",
    "text": "Simulations\nQuestion 7: We want to calibrate the model so that the hospitalizations predicted by the model (H compartment) roughly match those observed through March 20, 2020. You can do this visually by entering your best guess for the parameters below, then running the code and comparing the model prediction to the data. Keep tuning the parameters until you get a reasonable match, paying attention to what aspect of the curves each parameter is influencing. List the values of the parameters you used.\nHint: While you are welcome to ignore this, you might notice from the code that we have to do a few extra data manipulation steps before we can do that. The data represents the absolute number of individuals who are newly admitted to the hospital each day. However, in the model, the H compartment represents the fraction of the population that is currently in the hospital. In general people with COVID-19 stay in the hospital for multiple days to weeks, so this value includes individuals who were admitted over multiple days. To compare the model to data we need to first extract from the model the fraction of the population that newly enters the H compartment each day. We do this by making the model additionally track the cumulative number of people who have ever entered the H compartment, and then extract how much this value increases each day. We use the total population size to go from fractions of the population to total numbers.\n\nParameters\nFor the first set of parameters, try to start with reasonable values based on what you know about COVID-19 transmission\n\nLatentPeriod &lt;- 4  # latent period (days)\nDurInfect    &lt;- 7 # duration infectious (days)\nFracSevere   &lt;- 0.01  # fraction severe requiring hospitalization (0-1)\nDurHosp      &lt;- 7  # duration of hospitalization (days)\n\nFor the second set of parameters, you will have to change the values to try to match the model output to the existing data\n\nb            &lt;- 0.7  # probability per day of transmitting infection onwards\nInitialPrev  &lt;- 0.0001  # fraction of the population infected on Feb 15 (time 0)\n\n\n\nTranslate parameters into model form\n\nN &lt;- N_data  # equations are scale-free; keep N here for readability\n\nbeta &lt;- b / N          # transmission parameter in this formulation\na    &lt;- 1 / LatentPeriod\np    &lt;- FracSevere\ngI   &lt;- 1 / DurInfect\ngH   &lt;- 1 / DurHosp\n\nsprintf(\"Beta = %4.2e\", beta)\n\n[1] \"Beta = 8.40e-08\"\n\nsprintf(\"Rate of progression to infectiousness (a) = %4.2f\", a)\n\n[1] \"Rate of progression to infectiousness (a) = 0.25\"\n\nsprintf(\"Rate of recovery from infection (gamma_I) = %4.2f\", gI)\n\n[1] \"Rate of recovery from infection (gamma_I) = 0.14\"\n\nsprintf(\"Rate of recovery from hospitalization (gamma_H) = %4.2f\", gH)\n\n[1] \"Rate of recovery from hospitalization (gamma_H) = 0.14\"\n\nsprintf(\"Basic reproduction number (R_0) = %4.2f\", b/a)\n\n[1] \"Basic reproduction number (R_0) = 2.80\"\n\n\n\n\nSimulate without intervention\n\nRun and visualize model output\nNote that the output of the model is designed to be the number of individuals in the population that is in each state at each timepoint. We can transform this into a prevalence by dividing by the population size and changing to a %\n\ntmax &lt;- 120\ntvec &lt;- seq(0, tmax, by = 0.1)\n\nic &lt;- c(\n  S = N - InitialPrev * N,   # S0\n  E = InitialPrev * N,       # E0\n  I = 0,\n  H = 0,\n  R = 0,\n  CumInf  = 0,\n  CumHosp = 0\n)\n\nparms &lt;- c(beta = beta, a = a, gI = gI, p = p, gH = gH)\n\nsol &lt;- ode(y = ic, times = tvec, func = seir, parms = parms) %&gt;% as.data.frame()\n\nhead(sol)\n\n  time       S        E        I           H         R   CumInf     CumHosp\n1  0.0 8329167 833.0000  0.00000 0.000000000 0.0000000  0.00000 0.000000000\n2  0.1 8329166 813.1466 20.42597 0.001461413 0.1453783 20.57281 0.001468397\n3  0.2 8329164 795.1687 40.09296 0.005744688 0.5742426 40.67295 0.005799873\n4  0.3 8329161 778.9694 59.05707 0.012706184 1.2762995 60.34607 0.012890057\n5  0.4 8329156 764.4582 77.37125 0.022212032 2.2420248 79.63549 0.022642368\n6  0.5 8329150 751.5500 95.08558 0.034137540 3.4626189 98.58234 0.034967564\n\n\nTransform this output into long form using pivot_longer, creating a new column called compartment that contains the stage of infection and another called count which contains the number of observations in this stage at this timepoint. Just keep the compartments between S and R (ie not the “cumulative” ones). Create a new column called prevalence_pct which is the percent of individuals in the population in that stage of infection. Show a preview of this data frame:\n\nsol_long &lt;- sol %&gt;%\n  select(time, S:R) %&gt;%\n  pivot_longer(-time, names_to = \"compartment\", values_to = \"count\") %&gt;%\n  mutate(prevalence_pct = 100 * count / N)\n\nhead(sol_long)\n\n# A tibble: 6 × 4\n   time compartment    count prevalence_pct\n  &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1   0   S           8329167          100.0 \n2   0   E               833            0.01\n3   0   I                 0            0   \n4   0   H                 0            0   \n5   0   R                 0            0   \n6   0.1 S           8329166.         100.0 \n\n\nPlot the results over time (on both linear and log scale):\n\nplot_linear &lt;- ggplot(sol_long, aes(time, prevalence_pct, color = compartment)) +\n  geom_line() +\n  labs(x = \"Time (days)\", y = \"Prevalence (%)\", color = NULL)\n\nplot_log &lt;- ggplot(sol_long, aes(time, prevalence_pct, color = compartment)) +\n  geom_line() +\n  scale_y_log10(limits=c(0.001, 100)) + \n  labs(x = \"Time (days)\", y = \"Prevalence (%) (log10)\", color = NULL)\n\nplot_grid(plot_linear, plot_log, labels = \"AUTO\")\n\nWarning in scale_y_log10(limits = c(0.001, 100)): log-10 transformation\nintroduced infinite values.\n\n\nWarning: Removed 127 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\nCompare model to data\nNow, we’ll do the final steps to be able to compare our model output to the observed data! Data is only reported at daily intervals, and is reported as “incidence” values (# of new cases) instead of “prevalence” values, so we’ll manipulate our model output to mimic that.\n\n# Calculate daily incidence from cumulative\n\n# Sample at (approximately) integer days\nsol_daily &lt;- sol %&gt;%\n  mutate(day = floor(time)) %&gt;%\n  group_by(day) %&gt;%\n  slice_min(order_by = abs(time - day), n = 1, with_ties = FALSE) %&gt;%\n  ungroup() %&gt;%\n  filter(day &gt;= 0, day &lt;= (tmax - 1)) %&gt;%\n  arrange(day)\n\ninc_daily &lt;- sol_daily %&gt;%\n  transmute(\n    day = day,\n    inc_inf  = c(NA, diff(CumInf)),\n    inc_hosp = c(NA, diff(CumHosp))\n  ) %&gt;%\n  filter(!is.na(inc_hosp))\n\n# Scale model to NYC population (in case N differs)\nmodel_hosp &lt;- inc_daily %&gt;%\n  mutate(hosp = (N_data / N) * inc_hosp)\n\nymax &lt;- max(c(model_hosp$hosp, hosp_data$hosp), na.rm = TRUE) * 1.2\n\np_inc_lin &lt;- ggplot() +\n  geom_line(data = model_hosp, aes(day, hosp), linewidth = 0.7) +\n  geom_point(data = hosp_data, aes(day, hosp)) +\n  coord_cartesian(xlim = c(0, plot_tmax), ylim = c(0, ymax)) +\n  labs(x = \"Time (days)\", y = \"Daily incidence\")\n\np_inc_log &lt;- ggplot() +\n  geom_line(data = model_hosp, aes(day, hosp), linewidth = 0.7) +\n  geom_point(data = hosp_data, aes(day, hosp)) +\n  scale_y_log10() +\n  coord_cartesian(xlim = c(0, plot_tmax), ylim = c(1, ymax)) +\n  labs(x = \"Time (days)\", y = \"Daily incidence (log10)\")\n\nplot_grid(p_inc_lin,p_inc_log, labels = \"AUTO\")\n\n\n\n\n\n\n\n\nRepeat the last few cells, changing the parameter values until you find something that is a reasonable match.\n\n\nEstimate time of first case\nQuestion 8: When do you estimate that the first case of COVID-19 occurred in NYC? Try this two ways\nMethod 1: Use your estimated value from the full model for the initial fraction infected at time zero (InitialPrev), along with your simple estimate for the exponential growth rate, and then back out how may days ago infections were exactly 1 using the formula for exponential growth\n\ntime_since_index &lt;- log(InitialPrev * N_data / 1) / r\nsprintf(\"Inferred days before Feb 15 2020 that first infection occurred = %.1f days\", time_since_index)\n\n[1] \"Inferred days before Feb 15 2020 that first infection occurred = 25.3 days\"\n\n\nMethod 2: Run the full model as above, but force the initial fraction of infected (InitialPrev) to be 1/N, and figure out what t_zero value you need to get a match with the data (note: since t_zero is relative to Feb 15, 2020, you’ll likely need it to be negative, which is fine!)\n\nt_zero &lt;- -30  # TODO: choose a negative value so that it occurs BEFORE Feb 15\n\nsprintf(\"Inferred days before Feb 15 2020 that first infection occurred = %.1f days\", -t_zero)\n\n[1] \"Inferred days before Feb 15 2020 that first infection occurred = 30.0 days\"\n\nInitialPrev0 &lt;- 1 / N_data  # one infected person at time 0 (approx)\n\ntmax &lt;- 120\ntvec2 &lt;- seq(t_zero, tmax, by = 0.1)\n\nic2 &lt;- c(\n  S = N - InitialPrev0 * N,\n  E = InitialPrev0 * N,\n  I = 0,\n  H = 0,\n  R = 0,\n  CumInf  = 0,\n  CumHosp = 0\n)\n\nsol2 &lt;- ode(y = ic2, times = tvec2, func = seir, parms = parms) %&gt;% as_tibble()\n\n# Repeat daily-incidence comparison\nsol2_daily &lt;- sol2 %&gt;%\n  mutate(day = floor(time)) %&gt;%\n  group_by(day) %&gt;%\n  slice_min(order_by = abs(time - day), n = 1, with_ties = FALSE) %&gt;%\n  ungroup() %&gt;%\n  filter(day &gt;= 0, day &lt;= (tmax - 1)) %&gt;%\n  arrange(day)\n\ninc2_daily &lt;- sol2_daily %&gt;%\n  transmute(\n    day = day,\n    inc_hosp = c(NA, diff(CumHosp))\n  ) %&gt;%\n  filter(!is.na(inc_hosp)) %&gt;%\n  mutate(hosp = (N_data / N) * inc_hosp)\n\nymax2 &lt;- max(c(inc2_daily$hosp, hosp_data$hosp), na.rm = TRUE) * 1.2\n\np_inc_lin &lt;- ggplot() +\n  geom_line(data = inc2_daily, aes(day, hosp), linewidth = 0.7) +\n  geom_point(data = hosp_data, aes(day, hosp)) +\n  coord_cartesian(ylim = c(0, ymax2)) +\n  labs(x = \"Time (days)\", y = \"Daily incidence\")\n\np_inc_log &lt;- ggplot() +\n  geom_line(data = inc2_daily, aes(day, hosp), linewidth = 0.7) +\n  geom_point(data = hosp_data, aes(day, hosp)) +\n  scale_y_log10() +\n  coord_cartesian(ylim = c(1, ymax2)) +\n  labs(x = \"Time (days)\", y = \"Daily incidence (log10)\")\n\nplot_grid(p_inc_lin,p_inc_log, labels = \"AUTO\")\n\nDon't know how to automatically pick scale for object of type &lt;deSolve/matrix&gt;.\nDefaulting to continuous.\nDon't know how to automatically pick scale for object of type &lt;deSolve/matrix&gt;.\nDefaulting to continuous.\n\n\n\n\n\n\n\n\n\n\n\nEvaluate hospital capacity\nQuestion 9: Will COVID-19 hospitalizations exceed the capacity of NYC hospitals to treat them? When do we expect this to occur?\nTo estimate hospital capacity, we use national statistics on the average number of hospital beds per capita (2.6 per 1000), and the average occupancy of those beds (due to other health concerns, which is 66% on average throughout the year but increased by 10% during the winter months due to influenza and other seasonal respiratory viruses). Note that we can now go back to using the prevalence of hospitalizations, since this is what matches with the measured used for hospital capacity\n\nAvailHospBeds &lt;- 2.6 * (1 - 0.66 * 1.1)  # available beds per 1000, based on total beds and occupancy\n\nh_cap &lt;- AvailHospBeds * (N_data / 1000)\n\nhprev &lt;- sol %&gt;%\n  transmute(time = time, hosp = (N_data / N) * H)\n\np_hosp_lin &lt;- ggplot(hprev, aes(time, hosp)) +\n  geom_line() +\n  geom_hline(yintercept = h_cap, linetype = \"dotted\") +\n  coord_cartesian(xlim = c(0, 120)) +\n  labs(x = \"Time (days)\", y = \"Number currently hospitalized\")\n\np_hosp_log &lt;- ggplot(hprev, aes(time, hosp)) +\n  geom_line() +\n  geom_hline(yintercept = h_cap, linetype = \"dotted\") +\n  scale_y_log10() +\n  coord_cartesian(xlim = c(0, 120), ylim = c(1, NA)) +\n  labs(x = \"Time (days)\", y = \"Number currently hospitalized (log10)\")\n\nplot_grid(p_hosp_lin,p_hosp_log, labels = \"AUTO\")\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate the impact of interventions\nQuestion 10: What degree of social distancing is required to avoid infection levels growing to the point that hospitals no longer have the capacity to treat severe cases?\n\nRun and visualize model output with an intervention\nTry different values of the intervention efficacy parameter (which describes the extent to which transmission is reduced) to see how much you need to prevent hospitalizations from exceeding the capacity of the healthcare system. The value you need could vary a lot depending on which values you chose earlier for the disease parameters\nFirst plot all the model output once you add an intervention:\n\nIntervEff &lt;- 0.8  # TODO: intervention efficacy (0-1) fractional reduction in; transmission (ie 0.1 corresponds to 10% reduction in transmission, or 90% transmission continuing)\n\nt_int     &lt;- 35   # day intervention implemented (March 20, 2020)\ntmax_int  &lt;- 100  # days to run post intervention\ntmax_all  &lt;- t_int + tmax_int\n\n# Pre-intervention run\nt_pre &lt;- seq(0, t_int, by = 0.1)\nic_pre &lt;- ic\nsol_pre &lt;- ode(y = ic_pre, times = t_pre, func = seir, parms = parms) %&gt;% as.data.frame()\n\n# Use last state as initial condition for post-intervention\ny_last &lt;- sol_pre %&gt;% slice_tail(n = 1) %&gt;% select(-time) %&gt;% as.numeric()\nnames(y_last) &lt;- names(ic)\n\n# Post-intervention parameters: reduce beta\nparms_post &lt;- parms\nparms_post[\"beta\"] &lt;- parms[\"beta\"] * (1 - IntervEff)\n\nt_post &lt;- seq(t_int, tmax_all, by = 0.1)\nsol_post &lt;- ode(y = y_last, times = t_post, func = seir, parms = parms_post) %&gt;% as.data.frame()\n\n# Combine (avoid duplicating t_int row)\nsol_all &lt;- bind_rows(sol_pre, sol_post %&gt;% filter(time &gt; t_int))\n\n# Prevalence plots with intervention\nsol_all_long &lt;- sol_all %&gt;%\n  select(time, S:R) %&gt;%\n  pivot_longer(-time, names_to = \"compartment\", values_to = \"count\") %&gt;%\n  mutate(prevalence_pct = 100 * count / N)\n\np_lin &lt;- ggplot(sol_all_long, aes(time, prevalence_pct, color = compartment)) +\n  geom_line() +\n  coord_cartesian(xlim = c(0, 120), ylim = c(0, 100)) +\n  labs(x = \"Time (days)\", y = \"Prevalence (%)\", color = NULL)\n\np_log &lt;- ggplot(sol_all_long, aes(time, prevalence_pct, color = compartment)) +\n  geom_line() +\n  scale_y_log10() +\n  coord_cartesian(xlim = c(0, 120), ylim = c(0.001, 100)) +\n  labs(x = \"Time (days)\", y = \"Prevalence (%) (log10)\", color = NULL)\n\nplot_grid(p_lin,p_log, labels = \"AUTO\")\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n\n\n\n\n\n\n\n\n\nThen make a plot specifically to compare predicted hospitalizations to capacity\n\nhprev_pre  &lt;- sol_pre  %&gt;% transmute(time, hosp = (N_data / N) * H, scenario = \"no intervention\")\nhprev_post &lt;- sol_all  %&gt;% transmute(time, hosp = (N_data / N) * H, scenario = \"with intervention\")\n\np_int_lin &lt;- ggplot() +\n  geom_line(data = hprev_pre,  aes(time, hosp)) +\n  geom_line(data = hprev_post, aes(time, hosp), linetype = \"dashed\") +\n  geom_hline(yintercept = h_cap, linetype = \"dotted\") +\n  geom_vline(xintercept = t_int, linetype = \"dotted\") +\n  coord_cartesian(xlim = c(0, 120)) +\n  labs(x = \"Time (days)\", y = \"Number currently hospitalized\")\n\np_int_log &lt;- ggplot() +\n  geom_line(data = hprev_pre,  aes(time, hosp)) +\n  geom_line(data = hprev_post, aes(time, hosp), linetype = \"dashed\") +\n  geom_hline(yintercept = h_cap, linetype = \"dotted\") +\n  geom_vline(xintercept = t_int, linetype = \"dotted\") +\n  scale_y_log10() +\n  coord_cartesian(xlim = c(0, 120), ylim = c(1, NA)) +\n  labs(x = \"Time (days)\", y = \"Number currently hospitalized (log10)\")\n\nplot_grid(p_int_lin,p_int_log, labels = \"AUTO\")\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\n\n\n\n\n\n\n\n\n\nQuestion 11: Why do hospitalizations continue to increase for some time even after strong social distancing is implemented and infections are decreasing?",
    "crumbs": [
      "Miscellaneous notes",
      "Outbreak Investigation Answers"
    ]
  }
]