# Power analysis practice

The below function simulates genome sizes (which range from 1000 to 10000 base pairs) and associated mutation rates (measured in units of number of mutations per individual per base pair) under a linear model motivated by the observation that larger genomes tend to be more robust to mutation (i.e., the per-base mutation rate is a decreasing function of genome size). In the model, the effect of increasing genome size by one unit on mutation rate is set equal to -0.0005. The function takes sample size as an input.

```{r, warning=F, message=F, eval=F}
require(tidyverse)

data_generator <- function(sample_size = 100){
  genome_size <- sample(size = sample_size, x = seq(1000,10000,1), replace = T)
  
  mutation_rate <- c()
  
  for (i in 1:length(genome_size)){
    mutation_rate[i] <- (100 -0.0005*genome_size[i] + rnorm(1, mean=0, sd = 10))*1e-9
  }
  
  return(data.frame(genome_size = genome_size, mutation_rate = mutation_rate))
}
```

The below code chunk plots genome size and mutation rate data generated under the linear model above, and preforms a regression of mutation rate on genome size using the simulated data.

```{r, warning=F, message=F, eval=F}
data <- data_generator()

data %>% pivot_longer(! genome_size) %>% 
  ggplot(aes(x = genome_size, y = value)) + geom_point() + 
  geom_smooth(method = "lm") + ylab("mutation rate") + xlab("genome size")

summary(lm(mutation_rate~genome_size, data))
```

## Exercise

**Write a function to estimate, for a given sample size, the power at level $\alpha = 0.06$. That is, from a large number of simulations (generated using the `data_generator()` function), determine the fraction of simulations for which fitting a linear model (of mutation rate on genome size) the coefficient associated to genome size has a $p$ value which is $< \alpha$.**

Looping over many sample sizes, use the function you have written to determine the _minimum_ sample size needed to detect a significant effect >99% of the time.