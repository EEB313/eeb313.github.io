# Model selection

## Lesson preamble

> ### Learning objectives
>
> - Understand the difference between likelihood estimation/inference and model _selection_
> - Develop familiarity with common model selection approaches
> - Understand intuition for and the use of common information theoretic model selection criteria
> - Perform model selection on the RIKZ data from last class
> 
> ### Lesson outline
> 
> - Brief intro to model selection (20 mins)
> - Understanding information theoretic criteria (20 mins)
> - Model selection of RIKZ dataset (40 mins)
> - Model dredging and averaging (30 mins)
> 
> ### Required packages
>
> - `tidyverse`
> - `lme4`
> - `lmerTest`
> - `MuMIn`
> - `glmnet`

```{r include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

library(tidyverse)
library(lme4)
library(lmerTest)
library(MuMIn)
library(glmnet)

theme_set(theme_bw())
```

# Model selection: theory

Statistics is about making decisions under uncertainty. _Estimation_ involves deciding what parameter values were most likely to (under a model of the distribution of the data and, in some circumstances, any prior knowledge about the parameters) have given rise to that data. _Inference_ involves quantifying the uncertain around our estimates using confidence intervals. As we have seen, however, to any confidence interval there is an associated hypothesis test. Inference and hypothesis testing, then, involve deciding if a particular set of parameter values could have plausibly given rise to the data. 

It should come as no surprise, since both estimation and inference involve decision making, that in statistics we often interested in deciding if the models we build are appropriate descriptions of how the world works (given the data we have and use to fit those models), or what among a set of candidate models is the "best". This practice is called _model selection_ and is the focus of our lecture today.

## Examples

- Hypothesis testing is a kind of model selection. For example, for data $x_1,\dots,x_n \sim f(x|\theta)$, testing $H_0 \colon \theta = \theta_0$ vs $H_1 \colon \theta \neq \theta_0$ is equivalent to choosing between models $\mathcal{M}_0 \colon f(x|\theta)$ and $\mathcal{M}_1 \colon f(x|\theta_0)$.

- Suppose we regress $y$ on the 1st, 2nd, $\dots$, $p$th powers of a covariate $x$:

$$y = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_p x^p.$$

\vspace{12pt}

This gives rise to a sequence of models $\mathcal{M}_1,\mathcal{M}_2,\dots,\mathcal{M}_p$ of the data generative process. Which model is the best description of the data? Although the full model is more flexible in that it has more parameters than, say, the model in which all second and higher order terms are $=0$, it is more prone to overfitting. Choosing between a sequence of nested linear models like this is a classic model selection problem.

- Suppose we would like to model the relationship between expression (i.e., the number of transcripts produced) of _each_ coding gene in the human genome and, say, height. If there are $p$ genes for which we have measurements and only $n \ll p$ observations, it is not possible to fit a linear model of the form

$$y_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_p x_{pi}.$$

\vspace{12pt}

(The reason is that there are infinitely many solutions to the likelihood maximization problem.) In this case, we might want to select a subset of the covariates which best explain the available data. Of $x_1,\dots,x_p$, which are most informative? This too is a kind of model selection problem. We will discuss methods to do "regularized" regression and feature selection (e.g., ridge and LASSO regression) today.

## Common model selection techniques

1. AIC and related methods (BIC, $C_p$)
2. Cross validation
3. Regularized regression (ridge, LASSO, elastic net)
4. Stepwise regression

## The Akaike information criterion (AIC)

Suppose we have data $y_1,\dots,y_n$ that are drawn from a distribution $f$ and a set of candidate models

$$\mathcal{M}_j = \{p_j(y|\theta_j)\}.$$

\vspace{6pt}

It is possible under this setup to find the maximum likelihood estimators for each of the candidate models; it is, however, difficult to compare these models in that the parameters underlying each model might not match (i.e., the models may not be nested). The _Akaike information criterion_ overcomes this issue, and despite having a lengthy and complicated derivation, is a metric which balances two things: 1) the goodness-of-fit of the model and 2) the number of parameters fitted.

The intuition and formula for the AIC is as follows. If $\hat{\theta}_{j,\text{MLE}}$ is the MLE for $\theta$ under model $\mathcal{M}_j$, then we can measure the distance between the ground truth (i.e., distribution $f$ of the data) and fitted model $\hat{p}_j = p_j(y|\hat{\theta}_{j,\text{MLE}})$ using a metric called the Kullback-Leibler divergence:

$$D_{KL}(p, \hat{p}_j) = \int p(y) \log p(y) \text{d} y - \int p(y) \log \hat{p_j}(y) \text{d} y. $$

\vspace{6pt}

**Minimizing the Kullback-Leibler divergence (distance) between the ground truth and density $j$ is a principled way to preform model selection, and forms the basis for the AIC.** Note that minimizing only involves the second integral, and we can estimate the integral with an average

$$\frac{1}{n} \log L(\hat{\theta}_{j,\text{MLE}}) = \frac{1}{n} \sum_{i=1}^n \log p(y_i|\hat{\theta}_{j,\text{MLE}})$$

Importantly, AIC corrects for the fact this is an unbiased estimator of the divergence by adding $d_j/n$, where $d_j = \text{dim}(\mathcal{M}_j)$. This term is what penalizes models with a large number of parameters. So,

$$\text{AIC} = - 2 \log L(\hat{\theta}_{j,\text{MLE}}) + 2 d_j.$$

\vspace{6pt}

Notice we have multiplied the preceding quantities by $-2n$ to get the above expression; this does not change anything, and is largely for historical reasons. Based on the AIC expression, it is clear 1) the higher the likelihood, the lower the AIC; 2) introducing more parameters into the model without changing the likelihood results in a greater value for the AIC. The balance between goodness-of-fit (likelihood) and the number of parameters (the potential to overfit) is what AIC tries to optimize in choosing between candidate models. As we have shown here, that balance is struck by minimizing the distance between candidate models and the ground truth while correcting for bias introduced by having models of different dimensions.

### $\text{AIC}_c$ for small sample sizes

It is sometimes convenient to, when sample sizes are small ($n < 40$ is a commonly used rule-of-thumb) use the following metric to choose between candidate models:

$$\text{AIC}_c = \text{AIC} + \frac{2d_j(d_j+1)}{n-d_j-1}$$

## Stepwise and regularized regression

We will return to model selection via AIC later in the lecture, but in the meantime we will discuss two kinds of feature selection techniques that can help us choose between covariates in a linear model.

### Stepwise regression...and why you should not use it.

Stepwise selection involves iteratively adding and removing predictors according to some criterion (e.g., variance explained). The steps for forward stepwise regression are as follows:

1. Let $\mathcal{M}_0$ be a null model, i.e., one with no predictors.
2. For $k=0,1,\dots,p-1$, fit all models which add one predictor to those in model $\mathcal{M}_k$.
3. Choose the best among the $p-k$ models in step (2) and call it model $\mathcal{M}_{k+1}$. This is done according to some criterion. We might, e.g., choose the model with the greatest $R^2$.
4. Select from models $\mathcal{M}_0,\dots,\mathcal{M}_p$ using, for example, AIC.

**We, however, urge you to AVOID stepwise selection and to be critical of analyses which use it. No statistics can be a substitute for good theory which is informative about the predictors which have predictive power _because_ they are biologically meaningful. When choosing between models, careful consideration of the what predictors are informative and _why_ is key.**

\vspace{6pt}

For more on the problems with automated stepwise selection methods, see [here](https://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection?noredirect=1&lq=1).

### Regularized regression

A more principled approach to linear model selection is so-called _regularized regression_. These approaches constrain the least squares problem (i.e., maximizing the likelihood function with respect to the regression coefficients) in some way. These constraints help "pull" the regression coefficients to zero and filter the features which are most informative about the response. The induced sparsity (i.e., many of the regression coefficients being $=0$) is ideal in that it helps improve the interpretability and predictive power of the model.

Different types of constraint "regularize" the regression coefficients in different ways. Ridge regression fits the coefficients of a linear model $\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}$ subject to $||\boldsymbol{\beta}||_2^2 = \beta_0^2 + \beta_2^2 + \cdots + \beta_p^2 \leqslant t.$ The LASSO fits the same model, but subject to $||\boldsymbol{\beta}||_1 = |\beta_0| + |\beta_2| + \cdots + |\beta_p| \leqslant t.$ While ridge regression pulls coefficients close to zero, LASSO forces the fitted model to be sparse (i.e., most coefficients are exactly zero).

There are several packages which fit models subject to the ridge and LASSO constraints in R. The below example shows how to implement LASSO regression for simulated data using the function `glmnet`. (The syntax is similar for ridge and other forms of regularized regression.) 

```{r}
p <- 7; n <- 200
beta <- c(-10,0,2.5,5,10,25,50)
x <- matrix(rnorm(n*p, mean = 0, sd = 0.1), nrow = n, ncol = p)
y <- x %*% beta + rnorm(n, mean = 0, sd = 1)

data <- as.data.frame(cbind(y = y, x = x))
colnames(data) <- c("y", paste("x", 1:p, sep=""))

fit_lasso <- glmnet(x, y, alpha = 1) # alpha = 0 gives ridge constraint
plot(fit_lasso, xvar = "lambda", label = TRUE)
# solution path shows how size of fitted coefficients changes as a function of the penalty
# increasing lambda and t both result in more stringent constraint on the coeffs
```

How to choose $\lambda$ (or, equivalently, $t$)? Extreme values force the regression coefficients to be all $=0$, but small values may give rise to an over-saturated model with too many predictors. There are a couple ways to decide what constraint is best to use, but _cross-validation_ is quite popular. This involves splitting the data into "training" and "test" data and determining for which value of $\lambda$ the model minimizes, say, the mean squared error. The following implements so-called leave-one-out cross-validation:

```{r}
lambda_values <- exp(seq(log(0.001), log(1000), length = 100))
cross_val_lasso <- cv.glmnet(x, y, alpha = 1, nfolds = 10, lambda = lambda_values)
plot(cross_val_lasso)

lasso_predictions <- predict(cross_val_lasso, 
                             s = cross_val_lasso$lambda.min,
                             type = "coefficients")
lasso_VS_truth <- matrix(cbind(beta, lasso = lasso_predictions[-1]), ncol = 2)
colnames(lasso_VS_truth) <- c("true coeff","fitted coeff")
rownames(lasso_VS_truth) <- NULL

knitr::kable(lasso_VS_truth)
```

It appears the value of $\lambda$ that is ideal for the problem at hand is so small that LASSO regression returns the same coefficients as un-constrained least squares, i.e., the penalty does not affect estimation of the coefficients too much! The data we have used here are cherry-picked to illustrate how the method works with relative ease. Implementation of the LASSO is a bit trickier with more features. **However, when you need to preform feature selection on a large number of covariates and have limited data ($p$ large), a combination of cross-validation and LASSO regression is the way to go.**

# Model selection for LMMs

Model selection for mixed models is even more complicated, due to the addition of random effects. A procedure that is commonly used to preform selection of LMMs is as follows:

   1. Create a _saturated model_ that includes all fixed effects (and their interactions[^interaction]) and random effects you wish to test. 
   
   2. Optimize the random-effect structure by comparing models with the same saturated fixed effects structure but differing random effect structures. These models should be fitted using Restricted Maximum Likelihood (i.e., `REML = TRUE`), since varience component estimation will be biased otherwise. The _optimal_ random effect structure is the one that provides the lowest AIC. Do **not** remove random effects if they are included to account for non-independence in your data. Skip this step if random effects must be included given the hierarchical structure (i.e., nestedness) in the data.
   
   3. Optimize the fixed-effect structure by comparing models with the same optimized (or necessary) random effects but with differing fixed effect structures. These models should be fitted with Maximum Likelihood (i.e., `REML = FALSE`) to prevent biased fixed-effect parameter estimates. Models can be selected on the basis of AIC (the lower the better), by comparing nested models using a likelihood ratio test (LRT), or both. 
   
   4. Fit the final model with optimized fixed and random effects using REML and interpret your results.
   
We can apply this procedure to the RIKZ data as follows.

```{r}
rikz_data <- read_csv("rikz_data.csv")
```

### Step 1: Create saturated model

Let's recreate the random intercept and random intercept-slope models we created last class, and add in Exposure and the interaction between NAP and Exposure as additional fixed effects. 

```{r}
# Set `REML=TRUE` in anticipation of step 2 
mixed_model_IntOnly <- lmer(Richness~NAP*Exposure+(1|Beach), REML=TRUE, data=rikz_data)

mixed_model_IntSlope <- lmer(Richness~NAP*Exposure+(1+NAP|Beach), REML=TRUE, data=rikz_data)
```

### Step 2: Optimize random-effect structure

To optimize the random effects, we compare `mixed_model_IntSlope` with `mixed_model_IntOnly`. This will determine whether including a random slope for each beach improves the fit of the model to the observed data, and we couldn't decide otherwise because there isn't any prior research on whether we should expect the species richness and NAP relationship to differ by beach. We are **NOT** testing the `mixed_model_IntOnly` model against one in which there are no random effects since including a random intercept for each beach is required to account for the non-independence in the data. 

Let's get the $\text{AIC}_c$ for our two models are given by:

```{r}
AICc(mixed_model_IntOnly, mixed_model_IntSlope)
```

Based on the above, the random intercept only model is a better
fit to the data ($\text{AIC}_c$ is lower by 2.0157 units[^sleazy]). The optimal random-effect structure is thus one that includes only a random intercept for each beach but not a random slope.

### Step 3: Optimize the fixed effect structure

We now need to refit the model with the optimal random-effect structure using ML and compare different fixed effect structures. We will construct a series of models that captures all possible combinations of fixed effects terms in our saturated model in order to find the best one. Let's fit these models below and check their $\text{AIC}_c$s. Don't forget to include a model that doesn't have any fixed effects!

```{r}
# Full model with both fixed effects and their interaction
mixed_model_IntOnly_Full <- lmer(Richness~NAP*Exposure+(1|Beach), 
                                 REML=F, data=rikz_data)

# No interaction
mixed_model_IntOnly_NoInter <- lmer(Richness~NAP+Exposure+(1|Beach), 
                                    REML=F, data=rikz_data)

# No interaction or main effect of exposure
mixed_model_IntOnly_NAP <- lmer(Richness~NAP+(1|Beach), REML=F, data=rikz_data)

# No interaction or main effect of NAP
mixed_model_IntOnly_Exp <- lmer(Richness~Exposure+(1|Beach), REML=F, data=rikz_data)

# No fixed effects
mixed_model_IntOnly_NoFix <- lmer(Richness~1+(1|Beach), REML=F, data=rikz_data)

AICc(mixed_model_IntOnly_Full, mixed_model_IntOnly_NoInter,
    mixed_model_IntOnly_NAP, mixed_model_IntOnly_Exp,
    mixed_model_IntOnly_NoFix)
```

Wow, that was a lot of typing! And we only have two predictors plus an interaction! We can see that this can easily get out of hand when the model structure becomes more complex. But fear not! Obviously there is a package with a function that deals with this exact situation. Our hero today is the `MuMIn` package and the `dredge` function, which automates this dropping-of-terms process and summarizes and outputs all of the model results in one table. It is highly flexible, you can customize the criterion used (AIC, BIC, etc.), how the output is reported, what's included in the output (e.g., do you want F-stats and R^2^ to be included?), whether some terms should be represented in all models, and even only include some terms in models if other terms are included (a.k.a. dependency chain). So handy!!! In fact, _soooooooooo handy_, it is the perfect tool for $p$-hacking... but ... we can trust one another, right???

```{r}
# Argument `na.action="na.fail"` is required for `dredge` to run
mixed_model_IntOnly_Full <- lmer(Richness~NAP*Exposure+(1|Beach), REML=F, 
                                 data=rikz_data, na.action="na.fail")

mixed_model_dredge <- dredge(mixed_model_IntOnly_Full, rank=AICc)
mixed_model_dredge
```

And voila! SO. MUCH. FASTER. And look! Isn't this table just wonderful???

Based on the output above, it looks like the model that includes NAP, Exposure,
and their interaction provides the overall best fit to the data. However, this model is indistinguishable from the model without the interaction term ($\Delta \text{AIC}_c=1.552$). 

Let's try and see if we can resolve this using a _likelihood ratio test_ (LRT), which we learned about in the introductory statistics lecture. Recall that the LR test statistic has an approximate $\chi^2$ distribution with degrees of freedom equal to the difference in dimensional between _nested_ models.

```{r}
anova(mixed_model_IntOnly_Full, mixed_model_IntOnly_NoInter)
```

Ah-Ha! With $p$-value of 0.040, we reject the null hypothesis (i.e., the simpler model is a better description of the data) in favor of alternative hypothesis (the more complex model!).

### Step 4: Interpret model output

Finally, let's re-fit the best-fitting model via REML:

```{r}
mixed_model_IntOnly_Full2 <- lmer(Richness~NAP*Exposure+(1|Beach), 
                                  REML=T, data=rikz_data)
summary(mixed_model_IntOnly_Full2)
```

We see that increasing both NAP and Exposure results in a significant decrease in species richness ($p<0.05$). There is also a marginally insignificant interaction between NAP and Exposure which we won't read too much into ($p=0.051$!). Finally, while Beach is included in our model as a random effect, notice how little variation is attributed to differences between beaches relative to the model we ran in lecture 7 (~45% of residual variance vs. approx. 0). The only difference is that our current model includes Exposure as a fixed effect. This suggests that much of the variation between beaches in lecture 7 was likely attributable to differences in exposure, which is now being captured by the fixed effects.

# Model averaging

In Assignment 4, you will use data from Santangelo _et al._ (2019) who were interested in understanding how insect herbivores and plant defenses influence the expression of plant floral traits. While that was one component of the study, the main question was whether herbivores, pollinators, and plant defenses alter the shape and strength of _natural selection_ on plant floral traits. In other words, which of these 3 agents of selection (plant defenses, herbivores, or pollinators) are most important in driving the evolution of floral traits in plants?

The motivation for that experiment actually came a few year prior, in 2016, when Thompson and Johnson[^ref1] published an experiment examining how plant defenses alter natural selection on plant floral traits. They found some interesting patterns but it was unclear whether these were being driven by the plant's interactions with herbivores, pollinators, or both. This was because they didn't directly manipulate these agents: pollination was not quantified in their experiment and herbivore damage was measured observationally and thus these results were correlative. However, their experimental data provides a prime use of model selection in ecology and evolution.

The data consists of 140 observations (rows). Each row in the dataset corresponds to the mean trait value of one plant genotype (they had replicates for each genotype but took the average across these replicates) grown in a common garden. They measured 8 traits and quantified the total mass of seeds produced by plants as a measure of absolute fitness. Genotypes were either "cyanogenic" (i.e., containing plant defenses) or were "acyanogenic" (i.e., lacking plant defenses). In addition, they quantified the amount of herbivore damage (i.e., percent leaf area lost) on each plant twice throughout the growing season, although here we will only focus on the effects of plant defenses and avoid their herbivore damage measurements. We are going to estimate the strength of selection on each trait (controlling for correlations between traits due to the pleiotropic action of genes) and assess whether plant defenses alter the strength or direction of natural selection imposed on these traits. We developed the tools to do this in a previous lecture, where we learned that estimating different kinds of selection can be done by regressing fitness on standardized trait values (i.e., mean of 0 and standard deviation of 1).

Let's start by loading in the data.

```{r, echo=FALSE}
Thompson_data <- read_csv("Thompson-Johnson_2016_Evol.csv")
glimpse(Thompson_data)
```

We will now generate the global model. Remember, this should be a saturated model with all of the fixed effects and their interactions. We are including the presence of hydrogen cyanide (HCN, cyanide in model below), all standardized traits and the trait by HCN interactions as fixed effects in this model. There are no random effects in this model so we can go ahead and use `lm()`.

```{r}
# Create saturated model
GTSelnModel.HCN <- lm(RFSeed ~ VegGrowth.S*cyanide + BnrLgth.S*cyanide +
                          BnrWdt.S*cyanide + FrstFlwr.S*cyanide + 
                          InflNum.S*cyanide + FlwrCt.S*cyanide + 
                          InflWdt.S*cyanide + InflHt.S*cyanide,
                      data = Thompson_data)
```

Next, we will perform our model selection based on AIC~c~ (due to low sample sizes). We automate this process using the `dredge()` function from the `MuMIn` package. We warned earlier that this dredging approach has been criticized. However, in this case it's reasonable: we know from work in other systems that all of these traits could conceivably experience selection, and we know that that selection could vary due to plant defenses. In other words, all terms in this model represent biologically real hypotheses. Let's go ahead and dredge.

```{r}
GTSelnModel.HCN <- lm(RFSeed ~ VegGrowth.S*cyanide + BnrLgth.S*cyanide + 
                          BnrWdt.S*cyanide + FrstFlwr.S*cyanide + 
                          InflNum.S*cyanide + FlwrCt.S*cyanide + 
                          InflWdt.S*cyanide + InflHt.S*cyanide,
                      data = Thompson_data, 
                      na.action="na.fail")

GTmodel_dredge <- dredge(GTSelnModel.HCN,
                         beta = F, # or "none" corresponds to unstandardized coefficients
                         evaluate = T, # evaluate/rank models
                         rank = AICc) # rank function
```

Let's have a look at the first few lines returned by `dredge()`. Let's also print out how many models were compared.

```{r}
head(GTmodel_dredge)
nrow(GTmodel_dredge)
```

The output tells us the original model and then provides a rather large table with many rows and columns. The rows in this case are different models with different combinations of predictors ($n = 6,817$ models). The columns are the different terms from our model, which `dredge()` has abbreviated. The numbers in the cells are the estimates (i.e. beta coefficients) for each term that is present in the model; blank cells mean that term was not included in the model. The last 5 columns are important: they give us the degrees of freedom for the model (a function of the number of terms in the model), the log-likelihood of the model, the AIC score, the delta AIC, and the AIC weights. The $\Delta AIC$ is the difference between the AIC score of a model and the AIC score of the top model. The weight can be thought of as the probability that the model is the best model given the candidate set included in the model selection procedure.

We can retrieve the top model using the `get.models()` function and specifying that we want to top model using the `subset` argument. We need to further subset this output since it returns a
list.

```{r}
top_model <- get.models(GTmodel_dredge, subset = 1)[[1]]
summary(top_model)
```

But how much evidence do we actually have that this is the _best_ model? We have over 6,000 models, so it's unlikely that only one model explains the data. From the `dredge` output we can see there is little difference in the AIC and weights of the first few models. 

Is there really much of a difference between two models who’s AIC differ by only 0.14 points? How do we decide which model(s) to interpret? Statisticians have thought about this problem and it turns out that models with delta AIC (or other criterion) less than 2 are considered to be just as good as the top model and thus we shouldn’t just discount them. In fact, the top 5 models all have AIC~c~ scores within 2 units of the top model.

We could use likelihood ratio test(s) to try to test if more complex models are better descriptions of the data. Alternatively, we could use the weights: if a model has weight greater or equal to 95% then it is likely to be the top model. We can generate a “credibility” set consisting of all models whose cumulative sum of AIC weights is 0.95. In any case, the point is that we have no good reason to exclude models other than the top one when the next models after it are likely to be just as good.

To get around this, we will perform what’s called _model averaging_. This allows us to average the parameter estimates across multiple models that are performing equally well, and avoids the issue of model uncertainty. Let’s do this below by averaging all models with a delta $\text{AIC}_c \leqslant 2$.

```{r}
summary(model.avg(GTmodel_dredge, subset = delta <= 2))
```

The first part of the output breaks down which terms are part of which models and provides descriptive statistics for these models. Parameter estimates from the model averaging are also returned. Note there are two sets of estimates: the _full coefficients_ set terms to 0 if they are not included in the model while averaging, whereas the _conditional coefficients_ ignores the predictors entirely. The full coefficients are thus more conservative and it is best practice to interpret these.

To get a sense of what's going on, a visual might be helpful. (But there are 6000 regressions to deal with! However will we plot them all??) Although the code is admittedly somewhat clunky^[Whatever works! :P], following chunk plots how predicted fitness (RFSeed) changes as a function of one of standardized trait (BnrLgth.S) value for the top 100 models (in gray) and using the full coefficients estimated after averaging the top five models, i.e., those with $\Delta \text{AIC}_c < 2$ (in black). As expected, the averaged model lives somewhere between the top models. Given the slope of the averaged model, it seems the top fitting models are ones which predict a very strong relationship between fitness and the focal trait.

```{r}
### can ignore this code! focus on the plot.

BnrLgth.S_predictions <- seq(min(Thompson_data$BnrLgth.S),
                               max(Thompson_data$BnrLgth.S), 
                               length=100)

predct_values <- data.frame(BnrLgth.S = BnrLgth.S_predictions,
                       VegGrowth.S = 0, cyanide = 0, BnrWdt.S = 0, FrstFlwr.S = 0, 
                       InflNum.S = 0, FlwrCt.S = 0, InflWdt.S = 0, InflHt.S = 0)
# predicted values...only varying BnrLgth.S, all other covariates set = 0 for plotting purposes

predictions <- NULL

for (i in 1:100){
  model <- get.models(GTmodel_dredge, subset = i)
  cbind(RFSeed_predicted = predict(model[[1]], newdata = predct_values), 
        BnrLgth.S = BnrLgth.S_predictions,
        index = i) -> predictions[[i]]
}
# only top 100 models

predictions_all <- do.call(rbind, predictions)
predictions_ave <- data.frame(BnrLgth.S = BnrLgth.S_predictions,
                              RFSeed_predicted = 0.9786538 + 0.225054*BnrLgth.S_predictions)

as.data.frame(predictions_all) %>% 
  ggplot() +
  geom_line(aes(x = BnrLgth.S, y = RFSeed_predicted, group = as.factor(index)),
            alpha = 0.5, color = "lightgray") +
  geom_line(data = predictions_ave, inherit.aes = F,
            aes(x = BnrLgth.S, y = RFSeed_predicted), color = "black", size = 2)

# note: all other trait values are =0 for the purpose of visualization
```

# Summary...and some words of caution

- As we have seen, many problems can be formulated in terms of choosing between candidate models or deciding if a given model is an appropriate description of the data. Model selection is important and active area of statistics, and is complementary to the statistics we have so far discussed in the course.
- The tools to preform model selection will depend on the application. It is often difficult (or impossible) to fit linear models to high dimensional data, so tools like ridge and LASSO regression are very useful in that they 1) get the job done and 2) can be used to preform feature selection.
-  Model selection depends on the set set of models considered. You can't identify a model as being the "best" fit to the data if you exclude it from the analysis.
- Statistical significance and biological significance are different things!
- Stepwise selection is problematic. Don't do it. It gives rise to biased regression coefficients, $p$ values, etc. and provides little insight into how the world works.
- DO \*clap\* NOT \*clap\* USE \*clap\* `dredge` \*clap\* TO \*clap\* $p$ \*clap\* HACK \*clap\*.

[^interaction]: Note that models that include interaction terms will automatically also include the interacting terms as independent predictors. In other words, you cannot have a model that contain _only_ the interaction term but not the independent effects.

[^sleazy]: These AIC~c~ score are _close_! It barely made our predetermined cut off of 2 units. This is a very good example of why it is important to decide on a "recipe" for model selection before we do anything. Let's pretend we are very attached to the random slope. We see that the AIC~c~ scores are really close! We might think to ourselves "you know what, maybe 2 units is too stringent, let's set the cut off at 3 instead" and then proceed to happily not-reject the intercept-slope model because it's indistinguishable from the intercept-only model. Of course, there is nothing stopping anybody from just swapping out the recipe under the table half way through without telling anyone and just pretending it was like that all along.

# References and additional reading

  1. [Johnson & Omland 2004. Model selection in ecology and evolution. _Trends in Ecology and Evolution_.](https://www.sciencedirect.com/science/article/abs/pii/S0169534703003458)
  2. Burnham & Anderson 2002. _Model Selection and Multimodel Inference_ (2nd ed.).
  3. Zuur et al. 2009. _Mixed Effects Models and Extensions in Ecology with R_.