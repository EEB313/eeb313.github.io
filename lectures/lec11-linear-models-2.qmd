# Linear models II## Lesson preamble> ### Learning objectives>> - Learn how dependent data can be modeled, and the sitations in which one may encounter such data.> - Understand the structure, assumptions, and implementation of GLMs.> - Understand the structure, assumptions, and implementation of LLMs.> - Learn how to use simulation to inform what data that should be collected, and how.> > ### Lesson outline> -   Generalized linear models>     - Structure and assumptions, including interpretation of the effects>     - Implement logistic regression using dataset of Farrell and Davies (2019)>     - Other types of GLMs (Poisson, negative binomial, etc.)> -   Power analysis!> -   Dealing with dependent data>     - Splitting the data into groups...>     - Controlling for phylogeny>     - Linear mixed models: structure and assumptions>       - Difference between fixed, random effects; examples>       - Implement mixed models using sexual size dimorphism data```{r, message=F, warning=F}library(tidyverse)library(broom)library(lme4)```## Generalized linear models: theory and examplesSo far we have seen how linear models can be used to describe relationships between a response variable and predictors when the data is normally distributed or can be transformed so that this assumption is not violated. What if we were, say, interested in a *binary* response (e.g., infection status of a host with a particular parasite) and how it changes with a continuous predictor (e.g., age of the host)? In this case, one can use a special kind of linear model called *logistic regression* to estimate the additive effect of predictor(s) on the binary response. Logistic regression is a special kind of **generalized linear model**.### GLMs: structure and assumptionsGeneralized linear models describe how the mean of a response variable changes as a function of the predictors when important assumptions of the linear modeling framework (normality, constant error variance, etc.) are violated. In particular, **GLMs allow us to work with data that are not normal, whose range is restricted, or whose variance depends on the mean.** The latter might be important if, say, larger values of the response were also more variable.A GLM models the transformed mean of a response variable $Y_i$ as a linear combination of the predictors $x_1,\dots,x_p$. The goal of using a GLM is often to estimate how the predictors (e.g., sex and previous exposure to the disease) affect the response (e.g., infection status). sThe transformation of the response is done using a "link" function $g(\cdot)$ that is specific to the distribution which used to model the data. Written out, a GLM is of the form$$g(\mu_i) = \beta_0 + \beta_1 x_{1i} + \cdots \beta_p x_{pi}.$$The link functions for the Normal, Gamma, Exponential, Poisson, and Multinomial distributions are known. In general, GLMs apply when the data are modeled using a member of the [exponential family](https://en.wikipedia.org/wiki/Exponential_family). The distributions will have a mean parameter $\mu$ and, sometimes, a parameter $\tau$ which characterizes the dispersion around the mean. _GLMs are fitted using by maximizing the likelihood function that results from assuming the data arise from a distribution in the exponential family (with a mean and dispersion that depend on the predictors), using using numerical optimization methods._### Interpretation of the effectsNotice that, in a GLM, because the mean of the response been transformed in a particular way, the coefficients must be interpreted carefully. In particular, $\beta_{j}$ is how a per-unit change in $x_{j}$ increases or decreases the _transformed_ mean $g(\mu_i)$.### Example: logistic regressionIn a previous class, we estimated the probability of death given infection for the wild boar when infected with viruses in the family _Coronaviridae_. In your current homework, you have been tasked with extending that model to all host and parasite family combinations. Excitingly, the dataset which have used includes information about the mean evolutionary isolation of all hosts that are infected with a parasite from all other hosts which are known to be infected.Farrell and Davies tested if the mean evolutionary isolation affected the probability of death. They used a complex model to control for sampling bias and other confounding aspects of the data. We will ignore those complexities and see if, using a GLM, we can recapitulate their findings. To get started, load the `disease_distance.csv` dataset.```{r}disease_distance <- read_csv("data/disease_distance.csv")disease_distance %>%   mutate(AnyDeaths = case_when(Deaths > 0 ~ 1,                               Deaths == 0 ~ 0)) -> DataBernDataBern |>  ggplot(aes(x = EvoIso, y = AnyDeaths)) +   geom_point()```These are binary data. We CANNOT use a linear model, as this assumes the data are Normal. In fact, these data are distributed according to a Bernoulli($p$) distribution. This is a member of the exponential family and the type of generalized linear model when the data are distributed in this way (i.e., are binary) is called **logistic regression**.The mean of Bernoulli($p$) data is just $p$, and the link function for $p$ is$$ \text{logit}(p) = \log\frac{p}{1-p}.$$$\text{logit}(p)$ is called the *log-odds*, which can be thought of as a likelihood the response takes on the value one. In logistic regression, the log-odds ratio is modeled as a linear combination in the predictors:$$ \text{logit}(p_i) =  \beta_0 + \beta_1 x_{1i} + \cdots + \beta_p x_{pi}.$$Notice that increasing $x_j$ by one unit results in change $\beta_j$ to the link-transformed response.This is how the effect sizes are interpreted for GLMs such as this one.```{r}model <- glm(AnyDeaths ~ EvoIso,              family = "binomial",              data = DataBern)summary(model)```#### ChallengeHow do we interpret the regression coefficient above?#### ChallengeWhat are the log-odds of death if the evolutionary isolation of hosts is $EI = 200$? How much does this quantity change if the evolutionary isolation were to increase by 20 million years?#### Visualizing the fitted model```{r}DataBern |>  ggplot(aes(x = EvoIso, y = AnyDeaths)) +   geom_point() +  geom_smooth(method = "glm",               method.args = list(family = "binomial")              )### base R implementationEvoIso <- seq(0, 200, 0.1)predicted_prob <- predict(model, list(EvoIso = EvoIso), type = "response")plot(DataBern$EvoIso, DataBern$AnyDeaths)lines(EvoIso, predicted_prob)```### Other GLMsHere are some common types of response variables and their corresponding distributions:-   **Count data**: the **Poisson** distribution-   **Over-dispersed count data** (when the count data is more spread out than "expected"): the **negative binomial** distribution-   **Binary data** (two discrete categories): the **binomial** distribution-   Counts of occurrences of $K$ different types: the **multinomial** distribution-   **Times** between $r$ events: the **gamma** distribution, which is equivilent to the exponential when $r=1$You will have the opportunity to implement such models in your homework and on the challenge assignment!## Power analysisWhen designing experiments or how best to collect data, it is best to think about the model you will fit and the kind of experiment you need to design in order to have sufficient power to detect an effect of interest. For example, if we thought that age affected the likelihood that a host dies of a disease, then we would likely fit a logistic regression-type model. By _simulating_ data from such a model, we can determine- the minimum sample size required to reliably estimate an effect of a certain size- the minimum sample size required to detect an effect of a certain size at a fixed significance level- the minimum effect size that can be detected at a fixed significance level and sample size### ExampleBelow is an example of a simulation in which binary disease data (death/no death; 0/1) are simulated hosts of various ages, assuming the the log-odds of disease is a linear function of age. (Notice that we assume that host lifetimes are exponentially distributed with rate parameter $\lambda = 1/10$ years. This means that hosts, in this simulation, live for an average of 10 years.) We then fit a logistic regression to this data to determine the effect of age on disease risk.```{r}data_generator <- function(sample_size = 100, effect = 0.1){  age <- rexp(n = sample_size, rate = 1/10)  linear_predictor <- effect*age  prob <- 1/(1+exp(-linear_predictor))    disease_status <- c()    for (i in 1:length(prob)){  disease_status[i] <- rbinom(n = 1, size = 1, prob = prob[i])  }    return(data.frame(age = age, disease_status = disease_status))}data <- data_generator()data %>% pivot_longer(! age) %>%   ggplot(aes(x = age, y = value)) + geom_point() +   geom_smooth(method = "glm", method.args = list(family = "binomial")              ) + labs(y = "prob. of disease (i.e., disease status =1)")model <- glm(disease_status~scale(age), family = binomial, data = data)summary(model)```Next, we will write a function that performs a **power analysis**. We will use this function determine the sample size that is required so that simulating the age-disease data repeatedly we identify a significant effect of age on disease status (i.e., reject the null hypothesis) at level $\alpha = 0.01$ *at least $95\%$ of the time*.```{r}power_analysis_function <- function(sample_size){    sims <- 1000  pvalues <- c()  for (i in 1:sims){  data <- data_generator(sample_size)  model <- glm(disease_status~scale(age), family = binomial, data = data)  pvalues[i] <- summary(model)$coefficients[2,4]  }    power_estimate <- length(which(pvalues < 0.01))/length(pvalues)    return(power_estimate)}sample_sizes <- seq(150,200,10); power_estimates <- c()for (i in 1:length(sample_sizes)){  power_estimates[i] <- power_analysis_function(sample_size = sample_sizes[i])}knitr::kable(cbind(n = sample_sizes, power = power_estimates))```## Dealing with dependent data!To illustrate how mixed models work and what kinds of questions we can answer using them, we will use the sexual size dimorphism data which we analyzed last class. Recall that we did NOT find a significant effect of sex on the average body size. There was no effect of the interaction between Order and sex on body size. Indeed, this matches what you saw in the homework questions where you had to visualize the data --- most of the variation in body size was _between_ orders.In the models we built, we ignored a pretty important fact about the data: species have a common history (i.e., phylogeny). Thus, observations are not independent! This can make drawing inferences from comparative data difficult. **We will address how to deal with the non-independence of data (due to a common history of species, replication in blocks, etc.) using three approaches.**But, first, let's download the data we will use in this section!```{r}SSDdata <- read_csv("data/SSDinMammals.csv")``````{r}mammal_length <- SSDdata %>%  select(c("Order", "Scientific_Name", "lengthM", "lengthF")) %>%  pivot_longer(c(lengthM, lengthF), names_to = "sex", values_to = "length",               names_pattern = "length(.)")mammal_mass <- SSDdata %>%  select(c("Order", "Scientific_Name", "massM", "massF")) %>%  pivot_longer(c(massM, massF), names_to = "sex", values_to = "mass",               names_pattern = "mass(.)")mass_nodup <- mammal_mass %>%   group_by(Scientific_Name, sex) %>%  distinct(Scientific_Name, sex, .keep_all = TRUE)length_nodup <- mammal_length %>%   group_by(Scientific_Name, sex) %>%  distinct(Scientific_Name, sex, .keep_all = TRUE)mammal_long <- full_join(mass_nodup, length_nodup,                          by = join_by("Scientific_Name", "sex", "Order")) |>  drop_na()glimpse(mammal_long)```### Group-by-group analyses**One first way we can handle dependent data is to split observations into groups such that, within each group, observations are independent (or approximately so).** This is what we did last class when we fit order-specific regression coefficients of sex on body size. We saw that ALL order-specific effects had confidence intervals which overlapped zero!```{r, warning=F}# run linear model of size on sex for EACH OrderOrder_specific_models <-   mammal_long |>   group_by(Order) |>  do(model = tidy(lm(log(mass) ~ sex, data = .), conf.int = T))  # get coefficients for each OrderOrder_specific_models |>  unnest()# visualize effects, CIs, and p valuesOrder_specific_models |>  unnest() |>  subset(term == "sexM") |>  group_by(Order) |>  ggplot(aes(y = Order, x = estimate,              xmin = conf.low,             xmax = conf.high             )         ) +  geom_pointrange() +  geom_vline(xintercept = 0, lty = 2)```No effect of sex on body size, for any of the orders!#### ChallengeHow would you adjust the previous plot to show the estimated intercepts AND the effects of sex?```{r}Order_specific_models |>  unnest() |>  # subset(term == "sexM") |>  group_by(Order) |>  ggplot(aes(y = Order, x = estimate,              xmin = conf.low,             xmax = conf.high,             color = term             )         ) +  geom_pointrange() +  geom_vline(xintercept = 0, lty = 2)```#### ChallengeHow would you retrieve model fits for each Order in base R?```{r, include=F}for (i in 1:nrow(Order_specific_models)){  print(summary(Order_specific_models$mod[[i]]))}```#### ChallengeAdjust the plot above so that it the size of the estimates depend on the number of observations in an Order? _Hint: determine the number of observations per Order and then use the `merge()` function with the `Order_specific_models` tibble. To adjust the range of point sizes, use `scale_size()`._```{r, include=F}mammal_long |>   group_by(Order) |>  summarise(number_obs = n()) |>  merge(Order_specific_models) |>  unnest() |>  ggplot(aes(y = Order, x = estimate,              xmin = conf.low,             xmax = conf.high,             color = term,             size = number_obs             )         ) +  geom_pointrange() +  geom_vline(xintercept = 0, lty = 2) +  scale_size(range = c(0, 2))```#### Pros and consThere are several advantages to preforming a group-by-group analysis:- **Fitting more complex models (e.g., those with random effects) can be difficult.** There may be convergence issues, and interpretation of effects and $p$-values can be tricky.- The group-by-group analysis is **robust in the face of unbalanced data** (i.e., when there are more observations for some groups than others).- **The conclusions from a group-by-group analysis are _conservative_.**- The group-by-group analysis is **fairly easy to implement**.Among the disparages to this approach are the following:- With fewer samples per group, the analysis **may be under-powered**.- Splitting the data into groups means there are more coefficients to estimate, and thus confidence intervals to be estimated and hypothesis tests to be performed. This means there is a greater chance that we obtain a **spurious association**.*- It is **difficult to draw conclusions about the variance between groups.** In some applications, this is of interest. In others, it is not.*One solution is to adjust the significant level based on the number of tests conducted. If $k$ hypotheses are tested, an [common adjustment](https://en.wikipedia.org/wiki/Bonferroni_correction) is to set $\alpha = 0.05/k$.### Controlling for phylogenyA common reason data are dependent in biology is that **species share a common history of descent with modification.** When we assume that the observations are independent, we make implicit assumptions about the degree to which the characters at the tips of a phylogeny have underwent independent evolution. Sometimes, when species are diverged by many millions of years and traits evolve quickly, it is reasonable to ignore the phylogenetic constraints on the data. In other cases, it is essential to consider the role of phylogeny.Several methods can control for phylogeny (if known). In fact, such methods can _use_ information in the branching pattern of species to draw inferences about the evolution of ecologically-important traits (such as body and brain size, dispersal rate, etc.). We will not discuss how to implement phylogenetic comparative methods, but it is good to know they exist and how, at a surface level, they work.Intuition for how PCMs work is easiest to grasp when we consider a tree with $n$ species at the tips. If we know the pairwise divergence times for all species, then we can _transform_ the data so that observations are independent by looking at all _differences_ of traits. Not all differences are equally informative; if species have diverged a long time ago, there has been more time for differences to build up. PCMs account for this by specifying how the distribution of trait differences between species $i$ and $j$ depends on the time that has elapsed since these species diverged, In particular, the larger the divergence time, the greater the variance in $Y_i-Y_j$, the difference in trait values between species $i$ and $j$. A simple way to do this is to write $\sigma_{ij} = \sigma^2/T_{ij}$.^[Note: this gives rise to a likelihood function that is functionally VERY similar to the one for the linear model with constant error variance.]### Random effects!Another way one can account for dependent data is by including _random effects_. Random effects are often used to control for the fact that observations are clustered (e.g., trait data for species belonging to a higher taxonomic unit, measurements of plant biomass from plots of land).#### Structure and assumptionsA common way models with random effects are formulated is as follows:$$Y_{ij} \sim \text{Normal}(\beta_0 + \beta_1 x_{1ij} + \cdots + \beta_{p} x_{pij} + b_{0i} + b_{1i} z_{1ij} + \cdots + b_{qi} z_{qij}, \sigma_{ij}^2),$$where $ij$ is the $i$th observation from the $j$th cluster and$$b_{ki} \sim \text{Normal}(0,\tau_k^2).$$This gives rise to a distribution for $Y$ which depends on the values of the random effects $b_0,\dots,b_q$. Under the hood, methods that fit models of this form numerically maximize a version of the likelihood function that results from these assumptions.#### ChallengeIn each of the following examples, which effects might be reasonably treated as fixed vs. random? Justify your answer.1. Suppose we are working with rodents that have been infected with an evolved strain of the parasite that causes malaria. Some rodents have been treated with a prospective vaccine and others sham-vaccinated. We are interested if a proxy for virulence (e.g., density of infected red blood cells) depends on vaccination status.2. Suppose we conduct the same experiment, except rodents are caged are sets of four.3. Suppose we measure the time between sightings of a raccoon in a Toronto neighborhood using six camera traps (strategically placed in the neighborhood). We do this for year, and want to ask if mean daily temperature predicts the frequency of raccoon occurrence.4. Suppose that, every year, we go to the Amazon and measure the number of bird calls that come from $n=30$ trees. Assume that the trees are far enough apart we can treat them as independent. We measure the number of birds in a tree, characteristics of the tree (cover, height, width, age), the year in which a measurement was done, and the mean temperature that year. We want to know if tree cover affects the frequency of bird calls, and if it interacts with temperature.For more on the difference between fixed and random effects, check out the following resources- [this Cross Validated post](https://stats.stackexchange.com/questions/4700/what-is-the-difference-between-fixed-effect-random-effect-in-mixed-effect-model)- [this Dynamic Ecology post](https://dynamicecology.wordpress.com/2015/11/04/is-it-a-fixed-or-random-effect/)- [this book chapter](https://bookdown.org/steve_midway/DAR/random-effects.html)#### Using `lme4` to fit random and mixed effect modelsWe will start by fitting a model of log body size on log length where the intercept is random depending on the Order. Based on a quick visualization of the data, such a model may be appropriate.```{r}ggplot(data = mammal_long, aes(y = log(mass),                                x = log(length)                               )       ) +     geom_point(aes(color = Order), size = 3) -> pp ``````{r, include=F}# no Order-dependence:p + geom_smooth(method = "lm")# intercept AND slope fixed and specific to the Order:p + geom_smooth(method = "lm") + facet_wrap(~Order) ```To fit regress the response on a set of fixed effects with random _intercepts_ that depends the values assumed by a random effect `x`, we call `lmer` and write a linear model with `(1|x)`.```{r}## random intercept for Ordermodel <- lmer(formula = log(mass) ~ log(length) + (1|Order), data = mammal_long)summary(model)fixef(model)ranef(model)```From top to bottom, the output is telling us that the model was fit using a method called REML (restricted maximum likelihood). It returns information about the residuals, random effects, and fixed effects. The output also tells us about the **estimated variance for the random effects** in the model. Here, the variance associated with Order is 4.256. The variance explained by Order is 4.256/(4.256+0.847) _after controlling for the fixed effects_. Note the denominator here is the total variance, including from the residuals. As usual, we also have information about the fixed effects.To visualize the model, we can predict the overall and Order-specific relationship of log mass on log length for all of the Orders represented in the data.```{r}mammal_long |> ungroup() |> select(mass, length, Order) |>   mutate(fit.m = predict(model, re.form = NA), # does not include random effects         fit.c = predict(model, re.form = NULL) # includes random effects         ) ->  predicted_valuespredicted_values |>  ggplot(aes(x = log(length), y = log(mass), color = Order)) +  geom_point(size = 3) +  geom_line(inherit.aes = F, aes(x = log(length), y = fit.c, color = Order), size = 1) +  geom_line(inherit.aes = F, aes(x = log(length), y = fit.m), color = "black", size = 2)```The thick black line corresponds to the fitted values associated with the fixed-effect component of the model. The colored lines correspond to the fitted values estimated for each Order. Perhaps a model with Order-specific random intercepts AND slopes would be better. We fit this model using the code below. The key difference in syntax is that we write `(1+fixed effect|random effect)` to indicate that the random effect has an effect on both the intercept _and_ slope of the response on the fixed effect.```{r}## random intercept and slope for Ordermodel2 <- lmer(formula = log(mass) ~ log(length) + (1+log(length)|Order), data = mammal_long)summary(model2)fixef(model2)ranef(model2)``````{r}mammal_long |> ungroup() |> select(mass, length, Order) |>   mutate(fit.m = predict(model2, re.form = NA),         fit.c = predict(model2, re.form = NULL)         ) ->  predicted_valuespredicted_values |>  ggplot(aes(x = log(length), y = log(mass), color = Order)) +  geom_point(size = 3) +  geom_line(inherit.aes = F, aes(x = log(length), y = fit.c, color = Order), size = 1) +  geom_line(inherit.aes = F, aes(x = log(length), y = fit.m), color = "black", size = 2)```Why does this look like a mess? As stated above, **mixed models do not work well when datasets are unbalanced.** Indeed, the number of observations in the different Orders are quite variable.#### ChallengeRegress log body size on sex with Order as a random effect that affects the intercept AND slope of the sex-size relationship.```{r, include=F}Model <- lmer(log(mass) ~ sex + (1+sex|Order), mammal_long)mammal_long |> ungroup() |> select(Scientific_Name, mass, sex, Order) |>   mutate(fit.m = predict(Model, re.form = NA),         fit.c = predict(Model, re.form = NULL)         ) ->  predicted_valuespredicted_values |>  ggplot(aes(x = sex, y = log(mass))) +  geom_line(aes(group = Scientific_Name), color = "lightgrey") +  geom_point(color = "lightgrey", size = 2) +  geom_point(inherit.aes = F, aes(x = sex, y = fit.c, color = Order), size = 3) +  geom_line(inherit.aes = F, aes(x = sex, y = fit.c, color = Order, group = Order), size = 2) +  geom_point(inherit.aes = F, aes(x = sex, y = fit.m), color = "black", size = 3) +  geom_line(inherit.aes = F, aes(x = sex, y = fit.m, group = ""), color = "black", size = 3) +  theme_classic()```