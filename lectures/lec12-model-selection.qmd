# Model selection

```{r message=F,warning=F}
library(tidyverse)
library(MuMIn)
library(glmnet)
```

## Lesson preamble

> ### Learning objectives
>
>  - Understand the difference between likelihood estimation/inference and model *selection*
>  - Develop familiarity with common model selection approaches
>  - Understand intuition for and the use of common information theoretic model selection criteria
>  - Perform model selection on LMs, GLMs, LMMs

## What is model selection?

So far, we have covered central problems in statistics: determining what processes and parameters gave rise to data (*estimation*), and quantifying uncertainty in those estimates using confidence intervals (*inference*). As we have seen, however, to any confidence interval there is an associated hypothesis test. Inference and hypothesis testing, then, involve deciding if a particular set of parameter values could have plausibly given rise to the data. 

It should come as no surprise, since both estimation and inference involve decision making, that in statistics we often interested in deciding if the models we build are appropriate descriptions of how the world works (given the data we have and use to fit those models), or what among a set of candidate models is the "best". This practice is called *model selection* and is the focus of this lecture.

### Examples

- Hypothesis testing is a kind of model selection. For example, for data $x_1,\dots,x_n \sim f(x|\theta)$, testing $H_0 \colon \theta = \theta_0$ vs $H_1 \colon \theta \neq \theta_0$ is equivalent to choosing between models $\mathcal{M}_0 \colon f(x|\theta)$ and $\mathcal{M}_1 \colon f(x|\theta_0)$.

- Suppose we regress $y$ on the 1st, 2nd, $\dots$, $p$th powers of a covariate $x$:

$$y = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_p x^p.$$

\vspace{12pt}

This gives rise to a sequence of models $\mathcal{M}_1,\mathcal{M}_2,\dots,\mathcal{M}_p$ of the data generative process. Which model is the best description of the data? Although the full model is more flexible in that it has more parameters than, say, the model in which all second and higher order terms are $=0$, it is more prone to overfitting. Choosing between a sequence of nested linear models like this is a classic model selection problem.

- Suppose we would like to model the relationship between expression (i.e., the number of transcripts produced) of _each_ coding gene in the human genome and, say, height. If there are $p$ genes for which we have measurements and only $n \ll p$ observations, it is not possible to fit

$$y_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_p x_{pi}.$$

\vspace{12pt}

(The reason is that there are infinitely many solutions to the likelihood maximization problem.) In this case, we might want to select a subset of the covariates which best explain the available data. Of $x_1,\dots,x_p$, which are most informative? This too is a kind of model selection problem.

## Stepwise regression...and why you should not use it.

Suppose you want to model the effects of multiple predictors and their interactions on a particular response variable of interest. It may be tempting, then, to test all possible combinations of predictors and see which set fits the data the *best* (though this really sounds like p-hacking...). Stepwise selection involves iteratively adding and removing predictors according to some criterion (e.g., variance explained). The steps for forward stepwise regression are as follows:

1. Let $\mathcal{M}_0$ be a null model, i.e., one with no predictors.
2. For $k=0,1,\dots,p-1$, fit all models which add one predictor to those in model $\mathcal{M}_k$.
3. Choose the best among the $p-k$ models in step (2) and call it model $\mathcal{M}_{k+1}$. This is done according to some criterion. We might, e.g., choose the model with the greatest $R^2$.
4. Select from models $\mathcal{M}_0,\dots,\mathcal{M}_p$ using, for example, AIC (which we dive into later in this lecture).

**We, however, urge you to AVOID stepwise selection and to be critical of analyses which use it. No statistics can be a substitute for good theory which is informative about the predictors which have predictive power _because_ they are biologically meaningful. When choosing between models, careful consideration of the what predictors are informative and _why_ is key.**

\vspace{6pt}

For more on the problems with automated stepwise selection methods, see [here](https://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection?noredirect=1&lq=1).

## The Akaike information criterion (AIC)

Rather than (mindlessly) adding and removing predictors, it is important to build models that are based in evidence from good theory and the literature. We can then weigh how much the data supports each model using the *Aikake information criterion (AIC)* 

Suppose we have data $y_1,\dots,y_n$ that are drawn from a distribution $p$ and a set of candidate models

$$\mathcal{M}_j = \{p_j(y|\theta_j)\}.$$

\vspace{6pt}

It is possible under this setup to find the maximum likelihood estimators for each of the candidate models; it is, however, difficult to compare these models in that the parameters underlying each model might not match (i.e., the models may not be nested). The *AIC* overcomes this issue, and despite having a lengthy and complicated derivation, is a metric which balances two things: 1) the goodness-of-fit of the model and 2) the number of parameters fitted. There are other methods for model selection that are similar to AIC (such as BIC, $C_p$) that follow similar principles but use different penalties.

The intuition and formula for the AIC is as follows. If $\hat{\theta}_{j,\text{MLE}}$ is the MLE for $\theta$ under model $\mathcal{M}_j$, then we can measure the distance between the ground truth (i.e., distribution $p$ of the data) and fitted model $\hat{p}_j = p_j(y|\hat{\theta}_{j,\text{MLE}})$ using a metric called the Kullback-Leibler divergence:

$$D_{KL}(p, \hat{p}_j) = \int p(y) \log p(y) \text{d} y - \int p(y) \log \hat{p_j}(y) \text{d} y. $$

\vspace{6pt}

**Minimizing the Kullback-Leibler divergence (distance) between the ground truth and density $j$ is a principled way to preform model selection, and forms the basis for the AIC.** Note that minimizing only involves the second integral, and we can estimate the integral with an average

$$\frac{1}{n} \log L_j(\hat{\theta}_{j,\text{MLE}}) = \frac{1}{n} \sum_{i=1}^n \log p_j(y_i|\hat{\theta}_{j,\text{MLE}})$$

Importantly, AIC corrects for the fact this is an unbiased estimator of the divergence by adding $d_j/n$, where $d_j = \text{dim}(\mathcal{M}_j)$. This term is what penalizes models with a large number of parameters. So,

$$\text{AIC} = - 2 \log L_j(\hat{\theta}_{j,\text{MLE}}) + 2 d_j.$$

\vspace{6pt}

Notice we have multiplied the preceding quantities by $-2n$ to get the above expression; this does not change anything, and is largely for historical reasons. Based on the AIC expression, it is clear 1) the higher the likelihood, the lower the AIC; 2) introducing more parameters into the model without changing the likelihood results in a greater value for the AIC. **The balance between goodness-of-fit (likelihood) and the number of parameters (the potential to overfit) is what AIC tries to optimize in choosing between candidate models.** As we have shown here, that balance is struck by minimizing the distance between candidate models and the ground truth, while correcting for the bias introduced by having models with different numbers of parameters.

### $\text{AIC}_c$ for small sample sizes

It is sometimes convenient to, when sample sizes are small ($n < 40$ is a commonly used rule-of-thumb) use the following metric to choose between candidate models:

$$\text{AIC}_c = \text{AIC} + \frac{2d_j(d_j+1)}{n-d_j-1}$$

## Example with a linear model

In 2016, [Afkhami and Stinchcombe](https://doi.org/10.1111/mec.13809) investigated the effects of multiple microbial mutualists on the performance of the model legume, *Medicago truncatula*. These microbial mutualists offer different rewards to their plant host; rhizobia bacteria provide fixed nitrogen; mycorrhizal fungi provide phosphorus. Plants were inoculated with either both microbial partners, one microbial partner, or none. Measures of plant performance such as aboveground and belowground biomass, and mutualist performance (nodule count and biomass) were collected.

```{r}
medicago<-read.csv("data/medicago.csv")

# convert to factor
cols<-c("Block","Plant_ID","Myco_fungi","Rhizobia")
medicago[cols]<-lapply(medicago[cols],factor)

str(medicago)
```

Both rhizobia and mycorrhizal fungi interact with the root structures of their plant host. Rhizobia are housed in specialized root structures called nodules while mycorrhizal fungi can make filamentous branches, hyphae, on and inside plant roots. Therefore, we could be interested in testing the effects of both microbial partners on belowground (root) biomass. Let's plot the data!

```{r}
ggplot(aes(x = Myco_fungi, y = Root_mass, colour = Rhizobia), data = medicago) +
  geom_boxplot()
```

It looks like plants inoculated with rhizobia and fungi had higher root biomass. Let's look at the model.

```{r}
# main effects + interaction
root1 <- lm(Root_mass ~ Myco_fungi + Rhizobia + Myco_fungi:Rhizobia, data = medicago)
# or simplify to Myco_fungi*Rhizobia

summary(root1)


# no significant interaction! perhaps a model with just the main effects?

# main effects only
root2 <- lm(Root_mass ~ Myco_fungi + Rhizobia, data = medicago)

summary(root2)
```

Unfortunately, the models do not show significant differences between inoculation treatments. But let's compare these models using AIC. 

```{r}
AICc(root1, root2)
```

Based on the above, the model without the interaction term is a better fit to the data (lower AIC score). However, the scores for both models are really close! How do we decide which model(s) to interpret? Statisticians have thought about this problem and it turns out that models with delta AIC (or other criterion) less than 2 are considered to be just as good as the top model and thus we shouldnâ€™t just discount them.

Plant investment to the roots is important for microbial mutualist association, but what about aboveground biomass? Could this be impacted by microbial associations?

```{r}
ggplot(aes(x = Myco_fungi, y = Shoot_mass, colour = Rhizobia), data = medicago)+
  geom_boxplot()
```

```{r}
# main effects + interaction
shoot1 <- lm(Shoot_mass ~ Myco_fungi + Rhizobia + Myco_fungi:Rhizobia, data = medicago)

summary(shoot1)
```

Although there are similar trends to the data as with root biomass, it looks like shoot biomass is significantly different between treatments.

## Another example

*Note: this example comes from Dolph Schluter's ["Model Selection" R workshop](https://www.zoology.ubc.ca/~bio501/R/workshops/modelselection.html#Scaling_of_BMR_in_mammals).*

[Savage et al. (2004)](https://doi.org/10.1111/j.0269-8463.2004.00856.x) investigated competing claims for the value of the scaling parameter, $\beta$, in mammalian basal metabolic rate (BMR):

$$\text{BMR} = \alpha M^\beta,$$

where $\text{BMR}$ is basal metabolic rate, $M$ is body mass, and $\alpha$ is a constant.

On a log scale, this can be written as:

$$\log \text{BMR} = \log(\alpha) + \beta\log(M),$$

where $\beta$ is now a slope parameter of a linear model. Theory based on optimization of hydrodynamic flows through the circulation system predicts that the exponent should be $\beta = \frac{3}{4}$ but we would expect $\beta = \frac{2}{3}$ if metabolic rate scales with heat dissipation and therefore body surface area. These alternative scaling relationships represent distinct hypotheses. We will use them as candidate models and apply model selection procedures to compare their fits to data.

Savage et al. compiled data from 626 species of mammals. To simplify, and reduce possible effects of non-independence of species data points, they took the average of $\log(\text{BMR})$ among species in small intervals of $\log(M)$. Body mass is in grams, whereas basal metabolic rate is in watts.

```{r}
# download dataset
bmr <- read.csv(url("https://www.zoology.ubc.ca/~bio501/R/data/bmr.csv"),
                stringsAsFactors = FALSE)

bmr$logmass<-log(bmr$mass.g)
bmr$logbmr<-log(bmr$bmr.w)
head(bmr)
```

```{r}
# plot the data on a log scale
ggplot(aes(x = logmass, y = logbmr), data = bmr) +
  geom_point()
```

Let's fit a linear model on this data!

```{r}
mod <- lm(logbmr ~ logmass, data = bmr)
summary(mod) # notice the estimate of the slope
```

Let's fit the two candidate models, where $$\beta = \frac{3}{4}$$ or $$\beta = \frac{2}{3}$$

```{r}
# fit models
mod1 <- lm(logbmr ~ 1+offset((3/4)*logmass), data = bmr)
mod2 <- lm(logbmr ~ 1+offset((2/3)*logmass), data = bmr)

# plot
ggplot(aes(x = logmass,y = logbmr), data = bmr)+
  geom_point()+
  geom_abline(intercept = coef(mod1), slope = 3/4, colour = "blue")+
  geom_abline(intercept = coef(mod2), slope = 2/3, colour = "red")
```

Now let's compare the AIC scores of the two models.

```{r}
AICc(mod1, mod2)
```

By this criterion, which model is the best?

## Example with a Generalized Linear Model (GLM)

Last lecture, we tried to see if we could recapture the findings of [Farrell and Davies (2019)](https://www.pnas.org/doi/full/10.1073/pnas.1817323116) using a simple GLM to see if the mean evolutionary isolation affected the probability of death. Recall that the data is distributed according to a Bernoulli distribution, so we needed to conduct a logistic regression.

```{r}
disease_distance <- read_csv("data/disease_distance.csv")

DataBern<-disease_distance %>% 
  mutate(AnyDeaths = case_when(Deaths > 0 ~ 1,
                               Deaths == 0 ~ 0))

model <- glm(AnyDeaths ~ EvoIso, 
             family = "binomial", 
             data = DataBern)

summary(model)
```

Suppose we want to see if other factors affect the probability of death, in addition to or outside of evolutionary isolation. What factors could we test?

Perhaps the type of parasite also affects the probability of death. Let's compare the models.

```{r}
model2 <- glm(AnyDeaths ~ EvoIso + ParaType, 
              family = "binomial", 
              data = DataBern)

AICc(model,model2)
```

Note that the we could continue to add terms to the model that would make it a better fit to the data. But this veers towards p-hacking territory! This section is to show you that AIC works the same for GLM. **In practice, you should first build your models based on good theory and then compare them.**

## Model selection for linear mixed models

Model selection for mixed models is even more complicated, due to the addition of random effects. A procedure that is commonly used to preform selection of LMMs is as follows:

1. Create a _saturated model_ that includes all fixed effects (and their interactions[^interaction]) and random effects you wish to test. 
   
2. Optimize the random-effect structure by comparing models with the same saturated fixed effects structure but differing random effect structures. These models should be fitted using Restricted Maximum Likelihood (i.e., `REML = TRUE`), since varience component estimation will be biased otherwise. The _optimal_ random effect structure is the one that provides the lowest AIC. Do **not** remove random effects if they are included to account for non-independence in your data. Skip this step if random effects must be included given the hierarchical structure (i.e., nestedness) in the data.
   
3. Optimize the fixed-effect structure by comparing models with the same optimized (or necessary) random effects but with differing fixed effect structures. These models should be fitted with Maximum Likelihood (i.e., `REML = FALSE`) to prevent biased fixed-effect parameter estimates. Models can be selected on the basis of AIC (the lower the better), by comparing nested models using a likelihood ratio test (LRT), or both. 
   
4. Fit the final model with optimized fixed and random effects using REML and interpret your results.

## References and additional reading

  1. [Johnson & Omland 2004. Model selection in ecology and evolution. _Trends in Ecology and Evolution_.](https://www.sciencedirect.com/science/article/abs/pii/S0169534703003458)
  2. Burnham & Anderson 2002. _Model Selection and Multimodel Inference_ (2nd ed.).
  3. Zuur et al. 2009. _Mixed Effects Models and Extensions in Ecology with R_.
  4. [Tredennick et al. 2021. A practical guide to selecting models for exploration, inference, and prediction in ecology. _Ecology_.](https://doi.org/10.1002/ecy.3336)