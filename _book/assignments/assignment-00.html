<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>EEB313: Quantitative Methods in R for Biology - Assignment00: individual interest description</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../assignments/assignment-01.html" rel="next">
<link href="../lectures/lec11-linear-models-2.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../assignments/assignment-00.html">Assignments</a></li><li class="breadcrumb-item"><a href="../assignments/assignment-00.html">Assignment00: individual interest description</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">EEB313: Quantitative Methods in R for Biology</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/EEB313/eeb313.github.io" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Syllabus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../about-us.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2024 teaching team</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../downloadingR.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Installing R &amp; Ubuntu</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Lectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lec01-r_and_rstudio.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to the course</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lec02-basic-r.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to R: assignment, vectors, functions, strings, loops</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lec03-command_git.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Command line &amp; Git(Hub)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lec04-data-wrangling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Data wrangling!</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lec05-data-visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data visualization with ggplot2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lec06-exploratory-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Exploratory data analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lec07-self-directed-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Review using Farrell and Davies (2019) dataset</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lec08-intro-inference-1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Introduction to inference I</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lec09-intro-inference-2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Introduction to inference II</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lec10-linear-models-1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Linear models I</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lec11-linear-models-2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Linear models II

## Lesson preamble

&gt; ### Learning objectives
&gt;
&gt; - Learn how dependent data can be modeled, and the sitations in which one may encounter such data.
&gt; - Understand the structure, assumptions, and implementation of GLMs.
&gt; - Understand the structure, assumptions, and implementation of LLMs.
&gt; - Learn how to use simulation to inform what data that should be collected, and how.
&gt; 
&gt; ### Lesson outline
&gt; -   Generalized linear models
&gt;     - Structure and assumptions, including interpretation of the effects
&gt;     - Implement logistic regression using dataset of Farrell and Davies (2019)
&gt;     - Other types of GLMs (Poisson, negative binomial, etc.)
&gt; -   Power analysis!
&gt; -   Dealing with dependent data
&gt;     - Splitting the data into groups...
&gt;     - Controlling for phylogeny
&gt;     - Linear mixed models: structure and assumptions
&gt;       - Difference between fixed, random effects; examples
&gt;       - Implement mixed models using sexual size dimorphism data

```{r, message=F, warning=F}
library(tidyverse)
library(broom)
library(lme4)
```

## Generalized linear models: theory and examples

So far we have seen how linear models can be used to describe relationships between a response variable and predictors when the data is normally distributed or can be transformed so that this assumption is not violated. What if we were, say, interested in a *binary* response (e.g., infection status of a host with a particular parasite) and how it changes with a continuous predictor (e.g., age of the host)? In this case, one can use a special kind of linear model called *logistic regression* to estimate the additive effect of predictor(s) on the binary response. Logistic regression is a special kind of **generalized linear model**.

### GLMs: structure and assumptions

Generalized linear models describe how the mean of a response variable changes as a function of the predictors when important assumptions of the linear modeling framework (normality, constant error variance, etc.) are violated. In particular, **GLMs allow us to work with data that are not normal, whose range is restricted, or whose variance depends on the mean.** The latter might be important if, say, larger values of the response were also more variable.

A GLM models the transformed mean of a response variable $Y_i$ as a linear combination of the predictors $x_1,\dots,x_p$. The goal of using a GLM is often to estimate how the predictors (e.g., sex and previous exposure to the disease) affect the response (e.g., infection status). sThe transformation of the response is done using a "link" function $g(\cdot)$ that is specific to the distribution which used to model the data. Written out, a GLM is of the form

$$g(\mu_i) = \beta_0 + \beta_1 x_{1i} + \cdots \beta_p x_{pi}.$$

The link functions for the Normal, Gamma, Exponential, Poisson, and Multinomial distributions are known. In general, GLMs apply when the data are modeled using a member of the [exponential family](https://en.wikipedia.org/wiki/Exponential_family). The distributions will have a mean parameter $\mu$ and, sometimes, a parameter $\tau$ which characterizes the dispersion around the mean. _GLMs are fitted using by maximizing the likelihood function that results from assuming the data arise from a distribution in the exponential family (with a mean and dispersion that depend on the predictors), using using numerical optimization methods._

### Interpretation of the effects

Notice that, in a GLM, because the mean of the response been transformed in a particular way, the coefficients must be interpreted carefully. In particular, $\beta_{j}$ is how a per-unit change in $x_{j}$ increases or decreases the _transformed_ mean $g(\mu_i)$.

### Example: logistic regression

In a previous class, we estimated the probability of death given infection for the wild boar when infected with viruses in the family _Coronaviridae_. In your current homework, you have been tasked with extending that model to all host and parasite family combinations. Excitingly, the dataset which have used includes information about the mean evolutionary isolation of all hosts that are infected with a parasite from all other hosts which are known to be infected.

Farrell and Davies tested if the mean evolutionary isolation affected the probability of death. They used a complex model to control for sampling bias and other confounding aspects of the data. We will ignore those complexities and see if, using a GLM, we can recapitulate their findings. To get started, load the `disease_distance.csv` dataset.

```{r}
disease_distance &lt;- read_csv("data/disease_distance.csv")

disease_distance %&gt;% 
  mutate(AnyDeaths = case_when(Deaths &gt; 0 ~ 1,
                               Deaths == 0 ~ 0)) -&gt; DataBern

DataBern |&gt;
  ggplot(aes(x = EvoIso, y = AnyDeaths)) + 
  geom_point()
```

These are binary data. We CANNOT use a linear model, as this assumes the data are Normal. In fact, these data are distributed according to a Bernoulli($p$) distribution. This is a member of the exponential family and the type of generalized linear model when the data are distributed in this way (i.e., are binary) is called **logistic regression**.

The mean of Bernoulli($p$) data is just $p$, and the link function for $p$ is

$$ \text{logit}(p) = \log\frac{p}{1-p}.$$

$\text{logit}(p)$ is called the *log-odds*, which can be thought of as a likelihood the response takes on the value one. In logistic regression, the log-odds ratio is modeled as a linear combination in the predictors:

$$ \text{logit}(p_i) =  \beta_0 + \beta_1 x_{1i} + \cdots + \beta_p x_{pi}.$$

Notice that increasing $x_j$ by one unit results in change $\beta_j$ to the link-transformed response.This is how the effect sizes are interpreted for GLMs such as this one.


```{r}
model &lt;- glm(AnyDeaths ~ EvoIso, 
             family = "binomial", 
             data = DataBern)
summary(model)
```

#### Challenge

How do we interpret the regression coefficient above?

#### Challenge

What are the log-odds of death if the evolutionary isolation of hosts is $EI = 200$? How much does this quantity change if the evolutionary isolation were to increase by 20 million years?

#### Visualizing the fitted model

```{r}
DataBern |&gt;
  ggplot(aes(x = EvoIso, y = AnyDeaths)) + 
  geom_point() +
  geom_smooth(method = "glm", 
              method.args = list(family = "binomial")
              )

### base R implementation

EvoIso &lt;- seq(0, 200, 0.1)
predicted_prob &lt;- predict(model, list(EvoIso = EvoIso), type = "response")

plot(DataBern$EvoIso, DataBern$AnyDeaths)
lines(EvoIso, predicted_prob)
```

### Other GLMs

Here are some common types of response variables and their corresponding distributions:

-   **Count data**: the **Poisson** distribution
-   **Over-dispersed count data** (when the count data is more spread out than "expected"): the **negative binomial** distribution
-   **Binary data** (two discrete categories): the **binomial** distribution
-   Counts of occurrences of $K$ different types: the **multinomial** distribution
-   **Times** between $r$ events: the **gamma** distribution, which is equivilent to the exponential when $r=1$

You will have the opportunity to implement such models in your homework and on the challenge assignment!

## Power analysis

When designing experiments or how best to collect data, it is best to think about the model you will fit and the kind of experiment you need to design in order to have sufficient power to detect an effect of interest. For example, if we thought that age affected the likelihood that a host dies of a disease, then we would likely fit a logistic regression-type model. By _simulating_ data from such a model, we can determine

- the minimum sample size required to reliably estimate an effect of a certain size
- the minimum sample size required to detect an effect of a certain size at a fixed significance level
- the minimum effect size that can be detected at a fixed significance level and sample size

### Example

Below is an example of a simulation in which binary disease data (death/no death; 0/1) are simulated hosts of various ages, assuming the the log-odds of disease is a linear function of age. (Notice that we assume that host lifetimes are exponentially distributed with rate parameter $\lambda = 1/10$ years. This means that hosts, in this simulation, live for an average of 10 years.) We then fit a logistic regression to this data to determine the effect of age on disease risk.

```{r}
data_generator &lt;- function(sample_size = 100, effect = 0.1){
  age &lt;- rexp(n = sample_size, rate = 1/10)
  linear_predictor &lt;- effect*age
  prob &lt;- 1/(1+exp(-linear_predictor))
  
  disease_status &lt;- c()
  
  for (i in 1:length(prob)){
  disease_status[i] &lt;- rbinom(n = 1, size = 1, prob = prob[i])
  }
  
  return(data.frame(age = age, disease_status = disease_status))
}

data &lt;- data_generator()

data %&gt;% pivot_longer(! age) %&gt;% 
  ggplot(aes(x = age, y = value)) + geom_point() + 
  geom_smooth(method = "glm", method.args = list(family = "binomial")
              ) + labs(y = "prob. of disease (i.e., disease status =1)")

model &lt;- glm(disease_status~scale(age), family = binomial, data = data)
summary(model)
```

Next, we will write a function that performs a **power analysis**. We will use this function determine the sample size that is required so that simulating the age-disease data repeatedly we identify a significant effect of age on disease status (i.e., reject the null hypothesis) at level $\alpha = 0.01$ *at least $95\%$ of the time*.

```{r}
power_analysis_function &lt;- function(sample_size){
  
  sims &lt;- 1000
  pvalues &lt;- c()
  for (i in 1:sims){
  data &lt;- data_generator(sample_size)
  model &lt;- glm(disease_status~scale(age), family = binomial, data = data)
  pvalues[i] &lt;- summary(model)$coefficients[2,4]
  }
  
  power_estimate &lt;- length(which(pvalues &lt; 0.01))/length(pvalues)
  
  return(power_estimate)
}

sample_sizes &lt;- seq(150,200,10); power_estimates &lt;- c()

for (i in 1:length(sample_sizes)){
  power_estimates[i] &lt;- power_analysis_function(sample_size = sample_sizes[i])
}

knitr::kable(cbind(n = sample_sizes, power = power_estimates))
```

## Dealing with dependent data!

To illustrate how mixed models work and what kinds of questions we can answer using them, we will use the sexual size dimorphism data which we analyzed last class. Recall that we did NOT find a significant effect of sex on the average body size. There was no effect of the interaction between Order and sex on body size. Indeed, this matches what you saw in the homework questions where you had to visualize the data --- most of the variation in body size was _between_ orders.

In the models we built, we ignored a pretty important fact about the data: species have a common history (i.e., phylogeny). Thus, observations are not independent! This can make drawing inferences from comparative data difficult. **We will address how to deal with the non-independence of data (due to a common history of species, replication in blocks, etc.) using three approaches.**

But, first, let's download the data we will use in this section!

```{r}
SSDdata &lt;- read_csv("data/SSDinMammals.csv")
```

```{r}
mammal_length &lt;- SSDdata %&gt;%
  select(c("Order", "Scientific_Name", "lengthM", "lengthF")) %&gt;%
  pivot_longer(c(lengthM, lengthF), names_to = "sex", values_to = "length",
               names_pattern = "length(.)")

mammal_mass &lt;- SSDdata %&gt;%
  select(c("Order", "Scientific_Name", "massM", "massF")) %&gt;%
  pivot_longer(c(massM, massF), names_to = "sex", values_to = "mass",
               names_pattern = "mass(.)")

mass_nodup &lt;- mammal_mass %&gt;% 
  group_by(Scientific_Name, sex) %&gt;%
  distinct(Scientific_Name, sex, .keep_all = TRUE)

length_nodup &lt;- mammal_length %&gt;% 
  group_by(Scientific_Name, sex) %&gt;%
  distinct(Scientific_Name, sex, .keep_all = TRUE)

mammal_long &lt;- full_join(mass_nodup, length_nodup, 
                         by = join_by("Scientific_Name", "sex", "Order")) |&gt;
  drop_na()

glimpse(mammal_long)
```

### Group-by-group analyses

**One first way we can handle dependent data is to split observations into groups such that, within each group, observations are independent (or approximately so).** This is what we did last class when we fit order-specific regression coefficients of sex on body size. We saw that ALL order-specific effects had confidence intervals which overlapped zero!

```{r, warning=F}
# run linear model of size on sex for EACH Order

Order_specific_models &lt;- 
  mammal_long |&gt; 
  group_by(Order) |&gt;
  do(model = tidy(lm(log(mass) ~ sex, data = .), conf.int = T))
  
# get coefficients for each Order

Order_specific_models |&gt;
  unnest()

# visualize effects, CIs, and p values

Order_specific_models |&gt;
  unnest() |&gt;
  subset(term == "sexM") |&gt;
  group_by(Order) |&gt;
  ggplot(aes(y = Order, x = estimate, 
             xmin = conf.low,
             xmax = conf.high
             )
         ) +
  geom_pointrange() +
  geom_vline(xintercept = 0, lty = 2)
```

No effect of sex on body size, for any of the orders!

#### Challenge

How would you adjust the previous plot to show the estimated intercepts AND the effects of sex?

```{r}
Order_specific_models |&gt;
  unnest() |&gt;
  # subset(term == "sexM") |&gt;
  group_by(Order) |&gt;
  ggplot(aes(y = Order, x = estimate, 
             xmin = conf.low,
             xmax = conf.high,
             color = term
             )
         ) +
  geom_pointrange() +
  geom_vline(xintercept = 0, lty = 2)
```

#### Challenge

How would you retrieve model fits for each Order in base R?

```{r, include=F}
for (i in 1:nrow(Order_specific_models)){
  print(summary(Order_specific_models$mod[[i]]))
}
```

#### Challenge

Adjust the plot above so that it the size of the estimates depend on the number of observations in an Order? _Hint: determine the number of observations per Order and then use the `merge()` function with the `Order_specific_models` tibble. To adjust the range of point sizes, use `scale_size()`._

```{r, include=F}
mammal_long |&gt; 
  group_by(Order) |&gt;
  summarise(number_obs = n()) |&gt;
  merge(Order_specific_models) |&gt;
  unnest() |&gt;
  ggplot(aes(y = Order, x = estimate, 
             xmin = conf.low,
             xmax = conf.high,
             color = term,
             size = number_obs
             )
         ) +
  geom_pointrange() +
  geom_vline(xintercept = 0, lty = 2) +
  scale_size(range = c(0, 2))
```

#### Pros and cons

There are several advantages to preforming a group-by-group analysis:

- **Fitting more complex models (e.g., those with random effects) can be difficult.** There may be convergence issues, and interpretation of effects and $p$-values can be tricky.
- The group-by-group analysis is **robust in the face of unbalanced data** (i.e., when there are more observations for some groups than others).
- **The conclusions from a group-by-group analysis are _conservative_.**
- The group-by-group analysis is **fairly easy to implement**.

Among the disparages to this approach are the following:

- With fewer samples per group, the analysis **may be under-powered**.
- Splitting the data into groups means there are more coefficients to estimate, and thus confidence intervals to be estimated and hypothesis tests to be performed. This means there is a greater chance that we obtain a **spurious association**.*
- It is **difficult to draw conclusions about the variance between groups.** In some applications, this is of interest. In others, it is not.

*One solution is to adjust the significant level based on the number of tests conducted. If $k$ hypotheses are tested, an [common adjustment](https://en.wikipedia.org/wiki/Bonferroni_correction) is to set $\alpha = 0.05/k$.

### Controlling for phylogeny

A common reason data are dependent in biology is that **species share a common history of descent with modification.** When we assume that the observations are independent, we make implicit assumptions about the degree to which the characters at the tips of a phylogeny have underwent independent evolution. Sometimes, when species are diverged by many millions of years and traits evolve quickly, it is reasonable to ignore the phylogenetic constraints on the data. In other cases, it is essential to consider the role of phylogeny.

Several methods can control for phylogeny (if known). In fact, such methods can _use_ information in the branching pattern of species to draw inferences about the evolution of ecologically-important traits (such as body and brain size, dispersal rate, etc.). We will not discuss how to implement phylogenetic comparative methods, but it is good to know they exist and how, at a surface level, they work.

Intuition for how PCMs work is easiest to grasp when we consider a tree with $n$ species at the tips. If we know the pairwise divergence times for all species, then we can _transform_ the data so that observations are independent by looking at all _differences_ of traits. Not all differences are equally informative; if species have diverged a long time ago, there has been more time for differences to build up. PCMs account for this by specifying how the distribution of trait differences between species $i$ and $j$ depends on the time that has elapsed since these species diverged, In particular, the larger the divergence time, the greater the variance in $Y_i-Y_j$, the difference in trait values between species $i$ and $j$. A simple way to do this is to write $\sigma_{ij} = \sigma^2/T_{ij}$.^[Note: this gives rise to a likelihood function that is functionally VERY similar to the one for the linear model with constant error variance.]

### Random effects!

Another way one can account for dependent data is by including _random effects_. Random effects are often used to control for the fact that observations are clustered (e.g., trait data for species belonging to a higher taxonomic unit, measurements of plant biomass from plots of land).

#### Structure and assumptions

A common way models with random effects are formulated is as follows:

$$Y_{ij} \sim \text{Normal}(\beta_0 + \beta_1 x_{1ij} + \cdots + \beta_{p} x_{pij} + b_{0i} + b_{1i} z_{1ij} + \cdots + b_{qi} z_{qij}, \sigma_{ij}^2),$$

where $ij$ is the $i$th observation from the $j$th cluster and

$$b_{ki} \sim \text{Normal}(0,\tau_k^2).$$

This gives rise to a distribution for $Y$ which depends on the values of the random effects $b_0,\dots,b_q$. Under the hood, methods that fit models of this form numerically maximize a version of the likelihood function that results from these assumptions.

#### Challenge

In each of the following examples, which effects might be reasonably treated as fixed vs. random? Justify your answer.

1. Suppose we are working with rodents that have been infected with an evolved strain of the parasite that causes malaria. Some rodents have been treated with a prospective vaccine and others sham-vaccinated. We are interested if a proxy for virulence (e.g., density of infected red blood cells) depends on vaccination status.

2. Suppose we conduct the same experiment, except rodents are caged are sets of four.

3. Suppose we measure the time between sightings of a raccoon in a Toronto neighborhood using six camera traps (strategically placed in the neighborhood). We do this for year, and want to ask if mean daily temperature predicts the frequency of raccoon occurrence.

4. Suppose that, every year, we go to the Amazon and measure the number of bird calls that come from $n=30$ trees. Assume that the trees are far enough apart we can treat them as independent. We measure the number of birds in a tree, characteristics of the tree (cover, height, width, age), the year in which a measurement was done, and the mean temperature that year. We want to know if tree cover affects the frequency of bird calls, and if it interacts with temperature.

For more on the difference between fixed and random effects, check out the following resources

- [this Cross Validated post](https://stats.stackexchange.com/questions/4700/what-is-the-difference-between-fixed-effect-random-effect-in-mixed-effect-model)
- [this Dynamic Ecology post](https://dynamicecology.wordpress.com/2015/11/04/is-it-a-fixed-or-random-effect/)
- [this book chapter](https://bookdown.org/steve_midway/DAR/random-effects.html)

#### Using `lme4` to fit random and mixed effect models

We will start by fitting a model of log body size on log length where the intercept is random depending on the Order. Based on a quick visualization of the data, such a model may be appropriate.

```{r}
ggplot(data = mammal_long, aes(y = log(mass), 
                               x = log(length)
                               )
       ) + 
    geom_point(aes(color = Order), size = 3) -&gt; p

p 
```

```{r, include=F}
# no Order-dependence:
p + geom_smooth(method = "lm")

# intercept AND slope fixed and specific to the Order:
p + geom_smooth(method = "lm") + facet_wrap(~Order) 
```

To fit regress the response on a set of fixed effects with random _intercepts_ that depends the values assumed by a random effect `x`, we call `lmer` and write a linear model with `(1|x)`.

```{r}
## random intercept for Order

model &lt;- lmer(formula = log(mass) ~ log(length) + (1|Order), data = mammal_long)
summary(model)

fixef(model)
ranef(model)
```

From top to bottom, the output is telling us that the model was fit using a method called REML (restricted maximum likelihood). It returns information about the residuals, random effects, and fixed effects. The output also tells us about the **estimated variance for the random effects** in the model. Here, the variance associated with Order is 4.256. The variance explained by Order is 4.256/(4.256+0.847) _after controlling for the fixed effects_. Note the denominator here is the total variance, including from the residuals. As usual, we also have information about the fixed effects.

To visualize the model, we can predict the overall and Order-specific relationship of log mass on log length for all of the Orders represented in the data.

```{r}
mammal_long |&gt; ungroup() |&gt; select(mass, length, Order) |&gt; 
  mutate(fit.m = predict(model, re.form = NA), # does not include random effects
         fit.c = predict(model, re.form = NULL) # includes random effects
         ) -&gt;
  predicted_values

predicted_values |&gt;
  ggplot(aes(x = log(length), y = log(mass), color = Order)) +
  geom_point(size = 3) +
  geom_line(inherit.aes = F, aes(x = log(length), y = fit.c, color = Order), size = 1) +
  geom_line(inherit.aes = F, aes(x = log(length), y = fit.m), color = "black", size = 2)
```

The thick black line corresponds to the fitted values associated with the fixed-effect component of the model. The colored lines correspond to the fitted values estimated for each Order. 

Perhaps a model with Order-specific random intercepts AND slopes would be better. We fit this model using the code below. The key difference in syntax is that we write `(1+fixed effect|random effect)` to indicate that the random effect has an effect on both the intercept _and_ slope of the response on the fixed effect.

```{r}
## random intercept and slope for Order

model2 &lt;- lmer(formula = log(mass) ~ log(length) + (1+log(length)|Order), data = mammal_long)
summary(model2)

fixef(model2)
ranef(model2)
```

```{r}
mammal_long |&gt; ungroup() |&gt; select(mass, length, Order) |&gt; 
  mutate(fit.m = predict(model2, re.form = NA),
         fit.c = predict(model2, re.form = NULL)
         ) -&gt;
  predicted_values

predicted_values |&gt;
  ggplot(aes(x = log(length), y = log(mass), color = Order)) +
  geom_point(size = 3) +
  geom_line(inherit.aes = F, aes(x = log(length), y = fit.c, color = Order), size = 1) +
  geom_line(inherit.aes = F, aes(x = log(length), y = fit.m), color = "black", size = 2)
```

Why does this look like a mess? As stated above, **mixed models do not work well when datasets are unbalanced.** Indeed, the number of observations in the different Orders are quite variable.

#### Challenge

Regress log body size on sex with Order as a random effect that affects the intercept AND slope of the sex-size relationship.

```{r, include=F}
Model &lt;- lmer(log(mass) ~ sex + (1+sex|Order), mammal_long)

mammal_long |&gt; ungroup() |&gt; select(Scientific_Name, mass, sex, Order) |&gt; 
  mutate(fit.m = predict(Model, re.form = NA),
         fit.c = predict(Model, re.form = NULL)
         ) -&gt;
  predicted_values

predicted_values |&gt;
  ggplot(aes(x = sex, y = log(mass))) +
  geom_line(aes(group = Scientific_Name), color = "lightgrey") +
  geom_point(color = "lightgrey", size = 2) +
  geom_point(inherit.aes = F, aes(x = sex, y = fit.c, color = Order), size = 3) +
  geom_line(inherit.aes = F, aes(x = sex, y = fit.c, color = Order, group = Order), size = 2) +
  geom_point(inherit.aes = F, aes(x = sex, y = fit.m), color = "black", size = 3) +
  geom_line(inherit.aes = F, aes(x = sex, y = fit.m, group = ""), color = "black", size = 3) +
  theme_classic()
```</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Assignments</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../assignments/assignment-00.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Assignment00: individual interest description</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../assignments/assignment-01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Assignment 01: base R, command line, &amp; Git(Hub)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../assignments/assignment-02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Assignment 02: data wrangling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../assignments/assignment-03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Assignment 03: data visualisation and exploration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../assignments/assignment-04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Assignment 04: inference!</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Project</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Project description</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects_databases.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Some open-access databases</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Datasets</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/data/portal_data.csv" class="sidebar-item-text sidebar-link">
 <span class="menu-text">portal_data.csv</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/data/pseudo.ara.busco" class="sidebar-item-text sidebar-link">
 <span class="menu-text">pseudo.ara.busco</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/data/pseudo.LTRs" class="sidebar-item-text sidebar-link">
 <span class="menu-text">pseudo.LTRs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/data/disease_distance.csv" class="sidebar-item-text sidebar-link">
 <span class="menu-text">disease_distance.csv</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/data/SSDinMammals.csv" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SSDinMammals.csv</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/data/meas.csv" class="sidebar-item-text sidebar-link">
 <span class="menu-text">meas.csv</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Miscellaneous notes</span></span>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../assignments/assignment-00.html">Assignments</a></li><li class="breadcrumb-item"><a href="../assignments/assignment-00.html">Assignment00: individual interest description</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Assignment00: individual interest description</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Download the .Rmd file <a href="assignment-01.Rmd">here</a>.</p>
<p><em>To submit this assignment, upload the full document on Quercus, including the original questions, your code, and the output. Submit your assignment as a knitted <code>.pdf</code>.</em></p>
<ol type="1">
<li>Get set up at home (or on a lab computer after hours)
<ul>
<li>If you have not already done so, install <a href="https://cran.rstudio.com/">R</a> and <a href="https://www.rstudio.com/products/rstudio/download/">RStudio</a> (already installed on the lab computers).</li>
<li>Open a new R Notebook and read the instructions about how to use the R Markdown syntax.</li>
<li>Open this assignment file in RStudio or copy its content into an empty R Notebook.</li>
<li>Insert a code chunk below, above Question 2. Set <code>eval=FALSE</code></li>
<li>In the code chunk, use <code>install.packages("&lt;package_name&gt;")</code> to install <code>tidyverse</code> and <code>rmarkdown</code>. Remember to run the code chunk to execute the commands.</li>
<li>Load the two libraries you just installed into your environment with <code>library(&lt;package_name&gt;)</code> (no surrounding quotation marks). Add this to the same code chunk you created previously and execute it again.
<ul>
<li>Don’t worry that the <code>install.packages()</code> commands have already been executed once, R is smart and checks if you already have those installed.</li>
</ul></li>
<li>Run <code>sessionInfo()</code> to list all the loaded packages.
<ul>
<li>You should see the following packages under “other attached packages”: <code>rmarkdown</code>, <code>dplyr</code>, <code>purrr</code>, <code>readr</code>, <code>tidyr</code>, <code>tibble</code>, <code>ggplot</code>, and <code>tidyverse</code>.</li>
</ul></li>
<li>Since this is your first assignment, we have already completed most of this question below. You still need to run the code chunk on your computer to confirm that the packages installed and to get the <code>sessionInfo()</code> output for your computer. You might receive warnings that functions from other packages are masked when you load <code>tidyverse</code>, but this is fine.</li>
</ul></li>
</ol>
<ol start="2" type="1">
<li>In 4-5 sentences, what are some of the topics/questions in ecology and evolutionary biology that you are interested in for the group project?</li>
</ol>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../lectures/lec11-linear-models-2.html" class="pagination-link" aria-label="Linear models II

## Lesson preamble

> ### Learning objectives
>> - Learn how dependent data can be modeled, and the sitations in which one may encounter such data.
> - Understand the structure, assumptions, and implementation of GLMs.
> - Understand the structure, assumptions, and implementation of LLMs.
> - Learn how to use simulation to inform what data that should be collected, and how.
> 
> ### Lesson outline
> -   Generalized linear models
>     - Structure and assumptions, including interpretation of the effects
>     - Implement logistic regression using dataset of Farrell and Davies (2019)
>     - Other types of GLMs (Poisson, negative binomial, etc.)
> -   Power analysis!
> -   Dealing with dependent data
>     - Splitting the data into groups...
>     - Controlling for phylogeny
>     - Linear mixed models: structure and assumptions
>       - Difference between fixed, random effects; examples
>       - Implement mixed models using sexual size dimorphism data

```{r, message=F, warning=F}
library(tidyverse)
library(broom)
library(lme4)
```

## Generalized linear models: theory and examples

So far we have seen how linear models can be used to describe relationships between a response variable and predictors when the data is normally distributed or can be transformed so that this assumption is not violated. What if we were, say, interested in a *binary* response (e.g., infection status of a host with a particular parasite) and how it changes with a continuous predictor (e.g., age of the host)? In this case, one can use a special kind of linear model called *logistic regression* to estimate the additive effect of predictor(s) on the binary response. Logistic regression is a special kind of **generalized linear model**.

### GLMs: structure and assumptions

Generalized linear models describe how the mean of a response variable changes as a function of the predictors when important assumptions of the linear modeling framework (normality, constant error variance, etc.) are violated. In particular, **GLMs allow us to work with data that are not normal, whose range is restricted, or whose variance depends on the mean.** The latter might be important if, say, larger values of the response were also more variable.

A GLM models the transformed mean of a response variable $Y_i$ as a linear combination of the predictors $x_1,\dots,x_p$. The goal of using a GLM is often to estimate how the predictors (e.g., sex and previous exposure to the disease) affect the response (e.g., infection status). sThe transformation of the response is done using a &quot;link&quot; function $g(\cdot)$ that is specific to the distribution which used to model the data. Written out, a GLM is of the form

$$g(\mu_i) = \beta_0 + \beta_1 x_{1i} + \cdots \beta_p x_{pi}.$$

The link functions for the Normal, Gamma, Exponential, Poisson, and Multinomial distributions are known. In general, GLMs apply when the data are modeled using a member of the [exponential family](https://en.wikipedia.org/wiki/Exponential_family). The distributions will have a mean parameter $\mu$ and, sometimes, a parameter $\tau$ which characterizes the dispersion around the mean. _GLMs are fitted using by maximizing the likelihood function that results from assuming the data arise from a distribution in the exponential family (with a mean and dispersion that depend on the predictors), using using numerical optimization methods._

### Interpretation of the effects

Notice that, in a GLM, because the mean of the response been transformed in a particular way, the coefficients must be interpreted carefully. In particular, $\beta_{j}$ is how a per-unit change in $x_{j}$ increases or decreases the _transformed_ mean $g(\mu_i)$.

### Example: logistic regression

In a previous class, we estimated the probability of death given infection for the wild boar when infected with viruses in the family _Coronaviridae_. In your current homework, you have been tasked with extending that model to all host and parasite family combinations. Excitingly, the dataset which have used includes information about the mean evolutionary isolation of all hosts that are infected with a parasite from all other hosts which are known to be infected.

Farrell and Davies tested if the mean evolutionary isolation affected the probability of death. They used a complex model to control for sampling bias and other confounding aspects of the data. We will ignore those complexities and see if, using a GLM, we can recapitulate their findings. To get started, load the `disease_distance.csv` dataset.

```{r}
disease_distance <- read_csv(&quot;data/disease_distance.csv&quot;)

disease_distance %>% 
  mutate(AnyDeaths = case_when(Deaths > 0 ~ 1,
                               Deaths == 0 ~ 0)) -> DataBern

DataBern |>  ggplot(aes(x = EvoIso, y = AnyDeaths)) + 
  geom_point()
```

These are binary data. We CANNOT use a linear model, as this assumes the data are Normal. In fact, these data are distributed according to a Bernoulli($p$) distribution. This is a member of the exponential family and the type of generalized linear model when the data are distributed in this way (i.e., are binary) is called **logistic regression**.

The mean of Bernoulli($p$) data is just $p$, and the link function for $p$ is

$$ \text{logit}(p) = \log\frac{p}{1-p}.$$

$\text{logit}(p)$ is called the *log-odds*, which can be thought of as a likelihood the response takes on the value one. In logistic regression, the log-odds ratio is modeled as a linear combination in the predictors:

$$ \text{logit}(p_i) =  \beta_0 + \beta_1 x_{1i} + \cdots + \beta_p x_{pi}.$$

Notice that increasing $x_j$ by one unit results in change $\beta_j$ to the link-transformed response.This is how the effect sizes are interpreted for GLMs such as this one.


```{r}
model <- glm(AnyDeaths ~ EvoIso, 
             family = &quot;binomial&quot;, 
             data = DataBern)
summary(model)
```

#### Challenge

How do we interpret the regression coefficient above?

#### Challenge

What are the log-odds of death if the evolutionary isolation of hosts is $EI = 200$? How much does this quantity change if the evolutionary isolation were to increase by 20 million years?

#### Visualizing the fitted model

```{r}
DataBern |>  ggplot(aes(x = EvoIso, y = AnyDeaths)) + 
  geom_point() +
  geom_smooth(method = &quot;glm&quot;, 
              method.args = list(family = &quot;binomial&quot;)
              )

### base R implementation

EvoIso <- seq(0, 200, 0.1)
predicted_prob <- predict(model, list(EvoIso = EvoIso), type = &quot;response&quot;)

plot(DataBern$EvoIso, DataBern$AnyDeaths)
lines(EvoIso, predicted_prob)
```

### Other GLMs

Here are some common types of response variables and their corresponding distributions:

-   **Count data**: the **Poisson** distribution
-   **Over-dispersed count data** (when the count data is more spread out than &quot;expected&quot;): the **negative binomial** distribution
-   **Binary data** (two discrete categories): the **binomial** distribution
-   Counts of occurrences of $K$ different types: the **multinomial** distribution
-   **Times** between $r$ events: the **gamma** distribution, which is equivilent to the exponential when $r=1$

You will have the opportunity to implement such models in your homework and on the challenge assignment!

## Power analysis

When designing experiments or how best to collect data, it is best to think about the model you will fit and the kind of experiment you need to design in order to have sufficient power to detect an effect of interest. For example, if we thought that age affected the likelihood that a host dies of a disease, then we would likely fit a logistic regression-type model. By _simulating_ data from such a model, we can determine

- the minimum sample size required to reliably estimate an effect of a certain size
- the minimum sample size required to detect an effect of a certain size at a fixed significance level
- the minimum effect size that can be detected at a fixed significance level and sample size

### Example

Below is an example of a simulation in which binary disease data (death/no death; 0/1) are simulated hosts of various ages, assuming the the log-odds of disease is a linear function of age. (Notice that we assume that host lifetimes are exponentially distributed with rate parameter $\lambda = 1/10$ years. This means that hosts, in this simulation, live for an average of 10 years.) We then fit a logistic regression to this data to determine the effect of age on disease risk.

```{r}
data_generator <- function(sample_size = 100, effect = 0.1){
  age <- rexp(n = sample_size, rate = 1/10)
  linear_predictor <- effect*age
  prob <- 1/(1+exp(-linear_predictor))
  
  disease_status <- c()
  
  for (i in 1:length(prob)){
  disease_status[i] <- rbinom(n = 1, size = 1, prob = prob[i])
  }
  
  return(data.frame(age = age, disease_status = disease_status))
}

data <- data_generator()

data %>% pivot_longer(! age) %>% 
  ggplot(aes(x = age, y = value)) + geom_point() + 
  geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;)
              ) + labs(y = &quot;prob. of disease (i.e., disease status =1)&quot;)

model <- glm(disease_status~scale(age), family = binomial, data = data)
summary(model)
```

Next, we will write a function that performs a **power analysis**. We will use this function determine the sample size that is required so that simulating the age-disease data repeatedly we identify a significant effect of age on disease status (i.e., reject the null hypothesis) at level $\alpha = 0.01$ *at least $95\%$ of the time*.

```{r}
power_analysis_function <- function(sample_size){
  
  sims <- 1000
  pvalues <- c()
  for (i in 1:sims){
  data <- data_generator(sample_size)
  model <- glm(disease_status~scale(age), family = binomial, data = data)
  pvalues[i] <- summary(model)$coefficients[2,4]
  }
  
  power_estimate <- length(which(pvalues < 0.01))/length(pvalues)
  
  return(power_estimate)
}

sample_sizes <- seq(150,200,10); power_estimates <- c()

for (i in 1:length(sample_sizes)){
  power_estimates[i] <- power_analysis_function(sample_size = sample_sizes[i])
}

knitr::kable(cbind(n = sample_sizes, power = power_estimates))
```

## Dealing with dependent data!

To illustrate how mixed models work and what kinds of questions we can answer using them, we will use the sexual size dimorphism data which we analyzed last class. Recall that we did NOT find a significant effect of sex on the average body size. There was no effect of the interaction between Order and sex on body size. Indeed, this matches what you saw in the homework questions where you had to visualize the data --- most of the variation in body size was _between_ orders.

In the models we built, we ignored a pretty important fact about the data: species have a common history (i.e., phylogeny). Thus, observations are not independent! This can make drawing inferences from comparative data difficult. **We will address how to deal with the non-independence of data (due to a common history of species, replication in blocks, etc.) using three approaches.**

But, first, let's download the data we will use in this section!

```{r}
SSDdata <- read_csv(&quot;data/SSDinMammals.csv&quot;)
```

```{r}
mammal_length <- SSDdata %>%
  select(c(&quot;Order&quot;, &quot;Scientific_Name&quot;, &quot;lengthM&quot;, &quot;lengthF&quot;)) %>%
  pivot_longer(c(lengthM, lengthF), names_to = &quot;sex&quot;, values_to = &quot;length&quot;,
               names_pattern = &quot;length(.)&quot;)

mammal_mass <- SSDdata %>%
  select(c(&quot;Order&quot;, &quot;Scientific_Name&quot;, &quot;massM&quot;, &quot;massF&quot;)) %>%
  pivot_longer(c(massM, massF), names_to = &quot;sex&quot;, values_to = &quot;mass&quot;,
               names_pattern = &quot;mass(.)&quot;)

mass_nodup <- mammal_mass %>% 
  group_by(Scientific_Name, sex) %>%
  distinct(Scientific_Name, sex, .keep_all = TRUE)

length_nodup <- mammal_length %>% 
  group_by(Scientific_Name, sex) %>%
  distinct(Scientific_Name, sex, .keep_all = TRUE)

mammal_long <- full_join(mass_nodup, length_nodup, 
                         by = join_by(&quot;Scientific_Name&quot;, &quot;sex&quot;, &quot;Order&quot;)) |>  drop_na()

glimpse(mammal_long)
```

### Group-by-group analyses

**One first way we can handle dependent data is to split observations into groups such that, within each group, observations are independent (or approximately so).** This is what we did last class when we fit order-specific regression coefficients of sex on body size. We saw that ALL order-specific effects had confidence intervals which overlapped zero!

```{r, warning=F}
# run linear model of size on sex for EACH Order

Order_specific_models <- 
  mammal_long |> 
  group_by(Order) |>  do(model = tidy(lm(log(mass) ~ sex, data = .), conf.int = T))
  
# get coefficients for each Order

Order_specific_models |>  unnest()

# visualize effects, CIs, and p values

Order_specific_models |>  unnest() |>  subset(term == &quot;sexM&quot;) |>  group_by(Order) |>  ggplot(aes(y = Order, x = estimate, 
             xmin = conf.low,
             xmax = conf.high
             )
         ) +
  geom_pointrange() +
  geom_vline(xintercept = 0, lty = 2)
```

No effect of sex on body size, for any of the orders!

#### Challenge

How would you adjust the previous plot to show the estimated intercepts AND the effects of sex?

```{r}
Order_specific_models |>  unnest() |>  # subset(term == &quot;sexM&quot;) |>  group_by(Order) |>  ggplot(aes(y = Order, x = estimate, 
             xmin = conf.low,
             xmax = conf.high,
             color = term
             )
         ) +
  geom_pointrange() +
  geom_vline(xintercept = 0, lty = 2)
```

#### Challenge

How would you retrieve model fits for each Order in base R?

```{r, include=F}
for (i in 1:nrow(Order_specific_models)){
  print(summary(Order_specific_models$mod[[i]]))
}
```

#### Challenge

Adjust the plot above so that it the size of the estimates depend on the number of observations in an Order? _Hint: determine the number of observations per Order and then use the `merge()` function with the `Order_specific_models` tibble. To adjust the range of point sizes, use `scale_size()`._

```{r, include=F}
mammal_long |> 
  group_by(Order) |>  summarise(number_obs = n()) |>  merge(Order_specific_models) |>  unnest() |>  ggplot(aes(y = Order, x = estimate, 
             xmin = conf.low,
             xmax = conf.high,
             color = term,
             size = number_obs
             )
         ) +
  geom_pointrange() +
  geom_vline(xintercept = 0, lty = 2) +
  scale_size(range = c(0, 2))
```

#### Pros and cons

There are several advantages to preforming a group-by-group analysis:

- **Fitting more complex models (e.g., those with random effects) can be difficult.** There may be convergence issues, and interpretation of effects and $p$-values can be tricky.
- The group-by-group analysis is **robust in the face of unbalanced data** (i.e., when there are more observations for some groups than others).
- **The conclusions from a group-by-group analysis are _conservative_.**
- The group-by-group analysis is **fairly easy to implement**.

Among the disparages to this approach are the following:

- With fewer samples per group, the analysis **may be under-powered**.
- Splitting the data into groups means there are more coefficients to estimate, and thus confidence intervals to be estimated and hypothesis tests to be performed. This means there is a greater chance that we obtain a **spurious association**.*
- It is **difficult to draw conclusions about the variance between groups.** In some applications, this is of interest. In others, it is not.

*One solution is to adjust the significant level based on the number of tests conducted. If $k$ hypotheses are tested, an [common adjustment](https://en.wikipedia.org/wiki/Bonferroni_correction) is to set $\alpha = 0.05/k$.

### Controlling for phylogeny

A common reason data are dependent in biology is that **species share a common history of descent with modification.** When we assume that the observations are independent, we make implicit assumptions about the degree to which the characters at the tips of a phylogeny have underwent independent evolution. Sometimes, when species are diverged by many millions of years and traits evolve quickly, it is reasonable to ignore the phylogenetic constraints on the data. In other cases, it is essential to consider the role of phylogeny.

Several methods can control for phylogeny (if known). In fact, such methods can _use_ information in the branching pattern of species to draw inferences about the evolution of ecologically-important traits (such as body and brain size, dispersal rate, etc.). We will not discuss how to implement phylogenetic comparative methods, but it is good to know they exist and how, at a surface level, they work.

Intuition for how PCMs work is easiest to grasp when we consider a tree with $n$ species at the tips. If we know the pairwise divergence times for all species, then we can _transform_ the data so that observations are independent by looking at all _differences_ of traits. Not all differences are equally informative; if species have diverged a long time ago, there has been more time for differences to build up. PCMs account for this by specifying how the distribution of trait differences between species $i$ and $j$ depends on the time that has elapsed since these species diverged, In particular, the larger the divergence time, the greater the variance in $Y_i-Y_j$, the difference in trait values between species $i$ and $j$. A simple way to do this is to write $\sigma_{ij} = \sigma^2/T_{ij}$.^[Note: this gives rise to a likelihood function that is functionally VERY similar to the one for the linear model with constant error variance.]

### Random effects!

Another way one can account for dependent data is by including _random effects_. Random effects are often used to control for the fact that observations are clustered (e.g., trait data for species belonging to a higher taxonomic unit, measurements of plant biomass from plots of land).

#### Structure and assumptions

A common way models with random effects are formulated is as follows:

$$Y_{ij} \sim \text{Normal}(\beta_0 + \beta_1 x_{1ij} + \cdots + \beta_{p} x_{pij} + b_{0i} + b_{1i} z_{1ij} + \cdots + b_{qi} z_{qij}, \sigma_{ij}^2),$$

where $ij$ is the $i$th observation from the $j$th cluster and

$$b_{ki} \sim \text{Normal}(0,\tau_k^2).$$

This gives rise to a distribution for $Y$ which depends on the values of the random effects $b_0,\dots,b_q$. Under the hood, methods that fit models of this form numerically maximize a version of the likelihood function that results from these assumptions.

#### Challenge

In each of the following examples, which effects might be reasonably treated as fixed vs. random? Justify your answer.

1. Suppose we are working with rodents that have been infected with an evolved strain of the parasite that causes malaria. Some rodents have been treated with a prospective vaccine and others sham-vaccinated. We are interested if a proxy for virulence (e.g., density of infected red blood cells) depends on vaccination status.

2. Suppose we conduct the same experiment, except rodents are caged are sets of four.

3. Suppose we measure the time between sightings of a raccoon in a Toronto neighborhood using six camera traps (strategically placed in the neighborhood). We do this for year, and want to ask if mean daily temperature predicts the frequency of raccoon occurrence.

4. Suppose that, every year, we go to the Amazon and measure the number of bird calls that come from $n=30$ trees. Assume that the trees are far enough apart we can treat them as independent. We measure the number of birds in a tree, characteristics of the tree (cover, height, width, age), the year in which a measurement was done, and the mean temperature that year. We want to know if tree cover affects the frequency of bird calls, and if it interacts with temperature.

For more on the difference between fixed and random effects, check out the following resources

- [this Cross Validated post](https://stats.stackexchange.com/questions/4700/what-is-the-difference-between-fixed-effect-random-effect-in-mixed-effect-model)
- [this Dynamic Ecology post](https://dynamicecology.wordpress.com/2015/11/04/is-it-a-fixed-or-random-effect/)
- [this book chapter](https://bookdown.org/steve_midway/DAR/random-effects.html)

#### Using `lme4` to fit random and mixed effect models

We will start by fitting a model of log body size on log length where the intercept is random depending on the Order. Based on a quick visualization of the data, such a model may be appropriate.

```{r}
ggplot(data = mammal_long, aes(y = log(mass), 
                               x = log(length)
                               )
       ) + 
    geom_point(aes(color = Order), size = 3) -> p

p 
```

```{r, include=F}
# no Order-dependence:
p + geom_smooth(method = &quot;lm&quot;)

# intercept AND slope fixed and specific to the Order:
p + geom_smooth(method = &quot;lm&quot;) + facet_wrap(~Order) 
```

To fit regress the response on a set of fixed effects with random _intercepts_ that depends the values assumed by a random effect `x`, we call `lmer` and write a linear model with `(1|x)`.

```{r}
## random intercept for Order

model <- lmer(formula = log(mass) ~ log(length) + (1|Order), data = mammal_long)
summary(model)

fixef(model)
ranef(model)
```

From top to bottom, the output is telling us that the model was fit using a method called REML (restricted maximum likelihood). It returns information about the residuals, random effects, and fixed effects. The output also tells us about the **estimated variance for the random effects** in the model. Here, the variance associated with Order is 4.256. The variance explained by Order is 4.256/(4.256+0.847) _after controlling for the fixed effects_. Note the denominator here is the total variance, including from the residuals. As usual, we also have information about the fixed effects.

To visualize the model, we can predict the overall and Order-specific relationship of log mass on log length for all of the Orders represented in the data.

```{r}
mammal_long |> ungroup() |> select(mass, length, Order) |> 
  mutate(fit.m = predict(model, re.form = NA), # does not include random effects
         fit.c = predict(model, re.form = NULL) # includes random effects
         ) ->  predicted_values

predicted_values |>  ggplot(aes(x = log(length), y = log(mass), color = Order)) +
  geom_point(size = 3) +
  geom_line(inherit.aes = F, aes(x = log(length), y = fit.c, color = Order), size = 1) +
  geom_line(inherit.aes = F, aes(x = log(length), y = fit.m), color = &quot;black&quot;, size = 2)
```

The thick black line corresponds to the fitted values associated with the fixed-effect component of the model. The colored lines correspond to the fitted values estimated for each Order. 

Perhaps a model with Order-specific random intercepts AND slopes would be better. We fit this model using the code below. The key difference in syntax is that we write `(1+fixed effect|random effect)` to indicate that the random effect has an effect on both the intercept _and_ slope of the response on the fixed effect.

```{r}
## random intercept and slope for Order

model2 <- lmer(formula = log(mass) ~ log(length) + (1+log(length)|Order), data = mammal_long)
summary(model2)

fixef(model2)
ranef(model2)
```

```{r}
mammal_long |> ungroup() |> select(mass, length, Order) |> 
  mutate(fit.m = predict(model2, re.form = NA),
         fit.c = predict(model2, re.form = NULL)
         ) ->  predicted_values

predicted_values |>  ggplot(aes(x = log(length), y = log(mass), color = Order)) +
  geom_point(size = 3) +
  geom_line(inherit.aes = F, aes(x = log(length), y = fit.c, color = Order), size = 1) +
  geom_line(inherit.aes = F, aes(x = log(length), y = fit.m), color = &quot;black&quot;, size = 2)
```

Why does this look like a mess? As stated above, **mixed models do not work well when datasets are unbalanced.** Indeed, the number of observations in the different Orders are quite variable.

#### Challenge

Regress log body size on sex with Order as a random effect that affects the intercept AND slope of the sex-size relationship.

```{r, include=F}
Model <- lmer(log(mass) ~ sex + (1+sex|Order), mammal_long)

mammal_long |> ungroup() |> select(Scientific_Name, mass, sex, Order) |> 
  mutate(fit.m = predict(Model, re.form = NA),
         fit.c = predict(Model, re.form = NULL)
         ) ->  predicted_values

predicted_values |>  ggplot(aes(x = sex, y = log(mass))) +
  geom_line(aes(group = Scientific_Name), color = &quot;lightgrey&quot;) +
  geom_point(color = &quot;lightgrey&quot;, size = 2) +
  geom_point(inherit.aes = F, aes(x = sex, y = fit.c, color = Order), size = 3) +
  geom_line(inherit.aes = F, aes(x = sex, y = fit.c, color = Order, group = Order), size = 2) +
  geom_point(inherit.aes = F, aes(x = sex, y = fit.m), color = &quot;black&quot;, size = 3) +
  geom_line(inherit.aes = F, aes(x = sex, y = fit.m, group = &quot;&quot;), color = &quot;black&quot;, size = 3) +
  theme_classic()
```">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Linear models II## Lesson preamble&gt; ### Learning objectives&gt;&gt; - Learn how dependent data can be modeled, and the sitations in which one may encounter such data.&gt; - Understand the structure, assumptions, and implementation of GLMs.&gt; - Understand the structure, assumptions, and implementation of LLMs.&gt; - Learn how to use simulation to inform what data that should be collected, and how.&gt; &gt; ### Lesson outline&gt; - Generalized linear models&gt; - Structure and assumptions, including interpretation of the effects&gt; - Implement logistic regression using dataset of Farrell and Davies (2019)&gt; - Other types of GLMs (Poisson, negative binomial, etc.)&gt; - Power analysis!&gt; - Dealing with dependent data&gt; - Splitting the data into groups…&gt; - Controlling for phylogeny&gt; - Linear mixed models: structure and assumptions&gt; - Difference between fixed, random effects; examples&gt; - Implement mixed models using sexual size dimorphism data<code>{r, message=F, warning=F}library(tidyverse)library(broom)library(lme4)</code>## Generalized linear models: theory and examplesSo far we have seen how linear models can be used to describe relationships between a response variable and predictors when the data is normally distributed or can be transformed so that this assumption is not violated. What if we were, say, interested in a <em>binary</em> response (e.g., infection status of a host with a particular parasite) and how it changes with a continuous predictor (e.g., age of the host)? In this case, one can use a special kind of linear model called <em>logistic regression</em> to estimate the additive effect of predictor(s) on the binary response. Logistic regression is a special kind of <strong>generalized linear model</strong>.### GLMs: structure and assumptionsGeneralized linear models describe how the mean of a response variable changes as a function of the predictors when important assumptions of the linear modeling framework (normality, constant error variance, etc.) are violated. In particular, <strong>GLMs allow us to work with data that are not normal, whose range is restricted, or whose variance depends on the mean.</strong> The latter might be important if, say, larger values of the response were also more variable.A GLM models the transformed mean of a response variable <span class="math inline">\(Y_i\)</span> as a linear combination of the predictors <span class="math inline">\(x_1,\dots,x_p\)</span>. The goal of using a GLM is often to estimate how the predictors (e.g., sex and previous exposure to the disease) affect the response (e.g., infection status). sThe transformation of the response is done using a “link” function <span class="math inline">\(g(\cdot)\)</span> that is specific to the distribution which used to model the data. Written out, a GLM is of the form<span class="math display">\[g(\mu_i) = \beta_0 + \beta_1 x_{1i} + \cdots \beta_p x_{pi}.\]</span>The link functions for the Normal, Gamma, Exponential, Poisson, and Multinomial distributions are known. In general, GLMs apply when the data are modeled using a member of the </span></span></a><a href="https://en.wikipedia.org/wiki/Exponential_family">exponential family</a>. The distributions will have a mean parameter <span class="math inline">\(\mu\)</span> and, sometimes, a parameter <span class="math inline">\(\tau\)</span> which characterizes the dispersion around the mean. <em>GLMs are fitted using by maximizing the likelihood function that results from assuming the data arise from a distribution in the exponential family (with a mean and dispersion that depend on the predictors), using using numerical optimization methods.</em>### Interpretation of the effectsNotice that, in a GLM, because the mean of the response been transformed in a particular way, the coefficients must be interpreted carefully. In particular, <span class="math inline">\(\beta_{j}\)</span> is how a per-unit change in <span class="math inline">\(x_{j}\)</span> increases or decreases the <em>transformed</em> mean <span class="math inline">\(g(\mu_i)\)</span>.### Example: logistic regressionIn a previous class, we estimated the probability of death given infection for the wild boar when infected with viruses in the family <em>Coronaviridae</em>. In your current homework, you have been tasked with extending that model to all host and parasite family combinations. Excitingly, the dataset which have used includes information about the mean evolutionary isolation of all hosts that are infected with a parasite from all other hosts which are known to be infected.Farrell and Davies tested if the mean evolutionary isolation affected the probability of death. They used a complex model to control for sampling bias and other confounding aspects of the data. We will ignore those complexities and see if, using a GLM, we can recapitulate their findings. To get started, load the <code>disease_distance.csv</code> dataset.<code>{r}disease_distance &lt;- read_csv("data/disease_distance.csv")disease_distance %&gt;%   mutate(AnyDeaths = case_when(Deaths &gt; 0 ~ 1,                               Deaths == 0 ~ 0)) -&gt; DataBernDataBern |&gt;  ggplot(aes(x = EvoIso, y = AnyDeaths)) +   geom_point()</code>These are binary data. We CANNOT use a linear model, as this assumes the data are Normal. In fact, these data are distributed according to a Bernoulli(<span class="math inline">\(p\)</span>) distribution. This is a member of the exponential family and the type of generalized linear model when the data are distributed in this way (i.e., are binary) is called <strong>logistic regression</strong>.The mean of Bernoulli(<span class="math inline">\(p\)</span>) data is just <span class="math inline">\(p\)</span>, and the link function for <span class="math inline">\(p\)</span> is<span class="math display">\[ \text{logit}(p) = \log\frac{p}{1-p}.\]</span><span class="math inline">\(\text{logit}(p)\)</span> is called the <em>log-odds</em>, which can be thought of as a likelihood the response takes on the value one. In logistic regression, the log-odds ratio is modeled as a linear combination in the predictors:<span class="math display">\[ \text{logit}(p_i) =  \beta_0 + \beta_1 x_{1i} + \cdots + \beta_p x_{pi}.\]</span>Notice that increasing <span class="math inline">\(x_j\)</span> by one unit results in change <span class="math inline">\(\beta_j\)</span> to the link-transformed response.This is how the effect sizes are interpreted for GLMs such as this one.<code>{r}model &lt;- glm(AnyDeaths ~ EvoIso,              family = "binomial",              data = DataBern)summary(model)</code>#### ChallengeHow do we interpret the regression coefficient above?#### ChallengeWhat are the log-odds of death if the evolutionary isolation of hosts is <span class="math inline">\(EI = 200\)</span>? How much does this quantity change if the evolutionary isolation were to increase by 20 million years?#### Visualizing the fitted model<code>{r}DataBern |&gt;  ggplot(aes(x = EvoIso, y = AnyDeaths)) +   geom_point() +  geom_smooth(method = "glm",               method.args = list(family = "binomial")              )### base R implementationEvoIso &lt;- seq(0, 200, 0.1)predicted_prob &lt;- predict(model, list(EvoIso = EvoIso), type = "response")plot(DataBern$EvoIso, DataBern$AnyDeaths)lines(EvoIso, predicted_prob)</code>### Other GLMsHere are some common types of response variables and their corresponding distributions:- <strong>Count data</strong>: the <strong>Poisson</strong> distribution- <strong>Over-dispersed count data</strong> (when the count data is more spread out than “expected”): the <strong>negative binomial</strong> distribution- <strong>Binary data</strong> (two discrete categories): the <strong>binomial</strong> distribution- Counts of occurrences of <span class="math inline">\(K\)</span> different types: the <strong>multinomial</strong> distribution- <strong>Times</strong> between <span class="math inline">\(r\)</span> events: the <strong>gamma</strong> distribution, which is equivilent to the exponential when <span class="math inline">\(r=1\)</span>You will have the opportunity to implement such models in your homework and on the challenge assignment!## Power analysisWhen designing experiments or how best to collect data, it is best to think about the model you will fit and the kind of experiment you need to design in order to have sufficient power to detect an effect of interest. For example, if we thought that age affected the likelihood that a host dies of a disease, then we would likely fit a logistic regression-type model. By <em>simulating</em> data from such a model, we can determine- the minimum sample size required to reliably estimate an effect of a certain size- the minimum sample size required to detect an effect of a certain size at a fixed significance level- the minimum effect size that can be detected at a fixed significance level and sample size### ExampleBelow is an example of a simulation in which binary disease data (death/no death; 0/1) are simulated hosts of various ages, assuming the the log-odds of disease is a linear function of age. (Notice that we assume that host lifetimes are exponentially distributed with rate parameter <span class="math inline">\(\lambda = 1/10\)</span> years. This means that hosts, in this simulation, live for an average of 10 years.) We then fit a logistic regression to this data to determine the effect of age on disease risk.<code>{r}data_generator &lt;- function(sample_size = 100, effect = 0.1){  age &lt;- rexp(n = sample_size, rate = 1/10)  linear_predictor &lt;- effect*age  prob &lt;- 1/(1+exp(-linear_predictor))    disease_status &lt;- c()    for (i in 1:length(prob)){  disease_status[i] &lt;- rbinom(n = 1, size = 1, prob = prob[i])  }    return(data.frame(age = age, disease_status = disease_status))}data &lt;- data_generator()data %&gt;% pivot_longer(! age) %&gt;%   ggplot(aes(x = age, y = value)) + geom_point() +   geom_smooth(method = "glm", method.args = list(family = "binomial")              ) + labs(y = "prob. of disease (i.e., disease status =1)")model &lt;- glm(disease_status~scale(age), family = binomial, data = data)summary(model)</code>Next, we will write a function that performs a <strong>power analysis</strong>. We will use this function determine the sample size that is required so that simulating the age-disease data repeatedly we identify a significant effect of age on disease status (i.e., reject the null hypothesis) at level <span class="math inline">\(\alpha = 0.01\)</span> <em>at least <span class="math inline">\(95\%\)</span> of the time</em>.<code>{r}power_analysis_function &lt;- function(sample_size){    sims &lt;- 1000  pvalues &lt;- c()  for (i in 1:sims){  data &lt;- data_generator(sample_size)  model &lt;- glm(disease_status~scale(age), family = binomial, data = data)  pvalues[i] &lt;- summary(model)$coefficients[2,4]  }    power_estimate &lt;- length(which(pvalues &lt; 0.01))/length(pvalues)    return(power_estimate)}sample_sizes &lt;- seq(150,200,10); power_estimates &lt;- c()for (i in 1:length(sample_sizes)){  power_estimates[i] &lt;- power_analysis_function(sample_size = sample_sizes[i])}knitr::kable(cbind(n = sample_sizes, power = power_estimates))</code>## Dealing with dependent data!To illustrate how mixed models work and what kinds of questions we can answer using them, we will use the sexual size dimorphism data which we analyzed last class. Recall that we did NOT find a significant effect of sex on the average body size. There was no effect of the interaction between Order and sex on body size. Indeed, this matches what you saw in the homework questions where you had to visualize the data — most of the variation in body size was <em>between</em> orders.In the models we built, we ignored a pretty important fact about the data: species have a common history (i.e., phylogeny). Thus, observations are not independent! This can make drawing inferences from comparative data difficult. <strong>We will address how to deal with the non-independence of data (due to a common history of species, replication in blocks, etc.) using three approaches.</strong>But, first, let’s download the data we will use in this section!<code>{r}SSDdata &lt;- read_csv("data/SSDinMammals.csv")``````{r}mammal_length &lt;- SSDdata %&gt;%  select(c("Order", "Scientific_Name", "lengthM", "lengthF")) %&gt;%  pivot_longer(c(lengthM, lengthF), names_to = "sex", values_to = "length",               names_pattern = "length(.)")mammal_mass &lt;- SSDdata %&gt;%  select(c("Order", "Scientific_Name", "massM", "massF")) %&gt;%  pivot_longer(c(massM, massF), names_to = "sex", values_to = "mass",               names_pattern = "mass(.)")mass_nodup &lt;- mammal_mass %&gt;%   group_by(Scientific_Name, sex) %&gt;%  distinct(Scientific_Name, sex, .keep_all = TRUE)length_nodup &lt;- mammal_length %&gt;%   group_by(Scientific_Name, sex) %&gt;%  distinct(Scientific_Name, sex, .keep_all = TRUE)mammal_long &lt;- full_join(mass_nodup, length_nodup,                          by = join_by("Scientific_Name", "sex", "Order")) |&gt;  drop_na()glimpse(mammal_long)</code>### Group-by-group analyses<strong>One first way we can handle dependent data is to split observations into groups such that, within each group, observations are independent (or approximately so).</strong> This is what we did last class when we fit order-specific regression coefficients of sex on body size. We saw that ALL order-specific effects had confidence intervals which overlapped zero!<code>{r, warning=F}# run linear model of size on sex for EACH OrderOrder_specific_models &lt;-   mammal_long |&gt;   group_by(Order) |&gt;  do(model = tidy(lm(log(mass) ~ sex, data = .), conf.int = T))  # get coefficients for each OrderOrder_specific_models |&gt;  unnest()# visualize effects, CIs, and p valuesOrder_specific_models |&gt;  unnest() |&gt;  subset(term == "sexM") |&gt;  group_by(Order) |&gt;  ggplot(aes(y = Order, x = estimate,              xmin = conf.low,             xmax = conf.high             )         ) +  geom_pointrange() +  geom_vline(xintercept = 0, lty = 2)</code>No effect of sex on body size, for any of the orders!#### ChallengeHow would you adjust the previous plot to show the estimated intercepts AND the effects of sex?<code>{r}Order_specific_models |&gt;  unnest() |&gt;  # subset(term == "sexM") |&gt;  group_by(Order) |&gt;  ggplot(aes(y = Order, x = estimate,              xmin = conf.low,             xmax = conf.high,             color = term             )         ) +  geom_pointrange() +  geom_vline(xintercept = 0, lty = 2)</code>#### ChallengeHow would you retrieve model fits for each Order in base R?<code>{r, include=F}for (i in 1:nrow(Order_specific_models)){  print(summary(Order_specific_models$mod[[i]]))}</code>#### ChallengeAdjust the plot above so that it the size of the estimates depend on the number of observations in an Order? <em>Hint: determine the number of observations per Order and then use the <code>merge()</code> function with the <code>Order_specific_models</code> tibble. To adjust the range of point sizes, use <code>scale_size()</code>.</em><code>{r, include=F}mammal_long |&gt;   group_by(Order) |&gt;  summarise(number_obs = n()) |&gt;  merge(Order_specific_models) |&gt;  unnest() |&gt;  ggplot(aes(y = Order, x = estimate,              xmin = conf.low,             xmax = conf.high,             color = term,             size = number_obs             )         ) +  geom_pointrange() +  geom_vline(xintercept = 0, lty = 2) +  scale_size(range = c(0, 2))</code>#### Pros and consThere are several advantages to preforming a group-by-group analysis:- <strong>Fitting more complex models (e.g., those with random effects) can be difficult.</strong> There may be convergence issues, and interpretation of effects and <span class="math inline">\(p\)</span>-values can be tricky.- The group-by-group analysis is <strong>robust in the face of unbalanced data</strong> (i.e., when there are more observations for some groups than others).- <strong>The conclusions from a group-by-group analysis are <em>conservative</em>.</strong>- The group-by-group analysis is <strong>fairly easy to implement</strong>.Among the disparages to this approach are the following:- With fewer samples per group, the analysis <strong>may be under-powered</strong>.- Splitting the data into groups means there are more coefficients to estimate, and thus confidence intervals to be estimated and hypothesis tests to be performed. This means there is a greater chance that we obtain a <strong>spurious association</strong>.<em>- It is <strong>difficult to draw conclusions about the variance between groups.</strong> In some applications, this is of interest. In others, it is not.</em>One solution is to adjust the significant level based on the number of tests conducted. If <span class="math inline">\(k\)</span> hypotheses are tested, an <a href="https://en.wikipedia.org/wiki/Bonferroni_correction">common adjustment</a> is to set <span class="math inline">\(\alpha = 0.05/k\)</span>.### Controlling for phylogenyA common reason data are dependent in biology is that <strong>species share a common history of descent with modification.</strong> When we assume that the observations are independent, we make implicit assumptions about the degree to which the characters at the tips of a phylogeny have underwent independent evolution. Sometimes, when species are diverged by many millions of years and traits evolve quickly, it is reasonable to ignore the phylogenetic constraints on the data. In other cases, it is essential to consider the role of phylogeny.Several methods can control for phylogeny (if known). In fact, such methods can <em>use</em> information in the branching pattern of species to draw inferences about the evolution of ecologically-important traits (such as body and brain size, dispersal rate, etc.). We will not discuss how to implement phylogenetic comparative methods, but it is good to know they exist and how, at a surface level, they work.Intuition for how PCMs work is easiest to grasp when we consider a tree with <span class="math inline">\(n\)</span> species at the tips. If we know the pairwise divergence times for all species, then we can <em>transform</em> the data so that observations are independent by looking at all <em>differences</em> of traits. Not all differences are equally informative; if species have diverged a long time ago, there has been more time for differences to build up. PCMs account for this by specifying how the distribution of trait differences between species <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> depends on the time that has elapsed since these species diverged, In particular, the larger the divergence time, the greater the variance in <span class="math inline">\(Y_i-Y_j\)</span>, the difference in trait values between species <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>. A simple way to do this is to write <span class="math inline">\(\sigma_{ij} = \sigma^2/T_{ij}\)</span>.### Random effects!Another way one can account for dependent data is by including <em>random effects</em>. Random effects are often used to control for the fact that observations are clustered (e.g., trait data for species belonging to a higher taxonomic unit, measurements of plant biomass from plots of land).#### Structure and assumptionsA common way models with random effects are formulated is as follows:<span class="math display">\[Y_{ij} \sim \text{Normal}(\beta_0 + \beta_1 x_{1ij} + \cdots + \beta_{p} x_{pij} + b_{0i} + b_{1i} z_{1ij} + \cdots + b_{qi} z_{qij}, \sigma_{ij}^2),\]</span>where <span class="math inline">\(ij\)</span> is the <span class="math inline">\(i\)</span>th observation from the <span class="math inline">\(j\)</span>th cluster and<span class="math display">\[b_{ki} \sim \text{Normal}(0,\tau_k^2).\]</span>This gives rise to a distribution for <span class="math inline">\(Y\)</span> which depends on the values of the random effects <span class="math inline">\(b_0,\dots,b_q\)</span>. Under the hood, methods that fit models of this form numerically maximize a version of the likelihood function that results from these assumptions.#### ChallengeIn each of the following examples, which effects might be reasonably treated as fixed vs.&nbsp;random? Justify your answer.1. Suppose we are working with rodents that have been infected with an evolved strain of the parasite that causes malaria. Some rodents have been treated with a prospective vaccine and others sham-vaccinated. We are interested if a proxy for virulence (e.g., density of infected red blood cells) depends on vaccination status.2. Suppose we conduct the same experiment, except rodents are caged are sets of four.3. Suppose we measure the time between sightings of a raccoon in a Toronto neighborhood using six camera traps (strategically placed in the neighborhood). We do this for year, and want to ask if mean daily temperature predicts the frequency of raccoon occurrence.4. Suppose that, every year, we go to the Amazon and measure the number of bird calls that come from <span class="math inline">\(n=30\)</span> trees. Assume that the trees are far enough apart we can treat them as independent. We measure the number of birds in a tree, characteristics of the tree (cover, height, width, age), the year in which a measurement was done, and the mean temperature that year. We want to know if tree cover affects the frequency of bird calls, and if it interacts with temperature.For more on the difference between fixed and random effects, check out the following resources- <a href="https://stats.stackexchange.com/questions/4700/what-is-the-difference-between-fixed-effect-random-effect-in-mixed-effect-model">this Cross Validated post</a>- <a href="https://dynamicecology.wordpress.com/2015/11/04/is-it-a-fixed-or-random-effect/">this Dynamic Ecology post</a>- <a href="https://bookdown.org/steve_midway/DAR/random-effects.html">this book chapter</a>#### Using <code>lme4</code> to fit random and mixed effect modelsWe will start by fitting a model of log body size on log length where the intercept is random depending on the Order. Based on a quick visualization of the data, such a model may be appropriate.<code>{r}ggplot(data = mammal_long, aes(y = log(mass),                                x = log(length)                               )       ) +     geom_point(aes(color = Order), size = 3) -&gt; pp ``````{r, include=F}# no Order-dependence:p + geom_smooth(method = "lm")# intercept AND slope fixed and specific to the Order:p + geom_smooth(method = "lm") + facet_wrap(~Order)</code>To fit regress the response on a set of fixed effects with random <em>intercepts</em> that depends the values assumed by a random effect <code>x</code>, we call <code>lmer</code> and write a linear model with <code>(1|x)</code>.<code>{r}## random intercept for Ordermodel &lt;- lmer(formula = log(mass) ~ log(length) + (1|Order), data = mammal_long)summary(model)fixef(model)ranef(model)</code>From top to bottom, the output is telling us that the model was fit using a method called REML (restricted maximum likelihood). It returns information about the residuals, random effects, and fixed effects. The output also tells us about the <strong>estimated variance for the random effects</strong> in the model. Here, the variance associated with Order is 4.256. The variance explained by Order is 4.256/(4.256+0.847) <em>after controlling for the fixed effects</em>. Note the denominator here is the total variance, including from the residuals. As usual, we also have information about the fixed effects.To visualize the model, we can predict the overall and Order-specific relationship of log mass on log length for all of the Orders represented in the data.<code>{r}mammal_long |&gt; ungroup() |&gt; select(mass, length, Order) |&gt;   mutate(fit.m = predict(model, re.form = NA), # does not include random effects         fit.c = predict(model, re.form = NULL) # includes random effects         ) -&gt;  predicted_valuespredicted_values |&gt;  ggplot(aes(x = log(length), y = log(mass), color = Order)) +  geom_point(size = 3) +  geom_line(inherit.aes = F, aes(x = log(length), y = fit.c, color = Order), size = 1) +  geom_line(inherit.aes = F, aes(x = log(length), y = fit.m), color = "black", size = 2)</code>The thick black line corresponds to the fitted values associated with the fixed-effect component of the model. The colored lines correspond to the fitted values estimated for each Order. Perhaps a model with Order-specific random intercepts AND slopes would be better. We fit this model using the code below. The key difference in syntax is that we write <code>(1+fixed effect|random effect)</code> to indicate that the random effect has an effect on both the intercept <em>and</em> slope of the response on the fixed effect.<code>{r}## random intercept and slope for Ordermodel2 &lt;- lmer(formula = log(mass) ~ log(length) + (1+log(length)|Order), data = mammal_long)summary(model2)fixef(model2)ranef(model2)``````{r}mammal_long |&gt; ungroup() |&gt; select(mass, length, Order) |&gt;   mutate(fit.m = predict(model2, re.form = NA),         fit.c = predict(model2, re.form = NULL)         ) -&gt;  predicted_valuespredicted_values |&gt;  ggplot(aes(x = log(length), y = log(mass), color = Order)) +  geom_point(size = 3) +  geom_line(inherit.aes = F, aes(x = log(length), y = fit.c, color = Order), size = 1) +  geom_line(inherit.aes = F, aes(x = log(length), y = fit.m), color = "black", size = 2)</code>Why does this look like a mess? As stated above, <strong>mixed models do not work well when datasets are unbalanced.</strong> Indeed, the number of observations in the different Orders are quite variable.#### ChallengeRegress log body size on sex with Order as a random effect that affects the intercept AND slope of the sex-size relationship.<code>{r, include=F}Model &lt;- lmer(log(mass) ~ sex + (1+sex|Order), mammal_long)mammal_long |&gt; ungroup() |&gt; select(Scientific_Name, mass, sex, Order) |&gt;   mutate(fit.m = predict(Model, re.form = NA),         fit.c = predict(Model, re.form = NULL)         ) -&gt;  predicted_valuespredicted_values |&gt;  ggplot(aes(x = sex, y = log(mass))) +  geom_line(aes(group = Scientific_Name), color = "lightgrey") +  geom_point(color = "lightgrey", size = 2) +  geom_point(inherit.aes = F, aes(x = sex, y = fit.c, color = Order), size = 3) +  geom_line(inherit.aes = F, aes(x = sex, y = fit.c, color = Order, group = Order), size = 2) +  geom_point(inherit.aes = F, aes(x = sex, y = fit.m), color = "black", size = 3) +  geom_line(inherit.aes = F, aes(x = sex, y = fit.m, group = ""), color = "black", size = 3) +  theme_classic()</code>
                
  </div>
  <div class="nav-page nav-page-next">
      <a href="../assignments/assignment-01.html" class="pagination-link" aria-label="Assignment 01: base R, command line, &amp; Git(Hub)">
        <span class="nav-page-text">Assignment 01: base R, command line, &amp; Git(Hub)</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>