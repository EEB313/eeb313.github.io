[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "",
    "text": "1 Syllabus"
  },
  {
    "objectID": "index.html#land-acknowledgement",
    "href": "index.html#land-acknowledgement",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "1.1 Land Acknowledgement",
    "text": "1.1 Land Acknowledgement\nAlthough our students come from many locations around the world, we wish to recognize the land on which the University of Toronto was built. This land has historically been and still is the the home of the Huron-Wendat, the Seneca, and the Mississaugas of the Credit River.\nThere is a First Nations House for Indigenous Student Services on campus. Please refer to their web page for more resources and information about honouring our land and their services for students."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "1.2 Course Overview",
    "text": "1.2 Course Overview\nThis course covers statistical and data analysis, reproducible quantitative methods, and scientific computing in R to answer questions in ecology and evolutionary biology. Statistical and data analysis, modeling, and computing are essential skills for all biologists. This course is designed to meet a growing demand for reproducible, openly accessible, analytically thorough, and well-documented science. Students will learn to analyze and visualize data, develop mathematical models, and document their research using the R programming language. No prerequisite programming experience is required.\nPrerequisites: BIO220H1 and one of EEB225H1, STA288H1, or STA220H1\n\n1.2.1 Time\nTue and Thu 2:10 - 4:00 PM EST.\n\n\n1.2.2 Class Locations\nRW109 (Ramsay Wright first floor computer lab), St. George Campus.\n\n\n\nOffice hours (in EST)\n\n\n\n\n\n\nMete\nWeds 11-12pm\nESC3044\n\n\nVicki\nMon 11-12pm\nESC3044\n\n\nZoe\nThurs 4-5pm\nRW109\n\n\nJessie\nTues 4-5pm\nRW109\n\n\n\n\n\n1.2.3 Contact protocol\nPlease address all course-related and project issues to both Vicki and Mete, with the exception that questions regarding assignments should be addressed to Zoe and Jessie. Prefix the subject matter with “EEB313”. If you do not receive a reply within 48 hours (excluding weekends), please send us a reminder."
  },
  {
    "objectID": "index.html#diversity-and-inclusion-statement",
    "href": "index.html#diversity-and-inclusion-statement",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "1.3 Diversity and inclusion statement",
    "text": "1.3 Diversity and inclusion statement\nAs students, you all have something unique and special to offer to science. It is our intent that students from all backgrounds and perspectives be well served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that students bring to this class be recognized as a resource, strength, and benefit.\nDiversity can refer to multiple ways that we identify ourselves, including but not limited to race, national origin, language, cultural heritage, physical ability, neurodiversity, age, sexual orientation, gender identity, religion, and socio-economic class. Each of these varied, and often intersecting, identities, along with many others not mentioned here, shape the perspectives we bring to this class, to this department, and to the greater EEB community. We will work to promote diversity, equity, and inclusion not only because diversity fuels excellence and innovation, but because we want to pursue justice.\nWe expect that everybody in this class will respect each other, and demonstrate diligence in understanding how other people’s perspectives, behaviors, and worldviews may be different from their own. Racist, sexist, colonialist, homophobic, transphobic, and other abusive and discriminatory behavior and language will not be tolerated in this class and will result in disciplinary action, such as removal from class session or revocation of group working privileges. Please consult the University of Toronto Code of Student Conduct for details on unacceptable conduct and possible sanctions.\nPlease let us know if something said or done in this class, by either a member of the teaching team or other students, is particularly troubling or causes discomfort or offense. While our intention may not be to cause discomfort or offense, the impact of what happens throughout the course is not to be ignored and is something that we consider to be very important and deserving of attention. If and when this occurs, there are several ways to alleviate some of the discomfort or hurt you may experience:\n\nDiscuss the situation privately with a member of the teaching team. We are always open to listening to students’ experiences, and want to work with students to find acceptable ways to process and address the issue.\nNotify us of the issue through another source such as a trusted faculty member or a peer. If for any reason you do not feel comfortable discussing the issue directly with us, we encourage you to seek out another, more comfortable avenue to address the issue.\nContact the Anti-Racism and Cultural Diversity Office to report an incident and receive complaint resolution support, which may include consultations and referrals.\n\nWe acknowledge our imperfections while we also fully commit to the work, inside and outside of our classrooms, of building and sustaining a community that increasingly embraces these core values. Your suggestions and feedback are encouraged and appreciated. Please let us know ways to improve the effectiveness of the course for you personally or for other students or student groups."
  },
  {
    "objectID": "index.html#wellness-statement",
    "href": "index.html#wellness-statement",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Wellness statement",
    "text": "Wellness statement\nWe on the teaching team value your health and wellness. In order to succeed in this class, in university, and beyond, you must balance your work with rest, exercise, and attention to your mental and physical health. Working until exhaustion is NOT a badge of honor. If you are finding it difficult to balance your health and well-being with your work in this class, please do not hesitate to let us know. We are happy to help connect you with resources and services on campus and also to make accommodations to our course plan as needed. Our inboxes are always open, and we are also available for virtual chats by appointment. You have our support, and we believe in you."
  },
  {
    "objectID": "index.html#absence-policy",
    "href": "index.html#absence-policy",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Absence policy",
    "text": "Absence policy\nIf you are feeling unwell, please do not come to class. Instead, take the time to recover fully. Please let us know if you are feeling sick - you will not be penalized for missing a lecture, and we will do our best to ensure that you are up-to-date with class materials when you return."
  },
  {
    "objectID": "index.html#accessibility-needs",
    "href": "index.html#accessibility-needs",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "1.4 Accessibility needs",
    "text": "1.4 Accessibility needs\nIf you require accommodations for a disability, or have any accessibility concerns about the course or course materials, please notify your course instructors (Mete and Vicki), or contact Accessibility Services, as soon as possible regarding accommodations."
  },
  {
    "objectID": "index.html#course-learning-outcomes",
    "href": "index.html#course-learning-outcomes",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "1.5 Course learning outcomes",
    "text": "1.5 Course learning outcomes\n\nDevelop proficiency in the programming language R.\nUse R to apply statistical tools to analyze and interpret data.\nDevelop an understanding of mathematical models.\nDevelop proficiency in using the command line and Git.\nIntegrate appropriate techniques to analyze a variety of data types and formats.\nLearn and use techniques and best practices for reproducible, high-quality science.\nLearn how to work as part of a research team to produce a scientific product.\n\n\n1.5.1 Lecture schedule\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nInstructor\n\n\n\n\n1\nSep 7\nIntro to course, programming, RStudio\nVicki\n\n\n2\nSep 12\nR Markdown, project workflows\nVicki\n\n\n2\nSep 14\nBase R: Assignment, vectors, functions\nMete\n\n\n3\nSep 19\nData frames, intro to dplyr.\nVicki\n\n\n3\nSep 21\nData wrangling in dplyr\nVicki\n\n\n4\nSep 26\nData visualization in ggplot\nVicki\n\n\n4\nSep 28\nExploratory data analysis\nZoe\n\n\n5\nOct 03\nIntroduction to statistical inference\nMete\n\n\n5\nOct 05\nSimple linear models and generalized linear models\nMete\n\n\n6\nOct 10\nMixed models\nMete\n\n\n6\nOct 12\nMixed models & model selection\nVicki\n\n\n7\nOct 17\nMultivariate statistics\nJessie\n\n\n7\nOct 19\nIntro to command line and GitHub\nMete/Vicki\n\n\n8\nOct 24\nMore on LMs, GLMs, LMMs, and model selection!\nMete\n\n\n8\nOct 26\nMathematical models in ecology and evolution I\nMete\n\n\n9\nOct 31\nMathematical models in ecology and evolution II\nMete\n\n\n9\nNov 02\nWrap-up, review\nMete\n\n\n10\nNov 07\nReading break\n-\n\n\n10\nNov 09\nReading break\n-\n\n\n11\nNov 14\nProject work\n\n\n\n11\nNov 16\nProject work\n\n\n\n12\nNov 21\nProject work\n\n\n\n12\nNov 23\nProject work\n\n\n\n13\nNov 28\nProject work\n\n\n\n13\nNov 30\nProject work\n\n\n\n14\nDec 05\nGroup presentations\nEveryone\n\n\n\n\n\n1.5.2 Lecture readings\nYou will find a list of recommended readings posted under “Resources”. Since there are no exams in this class, you will not be tested on the readings directly. However, we highly recommend that you go through these readings as they were chosen to help you understand lecture material better (e.g., provides context for data that was used) as well as serve as resources for you if you wish to pursue any specific topic further. We also compiled a list of open-access R and statistics resources for your reference. You can find this list in the Readings folder on Quercus.\n\n\n1.5.3 Assessment schedule\n\n\n\n\n\n\n\n\n\n\nAssignment\nType\nSubmitted on\nDue date\nMarks\n\n\n\n\nBasic R and dplyr\nIndividual\nQuercus\nSep 28\n8\n\n\nProject proposal\nGroup\nGitHub\nOct 03\n4\n\n\ndplyr and tidy data\nIndividual\nQuercus\nOct 05\n8\n\n\nData exploration\nIndividual\nQuercus\nOct 12\n8\n\n\nLM, GLM, & LMM\nIndividual\nQuercus\nOct 19\n8\n\n\nMultivariate and command line\nIndividual\nQuercus\nOct 26\n8\n\n\nMid-project update\nGroup\nGitHub\nNov 02\n6\n\n\nChallenge assignment\nIndividual\nQuercus\nNov 17\n20\n\n\nPresentation\nGroup\nIn-class\nDec 05\n10\n\n\nFinal report\nGroup\nGitHub\nDec 08\n20\n\n\n\nThere are 100 marks in total. Your final course mark will be the sum of your assignment scores, which will be translated to a letter grade according to the official grading scale of the Faculty of Arts and Science.\nAssignments will be distributed and submitted in the R Markdown format via Quercus. Assignments will be handed out on Thursdays after class and are due at 8:00 PM on the following Thursday.\nThe Challenge Assignment is equivalent to a take home exam. The format will be the same as the other assignments, but this assignment is designed challenge you to go a little beyond what was taught in class. It will be distributed on 9:00 AM on Nov 13, and it will be due 11:59 PM on Nov 17. Students are welcome to work in a group on this assignment, but each student must submit their own original work. No extensions will be granted on this assignment except under the same extra-ordinary circumstances akin to those under which an exam might be deferred. We only expect you to do your best!\nAs per our stance on supporting student’s mental health, we are happy to accommodate a 72-hour extension for one of the assignments, no questions asked. Otherwise, except under extenuating circumstances, there will be a penalty of 5% per day (including weekends) for all late submissions. If you foresee needing an extension, please email both Vicki and Mete as soon as possible. This policy does not apply to the Challenge Assignment, Presentation and Final Report."
  },
  {
    "objectID": "index.html#improving-your-writing-skills",
    "href": "index.html#improving-your-writing-skills",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "1.6 Improving your writing skills",
    "text": "1.6 Improving your writing skills\nEffective communication is crucial in science. The University of Toronto provides services to help you improve your writing, from general advices on effective writing to writing centers and writing courses. The Faculty of Arts & Science also offers an English Language Learning (ELL) program, which provides free individualized instruction in English skills. Take advantage of these!"
  },
  {
    "objectID": "index.html#academic-integrity",
    "href": "index.html#academic-integrity",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "1.7 Academic integrity",
    "text": "1.7 Academic integrity\nYou should be aware of the University of Toronto Code of Behaviour on Academic Matters. Also see How Not to Plagiarize. Notably, it is NOT appropriate to use large sections from internet sources, and inserting a few words here and there does not make it an original piece of writing. Be careful in using internet sources – most online material are not reviewed and there are many errors out there. Make sure you read material from many sources (published, peer-reviewed, trusted internet sources) and that you write an original text using this information. Always cite your sources. In case of doubt about plagiarism, talk to your instructors and TAs. Please make sure that what you submit for the final project does not overlap with what you submit for other classes, such as the 4th-year research project.\n\n1.7.1 On the use of Generative AI\nThe knowing use of generative artificial intelligence tools, including ChatGPT and other AI writing and coding assistants, for the completion of, or to support the completion of, the assignments, the challenge assignment, or the final project is prohibited and may be considered an academic offense."
  },
  {
    "objectID": "index.html#fas-student-engagement-programs",
    "href": "index.html#fas-student-engagement-programs",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "1.8 FAS student engagement programs",
    "text": "1.8 FAS student engagement programs\nThere are a few programs on campus aimed at increasing student engagement with their coursework and keeping them socially connected. Recognized Study Groups are voluntary, peer-led study groups of up to 8 students enrolled in the same course. Meet to Complete are online drop-in study sessions for A&S undergrads. These are worth checking out if you are interested in participating in a study group."
  },
  {
    "objectID": "about-us.html#course-instructors",
    "href": "about-us.html#course-instructors",
    "title": "The 2023 teaching team",
    "section": "Course instructors",
    "text": "Course instructors\nMete Yuksel (mete.yuksel@mail.utoronto.ca)\nMete is a 2nd year PhD student in the Dept. Ecology and Evolutionary Biology at UTSG, and is co-advised by Matt Osmond & Nicole Mideo. He uses mathematical models to address questions in evolutionary genetics and ecology. He is currently developing theory to understand patterns, causes, and consequences of pathogen recombination. Mete was an undergraduate math & statistics student at the University of Idaho. There, he worked on the ecology of gene drive interventions against vectored diseases, understanding how continuous spatial structure can affect species coexistence, and forecasting the dynamics of Chinook salmon in the Willamette River system. He loves modeling, statistics, and programming in R (among other languages!) - and is thrilled to be teaching this course! Outside of science, he enjoys listening to podcasts, cooking, and cycling between Toronto neighborhoods in search of coffee.\nVicki Zhang (vm.zhang@mail.utoronto.ca)\nVicki is a PhD Candidate in Peter Kotanen’s lab at UTM. She studies invasive species at the edge of the Arctic, and how climate change and anthropogenic stressors might affect the persistence and spread of invasive species in Canada’s tundra and boreal forests. Vicki does fieldwork in Churchill, Manitoba (the polar bear capital of the world) in the summer, but does a lot of data analyses and modelling on R throughout the fall and winter semesters. She took EEB313 in 2018, and it was her favourite undergraduate course because everything about R just clicked, and she learned so much about coding, statistics, and modelling (her previous encounters with R and stats went very poorly). Now, she spends a lot of time making sure that her data is clean and her code is fast - it’s so satisfying when every line finally runs! For Vicki, the best way to work in R is with a big cup of coffee, soft classical music in the background, and her cat Remi napping next to her."
  },
  {
    "objectID": "about-us.html#teaching-assistants",
    "href": "about-us.html#teaching-assistants",
    "title": "The 2023 teaching team",
    "section": "Teaching assistants",
    "text": "Teaching assistants\nZoe Humphries (zoe.humphries@mail.utoronto.ca)\nZoë is a PhD student in the Wright and Barrett labs at UTSG. She studies the genome of a weedy plant to better understand how transposable elements affect sex chromosome evolution. She taught herself how to use R during her honours thesis and fell in love because it was much kinder than Lisp or C++ and, most importantly, because aesthetics. Zoë makes plots from aggressively large genomic data sets and spends a lot of time literally bash-ing her data into a file small enough for her computer to load into RStudio. For Zoë, the best way to work in R is while patting her puppies.\nJessie Wang (jae.wang@mail.utoronto.ca)\nJessie is a PhD student in the Frederickson lab at UTSG. She studies plant-microbe interactions using high-throughput experimentation in duckweeds. She fell in love with R during her time as an undergraduate and took EEB313 in 2020, simultaneously sharpening her coding skills while conducting research alone in the lab. Jessie loves to spend too much money on fancy coffee as she types away, making sure her code is well-annotated and her figures look beautiful. Outside of work, she enjoys caring for her many houseplants and aquariums, finding new delicious eats, and admiring other people’s pets."
  },
  {
    "objectID": "downloadingR.html#introduction",
    "href": "downloadingR.html#introduction",
    "title": "Downloading R",
    "section": "Introduction",
    "text": "Introduction\nThis course makes extensive use of R and RStudio. If you have any issues installing R or RStudio, let’s resolve them in advance so that you can make the most of our time together! As you follow these instructions, don’t worry if you don’t understand exactly what is going on - we will go through it all again during the first lecture! The goal here is to ensure that your computer is set up and ready for action. If you can’t complete these steps, please email Vicki and Mete as soon as possible."
  },
  {
    "objectID": "downloadingR.html#tech-requirements",
    "href": "downloadingR.html#tech-requirements",
    "title": "Downloading R",
    "section": "Tech Requirements",
    "text": "Tech Requirements\nFirst, make sure that your personal computer meets meets these requirements:\n\n1024×768 screen resolution,\n1.5GHz CPU,\n2GB RAM,\n10GB free disk space,\na recent version of Windows, Mac OS, or Linux is installed.\n\nNext, check that a robust internet browser such as Firefox, Safari or Google is installed. (Internet Explorer will not work.) Finally, make that a PDF viewer (e.g., Adobe Acrobat, Preview) is installed. It is fine if no PDF viewer is installed if you can view PDFs on your web browser."
  },
  {
    "objectID": "downloadingR.html#download-r-and-rstudio",
    "href": "downloadingR.html#download-r-and-rstudio",
    "title": "Downloading R",
    "section": "Download R and RStudio",
    "text": "Download R and RStudio\n\nDownload R, a free software environment for statistical computing and graphics from CRAN, the Comprehensive R Archive Network. We recommend you install a precompiled binary distribution for your operating system – use the links up at the top of the CRAN page!\n\nNote: MacBook users with an Apple Silicon chip (e.g., M1 or M2) should install the “arm64” version of R, while MacBook users with an Intel chip should install the regular (64-bit) version of R. You can check your laptop’s hardware specifications by clicking the Apple icon (top left corner) \\&gt; About This Mac.\n\nInstall RStudio, a graphical user interface (GUI) for R. Click the link under “2: Install RStudio”. RStudio comes with a text editor, so there is no immediate need to install a separate stand-alone editor.\n\nIf R is already installed, ensure that the R version is 4.0 or higher. You can do this by opening RStudio, where you should see a multi-section window like below. Locate the quadrant named “Console”, and put your cursor at the start of the prompt indicated by the &gt; symbol and where the red arrow is pointing. Type or copy sessionInfo() - make sure that only the I at the start of Info is capitalized and you are including the round brackets. Press enter to run this command and R should return an output to you. The first line shows what version of R is installed. Ensure that the R version installed is at least 4.0."
  },
  {
    "objectID": "downloadingR.html#download-packages",
    "href": "downloadingR.html#download-packages",
    "title": "Downloading R",
    "section": "Download Packages",
    "text": "Download Packages\n\nTinyTex\nThere is one package we have to install first before we can create PDF reports, which will be necessary for assignments and the project. Copy and paste into the console (where the \\&gt; symbol is) the two lines of code below to install a package called tinytex.\n\ninstall.packages(\"tinytex\") \ntinytex::install_tinytex()\n\n\n\nAll packages\n\nCopy and paste the below code into your console.\n\n\ninstall.packages(c(\"tidyverse\", \"here\", \"knitr\", \"datasauRus\", \"car\",\n                   \"lme4\", \"lmerTest\", \"ggalt\", \"MuMIn\", \"sjmisc\",\n                   \"lsmeans\", \"ggfortify\", \"vegan\"), dependencies = TRUE)\n\nDuring installation, if you ever get the below message, click “No”.\n\nIf you get the message “Do you want to install from sources the packages which need compilation? (Yes/no/cancel)” in the Console, type “Yes” and press enter.\n\nCheck that these packages have been installed correctly. To do this, go to the bottom right pane and click the tab for “Packages”. If you can search for and find the below packages, then they have been installed! They do not need to be checked off. Alternatively, go to the Console and type library(lme4) to verify that the lme4 package is installed. An error along the lines “there is no package called lme4” will be returned if the package is not installed."
  },
  {
    "objectID": "lec00-rstudio.html#lesson-preamble",
    "href": "lec00-rstudio.html#lesson-preamble",
    "title": "2  Getting started with RStudio and R Notebook",
    "section": "Lesson preamble",
    "text": "Lesson preamble\n\n2.0.1 Lesson objectives\n\nIntroduce students to the RStudio interface\nIntroduce the Markdown syntax and how to use it within the R Notebook\n\n2.0.2 Learning outline\n\nExplore RStudio interface (20 mins)\nRMarkdown (20 mins)\nGenerating reports (10 mins)\nKnit to PDF and submit on Quercus (10 mins)"
  },
  {
    "objectID": "lec00-rstudio.html#working-with-computers",
    "href": "lec00-rstudio.html#working-with-computers",
    "title": "2  Getting started with RStudio and R Notebook",
    "section": "Working with computers",
    "text": "Working with computers\nBefore we get into more practical matters, we want to provide a brief background to the idea of working with computers. Essentially, computer work is about humans communicating with a computer by modulating flows of current in the hardware in order to get the computer to carry out advanced calculations that we are unable to efficiently compute ourselves. Early examples of human computer communication were quite primitive and included physically disconnecting a wire and connecting it again in a different spot. Luckily, we are not doing this anymore; instead we have programs with graphical user interfaces with menus and buttons that enable more efficient human to computer communication.\n\n2.0.3 Graphical user interfaces vs. text based user interfaces\nAn example of such a program that many of you are familiar with is spreadsheet software such as Microsoft Excel. Here, all the functionality of the program is accessible via hierarchical menus, and clicking buttons sends instructions to the computer, which then responds and sends the results back to your screen. For instance, I can click a button to send the instruction of coloring this cell yellow, and the computer interprets my instructions and then displays the results on the screen.\nSpreadsheet software is great for viewing and entering small data sets and creating simple visualizations fast. However, it can be tricky to design figures, create automatic reproducible analysis workflows, perform advanced calculations, and reliably clean data sets. Even when using a spreadsheet program to record data, it is often beneficial to have some some basic programming skills to facilitate the analyses of those data.\nTyping commands directly instead of searching for them in menus is a more efficient approach to communicate with the computer and a powerful way of doing data analysis. This is initially intimidating for almost all people, but if you compare it to learning a new language, the process becomes more intuitive: when learning a language, you would initially string together sentences by looking up individual words in the dictionary. As you improve, you will only reference the dictionary occasionally since you already know most of the words. Eventually, you will throw the dictionary out altogether because it is faster and more precise to speak directly. In contrast, graphical programs force you to look up every word in the dictionary every time, even if you already know what to say.\n\n\n2.0.4 RStudio and the R Notebook\nRStudio includes the R console, but also many other convenient functionalities, which makes it easier to get started and to work with R. When you launch RStudio, you will see four panels. Starting at the top left and going clockwise, these panels are:\n\nThe text editor panel. This is where we can write scripts, i.e. putting several commands of code together and saving them as a text document so that they are accessible for later and so that we can execute them all at once by running the script instead of typing them in one by one.\nThe environment panel, which shows us all the files and objects we currently loaded into R.\nThe files-plots-help panel. This panel shows the files in the current directory (the folder we are working out of), any plots we make later, and also documentation for various packages and functions. Here, the documentation is formatted in a way that is easier to read and also provides links to the related sections.\nThe console is another space we can input code, only now the code is executed immediately and doesn’t get saved at the end.\n\nTo change the appearance of your RStudio, navigate to Tools &gt; Global Options &gt; Appearance. You can change the the font and size, and the editor theme. The default is “Textmate”, but if you like dark mode, I recommend “Tomorrow Night Bright”. You can also change how your panels are organized. I like to have my Console and history below my Source, and that way I can see my working environment next to my code. That way, I know if an error I am getting is because I am missing an object or I renamed something oddly. Let’s change that now. I recommend playing around with the appearance if you prefer a different layout or colour scheme. Do what makes you the most productive!\nAnother very useful thing with RStudio is that you have access to some excellent cheat sheets in PDF format straight from the menu: Help -&gt; Cheatsheets!\nIn the RStudio interface, we will be writing code in a format called the R Notebook. As the name entails, this interface works like a notebook for code, as it allows us to save notes about what the code is doing, the code itself, and any output we get, such as plots and tables, all together in the same document.\nWhen we are in the Notebook, the text we write is normal plain text, just as if we would be writing it in a text document. If we want to execute some R code, we need to insert a code chunk.\nYou insert a code chunk by either clicking the “Insert” button or pressing Ctrl/Command + Alt + i simultaneously. You could also type out the surrounding backticks, but this would take longer. To run a code chunk, you press the green arrow, or Ctrl/Command + Shift + Enter.\n\n1+1\n#&gt; [1] 2\n\nAs you can see, the output appears right under the code block.\nThis is a great way to perform explore your data, since you can do your analysis and write comments and conclusions right under it all in the same document. A powerful feature of this workflow is that there is no extra time needed for code documentation and note-taking, since you’re doing your analyses and taking notes at the same time. This makes it great for both taking notes at lectures and to have as a reference when you return to your code in the future."
  },
  {
    "objectID": "lec00-rstudio.html#r-markdown",
    "href": "lec00-rstudio.html#r-markdown",
    "title": "2  Getting started with RStudio and R Notebook",
    "section": "2.1 R Markdown",
    "text": "2.1 R Markdown\nThe text format we are using in the R Notebook is called R Markdown. This format allows us to combine R code with the Markdown text format, which enables the use of certain characters to specify headings, bullet points, quotations and even citations. A simple example of how to write in Markdown is to use a single asterisk or underscore to emphasize text (*emphasis*) and two asterisks or underscores to strongly emphasize text (**strong emphasis**). When we convert our R Markdown text to other file formats, these will show up as italics and bold typeface, respectively. If you have used WhatsApp, you might already be familiar with this style of writing. In case you haven’t seen it before, you have just learned something about WhatsApp in your quantitative methods class…\nTo learn more about R Markdown, you can read the cheat sheets in RStudio and RStudio Markdown reference online.\n\n2.1.1 Saving data and generating reports\nTo save our notes, code, and graphs, all we have to do is to save the R Notebook file, and the we can open it in RStudio next time again. However, if we want someone else to look at this, we can’t always just send them the R Notebook file, because they might not have RStudio installed. Another great feature of R Notebooks is that it is really easy to export them to HTML, MS word, or PDF documents with figures and professional typesetting. There are actually many academic papers that are written entirely in this format and it is great for assignments and reports. (You might even use it to communicate with your collaborators!) Since R Notebook files convert to HTML, it is also easy to publish simple and good-looking websites in it, in which code chunks are embedded nicely within the text.\nLet’s try to create a document in R.\nFirst, let’s set up the YAML block. This is found at the top of your document, and it is where you specify the title of your document, what kind of output you want, and a few other things such as author list and date.\n\n---\ntitle: \"Your title here\"\nauthor: \"Your name here\"\ndate: \"Insert date\"\n---\n\nThen, let’s type some notes and code together!\n\n# Attempt 1\n\n## Here goes!\n\n\n1+2+3+4\n#&gt; [1] 10\n\nx &lt;- seq(0,100,1)\n\nplot(x, type = \"l\")\n\n\n\n\n\n\n\n\nLet’s see what this looks like. To create the output document, we poetically say that we will knit our R Markdown into the HTML document. Luckily, it is much simpler than actually knitting something. Simply press the Knit button here and the new document will be created.\nAs you can see in the knitted document, the title showed up as we would expect, the lines with pound sign(s) in front of them were converted into headers and we can see both the code and its output! So the plots are generated directly in the report without us having to cut and paste images! If we change something in the code, we don’t have to find the new images and paste it in again, the correct one will appear right in your code.\nWhen you quit, R will ask you if you want to save the workspace (that is, all of the variables you have defined in this session); in general, you should say “no” to avoid clutter and unintentional confusion of results from different sessions. Note: When you say “yes” to saving your workspace, it is saved in a hidden file named .RData. By default, when you open a new R session in the same directory, this workspace is loaded and a message informing you so is printed: [Previously saved workspace restored]."
  },
  {
    "objectID": "lec00-rstudio.html#exercise",
    "href": "lec00-rstudio.html#exercise",
    "title": "2  Getting started with RStudio and R Notebook",
    "section": "Exercise",
    "text": "Exercise\n\n2.1.2 Knitting and Submitting on Quercus\nPractice knitting and uploading your file to Quercus!\nClick the dropdown “Knit” button at the top of the screen, and click “PDF”.\nNote: for assignments, submit PDF versions. If you are having trouble rendering your knitted file, you can submit HTML formats, or your .Rmd file as a last resort. Note that, if you are unable to knit your assignment, chances are there is an error. Make sure to double-check your code!\nHead on over to Quercus and submit your knitted PDF to “Assignment 0”."
  },
  {
    "objectID": "lec01-markdown-workflows.html#lesson-preamble",
    "href": "lec01-markdown-workflows.html#lesson-preamble",
    "title": "3  Markdown, project workflows",
    "section": "3.1 Lesson preamble",
    "text": "3.1 Lesson preamble\n\n3.1.1 Lesson objectives\n\nLearn about the Markdown syntax and how to use it within the R Notebook.\nSet up the R Notebook\nSet up an R Project\nUnderstand how to follow the conventional data science workflow\n\n\n\n3.1.2 Learning outline\n\nExplore RStudio interface (10 mins)\nReproducible science with Markdown (40 mins)\nProject-Oriented Workflow (20 mins)\nReproducible Science (10 mins)\nThe Scientific Method (10 mins)\nFind your groups! (10 mins)"
  },
  {
    "objectID": "lec01-markdown-workflows.html#markdown",
    "href": "lec01-markdown-workflows.html#markdown",
    "title": "3  Markdown, project workflows",
    "section": "3.2 Markdown",
    "text": "3.2 Markdown\nLast class, we introduced Markdown, the text format that RStudio uses. In fact, if you are following along this far, you should have already been typing your notes and code in a .Rmd file, or a RMarkdown file.\nLet’s introduce more Markdown features! We encourage groups to try completing their group projects in RMarkdown, but more about that later.\n\n3.2.1 In-line code\nWe talked about using RMarkdown for notes and inline code last class. This is extremely useful for when you need to update objects or rerun analyses. You can also comment out code within the code chunk.\nAs a reminder, the keyboard shortcut to insert a code chunk is:\n\ncommand + option + i (Mac)\nctrl + alt + i (Windows)\n\n\nx &lt;- 5\ny &lt;- 13\n\n# 5 plus 13\nx + y\n\n[1] 18\n\n\n\n3.2.1.1 R code chunk tricks\n\neval=TRUE evaluates the code chunk (runs it); eval=FALSE does not (will just print the chunk)\necho=TRUE prints the code chunk; echo=FALSE hides it but will still evaluate it\n\nthis is useful if you need to save something as an object but you don’t necessarily need it to print\n\nwarning=FALSE will hide warnings; message=FALSE will hide messages from output\n\nonly the case when eval=TRUE\n\n\n\n## eval=TRUE, echo=FALSE prints output, but does not print code chunk\nlibrary(ggplot2)\nqplot(mpg, wt, data = mtcars)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n3.2.1.2 Changing height and width of images/figures\nYou can also indicate the sizes of figures by using fig.width and fig.height in your code chunk.\n\n## in this R chuck, fig.width=2, fig.height=2\nqplot(mpg, wt, data = mtcars)\n\n\n\n\n\n\n\n3.2.2 Lists\nI’ve already used these! You can use either * or - to indicate unordered lists. To indent, hit tab. Make sure there’s a space before your items in the list.\nUnordered Lists (bullet points)\n\nfirst item\nsecond item\n\nsub-level item (two tabs)\n\nthird item\n\nFor ordered lists, use numbers. Again, hit tab to add sublists.\nOrdered Lists (numbers)\n\nOrdered item\nOrdered item\n\nOrdered sub-item\n\nItem (will automatically re-number)\n\n\n\n3.2.3 Tables\nkable displays r code chunks with tables nicely\nlibrary(knitr)\nkable(head(mtcars))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n\n\nIn comparison:\nlibrary(knitr)\nhead(mtcars) ## does not nicely format the table\n               mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1\n\n\n3.2.4 Images and Links\nUse the following format to add images:\n![Description/Caption](/path/to/image.png)\nFor example: \nAs an aside, let’s talk about rubber duck debugging. It is a simple but effective technique used by scientists to debug code. By explaining the code line-by-line to an inanimate object, such as a rubber duck, programmers can break down the code into smaller, more manageable pieces, and potentially identify the root cause of the problem. Or, explain it to a friend in the course! They can also help you find typos and other errors\nAnd use the following format to add weblinks: [Description/Caption](weblink).\nThe EEB website lives here!\n\n\n3.2.5 White Space and New Page\nUse \\vspace{2pt} to add white space, and use \\newpage to add a page break.\n\n\n3.2.6 In-line citations and bibliography\nExport a .bib file (e.g., from Zotero and Mendeley). Then, add the .bib to your YAML header.\n---\ntitle: My Report\noutput: html_document\nbibliography: bibliography.bib\n---\nNow, you can cite directly in your report, like below:\nSome fact[@Smith2018]\nSome fact supported by multiple papers [@Smith2018; @Tran1997]\n\n\n3.2.7 Footnotes\nThis is what a footnote looks like.1 Here is another.2"
  },
  {
    "objectID": "lec01-markdown-workflows.html#project-oriented-workflow",
    "href": "lec01-markdown-workflows.html#project-oriented-workflow",
    "title": "3  Markdown, project workflows",
    "section": "3.3 Project-Oriented Workflow",
    "text": "3.3 Project-Oriented Workflow\n\n3.3.1 Projects\nMaterial adapted from Posit and R-bloggers.\nRStudio projects make it straightforward to divide your work into multiple contexts, each with their own working directory, workspace, history, and source documents.\nTo create a new project in the RStudio IDE, use the Create Project command (top right). You can link your new project to an existing directory, or create a new directory entirely. Since we already have some RMarkdown written, let’s add our project to an existing directory.\nWhen a new project is created RStudio, it creates a project file (with an .Rproj extension) within the project directory. This file contains various project options (discussed below) and can also be used as a shortcut for opening the project directly from the filesystem. It also creates a hidden directory (named .Rproj.user) where project-specific temporary files (e.g., auto-saved source documents, window-state, etc.) are stored. This directory is also automatically added to .Rbuildignore, .gitignore, etc. if required.\nNow, whenever you open your project, it loads your project into RStudio and displays its name in the Projects toolbar (which is located on the far right side of the main toolbar). It even takes you back to where you were working last, so it opens all the scripts and files that were last open! You can also save your history, the objects in your environment, and an image of what your project looked like at the last time you closed it, but this can take up a lot of space if you do this each time.\nYou can work with more than one RStudio project at a time by simply opening each project in its own window of RStudio.\n\n\n3.3.2 Relative File Pathways\nThe goal of the here package is to enable easy file referencing in project-oriented workflows. In contrast to using setwd(), which is fragile and dependent on the way you organize your files, here uses the top-level directory of a project to easily build paths to files.\n\nlibrary(here)\n\nhere() starts at /Users/meteyuksel/eeb313website\n\nhere()\n\n[1] \"/Users/meteyuksel/eeb313website\"\n\n\nYou can build a path relative to the top-level directory in order to read or write a file:\n\nhere(\"1_lectures\", \"csv\", \"portal_data.csv\")\n\n[1] \"/Users/meteyuksel/eeb313website/1_lectures/csv/portal_data.csv\"\n\n\nThese relative paths work regardless of where the associated source file lives inside your project, like analysis projects with data and reports in different subdirectories.\n\n\n3.3.3 Demo Project\nThere is no a single “right” folder structure for analysis projects. However, this is a tried-and-true simple method that works (backed up by many data scientists!).\nCreate a folder on your computer that holds all the files relevant to that particular piece of work. This parent folder will contain the project’s .Rproj and all other subfolders, so it should have the same name as your project. Pick a good one. Spending an extra 5 minutes will save you from regrets in the future. The name should be short, concise, written in lower-case, and not containing any special symbols. One can apply similar strategies as for naming packages.\nUse subdirectories to organize data, code, figures, notes, etc.\n\nThe folder data typically contains two subfolders, namely, raw and processed. The content of raw directory is data files of any kind, such as .csv, SAS, Excel, text and database files, etc. The content of this folder is read-only, so that no scripts should change the original files or create new ones inside it. For this purpose, the processed directory is used: all processed, cleaned, and tidied datasets are saved here. It is a good practice to save files in R specific format, rather than in .csv, since the saving in .csv is a less efficient way of storing data (both in terms of space and time of reading/writing).\nThe folder figures is the place where you may store plots, diagrams, and other figures. There is not much to say about it. Common extensions of such files are .eps, .png, .pdf, etc. Again, file names in this folder should be meaningful (the name img1.png does not represent anything).\nAll reports live in a directory with the corresponding name reports. These reports can be of any formats, such as LaTeX, Markdown, R Markdown, Jupyter Notebooks, etc. Currently, more and more people prefer rich documents with text and executable code to LaTeX and such.\nPerhaps the most important folder is analyses or scripts. Here, you keep all your R scripts and codes. That is the exact place to use prefix numbers, if files should be run in a particular order. If you have files in other scripted languages (e.g., Python), it is better to keep them in this folder as well. There can also an important subfolder called deprecated. Whenever you want to remove one or the other script, it is a good idea to move it to deprecated at first iteration. I recommend never deleting your code until you are sure that you have finished everything you wanted to do, even if the code doesn’t work (going back to it after a break usually helps!).\n\n\n\n\nVicki’s demo of her file directory.\n\n\nThis convention guarantees that the project is self-contained, and it can be moved around on your computer or onto other computers and will still “just work”. This is a really important convention that creates reliable, reproducible science across different computers or users and over time.\n\n\n3.3.4 Documenting session info\nAs an overview of our environment, we can also use the sessionInfo() command. This is a good practice to have at the end of your code to document which packages you used and what version they were.\n\nsessionInfo()\n\nR version 4.2.3 (2023-03-15)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] here_1.0.1    knitr_1.43    ggplot2_3.4.3\n\nloaded via a namespace (and not attached):\n [1] rstudioapi_0.15.0 magrittr_2.0.3    tidyselect_1.2.0  munsell_0.5.0    \n [5] colorspace_2.1-0  R6_2.5.1          rlang_1.1.1       fastmap_1.1.1    \n [9] fansi_1.0.4       dplyr_1.1.2       tools_4.2.3       grid_4.2.3       \n[13] gtable_0.3.3      xfun_0.39         utf8_1.2.3        cli_3.6.1        \n[17] withr_2.5.0       htmltools_0.5.5   rprojroot_2.0.3   digest_0.6.33    \n[21] tibble_3.2.1      lifecycle_1.0.3   farver_2.1.1      htmlwidgets_1.6.2\n[25] vctrs_0.6.3       glue_1.6.2        evaluate_0.21     rmarkdown_2.23   \n[29] labeling_0.4.2    compiler_4.2.3    pillar_1.9.0      generics_0.1.3   \n[33] scales_1.2.1      jsonlite_1.8.7    pkgconfig_2.0.3  \n\n\nNotice that we have some base packages active even though we did not explicitly call for them."
  },
  {
    "objectID": "lec01-markdown-workflows.html#reproducible-science",
    "href": "lec01-markdown-workflows.html#reproducible-science",
    "title": "3  Markdown, project workflows",
    "section": "3.4 Reproducible Science",
    "text": "3.4 Reproducible Science\n\nComputational reproducibility\n\ncode, software, etc.\n\nScientific reproducibility\n\ndata (able to run the same statistical analysis on the same data)\nmethod, details about how the data was collected\nN.B., field work - hard to reproduce method\n\nStatistical reproducibility\n\n\nWhy does reproducibility matter in science?\n\nMust trust findings to build on scientific progress!\n\nWhat do you think about when you hear the term “open science”?\n\nOpen science: everything about a project (data, stats, code, funding, etc.) is open and available to viewing\n\nHow does open science affect collaboration and the progress of science?\n\nWhat are the barriers to reproducibility?\n\npeople won’t care\npeople might want to steal results\nresearchers do not want to share (embarrassed, code is rushed, etc.)\n\n\n3.4.1 Reproducibility in Data Science\nMaterial from this section adapted from R for Data Science.\nThe main tools of data science are: importing, tidying, transforming, and visualizing data. We will go through all of these steps of data science so that you can tackle real datasets.\n\n\n\nData Science Workflow\n\n\n\nBefore you can transform and visualize your data, you need to first get your data into R.\nThen, you’ll learn about tidy data, a consistent way of storing your data that makes transformation, visualization, and modelling easier. You’ll learn the underlying principles, and how to get your data into a tidy form.\nYou’ll learn the key verbs that allow you to select important variables, filter out key observations, create new variables, and compute summaries.\nFinally, we will visualize by making elegant and informative plots that help you understand data.\n\nNote that if you are interested in doing a modelling project, these steps are different. Talk to Vicki and Mete if you want to tackle mathematical models!"
  },
  {
    "objectID": "lec01-markdown-workflows.html#the-scientific-method",
    "href": "lec01-markdown-workflows.html#the-scientific-method",
    "title": "3  Markdown, project workflows",
    "section": "3.5 The Scientific Method",
    "text": "3.5 The Scientific Method\n\n3.5.1 Steps in scientific process\n\nIdentify research question(s).\nLook into what the previous literature shows.\nCreate one or more hypotheses or objectives.\nWrite up an outline or expected approach to answering those questions/objectives (analysis and presentation plan):\n\n\nHow will the data be obtained and what is the data (i.e. the source)?\nWhat statistical/mathematical techniques have previous researchers used? Will you use them? Are they appropriate (optional, may need expert)? ( How will the results/data be presented or visualized (possible tables/figures)?\n\n\nRun the planned analyses (or additional ones that come may up).\nVisualize or present all results from the analyses.\nInterpret the results and how they fit with the previous literature.\nDraw conclusions based on the hypotheses/objectives.\nDisseminate your results (in blogs, pre-print archives, journals, conferences)\n\n\n\n\nSimplified diagram of the scientific method\n\n\n\n\n3.5.2 Basics of succeeding as a team\nFinal assignment is in a group and to succeed, you need to understand team dynamics:\n\nCommunication is vital to work together and to achieve the goal\nTeams go through various stages\nNeed consensus for group norms, goals, duties/responsibilities, and conduct/behaviour\nImportant that everyone has a stake in the project\nRotate roles (specifically for the leader/facilitator)\n\n\n\n3.5.3 Roles and responsibilities\n\nLeader/Facilitator’s duty:\n\nGoal is to keep things running smoothly, focused on the task, and on track for time\nKeep everyone on topic and on task; stay aware of the time\n(Try to) mediate or resolve any conflicts (there will always be some type of conflict; how it’s dealt with is what matters)\n(Try to) encourage everyone to participate and allow everyone a chance at talking\n\nRecorder’s duty:\n\nGoal is to write/type down main or important points raised or discussed when team is meeting\nKeep notes and files organized and orderly\n\nOrganizer’s duty:\n\nArrange for next meeting time and location\n\nSend reminders to members a day or two before meeting\n\nMake and email a simple agenda of tasks to do or to discuss\n\n\n\n\n3.5.4 Code of Conduct\n\nVital to establishing boundaries and expectations of being a team member\n\nHow do you want each member to treat each other?\nHow do you deal with conflict?\nWhat is acceptable and unacceptable behaviour?\n\nThese are outlined in the code of conduct\nMostly its common sense (be kind and respectful)\n\nBut its important that you as a team write out what everyone wants and agrees to\n\n\n\n3.5.4.1 Example Code of Conduct\nCheck out the Contributor Convenent and the UofT Coders Code of Conduct"
  },
  {
    "objectID": "lec01-markdown-workflows.html#projects-1",
    "href": "lec01-markdown-workflows.html#projects-1",
    "title": "3  Markdown, project workflows",
    "section": "3.6 Projects!",
    "text": "3.6 Projects!\n\n3.6.1 Exercise after groups are formed\nAs a group, complete these tasks:\n\nGet into your groups\nIntroduce one another:\n\n\nName, year of study\nOther stuff (e.g., interests, plans for next years, etc.)\n\n\nCreate a “team name”\nAssign roles to each other:\n\n\nThese will be rotated: Leader/Facilitator, Recorder, Organizer\nDiscuss how and when the roles will be rotated.\n\n\nDiscuss and brainstorm code of conduct (record it).\nTake a minute to think about your own skills\n\n\nShare these with your group (record them).\n\n\nDiscuss how responsibilities of each member will be decided on (record it).\n\n\n\n3.6.2 GitHub\nMake a GitHub account."
  },
  {
    "objectID": "lec01-markdown-workflows.html#footnotes",
    "href": "lec01-markdown-workflows.html#footnotes",
    "title": "3  Markdown, project workflows",
    "section": "",
    "text": "This is the first footnote.↩︎\nThis is the second footnote.↩︎"
  },
  {
    "objectID": "lec02-base-r.html#lesson-preamble",
    "href": "lec02-base-r.html#lesson-preamble",
    "title": "4  Base R: assignment, vectors, functions, and loops",
    "section": "4.1 Lesson Preamble",
    "text": "4.1 Lesson Preamble\n\n4.1.1 Learning Objectives\n\nDefine the following terms as they relate to R: call, function, arguments, options.\nDo simple arithmetic operations in R using values and objects.\nCall functions and use arguments to change their default options.\nUnderstand the logic and use of if else statements.\nDefine our own functions.\nCreate for and while loops.\nInspect the content of vectors and manipulate their content.\n\n4.1.2 Learning outline\n\nCreating objects/variables in R\nIf else statements\nUsing and writing functions\nVectors and data types\nSubsetting vectors\nMissing data\nLoops and vectorization\n\n\nNote: Parts of this lecture were originally created by combining contributions to Data Carpentry and has been modified to align with the aims of EEB313.\n\n\nprint(\"hello! today we are talking about vectors, functions, and the like.\")\n\n[1] \"hello! today we are talking about vectors, functions, and the like.\""
  },
  {
    "objectID": "lec02-base-r.html#clear-your-worksapce",
    "href": "lec02-base-r.html#clear-your-worksapce",
    "title": "4  Base R: assignment, vectors, functions, and loops",
    "section": "4.2 Clear your worksapce",
    "text": "4.2 Clear your worksapce\nWhen using Rstudio, it is best practice to turn off automatic save and restore of global workspace. To do this, go to the “Tools” menu in Rstudio, select “Global Options”, and make sure the “Restore .RData into workspace at startup” box is not selected For good measure, set the “Save workspace to .RData on exit” to “Never”. The command to clear your workspace in a script is\n\nrm(list=ls())\n\nToday we will go through some R basics, including how to create objects, assign values, define functions, and use for and while loops to iteratively preform calculations."
  },
  {
    "objectID": "lec02-base-r.html#creating-objects-in-r",
    "href": "lec02-base-r.html#creating-objects-in-r",
    "title": "4  Base R: assignment, vectors, functions, and loops",
    "section": "4.3 Creating objects in R",
    "text": "4.3 Creating objects in R\nAs we saw in our first class, you can get output from R simply by typing math in the console:\n\n3 + 5\n\n[1] 8\n\n12 / 7\n\n[1] 1.714286\n\n\nHowever, to do more complex calcualtions, we need to assign values to objects.\n\nx &lt;- 3\ny &lt;- x + 5\ny\n\n[1] 8\n\n\nYou can name an object in R almost anything you want:\n\njoel &lt;- 3\njoel + 5\n\n[1] 8\n\nTRUE &lt;- 3\n\nError in TRUE &lt;- 3: invalid (do_set) left-hand side to assignment\n\n### not allowed to overwrite logical operators\n\nT &lt;- 3 \n### for some reason this is allowed, but problematic\n### T and TRUE are often used interchangeably\n\nThere are some names that cannot be used because they are they are reserved for commands, operators, functions, etc. in base R (e.g., while, TRUE). See ?Reserved for a list these names. Even if it’s allowed, it’s best to not use names of functions that already exist in R (e.g., c, T, mean, data, df, weights). When in doubt, check the help or use tab completion to see if the name is already in use.\n\n4.3.0.1 Challenge\nWe have created two variables, joel and x. What is their sum? The sum of joel six times?\n\njoel + x\n\n[1] 6\n\njoel + joel + joel + joel + joel + joel\n\n[1] 18"
  },
  {
    "objectID": "lec02-base-r.html#some-tips-on-naming-objects",
    "href": "lec02-base-r.html#some-tips-on-naming-objects",
    "title": "4  Base R: assignment, vectors, functions, and loops",
    "section": "4.4 Some tips on naming objects",
    "text": "4.4 Some tips on naming objects\n\nObjects can be given any name: x, current_temperature, thing, or subject_id.\nYou want your object names to be explicit and not too long.\nObject names cannot start with a number: x2 is valid, but 2x is not valid.\nR is also case sensitive: joel is different from Joel.\n\nIt is recommended to use nouns for variable names, and verbs for function names. It’s important to be consistent in the styling of your code (where you put spaces, how you name variables, etc.). Using a consistent coding style1 makes your code clearer to read for your future self and your collaborators. RStudio will format code for you if you highlight a section of code and press Ctrl/Cmd + Shift + a.\n\n4.4.1 Preforming calculations\nWhen assigning a value to an object, R does not print anything. You can force R to print the value by using parentheses or by typing the object name:\n\nweight_kg &lt;- 55    # doesn't print anything\n(weight_kg &lt;- 55)  # putting parentheses around the call prints the value of `weight_kg`\n\n[1] 55\n\nweight_kg          # and so does typing the name of the object\n\n[1] 55\n\n\nThe variable weight_kg is stored in the computer’s memory where R can access it, and we can start doing arithmetic with it efficiently. For instance, we may want to convert this weight into pounds:\n\n2.2 * weight_kg\n\n[1] 121\n\n\nWe can also change a variable’s value by assigning it a new one:\n\nweight_kg &lt;- 57.5\n2.2 * weight_kg\n\n[1] 126.5\n\n\nImportantly, assigning a value to one variable does not change the values of other variables. For example, let’s store the animal’s weight in pounds in a new variable, weight_lb:\n\nweight_lb &lt;- 2.2 * weight_kg\n\nand then change weight_kg to 100.\n\nweight_kg &lt;- 100\nweight_lb\n\n[1] 126.5\n\n\nNotice that weight_lb is unchanged.\n\n4.4.1.1 Challenge\nWhat are the values of these variables after each statement in the following?\n\nmass &lt;- 47.5\nage  &lt;- 122\nmass &lt;- mass * 2.0      \nage  &lt;- age - 20  \nmass_index &lt;- mass/age"
  },
  {
    "objectID": "lec02-base-r.html#functions-and-their-arguments",
    "href": "lec02-base-r.html#functions-and-their-arguments",
    "title": "4  Base R: assignment, vectors, functions, and loops",
    "section": "4.5 Functions and their arguments!",
    "text": "4.5 Functions and their arguments!\nFunctions are sets of statements that are organized to preform certain tasks. They can be understood through analogy with cooking. Ingredients (called inputs or arguments) combine according to some set of reactions (the statements and commands of the function) to yield a product or output. A function does not have to return a number: a list of values could be returned, another function, or a list of functions.\nMany functions are built into R, including sqrt(). For sqrt(), the input must be a number larger than zero, and the value that is returned by the function is the square root of that number. Executing a function is called running or calling the function. An example of a function call is:\n\nsqrt(9)\n\n[1] 3\n\n# the input must be in the domain of the function:\nsqrt(\"hello\")\n\nError in sqrt(\"hello\"): non-numeric argument to mathematical function\n\nsqrt(-1) # note: sqrt() can take in *complex* numbers, including -1+0i\n\nWarning in sqrt(-1): NaNs produced\n\n\n[1] NaN\n\n\nThis is the same as assigning the value to a variable and then passing that variable to the function:\n\na &lt;- 9\nb &lt;- sqrt(a)\nb\n\n[1] 3\n\n\nHere, the value of a is given to the sqrt() function, the sqrt() function calculates the square root, and returns the value which is then assigned to variable b. This set up is important when you write more complex functions where multiple variables are passed to different arguments in different parts of a function.\nsqrt() is very simple because it takes just one argument. Arguments can be anything, not only numbers or files. Some functions take arguments which may either be specified by the user, or, if left out, take on a default value: these are called options. Options are typically used to alter the way the function operates, such as whether it ignores ‘bad values’, or what symbol to use in a plot. However, if you want something specific, you can specify a value of your choice which will be used instead of the default.\n\n4.5.1 Tab-completion\nTo access help about sqrt, tab-completion can be a useful tool. Type s and press Tab. You can see that R gives you suggestions of what functions and variables are available that start with the letter s, and thanks to RStudio they are formatted in this nice list. There are many suggestions here, so let’s be a bit more specific and append a q, to find what we want. If we press tab again, R will helpfully display all the available parameters for this function that we can pass an argument to.\n\n#s&lt;tab&gt;q\n#sqrt(&lt;tab&gt;)\n\nTo read the full help about sqrt, we can use the question mark, or type it directly into the help document browser.\n\n?sqrt\n\nAs you can see, sqrt() takes only one argument, x, which needs to be a numerical vector. Don’t worry too much about the fact that it says vector here; we will talk more about that later. Briefly, a numerical vector is one or more numbers. In R, every number is a vector, so you don’t have to do anything special to create a vector. More on vectors later!\nLet’s try a function that can take multiple arguments: round().\n\n#round(&lt;tab&gt;)\n?round\n\nIf we try round with a value:\n\nround(3.14159)\n\n[1] 3\n\n\nHere, we’ve called round() with just one argument, 3.14159, and it has returned the value 3. That’s because the default is to round to the nearest whole number, or integer. If we want more digits we can pass an argument to the digits parameter, to specify how many decimals we want to round to.\n\nround(3.14159, digits = 2)\n\n[1] 3.14\n\n\nAbove we have passed the argument 2 to the parameter digits. We can leave out the word digits since we know it comes as the second parameter, after x.\n\nround(3.14159, 2)\n\n[1] 3.14\n\n\nAs you notice, we have been leaving out x from the beginning. If you provide the names for both the arguments, we can switch their order:\n\nround(digits = 2, x = 3.14159)\n\n[1] 3.14\n\n\nIt’s good practice to put non-optional arguments before optional arguments, and to specify the names of all optional arguments. If you don’t, someone reading your code might have to look up the definition of a function with unfamiliar arguments to understand what you’re doing."
  },
  {
    "objectID": "lec02-base-r.html#if-else-statements",
    "href": "lec02-base-r.html#if-else-statements",
    "title": "4  Base R: assignment, vectors, functions, and loops",
    "section": "4.6 If else statements",
    "text": "4.6 If else statements\nIt is often useful to preform calculations only when certain conditions are met. One way to do this is using an “if else” statement. The syntax of such a statement is below:\n\n# if (condition){\n#   computation\n# } else{ \n#   another computation\n# }\n\nWithout the else bit, the computation will be preformed if the condition is satisfied and nothing will be done (and variables in the environment will be unchanged) otherwise.\n\nt &lt;- 1\n\nt &lt; 10 # returns the truth value of this statement\n\n[1] TRUE\n\nt == 10\n\n[1] FALSE\n\nt &gt; 10\n\n[1] FALSE\n\nt &gt; 10 | t == 10\n\n[1] FALSE\n\n### &lt; (less than), &gt; (greater than), == (equals)\n### & (and), | (or), ! (not) are common logical operators\n\nif (t &lt; 10){\n  print(t)\n} else{\n  print(t-1)\n}\n\n[1] 1\n\n### setting t &lt;- 10 and executing the above returns 9\n\nIn fact, if else statements lend themselves naturally to deciding which of &gt;2 alternative computations should be preformed, based on a set of appropriate conditions. For example,\n\nt &lt;- 10\nt2 &lt;- 20\n\nif (t &lt; 10 & t2 &gt; 19){\n  print(\"1\")\n} else if (t &lt; 10 & t2 &gt; 19){\n  print(\"2\")\n} else if (t &lt;= 10 & t2 &gt; 19){\n  print(\"3\")\n}\n\n[1] \"3\"\n\n### notice how the third condition is met, but the others are not\n### when the first condition is met (even if the others are too), \"1\" is printed:\n\nif (t &lt;= 10 & t2 &gt; 19){\n  print(\"1\")\n} else if (t &lt;= 10 & t2 &gt; 19){\n  print(\"2\")\n} else if (t &lt;= 10 & t2 &gt; 19){\n  print(\"3\")\n}\n\n[1] \"1\""
  },
  {
    "objectID": "lec02-base-r.html#writing-functions",
    "href": "lec02-base-r.html#writing-functions",
    "title": "4  Base R: assignment, vectors, functions, and loops",
    "section": "4.7 Writing functions",
    "text": "4.7 Writing functions\nWe have seen there are many built-in functions in R, which we will use throughout the semester: sum, c(), mean(), all(), plot(), ifelse(), print(). We can also write our own functions for custom use. For example, the below chuck defines two functions which check if two scalar inputs are positive.\n\ncheck_if_numbers_are_postive_function1 &lt;- function(num1, num2) {\n  if (num1 &gt; 0 & num2 &gt; 0){\n    return(\"both numbers are postive!\")\n  } else{\n    return(\"one or both numbers are not postive.\")\n  }\n}\n\ncheck_if_numbers_are_postive_function1(4, 5)\n\n[1] \"both numbers are postive!\"\n\ncheck_if_numbers_are_postive_function1(-4, 5)\n\n[1] \"one or both numbers are not postive.\"\n\ncheck_if_numbers_are_postive_function2 &lt;- function(num1, num2) {\n  if (num1 &gt; 0){\n    if (num2 &gt; 0){\n      return(\"both numbers are postive!\")\n    }\n  }\n}\n\ncheck_if_numbers_are_postive_function2(4, 5)\n\n[1] \"both numbers are postive!\"\n\ncheck_if_numbers_are_postive_function2(-4, 5)\n\nAlthough these functions agree when both inputs are positive (i.e., they return the same output), the second function does not return a statement indicating one or both of the inputs are non-positive when this is the case. This is because we have not indicated what should be returned when the condition in one or the other if the statement in check_if_numbers_are_postive_function2 is not met.\nWe can do this as follows:\n\ncheck_if_numbers_are_postive_function2 &lt;- function(num1, num2) {\n  \n  if (! num1 &gt; 0){\n    return(\"one or both numbers are not postive.\")\n  }\n  \n  if (num1 &gt; 0){\n    if (num2 &gt; 0){\n      return(\"both numbers are postive!\")\n    }\n     if (! num2 &gt; 0){\n      return(\"one or both numbers are not postive.\")\n    }\n  }\n  \n}\n\ncheck_if_numbers_are_postive_function2(4, 5)\n\n[1] \"both numbers are postive!\"\n\ncheck_if_numbers_are_postive_function2(-4, 5)\n\n[1] \"one or both numbers are not postive.\"\n\ncheck_if_numbers_are_postive_function2(4, -5)\n\n[1] \"one or both numbers are not postive.\"\n\n\nImportantly, these functions are not written with elegance in mind. There are better ways to check if two numbers are both positive. We encourage you to think more about how to write functions (like the above) with elegance and efficiency in mind, and how trade-offs between the two might come up.\n\n4.7.0.1 Challenge\nCan you write a function that calculates the mean of 3 numbers?\n\nmean_of_three_numbers &lt;- function(num1, num2, num3) {\n   my_sum &lt;- num1 + num2 + num3\n   my_mean &lt;- my_sum / 3\n   return(my_mean)\n}\nmean_of_three_numbers(2, 4, 6)\n\n[1] 4"
  },
  {
    "objectID": "lec02-base-r.html#vectors-and-data-types",
    "href": "lec02-base-r.html#vectors-and-data-types",
    "title": "4  Base R: assignment, vectors, functions, and loops",
    "section": "4.8 Vectors and data types",
    "text": "4.8 Vectors and data types\nA vector is the most common data type in R, and is the workhorse of the language. A vector is composed of a series of values, which can be numbers (0, \\(\\pi\\), 72) or characters (“hello”, “I’m a ChaRaCTER”). We can assign a series of values to a vector using the c() function, which stands for concatenate. For example we can create a vector of animal weights and assign it to a new object weight_g:\n\nweight_g &lt;- c(50, 60, 65, 82) # concatenate values into a vector\nweight_g\n\n[1] 50 60 65 82\n\n\nYou can also use the command seq to create a sequence of numbers.\n\nseq(from = 0, to = 30) # default spacing is =1\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n[26] 25 26 27 28 29 30\n\nseq(from = 0, to = 30, by = 3) # returns every third number in c(0,1,2,...,30)\n\n [1]  0  3  6  9 12 15 18 21 24 27 30\n\n\nA vector can also contain characters (in addition to numbers):\n\nanimals &lt;- c('mouse', 'rat', 'dog')\nanimals\n\n[1] \"mouse\" \"rat\"   \"dog\"  \n\n\nThe quotes around “mouse”, “rat”, etc. are essential here and can be either single or double quotes. Without the quotes R will assume there are objects called mouse, rat and dog. As these objects don’t exist in R’s memory, there will be an error message.\nThere are many functions that allow you to inspect the content of a vector. length() tells you how many elements are in a particular vector:\n\nlength(weight_g)\n\n[1] 4\n\nlength(animals)\n\n[1] 3\n\n\nAn important feature of a vector is that all of the elements are the same type of data. The function class() indicates the class (the type of element) of an object:\n\nclass(weight_g)\n\n[1] \"numeric\"\n\nclass(animals)\n\n[1] \"character\"\n\n\nThe function str() provides an overview of the structure of an object and its elements. It is a useful function when working with large and complex objects:\n\nstr(weight_g)\n\n num [1:4] 50 60 65 82\n\nstr(animals)\n\n chr [1:3] \"mouse\" \"rat\" \"dog\"\n\n\nYou can use the c() function to add other elements to your vector:\n\nweight_g &lt;- c(weight_g, 90) # add to the end of the vector\nweight_g &lt;- c(30, weight_g) # add to the beginning of the vector\nweight_g\n\n[1] 30 50 60 65 82 90\n\n\nIn the first line, we take the original vector weight_g, add the value 90 to the end of it, and save the result back into weight_g. Then we add the value 30 to the beginning, again saving the result back into weight_g.\nWe can do this over and over again to grow a vector, or assemble a dataset. As we program, this may be useful to add results that we are collecting or calculating.\nAn atomic vector is the simplest R data type and it is a linear vector of a single type, e.g., all numbers. Above, we saw two of the six atomic vector types that R uses: \"character\" and \"numeric\" (or \"double\"). These are the basic building blocks that all R objects are built from.\nThe other four atomic vector types are:\n\n\"logical\" for TRUE and FALSE (the boolean data type)\n\"integer\" for integer numbers (e.g., 2L, the L indicates to R that it’s an integer)\n\"complex\" to represent complex numbers with real and imaginary parts (e.g., 1 + 4i).\n\"raw\" for bitstreams. We will not discuss this type further.\n\nVectors are one of the many data structures that R uses. Other important ones are lists (list), matrices (matrix), data frames (data.frame), factors (factor) and arrays (array). In this class, we will focus on data frames, which is most commonly used one for data analyses.\n\n4.8.0.1 Challenge\nWe’ve seen that atomic vectors can be of type character, numeric (or double), integer, and logical. What happens if we try to mix these types? Find out by using class to test these examples.\n\nnum_char &lt;- c(1, 2, 3, 'a')\nnum_logical &lt;- c(1, 2, 3, TRUE)\nchar_logical &lt;- c('a', 'b', 'c', TRUE)\ntricky &lt;- c(1, 2, 3, '4')\n\n\n# Answer\nclass(num_char)\n\n[1] \"character\"\n\nclass(num_logical)\n\n[1] \"numeric\"\n\nclass(char_logical)\n\n[1] \"character\"\n\nclass(tricky)\n\n[1] \"character\"\n\n\nThis happens because vectors can be of only one data type. Instead of throwing an error and saying that you are trying to mix different types in the same vector, R tries to convert (coerce) the content of this vector to find a “common denominator”. A logical can be turn into 1 or 0, and a number can be turned into a string/character representation. It would be difficult to do it the other way around: would 5 be TRUE or FALSE? What number would ‘t’ be? This establishes a hierarchy for conversions/coercions, whereby some types get preferentially coerced into other types. From the above example, we can see that the hierarchy goes logical -&gt; numeric -&gt; character, and logical can also be directly coerced into character."
  },
  {
    "objectID": "lec02-base-r.html#subsetting-vectors",
    "href": "lec02-base-r.html#subsetting-vectors",
    "title": "4  Base R: assignment, vectors, functions, and loops",
    "section": "4.9 Subsetting vectors",
    "text": "4.9 Subsetting vectors\nIf we want to extract one or several values from a vector, we provide one or several indices in square brackets:\n\nanimals &lt;- c(\"mouse\", \"rat\", \"dog\", \"cat\")\nanimals[2]\n\n[1] \"rat\"\n\nanimals[c(3, 2)] # Provide multiple indices simultaneously\n\n[1] \"dog\" \"rat\"\n\n\nWe can also repeat the indices to create an object with more elements than the original one:\n\nmore_animals &lt;- animals[c(1, 2, 3, 2, 1, 4)]\nmore_animals\n\n[1] \"mouse\" \"rat\"   \"dog\"   \"rat\"   \"mouse\" \"cat\"  \n\n\nR indices start at 1. Programming languages like Fortran, MATLAB, Julia, and R start counting at 1, because that’s what human beings typically do. Languages in the C family (including C++, Java, Perl, and Python) start counting at 0.\n\n4.9.1 Conditional subsetting\nAnother common way of subsetting is by using a logical vector. TRUE will select the element with the same index, while FALSE will not:\n\nweight_g &lt;- c(21, 34, 39, 54, 55)\nweight_g[c(TRUE, FALSE, TRUE, TRUE, FALSE)]\n\n[1] 21 39 54\n\n\nTypically, these logical vectors are not typed by hand, but are the output of other functions or logical tests."
  },
  {
    "objectID": "lec02-base-r.html#na-na-na-na-na-na-missing-data",
    "href": "lec02-base-r.html#na-na-na-na-na-na-missing-data",
    "title": "4  Base R: assignment, vectors, functions, and loops",
    "section": "4.10 NA NA NA NA NA NA… Missing data??",
    "text": "4.10 NA NA NA NA NA NA… Missing data??\nDue to its origins as a statistical computing language, R includes tools to deal with missing data easily. Missing data are represented in vectors as NA.\nImportantly, many built-in R functions will return NA if the data you are working with include missing values. This feature makes it harder to overlook the cases where you are dealing with missing data.\n\nheights &lt;- c(2, 4, 4, NA, 6)\nmean(heights)\n\n[1] NA\n\nmax(heights)\n\n[1] NA\n\n\nFor functions such as mean(), you can add the argument na.rm = TRUE to preform calculations ignoring the missing values:\n\nmean(heights, na.rm = TRUE)\n\n[1] 4\n\nmax(heights, na.rm = TRUE)\n\n[1] 6\n\n\nIt is also possible to use conditional subsetting to remove NAs. The function is.na() is helpful in this case. This function examines each element in a vector to see whether it is NA, and returns a logical vector.\n\nis.na(heights)\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n\nCombining this function and ! (the logical operator not), we can extract elements that are not NAs:\n\n## Extract those elements which are not missing values.\nheights[!is.na(heights)]\n\n[1] 2 4 4 6\n\n\nAlternatively, we can use the these functions to achieve the same outcome.\n\n# Returns the object with incomplete cases removed. \nna.omit(heights)\n\n[1] 2 4 4 6\nattr(,\"na.action\")\n[1] 4\nattr(,\"class\")\n[1] \"omit\"\n\n# Extract those elements which are complete cases. \nheights[complete.cases(heights)]\n\n[1] 2 4 4 6\n\n\nImportant note: missing data are ubiquitous. Make sure you know why NAs exist in your data before removing them. If NAs are removed, document why and be sure to store the data pre- and post-processing."
  },
  {
    "objectID": "lec02-base-r.html#loops-and-vectorization",
    "href": "lec02-base-r.html#loops-and-vectorization",
    "title": "4  Base R: assignment, vectors, functions, and loops",
    "section": "4.11 Loops and vectorization",
    "text": "4.11 Loops and vectorization\nLoops are essential in programming. They come in two types: for and while.\nThe syntax for a for loop is as follows:\n\n# for (iterator in values_iterator_can_assume){\n#   computation\n# }\n\nThe syntax for a while loop is as follows:\n\n# while (condition){\n#   computation\n# }\n\nThe key difference between these types of loop is that a while loop breaks when the condition fails to be met; the loop preforms calculations while the condition is met. A for loop preforms the computation for all values of the iterator in the list/vector/etc. of values specified in the “for” statement.\nThe below for loop prints the values in the vector the iterator num can assume (one by one):\n\nv &lt;- c(2, 4, 6, 8, 10)\nfor (num in v) {\n    print(num)\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n[1] 10\n\n\nEquivalently, we could write\n\nfor (i in 1:5) {\n  print(v[i])\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n[1] 10\n\n\nThis set up is quite powerful. We can now perform tasks iteratively:\n\n# creates vector where each number is 3 more than the previous number:\n\nx &lt;- c(0.4)\n\nfor (i in 1:5) {\n  x[i+1] &lt;- x[i] + 3 \n}\n\nx\n\n[1]  0.4  3.4  6.4  9.4 12.4 15.4\n\n# calls sqrt() function from inside loop\n\nx &lt;- c(0.4)\n\nfor (i in 1:5) {\n  x[i+1] &lt;- sqrt(x[i])\n}\n\nx\n\n[1] 0.4000000 0.6324555 0.7952707 0.8917795 0.9443408 0.9717720\n\n\nTo constrast for and while loops, consider the following:\n\nx &lt;- 0.4\ni &lt;- 1\ny &lt;- c() ### need to declare y so that values can be added in below loop\n\nwhile (x &lt;= 0.9999) {\n  y[i] &lt;- x\n  x &lt;- sqrt(x)\n  i &lt;- i + 1 # updating i so that y can be updated in next step\n}\n\n### note we could just keep track of x if we:\n### 1) use the condition x[i] &lt;= 0.9999\n### 2) calculate the next term in the sequence of sqrts using x[i+1] &lt;- sqrt(x[i])\n\ny\n\n [1] 0.4000000 0.6324555 0.7952707 0.8917795 0.9443408 0.9717720 0.9857850\n [8] 0.9928670 0.9964271 0.9982120 0.9991056 0.9995527 0.9997763 0.9998882\n\n\nThe above loop returns the sequence of square roots \\(0.4, \\sqrt{0.4}, \\sqrt{\\sqrt{0.4}}, \\dots\\). Importantly, the loop terminates when an element of this sequence is greater than 0.9999. The number of iterations until this happens is not specified. This means while loops can run for infinite time if their conditions are never violated. It is best to have checks in place to make sure this doesn’t happen!"
  },
  {
    "objectID": "lec02-base-r.html#footnotes",
    "href": "lec02-base-r.html#footnotes",
    "title": "4  Base R: assignment, vectors, functions, and loops",
    "section": "",
    "text": "Refer to the tidy style guide for which style to adhere to.↩︎"
  },
  {
    "objectID": "lec03-dataframes-dplyr.html#lesson-preamble",
    "href": "lec03-dataframes-dplyr.html#lesson-preamble",
    "title": "5  Data frames, intro to dplyr",
    "section": "5.1 Lesson Preamble",
    "text": "5.1 Lesson Preamble\n\n5.1.1 Learning Objectives\n\nDescribe what a data frame is.\nLoad external data from a .csv file into a data frame in R.\nSummarize the contents of a data frame in R.\nUnderstand the purpose of the dplyr package.\n\n5.1.2 Lecture outline\n\nR packages for data analyses (20 mins)\nData set background (10 mins)\nWhat are data frames (10 mins)\nIntroduction to data and wrangling (40 mins)"
  },
  {
    "objectID": "lec03-dataframes-dplyr.html#setting-up-the-workspace",
    "href": "lec03-dataframes-dplyr.html#setting-up-the-workspace",
    "title": "5  Data frames, intro to dplyr",
    "section": "5.2 Setting up the workspace",
    "text": "5.2 Setting up the workspace\nOne of the keys to a successful R working session is to make sure you have everything you need ready to go, and neatly organized. Just like when you are cooking or baking, having everything pre-measured and organized in neat way can be so helpful to ensure efficient, no fuss, and nice dishes in the end.\nNow we’ll take another couple of minutes to get our R workspace ready.\n\n5.2.1 Set up your Notebook\nFirst, we are going to set up our Notebook.\n---\ntitle: Introduction to R\n---\nRecall that this header block is called the YAML header. This is optional, without this your document will still knit, but this is useful for fine-tuning the output of your document, such as font size, figure sizes, and generating a table of content. We will mostly be using it to give our documents nice headers. If you are interested in playing with other customizations options the YAML provides, check out this guide.\n\n\n5.2.2 Check your directory\nOk, now the Notebook is ready. Next, we want to make sure we are working out of the correct directory, which is a fancy way to say the folder that we will be reading files from, and saving files to.\nBy default, R works out of your Documents folder. We can check this with the following function getwd(), which stands for get working directory.\n\ngetwd()\n\nIf this is the folder you wish to work out of, then we are good to go. If not, then we need to redirect R to some place else. Normally, in an R Script, you would do so by using the function setwd() (set working directory). In R Notebook, this is a slightly more involved process, because you cannot just quickly switch directories with setwd(). The reason for this is because R is trying to be helpful making sure you don’t lose your files half way through a script when you switch folders. In order to change directories in R Notebook, we have to reach all the way in and change the root directory in a special “setup code chunk”. To make this setup code chunk, add the words setup after r in your code chunk header.\n\n\nThe setup code chunk must NOT contain any commands other than the setup options you wish to change.\nWe then need to find the path to the folder we want R to work out of. In PC, right-click on the desired folder, and you can find its location under the General tab. In Mac, right-click on the desired folder and choose Get Info to find it’s location.\n\n# For PC (remember to change forward-slash to back-slash):\n# knitr::opts_knit$set(root.dir = \"C:/Users/Documents/UofT/eeb313\")\n\n# For Mac (can get away with using ~ as short hand):\n# knitr::opts_knit$set(root.dir = \"~/Documents/UofT/eeb313\")\n\ngetwd()\n\nHowever, remember last week when we talked about relative file paths? The functions to set your working directory is slowly becoming outdated as project-oriented workflows become the norm.\nEach R script or .Rmd report should contain a call to here::i_am('path/to/this/file') at the top to establish the root directory.\nSubsequent file paths can be made using the here function. You should also be saving your files and plots by using the here() function. More on that later.\n\nlibrary(here)\nhere()\n\n# use to set \"lectures\" as working directory\n# here::i_am(\"lectures\", \"lec03-dataframes-dplyr.Rmd\")\n\nOne important distinction from the working directory is that this remains stable even if the working directory is changed. I recommend that you steer clear from ever changing the working directory.\n\n\n5.2.3 Load your packages\nNext, we want to check to see that all of our required packages are properly loaded and ready to go.\nIn a nutshell, packages are bundles of functions that perform related tasks (like a book of recipes). Our installation of R comes with some packages already, since they are considered critical for using R, e.g., c(), mean(), +, -, etc. These pre-installed packages and function are commonly collectively referred to as base R.\nIn addition to these foundational packages and functions, CRAN (Comprehensive R Archive Network), the official repository for R-packages, houses thousands more packages. There is literally a package for every occasion you can and cannot imagine. Did you know that you can call an Uber from R (ubeR)? How about, instead of boring dots as data points in figures, you can plot emojis (emojifont)? Frustrated with code and need a pick-me-up? The packages praise1 and kittyR2 got you covered! All of this is possible due to R being an open language, and many people generously contribute their time to its development.\nThese additionally packages are not installed by default, because then base R installation would be huge and most people would only be using a fraction of everything installed on their machine. It would be like if you downloaded the Firefox or Chrome browser and you would get all extensions and add-ons installed by default, or as if your phone came with every app ever made for it already installed when you bought it: quite impractical.\nTo install a package in R, we use the function install.packages(). Think of this function as downloading the package to your machine - you only need to do this once.\n\ninstall.packages('tidyverse') # This is a chonky package, takes a while to download\n\ntidyverse3 is a conglomerate package that is a collection of packages that has similar functions, just like Microsoft Word is part of Microsoft Office. tidyverse, as its name may suggest, contains many packages that makes data cleaning and exploring more intuitive and effective. It is basically an entire philosophy on how to handle data and has a massive following.\nThe two tidyverse packages we will be using the most frequently in this course is dplyr and ggplot2. dplyr is great for data wrangling (Lecture 2) and ggplot2 makes killer plots (Lecture 3).\nTo use functions in the dplyr package, type dplyr:: and then the function name.\n\ndplyr::glimpse(cars) \n\nRows: 50\nColumns: 2\n$ speed &lt;dbl&gt; 4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13…\n$ dist  &lt;dbl&gt; 2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34…\n\n# `glimpse` gives us a glimpse... of an object\n# `cars` is a base R dataset\n\nSince we will be using this package a lot, it would be a little annoying to have to type dplyr:: every time. We can bypass this step by loading the package into our current environment. Think of this is “opening” the package for your work session.\n\nlibrary(tidyverse) # More convenient to load all tidy packages at once\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nglimpse(cars)\n\nRows: 50\nColumns: 2\n$ speed &lt;dbl&gt; 4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13…\n$ dist  &lt;dbl&gt; 2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34…\n\n\nThis needs to be done once for every new R session, and so it is common practice to keep a list of all the packages used at the top of your script or notebook for convenience and load all of it at start up.\nThat’s a lot of red though! What are these warning signs and checks?\nAll the warning signs indicate are the version of R that they were built under. They can frequently be ignored unless your version of R is so old that the packages can no longer be run on R! Note that packages are frequently updated, and functions may become deprecated.\nNext, the warning shows you all the packages that were successfully installed.\nFinally, there are some conflicts! All this means is that there are multiple functions with the same name that may do different things. R prioritizes functions from certain packages over others. So, in this case, the filter() function from dplyr will take precedent over the filter() function from the stats package. If you want to use the latter, use double colons :: to indicate that you are calling a function from a certain package:\n\nstats::filter()\n\n\n5.2.3.1 An Aside on Reproducible Environments\nAt this point, you are already living a project-centric lifestyle. With renv, you’ll start using project libraries, giving each project its own independent collection of packages. To increase reproducibility of a project, we must keep track of what packages are used. This also helps us avoid error messages if we update R or update a package, and things no longer work.\nTo convert a project to use renv, call renv::init(). It adds three new files and directories to your project. Make sure you indicate where your project lives by using here()\n\n# install.packages(\"renv\")\nlibrary(renv)\nrenv::init(here())\n\nThat’s a lot of output! What does it mean?\nYou should now see lockfile, project_name/renv.lock. This lockfile records all of the information about packages needed by your project, such as the version of package and where was it installed from. It knows all this because renv scans all files in your project directory, and looks for functions like library, require, or package::function.\nWe also see project_name/.Rprofile, containing one line. When R starts, it searches for .Rprofile and runs what it finds. We can use this file to change various options.\nYou will also see a folder project_name/renv/, which contains your project library. If you already have a package installed elsewhere, renv will link to it. Otherwise, it’ll be installed in renv/library.\nEssentially, whenever we open our project, we activate our R environment using these three files. renv will automatically be active in any R session that is run from the project directory. To activate\n\nactivate(here())\n\n# or use\nknitr::opts_knit$set(root.dir = here())\n\nNow, every time you open your project, run restore() in order to start from where you left off!\n\nrestore()\n\nIf you are adding new packages that you require to your code, uses snapshot() to record changes to the renv.lock file. This function captures the state of your project at that point in time.\n\nsnapshot()\n\nTo summarize, the general workflow[^1] when working with renv is:\n\nCall renv::init() to initialize a new project-local environment with a private R library. Work in the project as normal, installing and removing new R packages as they are needed in the project.\nCall renv::snapshot() to save the state of the project library to the lockfile (called renv.lock). Continue working on your project, installing and updating R packages as needed.\nCall renv::snapshot() again to save the state of your project library if your attempts to update R packages were successful, or call renv::restore() to revert to the previous state as encoded in the lockfile if your attempts to update packages introduced some new problems.\n\n[^1] Material taken from Posit.\nThe last thing required for reproducibility is a version control system, like Git and GitHub. That lecture will be coming later on in the semester!\n\n\n\n5.2.4 Load your data\nJust a moment ago we took a glimpse of the dataset cars, which is one of the datasets that came with R. Now we are going to try loading our own data.\nWe will be working with real data from a longitudinal study of the species abundance in the Chihuahuan desert ecosystem near Portal, Arizona, USA. This study includes observations of plants, ants, and rodents from 1977 - 2002, and has been used in over 100 publications. More information is available in the abstract of this paper from 2009. There are several datasets available related to this study, and we will be working with datasets that have been preprocessed by Data Carpentry to facilitate teaching. These are made available online as The Portal Project Teaching Database, both at the Data Carpentry website, and on Figshare. Figshare is a great place to publish data, code, figures, and more openly to make them available for other researchers and to communicate findings that are not part of a longer paper.\nWe are studying the species and weight of animals caught in plots in our study area. The dataset is stored as a comma separated value (CSV) file. Each row holds information for a single animal, and the columns represent:\n\n\n\nColumn\nDescription\n\n\n\n\nrecord_id\nunique id for the observation\n\n\nmonth\nmonth of observation\n\n\nday\nday of observation\n\n\nyear\nyear of observation\n\n\nplot_id\nID of a particular plot\n\n\nspecies_id\n2-letter code\n\n\nsex\nsex of animal (“M”, “F”)\n\n\nhindfoot_length\nlength of the hindfoot in mm\n\n\nweight\nweight of the animal in grams\n\n\ngenus\ngenus of animal\n\n\nspecies\nspecies of animal\n\n\ntaxa\ne.g. rodent, reptile, bird, rabbit\n\n\nplot_type\ntype of plot\n\n\n\nTo read data into R, we are going to use a function called read_csv. One useful option that read_csv includes is the ability to read a CSV file directly from a URL, without downloading it in a separate step:\n\nlibrary(readr)\nsurveys &lt;- read_csv('https://ndownloader.figshare.com/files/2292169')\n\nRows: 34786 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): species_id, sex, genus, species, taxa, plot_type\ndbl (7): record_id, month, day, year, plot_id, hindfoot_length, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHowever, it is often a good idea to download the data first, so you have a copy stored locally on your computer in case you want to do some offline analyses, or the online version of the file changes or the file is taken down. You can either download the data manually or from within R:\n\ndownload.file(\"https://ndownloader.figshare.com/files/2292169\",\n              \"portal_data.csv\")\n# Saves file to current or root directory with this name\n# You can use `here()` to indicate sublevels in your directory\n\nThe data is read in by specifying its file name. R knows to look in your current directory for something with this name (don’t forget the quotation marks).\n\nsurveys &lt;- read_csv(\"portal_data.csv\")\n\nRows: 34786 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): species_id, sex, genus, species, taxa, plot_type\ndbl (7): record_id, month, day, year, plot_id, hindfoot_length, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s have a look at this dataset.\n\nsurveys\n\n# A tibble: 34,786 × 13\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2        72     8    19  1977       2 NL         M                  31     NA\n 3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n 4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n 5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n 6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n 7       435    12    10  1977       2 NL         &lt;NA&gt;               NA     NA\n 8       506     1     8  1978       2 NL         &lt;NA&gt;               NA     NA\n 9       588     2    18  1978       2 NL         M                  NA    218\n10       661     3    11  1978       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 34,776 more rows\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\nThis displays a nice tabular view of the data, which also includes pagination; there are many rows and we can click the arrow to view all the columns. Technically, this object is actually a tibble rather than a data frame, as indicated in the output. The reason for this is that read_csv automatically converts the data into to a tibble when loading it. Since a tibble is just a data frame with some convenient extra functionality, we will use these words interchangeably from now on.\nIf we just want to glance at how the data frame looks, it is sufficient to display only the top (the first 6 lines) using the function head():\n\nhead(surveys)\n\n# A tibble: 6 × 13\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1         1     7    16  1977       2 NL         M                  32     NA\n2        72     8    19  1977       2 NL         M                  31     NA\n3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\nAwesome! Everything is set, and now we are finally ready to start working with real data in R!"
  },
  {
    "objectID": "lec03-dataframes-dplyr.html#what-are-data-frames",
    "href": "lec03-dataframes-dplyr.html#what-are-data-frames",
    "title": "5  Data frames, intro to dplyr",
    "section": "5.3 What are data frames?",
    "text": "5.3 What are data frames?\nData frames are the de facto data structure for most tabular data, and what we use for statistics and plotting. A data frame can be created by hand, but most commonly they are generated by the function read_csv(); in other words, when importing spreadsheets from your hard drive (or the web).\nA data frame is a representation of data in the format of a table where the columns are vectors that all have the same length. Because the columns are vectors, each column contain the same type of data (e.g., characters, integers, factors). We can see this when inspecting the structure of a data frame with the function str() (structure):\n\nstr(surveys)\n\nspc_tbl_ [34,786 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ record_id      : num [1:34786] 1 72 224 266 349 363 435 506 588 661 ...\n $ month          : num [1:34786] 7 8 9 10 11 11 12 1 2 3 ...\n $ day            : num [1:34786] 16 19 13 16 12 12 10 8 18 11 ...\n $ year           : num [1:34786] 1977 1977 1977 1977 1977 ...\n $ plot_id        : num [1:34786] 2 2 2 2 2 2 2 2 2 2 ...\n $ species_id     : chr [1:34786] \"NL\" \"NL\" \"NL\" \"NL\" ...\n $ sex            : chr [1:34786] \"M\" \"M\" NA NA ...\n $ hindfoot_length: num [1:34786] 32 31 NA NA NA NA NA NA NA NA ...\n $ weight         : num [1:34786] NA NA NA NA NA NA NA NA 218 NA ...\n $ genus          : chr [1:34786] \"Neotoma\" \"Neotoma\" \"Neotoma\" \"Neotoma\" ...\n $ species        : chr [1:34786] \"albigula\" \"albigula\" \"albigula\" \"albigula\" ...\n $ taxa           : chr [1:34786] \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ...\n $ plot_type      : chr [1:34786] \"Control\" \"Control\" \"Control\" \"Control\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   record_id = col_double(),\n  ..   month = col_double(),\n  ..   day = col_double(),\n  ..   year = col_double(),\n  ..   plot_id = col_double(),\n  ..   species_id = col_character(),\n  ..   sex = col_character(),\n  ..   hindfoot_length = col_double(),\n  ..   weight = col_double(),\n  ..   genus = col_character(),\n  ..   species = col_character(),\n  ..   taxa = col_character(),\n  ..   plot_type = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nInteger refers to a whole number, such as 1, 2, 3, 4, etc. Numbers with decimals, 1.0, 2.4, 3.333, are referred to as floats. Factors are used to represent categorical data. Factors can be ordered or unordered, and understanding them is necessary for statistical analysis and for plotting. Factors are stored as integers, and have labels (text) associated with these unique integers. While factors look (and often behave) like character vectors, they are actually integers under the hood, and you need to be careful when treating them like strings.\n\n5.3.1 Inspecting data.frame objects\nWe already saw how the functions head() and str() can be useful to check the content and the structure of a data frame. Here is a non-exhaustive list of functions to get a sense of the content/structure of the data. Let’s try them out!\n\nSize:\n\ndim(surveys) - returns a vector with the number of rows in the first element and the number of columns as the second element (the dimensions of the object)\nnrow(surveys) - returns the number of rows\nncol(surveys) - returns the number of columns\n\nContent:\n\nhead(surveys) - shows the first 6 rows\ntail(surveys) - shows the last 6 rows\n\nNames:\n\nnames(surveys) - returns the column names (synonym of colnames() for data.frame objects)\nrownames(surveys) - returns the row names\n\nSummary:\n\nstr(surveys) - structure of the object and information about the class, length, and content of each column\nsummary(surveys) - summary statistics for each column\n\n\nNote: most of these functions are “generic”, they can be used on other types of objects besides data.frame.\n\n5.3.1.1 Challenge\nBased on the output of str(surveys), can you answer the following questions?\n\nWhat is the class of the object surveys?\nHow many rows and how many columns are in this object?\nHow many species have been recorded during these surveys?\n\n\n\n\n5.3.2 Indexing and subsetting data frames\nOur survey data frame has rows and columns (that is, it has 2 dimensions). If we want to extract some specific data from it, we need to specify the “coordinates” we want from it in a manner similar to when we indexed vectors. Row numbers come first, followed by column numbers. When indexing, base R data frames return a different format depending on how we index the data (i.e., either a vector or a data frame), but with enhanced data frames, tibbles, the returned object is almost always a data frame.\n\nsurveys[1, 1]   # First element in the first column of the data frame\n\n# A tibble: 1 × 1\n  record_id\n      &lt;dbl&gt;\n1         1\n\nsurveys[1, 6]   # First element in the 6th column\n\n# A tibble: 1 × 1\n  species_id\n  &lt;chr&gt;     \n1 NL        \n\nsurveys[, 1]    # First column in the data frame\n\n# A tibble: 34,786 × 1\n   record_id\n       &lt;dbl&gt;\n 1         1\n 2        72\n 3       224\n 4       266\n 5       349\n 6       363\n 7       435\n 8       506\n 9       588\n10       661\n# ℹ 34,776 more rows\n\nsurveys[1]      # First column in the data frame\n\n# A tibble: 34,786 × 1\n   record_id\n       &lt;dbl&gt;\n 1         1\n 2        72\n 3       224\n 4       266\n 5       349\n 6       363\n 7       435\n 8       506\n 9       588\n10       661\n# ℹ 34,776 more rows\n\nsurveys[1:3, 7] # First three elements in the 7th column\n\n# A tibble: 3 × 1\n  sex  \n  &lt;chr&gt;\n1 M    \n2 M    \n3 &lt;NA&gt; \n\nsurveys[3, ]    # The 3rd element for all columns\n\n# A tibble: 1 × 13\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\nsurveys[1:6, ]  # Equivalent to head(surveys)\n\n# A tibble: 6 × 13\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1         1     7    16  1977       2 NL         M                  32     NA\n2        72     8    19  1977       2 NL         M                  31     NA\n3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\nYou can also exclude certain parts of a data frame using the “-” sign:\n\nsurveys[,-1]    # All columns, except the first\n\n# A tibble: 34,786 × 12\n   month   day  year plot_id species_id sex   hindfoot_length weight genus  \n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  \n 1     7    16  1977       2 NL         M                  32     NA Neotoma\n 2     8    19  1977       2 NL         M                  31     NA Neotoma\n 3     9    13  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 4    10    16  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 5    11    12  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 6    11    12  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 7    12    10  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 8     1     8  1978       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 9     2    18  1978       2 NL         M                  NA    218 Neotoma\n10     3    11  1978       2 NL         &lt;NA&gt;               NA     NA Neotoma\n# ℹ 34,776 more rows\n# ℹ 3 more variables: species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\nsurveys[-c(7:34786),] # Equivalent to head(surveys)\n\n# A tibble: 6 × 13\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1         1     7    16  1977       2 NL         M                  32     NA\n2        72     8    19  1977       2 NL         M                  31     NA\n3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\nIn addition to using numeric values to subset a data.frame (or matrix), columns can be called by name, using the following notations:\n\n# Only printing the first six rows as a demonstration\n\nsurveys[1:6, \"species_id\"] # Result is a data.frame\n\n# A tibble: 6 × 1\n  species_id\n  &lt;chr&gt;     \n1 NL        \n2 NL        \n3 NL        \n4 NL        \n5 NL        \n6 NL        \n\nhead(surveys$species_id) # Result is a vector\n\n[1] \"NL\" \"NL\" \"NL\" \"NL\" \"NL\" \"NL\"\n\n\n\n5.3.2.1 Challenge\n\nCreate a data.frame (surveys_200) containing only the observations from row 200 of the surveys dataset.\nNotice how nrow() gave you the number of rows in a data.frame?\n\nUse that number to pull out just that last row in the data frame.\nCompare that with what you see as the last row using tail() to make sure it’s meeting expectations.\nPull out that last row using nrow() instead of the row number.\nCreate a new data frame object (surveys_last) from that last row.\n\nUse nrow() to extract the row that is in the middle of the data frame. Store the content of this row in an object named surveys_middle.\nCombine nrow() with the - notation above to reproduce the behavior of head(surveys) keeping just the first through 6th rows of the surveys dataset."
  },
  {
    "objectID": "lec03-dataframes-dplyr.html#working-with-a-subset-of-the-data",
    "href": "lec03-dataframes-dplyr.html#working-with-a-subset-of-the-data",
    "title": "5  Data frames, intro to dplyr",
    "section": "5.4 Working with a subset of the data",
    "text": "5.4 Working with a subset of the data\nSometimes when we are working with a big dataset, we might want to test our code on a smaller portion of the data first to make sure it works before running the code on the entire thing, which could take a long time. There are many ways to subset your data. Common practices include pulling a random number of samples from the full dataset or take the first n rows of your dataset. Here, we take every 8th row from the dataset to work with for the rest of the lecture. This ensures that everybody is working with the same dataset (we don’t want to randomize) and we also have a good representation of observations from different plot types (the first 5000 rows only captured two out of a total of five plot types). Think about these issues when you create your own test data set.\nWhen you work on your own projects, don’t forget to go back to the full data set for your final, official analysis!!!\n\nstr(surveys)\n\nspc_tbl_ [34,786 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ record_id      : num [1:34786] 1 72 224 266 349 363 435 506 588 661 ...\n $ month          : num [1:34786] 7 8 9 10 11 11 12 1 2 3 ...\n $ day            : num [1:34786] 16 19 13 16 12 12 10 8 18 11 ...\n $ year           : num [1:34786] 1977 1977 1977 1977 1977 ...\n $ plot_id        : num [1:34786] 2 2 2 2 2 2 2 2 2 2 ...\n $ species_id     : chr [1:34786] \"NL\" \"NL\" \"NL\" \"NL\" ...\n $ sex            : chr [1:34786] \"M\" \"M\" NA NA ...\n $ hindfoot_length: num [1:34786] 32 31 NA NA NA NA NA NA NA NA ...\n $ weight         : num [1:34786] NA NA NA NA NA NA NA NA 218 NA ...\n $ genus          : chr [1:34786] \"Neotoma\" \"Neotoma\" \"Neotoma\" \"Neotoma\" ...\n $ species        : chr [1:34786] \"albigula\" \"albigula\" \"albigula\" \"albigula\" ...\n $ taxa           : chr [1:34786] \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ...\n $ plot_type      : chr [1:34786] \"Control\" \"Control\" \"Control\" \"Control\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   record_id = col_double(),\n  ..   month = col_double(),\n  ..   day = col_double(),\n  ..   year = col_double(),\n  ..   plot_id = col_double(),\n  ..   species_id = col_character(),\n  ..   sex = col_character(),\n  ..   hindfoot_length = col_double(),\n  ..   weight = col_double(),\n  ..   genus = col_character(),\n  ..   species = col_character(),\n  ..   taxa = col_character(),\n  ..   plot_type = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsurveys_subset&lt;- surveys[seq(1, 34786, 8), ]\nstr(surveys_subset)\n\ntibble [4,349 × 13] (S3: tbl_df/tbl/data.frame)\n $ record_id      : num [1:4349] 1 588 1453 3000 5558 ...\n $ month          : num [1:4349] 7 2 11 5 3 8 11 9 12 9 ...\n $ day            : num [1:4349] 16 18 5 18 29 16 13 30 8 7 ...\n $ year           : num [1:4349] 1977 1978 1978 1980 1982 ...\n $ plot_id        : num [1:4349] 2 2 2 2 2 2 2 2 2 2 ...\n $ species_id     : chr [1:4349] \"NL\" \"NL\" \"NL\" \"NL\" ...\n $ sex            : chr [1:4349] \"M\" \"M\" \"M\" \"F\" ...\n $ hindfoot_length: num [1:4349] 32 NA NA 31 33 33 32 32 32 32 ...\n $ weight         : num [1:4349] NA 218 218 87 211 152 158 173 160 135 ...\n $ genus          : chr [1:4349] \"Neotoma\" \"Neotoma\" \"Neotoma\" \"Neotoma\" ...\n $ species        : chr [1:4349] \"albigula\" \"albigula\" \"albigula\" \"albigula\" ...\n $ taxa           : chr [1:4349] \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ...\n $ plot_type      : chr [1:4349] \"Control\" \"Control\" \"Control\" \"Control\" ..."
  },
  {
    "objectID": "lec03-dataframes-dplyr.html#exporting-data",
    "href": "lec03-dataframes-dplyr.html#exporting-data",
    "title": "5  Data frames, intro to dplyr",
    "section": "5.5 Exporting data",
    "text": "5.5 Exporting data\nAs you begin to play with your raw data, you may want to export these new, processed, datasets to share them with your collaborators or for archival.\nSimilar to the read_csv() function used for reading CSV files into R, there is a write_csv() function that generates CSV files from data frames.\nBefore using write_csv(), we are going to create a new folder, data-processed, in our working directory that will store this generated dataset. We don’t want to store manipulated datasets in the same directory as our raw data. It’s good practice to keep them separate. The raw data would ideally be put in a data-raw folder, which should only contain the raw, unaltered data, and should be left alone to make sure we don’t delete or modify it from how it was when we downloaded or recorded it ourself. Keep good notes on this original data, e.g., when did you download it and which links were used.\nManually create a new folder called “data-processed” in your directory. Alternatively, get R to help you with it. Remember to make sure that we are in the correct relative file directory!\n\nhere()\n\ndir.create(here(\"1_lectures\", \"processed_data\"))\n\nSave the new dataset to a directory where you will be able to access it for next class. There are a couple ways you could do this depending on which function you use. The function write_csv exists in the readr package, while write.csv is from base R. These two functions have slightly different arguments. Or, you can use save from base, which writes the R object into a specific file format.\n\nwrite_csv(surveys_subset,\n          file = here(\"survey_subset.csv\"))\n\n# If that fails you can always go back to base R! \nwrite.csv(surveys_subset,\n          file = here(\"survey_subset.csv\"))\n\nsave(surveys_subset, file = here(\"survey_subset.csv\"))\n\nWe are going to prepare a cleaned up version of the data without NAs.\n\n# Note that this omits observations with NA in *any* column.\n# There is no way to control which columns to use.\nsurveys_complete_naomit &lt;- na.omit(surveys)\n\n# Compare the dimensions of the original and the cleaned data frame\ndim(surveys)\n\n[1] 34786    13\n\ndim(surveys_complete_naomit)\n\n[1] 30676    13\n\n\nNow that our dataset is ready, we can save it as a CSV file in our Processed data folder.\n\n# To save to newly created directory\nwrite_csv(surveys_complete_naomit, here(\"surveys_complete_naomit.csv\"))"
  },
  {
    "objectID": "lec03-dataframes-dplyr.html#data-wrangling-with-dplyr",
    "href": "lec03-dataframes-dplyr.html#data-wrangling-with-dplyr",
    "title": "5  Data frames, intro to dplyr",
    "section": "5.6 Data wrangling with dplyr",
    "text": "5.6 Data wrangling with dplyr\nWrangling here is used in the sense of maneuvering, managing, controlling, cleaning, and turning your data upside down and inside out to look at it from different angles in order to understand it and prepare it for analyses. The package dplyr provides easy tools for the most common data manipulation tasks. It is built to work directly with data frames, with many common tasks optimized by being written in a compiled language (C++), which means that many operations run much faster than similar tools in R. An additional feature is the ability to work directly with data stored in an external database, such as SQL-databases. The ability to work with databases is great because you are able to work with much bigger datasets (100s of GB) than your computer could normally handle. We will not talk in detail about this in class, but there are great resources online to learn more (e.g., this lecture from Data Carpentry).\n\n5.6.1 Coercing as factors\nNotice for the sex of the observations, the summary is returning that there are characters in this column but not much else. Let’s take a look at the data in this column closer. Again, previous function parameters have words read in as characters but recent defaults have changed to read words in as factors.\nAs a column of character values, the relationship between the observations being recorded as “M”, or “F” are not being recognized. We will need convert this column to factor.\nLet’s overwrite the column in the original dataset. Remember, there is no undo button in programming. Double check your work before you overwrite objects\n\nsurveys_subset$sex &lt;- as.factor(surveys_subset$sex)\n\n\n\n5.6.2 Renaming columns in a dataframe\nFirst, let’s check out our surveys_subset dataframe.\n\nsurveys_subset\n\n# A tibble: 4,349 × 13\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;fct&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2       588     2    18  1978       2 NL         M                  NA    218\n 3      1453    11     5  1978       2 NL         M                  NA    218\n 4      3000     5    18  1980       2 NL         F                  31     87\n 5      5558     3    29  1982       2 NL         M                  33    211\n 6      6500     8    16  1982       2 NL         F                  33    152\n 7      8657    11    13  1983       2 NL         F                  32    158\n 8      9605     9    30  1984       2 NL         F                  32    173\n 9     11215    12     8  1985       2 NL         F                  32    160\n10     11879     9     7  1986       2 NL         F                  32    135\n# ℹ 4,339 more rows\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\nThis dataframe contains hindfoot length and weight data of several species in different taxanomic groups. While we were collecting data, we notice that we didn’t write down the units in our lab notebook of hindfoot_length and weight! Let’s add it into the column manually here so that we don’t forget.\nWe can do that by renaming our column, using the rename() function.\n\nrename(surveys_subset,\n       hindfoot_length_cm = hindfoot_length,\n       weight_g = weight)\n\n# A tibble: 4,349 × 13\n   record_id month   day  year plot_id species_id sex   hindfoot_length_cm\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;fct&gt;              &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                     32\n 2       588     2    18  1978       2 NL         M                     NA\n 3      1453    11     5  1978       2 NL         M                     NA\n 4      3000     5    18  1980       2 NL         F                     31\n 5      5558     3    29  1982       2 NL         M                     33\n 6      6500     8    16  1982       2 NL         F                     33\n 7      8657    11    13  1983       2 NL         F                     32\n 8      9605     9    30  1984       2 NL         F                     32\n 9     11215    12     8  1985       2 NL         F                     32\n10     11879     9     7  1986       2 NL         F                     32\n# ℹ 4,339 more rows\n# ℹ 5 more variables: weight_g &lt;dbl&gt;, genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;,\n#   plot_type &lt;chr&gt;\n\n\nLet’s assign this dataframe to an object, by using the put &lt;- function.\n\nsurveys_subset_units &lt;- rename(surveys_subset,\n       hindfoot_length_cm = hindfoot_length,\n       weight_g = weight)\n\n\n5.6.2.1 An aside on naming conventions\nThere are several different styles of naming objects in R. We talked about this briefly last class when discussing how to name our functions. Naming things is one of the most difficult problems in data science since it can be instrumental in helping with maintenance of code and code-sharing These styles include:\n\ncamelCase\n\nThese names start with small letter and every subsequent word will start with upperCase letter\n\nPascalCase\n\nPascalCase is just like camel case but the only difference is the first letter is also UpperCase.\n\nsnake_case\n\nThese names are all lower case with underscore between the name.\n\nkebab-case\n\nthis is the style that is recommended to be avoided, because the - sign can be mistaken for the sign for subtraction! If you have a value saved in kebab, and another value saved in case, then R will try to find the difference between these two objects, as opposed to saving it as an object.\n\n\nWhere possible, avoid re-using names of common functions and variables. This will cause confusion for the readers of your code. This comes with more practice in common R packages - once you get a sense of the names of functions and data frames that already exist, you will know to avoid them!\nOther object-naming tips:\n\nchoose a convention and stick with it!\ndon’t use dots .\nuse names that are concise, meaningful, and consistent throughout your document. This is not easy!\ngenerally, variable names should be nouns, and function names should be verbs."
  },
  {
    "objectID": "lec03-dataframes-dplyr.html#footnotes",
    "href": "lec03-dataframes-dplyr.html#footnotes",
    "title": "5  Data frames, intro to dplyr",
    "section": "",
    "text": "Every time you call the function praise() you get praised once. Can you think of creative ways to get praised faster? (Hint 1: What about those loops we talked about earlier?) (Hint 2: Can we vectorize this operation and make the praising even more efficient?)↩︎\nTo install the kittyR package, use this code: devtools::install_github(\"IndrajeetPatil/kittyR\")↩︎\nThis course is focused on tidyverse functions, because that seems to be the trend these days. Although all of our teaching material is written in tidy lingo, it is mostly for the sake of consistency. In all honesty, tidy is pretty great, but some functions are more intuitive in base, so most people code in a mix of the two. If you learned base R elsewhere and perfer to use those functions instead, by all means, go ahead. The correct code is code that does what you want it to do.↩︎"
  },
  {
    "objectID": "lec04-data-wrangling.html#lesson-preamble",
    "href": "lec04-data-wrangling.html#lesson-preamble",
    "title": "6  Data wrangling in dplyr",
    "section": "6.1 Lesson preamble",
    "text": "6.1 Lesson preamble\n\n6.1.1 Learning Objectives\n\nLearn to use data wrangling commands select, filter, %&gt;%, and mutate from the dplyr package.\nUnderstand the split-apply-combine concept for data analysis.\nUse summarize, group_by, and tally to split a data frame into groups of observations, apply a summary statistics for each group, and then combine the results.\nLearn to switch between long and wide format\n\n6.1.2 Lesson outline\n\nContinue data wrangling in dplyr (30 mins)\nSplit-apply-combine techniques in dplyr (20 mins)\nUsing group_by and tally to summarize categorical data (25 mins)\nReshaping data (15 mins)"
  },
  {
    "objectID": "lec04-data-wrangling.html#getting-ready-to-code",
    "href": "lec04-data-wrangling.html#getting-ready-to-code",
    "title": "6  Data wrangling in dplyr",
    "section": "6.2 Getting ready to code",
    "text": "6.2 Getting ready to code\nWe are going to pick up where we left off last lecture, and continue to work with the same desert animal survey data.\nWe’ll start by doing our checks. You can set up your yaml block and check your working directory with getwd() on your own. Make sure you are in the directory where you saved our dataset last time and clear your workspace for our new session. We’ll also load the required packages (mostly tidyverse for this class) and data. Make sure what you need to be comfortable is within reach such as tea, coffee, cookies, blankets.\n\n\n# setwd(\"~/Documents/UofT/PhD/Teaching/2022-2023/EEB313/2022/Lectures\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nsurveys &lt;- read.csv(\"portal_data.csv\", na.strings = c(\"\",\".\",\"NA\"))\nsurveys %&gt;% head()\n\n  record_id month day year plot_id species_id  sex hindfoot_length weight\n1         1     7  16 1977       2         NL    M              32     NA\n2        72     8  19 1977       2         NL    M              31     NA\n3       224     9  13 1977       2         NL &lt;NA&gt;              NA     NA\n4       266    10  16 1977       2         NL &lt;NA&gt;              NA     NA\n5       349    11  12 1977       2         NL &lt;NA&gt;              NA     NA\n6       363    11  12 1977       2         NL &lt;NA&gt;              NA     NA\n    genus  species   taxa plot_type\n1 Neotoma albigula Rodent   Control\n2 Neotoma albigula Rodent   Control\n3 Neotoma albigula Rodent   Control\n4 Neotoma albigula Rodent   Control\n5 Neotoma albigula Rodent   Control\n6 Neotoma albigula Rodent   Control\n\nsurveys_subset &lt;- surveys[seq(1, 34786, 8), ]"
  },
  {
    "objectID": "lec04-data-wrangling.html#data-wrangling-with-dplyr",
    "href": "lec04-data-wrangling.html#data-wrangling-with-dplyr",
    "title": "6  Data wrangling in dplyr",
    "section": "6.3 Data wrangling with dplyr",
    "text": "6.3 Data wrangling with dplyr\nWrangling here is used in the sense of maneuvering, managing, controlling, and turning your data upside down and inside out to look at it from different angles in order to understand it. The package dplyr provides easy tools for the most common data manipulation tasks. It is built to work directly with data frames, with many common tasks optimized by being written in a compiled language (C++), which means that many operations run much faster than similar tools in R. An additional feature is the ability to work directly with data stored in an external database, such as SQL-databases. The ability to work with databases is great because you are able to work with much bigger datasets (100s of GB) than your computer could normally handle. We will not talk in detail about this in class, but there are great resources online to learn more (e.g. this lecture from Data Carpentry).\n\n6.3.1 Selecting columns and filtering rows\nWe’re going to learn some of the most common dplyr functions: select(), filter(), mutate(), group_by(), arrange, and summarise(). To select columns of a data frame, use select(). The first argument to this function is the data frame (surveys_subset), and the subsequent arguments are the columns to keep.\n\nselect(surveys_subset, plot_id, species_id, weight, year) %&gt;% head()\n\n   plot_id species_id weight year\n1        2         NL     NA 1977\n9        2         NL    218 1978\n17       2         NL    218 1978\n25       2         NL     87 1980\n33       2         NL    211 1982\n41       2         NL    152 1982\n\n\nNote: Unlike in base R, here we don’t need to use quotations around column names.\nTo choose rows based on a specific criteria, use filter():\n\nfilter(surveys_subset, year == 1995) %&gt;% head()\n\n  record_id month day year plot_id species_id sex hindfoot_length weight\n1     22044     2   4 1995       2         DM   M              37     46\n2     22550     8  26 1995       2         DM   F              37     26\n3     22997    12   2 1995       2         DM   M              36     50\n4     23136    12  21 1995       2         DM   M              33     27\n5     22441     7  20 1995       2         PP   F              19     17\n6     22669     9  23 1995       2         PP   F              20     17\n        genus      species   taxa plot_type\n1   Dipodomys     merriami Rodent   Control\n2   Dipodomys     merriami Rodent   Control\n3   Dipodomys     merriami Rodent   Control\n4   Dipodomys     merriami Rodent   Control\n5 Chaetodipus penicillatus Rodent   Control\n6 Chaetodipus penicillatus Rodent   Control\n\n\nNote2: To check for equality, R requires two equal signs (==). This is different than object assignment where we use (‘&lt;-’) or (‘=’) to assign values to an object. With filter we want to pull out all rows where year is equal to 1995 not assign the value 1995 to an object named year, so we use the == symbol.\nNote3: In general, when you want to find all rows that equal a numeric number you don’t have to use quotes. However, when you want to find all rows that equal a character you do need to put quotes around the value (for instance taxa == \"Rodent\").\n\n6.3.1.1 An aside on conditionals\nWithin filter you might want to filter rows using conditionals. Basic conditionals in R are broadly similar to how they’re expressed mathematically:\n\n2 &lt; 3\n\n[1] TRUE\n\n5 &gt; 9\n\n[1] FALSE\n\n5 == 5\n\n[1] TRUE\n\n\nHowever, there are a few idiosyncrasies to be mindful of for other conditionals:\n\n2 != 3 # Not equal\n\n[1] TRUE\n\n2 &lt;= 3 # Less than or equal to\n\n[1] TRUE\n\n5 &gt;= 9 # Greater than or equal to\n\n[1] FALSE\n\n\nFinally, the %in% operator is used to check for membership:\n\n2 %in% c(2, 3, 4) # Checks whether 2 is in c(2, 3, 4), returns logical vector\n\n[1] TRUE\n\n\nAll of the above conditionals are compatible with filter, with the key difference being that filter expects column names as part of conditional statements instead of individual numbers.\n\n\n\n6.3.2 Chaining functions together using pipes\nBut what if you wanted to select and filter at the same time? There are three ways to do this: use intermediate steps, nesting functions, or pipes.\nWith intermediate steps, you essentially create a temporary data frame and use that as input to the next function:\n\ntemp_df &lt;- select(surveys_subset, plot_id, species_id, weight, year)\nfilter(temp_df, year == 1995) %&gt;% head()\n\n  plot_id species_id weight year\n1       2         DM     46 1995\n2       2         DM     26 1995\n3       2         DM     50 1995\n4       2         DM     27 1995\n5       2         PP     17 1995\n6       2         PP     17 1995\n\n\nThis can quickly clutter up your workspace with lots of objects.\nYou can also nest functions (i.e., one function inside of another).\n\nfilter(select(surveys_subset, plot_id, species_id, weight, year), year == 1995) %&gt;% head()\n\n  plot_id species_id weight year\n1       2         DM     46 1995\n2       2         DM     26 1995\n3       2         DM     50 1995\n4       2         DM     27 1995\n5       2         PP     17 1995\n6       2         PP     17 1995\n\n\nThis is handy, but can be difficult to read if too many functions are nested as they are evaluated from the inside out.\nThe last option, forward pipes, are a fairly recent addition to R. Pipes let you take the output of one function and send it directly to the next, which is useful when you need to do many things to the same dataset. Pipes in R look like %&gt;% and are made available via the magrittr package that is a part of the tidyverse. If you use RStudio, you can type the pipe with Ctrl/Cmd + Shift + M.\nFun fact: The name magrittr comes from the Belgian artist Rene Magritte, who has a painting called “The Treachery of Images” that says in French “This is not a pipe”.\n\nsurveys_subset %&gt;% \n    select(., plot_id, species_id, weight, year) %&gt;% \n    filter(., year == 1995) %&gt;% head()\n\n  plot_id species_id weight year\n1       2         DM     46 1995\n2       2         DM     26 1995\n3       2         DM     50 1995\n4       2         DM     27 1995\n5       2         PP     17 1995\n6       2         PP     17 1995\n\n\nThe . refers to the object that is passed from the previous line. In this example, the data frame surveys_subset is passed to the . in the select() statement. Then, the modified data frame (which is the result of the select() operation) is passed to the . in the filter() statement. Put more simply: whatever was the result from the line above will be used in the current line.\nSince it gets a bit tedious to write out all the dots, dplyr allows for them to be omitted. In the dplyr family of functions, the first argument is always a data frame, and by default the pipe will pass the output from the line above to this argument. The chunk below, with the . omitted, gives the same output as the one above:\n\nsurveys_subset %&gt;% \n    select(plot_id, species_id, weight, year) %&gt;% \n    filter(year == 1995) %&gt;% head()\n\n  plot_id species_id weight year\n1       2         DM     46 1995\n2       2         DM     26 1995\n3       2         DM     50 1995\n4       2         DM     27 1995\n5       2         PP     17 1995\n6       2         PP     17 1995\n\n\nIf this runs off your screen and you just want to see the first few rows, you can use a pipe to view the head() of the data. Pipes work with non-dplyr functions, too, as long as either the dplyr or magrittr package is loaded.\n\nsurveys_subset %&gt;% \n    select(plot_id, species_id, weight, year) %&gt;% \n    filter(year == 1995) %&gt;%\n    head()\n\n  plot_id species_id weight year\n1       2         DM     46 1995\n2       2         DM     26 1995\n3       2         DM     50 1995\n4       2         DM     27 1995\n5       2         PP     17 1995\n6       2         PP     17 1995\n\n\nIf we wanted to create a new object with this smaller version of the data, we could do so by assigning it a new name:\n\nsurveys_1995 &lt;- surveys_subset %&gt;% \n    select(plot_id, species_id, weight, year) %&gt;% \n    filter(year == 1995)\n\nsurveys_1995 %&gt;% head()\n\n  plot_id species_id weight year\n1       2         DM     46 1995\n2       2         DM     26 1995\n3       2         DM     50 1995\n4       2         DM     27 1995\n5       2         PP     17 1995\n6       2         PP     17 1995\n\n\n\n6.3.2.1 Challenge\nUse the pipe to subset the data frame, keeping only rows where weight is less than 10, and only the columns species_id, sex, and weight.\n\nsurveys_subset %&gt;%\n  filter(weight &lt; 10) %&gt;%\n  select(species_id, sex, weight) %&gt;% head()\n\n  species_id sex weight\n1         PF   M      8\n2         PF   M      6\n3         RM   F      8\n4         RM   F      8\n5         PF   F      6\n6         PF   M      8\n\n\nWe could write a single expression to filter for several criteria, either matching all criteria (&) or any criteria (|):\n\nsurveys_subset %&gt;% \n    filter(taxa == 'Rodent' & sex == 'F') %&gt;% \n    select(sex, taxa) %&gt;% head()\n\n  sex   taxa\n1   F Rodent\n2   F Rodent\n3   F Rodent\n4   F Rodent\n5   F Rodent\n6   F Rodent\n\n\n\nsurveys_subset %&gt;% \n    filter(species == 'clarki' | species == 'audubonii') %&gt;% \n    select(species, taxa) %&gt;% head()\n\n    species    taxa\n1 audubonii  Rabbit\n2 audubonii  Rabbit\n3 audubonii  Rabbit\n4 audubonii  Rabbit\n5    clarki Reptile\n6 audubonii  Rabbit\n\n\n\n\n\n6.3.3 Creating new columns with mutate\nFrequently, you’ll want to create new columns based on the values in existing columns. For instance, you might want to do unit conversions, or find the ratio of values in two columns. For this we’ll use mutate().\nTo create a new column of weight in kg:\n\nsurveys_subset %&gt;%\n    mutate(weight_kg = weight / 1000) %&gt;% head()\n\n   record_id month day year plot_id species_id sex hindfoot_length weight\n1          1     7  16 1977       2         NL   M              32     NA\n9        588     2  18 1978       2         NL   M              NA    218\n17      1453    11   5 1978       2         NL   M              NA    218\n25      3000     5  18 1980       2         NL   F              31     87\n33      5558     3  29 1982       2         NL   M              33    211\n41      6500     8  16 1982       2         NL   F              33    152\n     genus  species   taxa plot_type weight_kg\n1  Neotoma albigula Rodent   Control        NA\n9  Neotoma albigula Rodent   Control     0.218\n17 Neotoma albigula Rodent   Control     0.218\n25 Neotoma albigula Rodent   Control     0.087\n33 Neotoma albigula Rodent   Control     0.211\n41 Neotoma albigula Rodent   Control     0.152\n\n\nYou can also create a second new column based on the first new column within the same call of mutate():\n\nsurveys_subset %&gt;%\n    mutate(weight_kg = weight / 1000,\n           weight_kg2 = weight_kg * 2) %&gt;% head()\n\n   record_id month day year plot_id species_id sex hindfoot_length weight\n1          1     7  16 1977       2         NL   M              32     NA\n9        588     2  18 1978       2         NL   M              NA    218\n17      1453    11   5 1978       2         NL   M              NA    218\n25      3000     5  18 1980       2         NL   F              31     87\n33      5558     3  29 1982       2         NL   M              33    211\n41      6500     8  16 1982       2         NL   F              33    152\n     genus  species   taxa plot_type weight_kg weight_kg2\n1  Neotoma albigula Rodent   Control        NA         NA\n9  Neotoma albigula Rodent   Control     0.218      0.436\n17 Neotoma albigula Rodent   Control     0.218      0.436\n25 Neotoma albigula Rodent   Control     0.087      0.174\n33 Neotoma albigula Rodent   Control     0.211      0.422\n41 Neotoma albigula Rodent   Control     0.152      0.304\n\n\nWe can see that there is some NAs in our new column. If we wanted to remove those we could insert a filter() in the chain, paired with the !is.na notation we learned in the last lecture:\n\nsurveys_subset %&gt;%\n    filter(!is.na(weight)) %&gt;%\n    mutate(weight_kg = weight / 1000) %&gt;% head()\n\n  record_id month day year plot_id species_id sex hindfoot_length weight\n1       588     2  18 1978       2         NL   M              NA    218\n2      1453    11   5 1978       2         NL   M              NA    218\n3      3000     5  18 1980       2         NL   F              31     87\n4      5558     3  29 1982       2         NL   M              33    211\n5      6500     8  16 1982       2         NL   F              33    152\n6      8657    11  13 1983       2         NL   F              32    158\n    genus  species   taxa plot_type weight_kg\n1 Neotoma albigula Rodent   Control     0.218\n2 Neotoma albigula Rodent   Control     0.218\n3 Neotoma albigula Rodent   Control     0.087\n4 Neotoma albigula Rodent   Control     0.211\n5 Neotoma albigula Rodent   Control     0.152\n6 Neotoma albigula Rodent   Control     0.158\n\n\n\n6.3.3.1 Challenge\nCreate a new data frame from the surveys_subset data that meets the following criteria: contains only the species_id column and a new column called hindfoot_half containing values that are half the hindfoot_length values. In this hindfoot_half column, there should be no NAs and all values should be less than 30. (Hint: think about how the commands should be ordered to produce this data frame.)\n\n## Answer\nsurveys_hindfoot_half &lt;- surveys_subset %&gt;%\n    filter(!is.na(hindfoot_length)) %&gt;%\n    mutate(hindfoot_half = hindfoot_length / 2) %&gt;%\n    filter(hindfoot_half &lt; 30) %&gt;%\n    select(species_id, hindfoot_half)\n\nsurveys_hindfoot_half %&gt;% head()\n\n  species_id hindfoot_half\n1         NL          16.0\n2         NL          15.5\n3         NL          16.5\n4         NL          16.5\n5         NL          16.0\n6         NL          16.0"
  },
  {
    "objectID": "lec04-data-wrangling.html#split-apply-combine-techniques-in-dplyr",
    "href": "lec04-data-wrangling.html#split-apply-combine-techniques-in-dplyr",
    "title": "6  Data wrangling in dplyr",
    "section": "6.4 Split-apply-combine techniques in dplyr",
    "text": "6.4 Split-apply-combine techniques in dplyr\nMany data analysis tasks can be approached using the split-apply-combine paradigm: split the data into groups, apply some analysis to each group, and then combine the results.\ndplyr facilitates this workflow through the use of group_by() and summarize(), which collapses each group into a single-row summary of that group. The arguments to group_by() are the column names that contain the categorical variables for which you want to calculate the summary statistics. Let’s view the mean weight by sex.\n\nsurveys_subset %&gt;%\n    group_by(sex) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n# A tibble: 3 × 2\n  sex   mean_weight\n  &lt;chr&gt;       &lt;dbl&gt;\n1 F              NA\n2 M              NA\n3 &lt;NA&gt;           NA\n\n\nThe mean weights become NA since there are individual observations that are NA. Let’s remove those observations.\n\nsurveys_subset %&gt;%\n    filter(!is.na(weight)) %&gt;%\n    group_by(sex) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n# A tibble: 3 × 2\n  sex   mean_weight\n  &lt;chr&gt;       &lt;dbl&gt;\n1 F            43.0\n2 M            42.1\n3 &lt;NA&gt;         60.2\n\n\nThere is one row here that is neither male nor female. These are observations where the animal escaped before the sex could be determined. Let’s remove those as well.\n\nsurveys_subset %&gt;%\n    filter(!is.na(weight) & !is.na(sex)) %&gt;%\n    group_by(sex) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n# A tibble: 2 × 2\n  sex   mean_weight\n  &lt;chr&gt;       &lt;dbl&gt;\n1 F            43.0\n2 M            42.1\n\n\nYou can also group by multiple columns:\n\nsurveys_subset %&gt;%\n    filter(!is.na(weight) & !is.na(sex)) %&gt;%\n    group_by(genus, sex) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n`summarise()` has grouped output by 'genus'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 18 × 3\n# Groups:   genus [9]\n   genus           sex   mean_weight\n   &lt;chr&gt;           &lt;chr&gt;       &lt;dbl&gt;\n 1 Baiomys         F            7.5 \n 2 Baiomys         M            8   \n 3 Chaetodipus     F           23.8 \n 4 Chaetodipus     M           24.3 \n 5 Dipodomys       F           55.8 \n 6 Dipodomys       M           55.5 \n 7 Neotoma         F          157.  \n 8 Neotoma         M          180.  \n 9 Onychomys       F           26.2 \n10 Onychomys       M           25.9 \n11 Perognathus     F            9   \n12 Perognathus     M            8.12\n13 Peromyscus      F           22.7 \n14 Peromyscus      M           20.3 \n15 Reithrodontomys F           11.5 \n16 Reithrodontomys M            9.96\n17 Sigmodon        F           84   \n18 Sigmodon        M           57.1 \n\n\nSince we will use the same filtered and grouped data frame in multiple code chunks below, we could assign this subset of the data to a new name and use this data frame in the subsequent code chunks instead of typing out the functions each time.\n\nfiltered_surveys &lt;- surveys_subset %&gt;%\n    filter(!is.na(weight) & !is.na(sex)) %&gt;%\n    group_by(genus, sex)\n\nOnce the data are grouped, you can also summarize multiple variables at the same time. For instance, we could add a column indicating the minimum weight for each species for each sex:\n\nfiltered_surveys %&gt;%\n    summarize(mean_weight = mean(weight),\n              min_weight = min(weight))\n\n`summarise()` has grouped output by 'genus'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 18 × 4\n# Groups:   genus [9]\n   genus           sex   mean_weight min_weight\n   &lt;chr&gt;           &lt;chr&gt;       &lt;dbl&gt;      &lt;int&gt;\n 1 Baiomys         F            7.5           7\n 2 Baiomys         M            8             8\n 3 Chaetodipus     F           23.8           5\n 4 Chaetodipus     M           24.3           7\n 5 Dipodomys       F           55.8          13\n 6 Dipodomys       M           55.5          18\n 7 Neotoma         F          157.           68\n 8 Neotoma         M          180.           48\n 9 Onychomys       F           26.2          10\n10 Onychomys       M           25.9           9\n11 Perognathus     F            9             6\n12 Perognathus     M            8.12          5\n13 Peromyscus      F           22.7          10\n14 Peromyscus      M           20.3           9\n15 Reithrodontomys F           11.5           5\n16 Reithrodontomys M            9.96          4\n17 Sigmodon        F           84            50\n18 Sigmodon        M           57.1          35\n\n\n\n6.4.0.1 Challenge\n\nUsing the surveys_subset dataframe, use group_by() and summarize() to find the mean hindfoot length for the species Ammospermophilus harrisi.\nWhat was the heaviest animal measured in 1979?\n\n\n# Answer 1\nsurveys_subset %&gt;%\n    filter(!is.na(hindfoot_length)) %&gt;%\n    group_by(species) %&gt;%\n    summarize(mean_hindfoot_length = mean(hindfoot_length)) # 31\n\n# A tibble: 22 × 2\n   species     mean_hindfoot_length\n   &lt;chr&gt;                      &lt;dbl&gt;\n 1 albigula                    32.4\n 2 baileyi                     26.2\n 3 eremicus                    20.1\n 4 flavus                      15.8\n 5 fulvescens                  17.4\n 6 fulviventer                 25  \n 7 harrisi                     31  \n 8 hispidus                    28.3\n 9 intermedius                 22.5\n10 leucogaster                 20.7\n# ℹ 12 more rows\n\n# Answer 2\nsurveys_subset %&gt;%\n    filter(!is.na(weight)) %&gt;%\n    filter(year == 1979) %&gt;% \n    filter(weight == max(weight)) %&gt;% \n    select(genus, species) # Neotoma albigula (white-throated woodrat)\n\n    genus  species\n1 Neotoma albigula"
  },
  {
    "objectID": "lec04-data-wrangling.html#using-tally-to-summarize-categorical-data",
    "href": "lec04-data-wrangling.html#using-tally-to-summarize-categorical-data",
    "title": "6  Data wrangling in dplyr",
    "section": "6.5 Using tally to summarize categorical data",
    "text": "6.5 Using tally to summarize categorical data\nWhen working with data, it is also common to want to know the number of observations found for each factor or combination of factors. For this, dplyr provides tally(). For example, if we want to group by taxa and find the number of observations for each taxa, we would do:\n\nsurveys_subset %&gt;%\n    group_by(taxa) %&gt;%\n    tally()\n\n# A tibble: 4 × 2\n  taxa        n\n  &lt;chr&gt;   &lt;int&gt;\n1 Bird       57\n2 Rabbit      7\n3 Reptile     3\n4 Rodent   4282\n\n\nYou can also use count() to quickly count the unique values of one or more variables. count() combines group_by() and summarise(), so the following will give the same result as the code above:\n\nsurveys_subset %&gt;%\n    count(taxa)\n\n     taxa    n\n1    Bird   57\n2  Rabbit    7\n3 Reptile    3\n4  Rodent 4282\n\n\nWe can also use tally() or count() when grouping on multiple variables:\n\nsurveys_subset %&gt;%\n    group_by(taxa, sex) %&gt;%\n    tally()\n\n# A tibble: 6 × 3\n# Groups:   taxa [4]\n  taxa    sex       n\n  &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;\n1 Bird    &lt;NA&gt;     57\n2 Rabbit  &lt;NA&gt;      7\n3 Reptile &lt;NA&gt;      3\n4 Rodent  F      1979\n5 Rodent  M      2141\n6 Rodent  &lt;NA&gt;    162\n\nsurveys_subset %&gt;% \n  count(taxa, sex)\n\n     taxa  sex    n\n1    Bird &lt;NA&gt;   57\n2  Rabbit &lt;NA&gt;    7\n3 Reptile &lt;NA&gt;    3\n4  Rodent    F 1979\n5  Rodent    M 2141\n6  Rodent &lt;NA&gt;  162\n\n\nHere, tally() is the action applied to the groups created by group_by() and counts the total number of records for each category.\nIf there are many groups, tally() is not that useful on its own. For example, when we want to view the five most abundant species among the observations:\n\nsurveys_subset %&gt;%\n    group_by(species) %&gt;%\n    tally()\n\n# A tibble: 32 × 2\n   species             n\n   &lt;chr&gt;           &lt;int&gt;\n 1 albigula          158\n 2 audubonii           7\n 3 baileyi           356\n 4 bilineata          36\n 5 brunneicapillus     5\n 6 chlorurus           6\n 7 clarki              1\n 8 eremicus          164\n 9 flavus            199\n10 fulvescens          8\n# ℹ 22 more rows\n\n\nSince there are 32 rows in this output, we would like to order the table to display the most abundant species first. In dplyr, we say that we want to arrange() the data.\n\nsurveys_subset %&gt;%\n    group_by(species) %&gt;%\n    tally() %&gt;%\n    arrange(n) # `n` is the name of the column `tally` generated\n\n# A tibble: 32 × 2\n   species             n\n   &lt;chr&gt;           &lt;int&gt;\n 1 clarki              1\n 2 montanus            1\n 3 savannarum          1\n 4 squamata            1\n 5 intermedius         2\n 6 melanocorys         2\n 7 taylori             4\n 8 brunneicapillus     5\n 9 gramineus           5\n10 leucopus            5\n# ℹ 22 more rows\n\n\nStill not that useful. Since we are interested in the most abundant species, we want to display those with the highest count first, in other words, we want to arrange the column n in descending order:\n\nsurveys_subset %&gt;%\n    group_by(species) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n)) %&gt;%\n    head(5)\n\n# A tibble: 5 × 2\n  species          n\n  &lt;chr&gt;        &lt;int&gt;\n1 merriami      1324\n2 penicillatus   394\n3 ordii          378\n4 baileyi        356\n5 megalotis      328\n\n\nIf we want to include more attributes (like taxa and genus) about these species, we can include these in the call to group_by():\n\nsurveys_subset %&gt;%\n    group_by(species, taxa, genus) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n)) %&gt;%\n    head(5)\n\n# A tibble: 5 × 4\n# Groups:   species, taxa [5]\n  species      taxa   genus               n\n  &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;           &lt;int&gt;\n1 merriami     Rodent Dipodomys        1324\n2 penicillatus Rodent Chaetodipus       394\n3 ordii        Rodent Dipodomys         378\n4 baileyi      Rodent Chaetodipus       356\n5 megalotis    Rodent Reithrodontomys   328\n\n\nHere, we are using additional columns that are unique. Be careful not to include anything that would split the group into subgroups, such as sex, year etc.\n\n6.5.0.1 Challenge\nHow many individuals were caught in the rodent enclosure plot type?\n\nsurveys_subset %&gt;%\n    group_by(plot_type) %&gt;%\n    tally()\n\n# A tibble: 5 × 2\n  plot_type                     n\n  &lt;chr&gt;                     &lt;int&gt;\n1 Control                    1951\n2 Long-term Krat Exclosure    639\n3 Rodent Exclosure            530\n4 Short-term Krat Exclosure   739\n5 Spectab exclosure           490\n\n\n\n\n6.5.0.2 Challenge\nYou saw above how to count the number of individuals of each sex using a combination of group_by() and tally(). How could you get the same result using group_by() and summarize()? (Hint: see ?n.)\n\nsurveys_subset %&gt;%\n  group_by(sex) %&gt;%\n  summarize(n = n())\n\n# A tibble: 3 × 2\n  sex       n\n  &lt;chr&gt; &lt;int&gt;\n1 F      1979\n2 M      2141\n3 &lt;NA&gt;    229"
  },
  {
    "objectID": "lec04-data-wrangling.html#reshaping-with-gather-and-spread",
    "href": "lec04-data-wrangling.html#reshaping-with-gather-and-spread",
    "title": "6  Data wrangling in dplyr",
    "section": "6.6 Reshaping with gather and spread",
    "text": "6.6 Reshaping with gather and spread\nThe survey data presented here is almost in what we call a long format – every observation of every individual is its own row. This is an ideal format for data with a rich set of information per observation. It makes it difficult, however, to look at the relationships between measurements across plots. For example, what is the relationship between mean weights of different genera across all plots?\nTo answer that question, we want each plot to have a single row, with all of the measurements in a single plot having their own column. This is called a wide data format. For the surveys_subset data as we have it right now, this is going to be one heck of a wide data frame! However, if we were to summarize data within plots and species, we can reduce the dataset and begin to look for some relationships we’d want to examine. We need to create a new table where each row is the values for a particular variable associated with each plot. In practical terms, this means the values in genus would become the names of column variables and the cells would contain the values of the mean weight observed on each plot by genus.\nSo, in summary:\nLong format:\n\nevery column is a variable\n\nfirst column(s) repeat\n\nevery row is an observation\n\nWide format:\n\neach row is a measured thing\neach column is an independent observation\n\nfirst column does not repeat\n\n\nWe can use the functions called pivot_wider() and pivot_longer() (these are newer replacements for spread() and gather(), which were the older functions). Both functions are explained, but take some time to see what you prefer using!\nLet’s start by using dplyr to create a data frame with the mean body weight of each genus by plot.\n\nsurveys_gw &lt;- surveys_subset %&gt;%\n    filter(!is.na(weight)) %&gt;%\n    group_by(genus, plot_id) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n`summarise()` has grouped output by 'genus'. You can override using the\n`.groups` argument.\n\nsurveys_gw %&gt;% head()\n\n# A tibble: 6 × 3\n# Groups:   genus [2]\n  genus       plot_id mean_weight\n  &lt;chr&gt;         &lt;int&gt;       &lt;dbl&gt;\n1 Baiomys           3         8  \n2 Baiomys           5         7  \n3 Baiomys          19         8  \n4 Chaetodipus       1        21.4\n5 Chaetodipus       2        24.5\n6 Chaetodipus       3        24.4\n\n\n\n6.6.1 Long to Wide with spread and pivot_wider\nNow, to make this long data wide, we use spread() from tidyr to spread out the different taxa into columns. spread() takes three arguments: the data, the key column (or column with identifying information), and the values column (the one with the numbers/values). We’ll use a pipe so we can ignore the data argument.\n\nsurveys_gw_wide1 &lt;- surveys_gw %&gt;%\n  spread(key = genus, value = mean_weight) \n\nhead(surveys_gw_wide1)\n\n# A tibble: 6 × 10\n  plot_id Baiomys Chaetodipus Dipodomys Neotoma Onychomys Perognathus Peromyscus\n    &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1       1      NA        21.4      60.8    155.      28.7       13          20.2\n2       2      NA        24.5      55.9    175.      26.6        7          22.4\n3       3       8        24.4      47.6    157.      23.5        7.5        22.4\n4       4      NA        23.1      57.6    200       29.9        7.67       20.5\n5       5       7        16.1      51.3    198.      26.3        7.6        20.5\n6       6      NA        24.2      59.1    184.      26.2        7.86       22  \n# ℹ 2 more variables: Reithrodontomys &lt;dbl&gt;, Sigmodon &lt;dbl&gt;\n\n\nNotice that some genera have NA values. That’s because some of those genera don’t have any record in that plot. Sometimes it is fine to leave those as NA. Sometimes we want to fill them as zeros, in which case we would add the argument fill=0. Remember, if arguments are presented in the correct order, you don’t have to specify them.\n\nsurveys_gw_wide1_fill0 &lt;- surveys_gw %&gt;%\n  spread(genus, mean_weight, fill = 0)\n\nhead(surveys_gw)\n\n# A tibble: 6 × 3\n# Groups:   genus [2]\n  genus       plot_id mean_weight\n  &lt;chr&gt;         &lt;int&gt;       &lt;dbl&gt;\n1 Baiomys           3         8  \n2 Baiomys           5         7  \n3 Baiomys          19         8  \n4 Chaetodipus       1        21.4\n5 Chaetodipus       2        24.5\n6 Chaetodipus       3        24.4\n\n\nAnother way to spread your data out is to use pivot_wider(), which takes 3 arguments as well: the data, the names_from column variable that will eventually become the column names, and the values_from column variable that will fill in the values.\n\nsurveys_gw_wide2 &lt;- surveys_gw %&gt;% \n  pivot_wider(names_from = genus, values_from = mean_weight)\n\nhead(surveys_gw_wide2)\n\n# A tibble: 6 × 10\n  plot_id Baiomys Chaetodipus Dipodomys Neotoma Onychomys Perognathus Peromyscus\n    &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1       3       8        24.4      47.6    157.      23.5        7.5        22.4\n2       5       7        16.1      51.3    198.      26.3        7.6        20.5\n3      19       8        25.3      41       NA       23.3        8.06       20.4\n4       1      NA        21.4      60.8    155.      28.7       13          20.2\n5       2      NA        24.5      55.9    175.      26.6        7          22.4\n6       4      NA        23.1      57.6    200       29.9        7.67       20.5\n# ℹ 2 more variables: Reithrodontomys &lt;dbl&gt;, Sigmodon &lt;dbl&gt;\n\n\nNow we can go back to our original question: what is the relationship between mean weights of different genera across all plots? We can easily see the weights for each genus in each plot!\n\n\n6.6.2 Wide to long with gather and pivot_longer\nWhat if we had the opposite problem, and wanted to go from a wide to long format? For that, we can use gather() to sweep up a set of columns into one key-value pair. We give it the arguments of a new key and value column name, and then we specify which columns we either want or do not want gathered up. So, to go backwards from surveys_gw_wide, and exclude plot_id from the gathering, we would do the following:\n\nsurveys_gw_long1 &lt;- surveys_gw_wide1 %&gt;%\n  gather(genus, mean_weight, -plot_id) \n\nhead(surveys_gw_long1)\n\n# A tibble: 6 × 3\n  plot_id genus   mean_weight\n    &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n1       1 Baiomys          NA\n2       2 Baiomys          NA\n3       3 Baiomys           8\n4       4 Baiomys          NA\n5       5 Baiomys           7\n6       6 Baiomys          NA\n\n\nNote that now the NA genera are included in the long format. Going from wide to long to wide can be a useful way to balance out a dataset so every replicate has the same composition.\nWe could also have used a specification for what columns to include. This can be useful if you have a large number of identifying columns, and it’s easier to specify what to gather than what to leave alone. And if the columns are sequential, we don’t even need to list them all out - just use the : operator! Say we only wanted the columns from Baiomys to Onychomys to be gathered together.\n\nsurveys_gw_wide1 %&gt;%\n  gather(genus, mean_weight_of_some_genera, Baiomys:Onychomys)\n\n# A tibble: 120 × 7\n   plot_id Perognathus Peromyscus Reithrodontomys Sigmodon genus  \n     &lt;int&gt;       &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  \n 1       1       13          20.2           13        NA   Baiomys\n 2       2        7          22.4           10.6      71.5 Baiomys\n 3       3        7.5        22.4            9.85     70.7 Baiomys\n 4       4        7.67       20.5            9.5      NA   Baiomys\n 5       5        7.6        20.5           11.3      NA   Baiomys\n 6       6        7.86       22             11.7      83.5 Baiomys\n 7       7       NA          21.3           11.2      NA   Baiomys\n 8       8        6.83       17.3           10        NA   Baiomys\n 9       9        7.75       18.8           11.8      36   Baiomys\n10      10       NA          20.5           10.2     110   Baiomys\n# ℹ 110 more rows\n# ℹ 1 more variable: mean_weight_of_some_genera &lt;dbl&gt;\n\n\nAnother method is to use pivot_longer(), which takes 4 arguments: the data, the names_to column variable that comes from the column names, the values_to column with the values, and cols which specifies which columns we want to keep or drop. Again, we will pipe from the dataset so we don’t have to specify the data argument:\n\nsurveys_gw_long2 &lt;- surveys_gw_wide2 %&gt;% \n  pivot_longer(names_to = \"genus\", values_to = \"mean_weight\", cols = -plot_id)\n\nsurveys_gw_long2\n\n# A tibble: 216 × 3\n   plot_id genus           mean_weight\n     &lt;int&gt; &lt;chr&gt;                 &lt;dbl&gt;\n 1       3 Baiomys                8   \n 2       3 Chaetodipus           24.4 \n 3       3 Dipodomys             47.6 \n 4       3 Neotoma              157.  \n 5       3 Onychomys             23.5 \n 6       3 Perognathus            7.5 \n 7       3 Peromyscus            22.4 \n 8       3 Reithrodontomys        9.85\n 9       3 Sigmodon              70.7 \n10       5 Baiomys                7   \n# ℹ 206 more rows\n\n\nIf the columns are directly adjacent as they are here, we don’t even need to list the all out: we can just use the : operator, as before.\n\nsurveys_gw_wide2 %&gt;% \n  pivot_longer(names_to = \"genus\", values_to = \"mean_weight\", cols = Baiomys:Sigmodon)\n\n# A tibble: 216 × 3\n   plot_id genus           mean_weight\n     &lt;int&gt; &lt;chr&gt;                 &lt;dbl&gt;\n 1       3 Baiomys                8   \n 2       3 Chaetodipus           24.4 \n 3       3 Dipodomys             47.6 \n 4       3 Neotoma              157.  \n 5       3 Onychomys             23.5 \n 6       3 Perognathus            7.5 \n 7       3 Peromyscus            22.4 \n 8       3 Reithrodontomys        9.85\n 9       3 Sigmodon              70.7 \n10       5 Baiomys                7   \n# ℹ 206 more rows\n\n\n\n6.6.2.1 Challenge\nStarting with the surveys_gw_wide2 dataset, how would you create a new dataset that gathers the mean weight of all the genera (excluding NAs) except for the genus Perognathus?\n\nsurveys_gw_wide2 %&gt;%\n  gather(genus, mean_weight, -plot_id, -Perognathus) %&gt;%\n  filter(!is.na(mean_weight)) %&gt;%\n  head()\n\n# A tibble: 6 × 4\n  plot_id Perognathus genus       mean_weight\n    &lt;int&gt;       &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n1       3        7.5  Baiomys             8  \n2       5        7.6  Baiomys             7  \n3      19        8.06 Baiomys             8  \n4       3        7.5  Chaetodipus        24.4\n5       5        7.6  Chaetodipus        16.1\n6      19        8.06 Chaetodipus        25.3"
  },
  {
    "objectID": "lec05-data-visualization.html#lesson-preamble",
    "href": "lec05-data-visualization.html#lesson-preamble",
    "title": "7  Data visualization in ggplot",
    "section": "7.1 Lesson preamble",
    "text": "7.1 Lesson preamble\n\n7.1.1 Learning Objectives\n\nProduce scatter plots, line plots, and histograms using ggplot.\nSet universal plot settings.\nUnderstand how to combine dplyr and ggplot.\nUnderstand and apply faceting in ggplot.\n\n7.1.2 Lesson outline\n\nPlotting with ggplot2 (10 mins)\nBuilding plots iteratively (30 mins)\nSplit-apply-combine… plot! (20 mins)\nFaceting (10 mins)\nWhy plot? (15 mins)\nGeneralizable plots (15 mins)\nExporting (10 mins)\n\n\n\nNow we have seen how to get our dataset in our desired shape and form (aka “tidy”, where every column is a variable, and every row is an observation), we are of course itching to actually see what the data actually looks like. Luckily, our favourite package-of-packages tidyverse got us covered – it comes with a wonderful package for generating graphics called ggplot2!\nSo we’ll go ahead and load that up and let’s get graphing! We will use the subset data again but remember to plot everything for your projects.\n\nlibrary(tidyverse)\nlibrary(here)\n\n# from last class\n# download file\ndownload.file(\"https://ndownloader.figshare.com/files/2292169\",\n              here(\"portal_data.csv\"))\n\n# read file into R\nportal_data &lt;- read_csv(here('portal_data.csv'))\n\n# subset every 8th row\nsurveys_subset &lt;- portal_data[seq(1, 34786, 8), ]\n\n# save subsetted dataframe into a new .csv in your file folder\nwrite.csv(surveys_subset, file = here(\"survey_subset.csv\"))\n\n# if you already have surveys_subset.csv in your file folder, you only have to run this\nsurveys_subset &lt;- read_csv(here('survey_subset.csv'))\n\n# always good to inspect your data frame to make sure there are no errors\nhead(surveys_subset)\n\n# A tibble: 6 × 14\n   ...1 record_id month   day  year plot_id species_id sex   hindfoot_length\n  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n1     1         1     7    16  1977       2 NL         M                  32\n2     2       588     2    18  1978       2 NL         M                  NA\n3     3      1453    11     5  1978       2 NL         M                  NA\n4     4      3000     5    18  1980       2 NL         F                  31\n5     5      5558     3    29  1982       2 NL         M                  33\n6     6      6500     8    16  1982       2 NL         F                  33\n# ℹ 5 more variables: weight &lt;dbl&gt;, genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;,\n#   plot_type &lt;chr&gt;\n\nstr(surveys_subset)\n\nspc_tbl_ [4,349 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ ...1           : num [1:4349] 1 2 3 4 5 6 7 8 9 10 ...\n $ record_id      : num [1:4349] 1 588 1453 3000 5558 ...\n $ month          : num [1:4349] 7 2 11 5 3 8 11 9 12 9 ...\n $ day            : num [1:4349] 16 18 5 18 29 16 13 30 8 7 ...\n $ year           : num [1:4349] 1977 1978 1978 1980 1982 ...\n $ plot_id        : num [1:4349] 2 2 2 2 2 2 2 2 2 2 ...\n $ species_id     : chr [1:4349] \"NL\" \"NL\" \"NL\" \"NL\" ...\n $ sex            : chr [1:4349] \"M\" \"M\" \"M\" \"F\" ...\n $ hindfoot_length: num [1:4349] 32 NA NA 31 33 33 32 32 32 32 ...\n $ weight         : num [1:4349] NA 218 218 87 211 152 158 173 160 135 ...\n $ genus          : chr [1:4349] \"Neotoma\" \"Neotoma\" \"Neotoma\" \"Neotoma\" ...\n $ species        : chr [1:4349] \"albigula\" \"albigula\" \"albigula\" \"albigula\" ...\n $ taxa           : chr [1:4349] \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ...\n $ plot_type      : chr [1:4349] \"Control\" \"Control\" \"Control\" \"Control\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   ...1 = col_double(),\n  ..   record_id = col_double(),\n  ..   month = col_double(),\n  ..   day = col_double(),\n  ..   year = col_double(),\n  ..   plot_id = col_double(),\n  ..   species_id = col_character(),\n  ..   sex = col_character(),\n  ..   hindfoot_length = col_double(),\n  ..   weight = col_double(),\n  ..   genus = col_character(),\n  ..   species = col_character(),\n  ..   taxa = col_character(),\n  ..   plot_type = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt;"
  },
  {
    "objectID": "lec05-data-visualization.html#plotting-with-ggplot2",
    "href": "lec05-data-visualization.html#plotting-with-ggplot2",
    "title": "7  Data visualization in ggplot",
    "section": "7.2 Plotting with ggplot2",
    "text": "7.2 Plotting with ggplot2\nggplot2 is a plotting package that makes it simple to create complex plots from data frames. The name ggplot2 comes from its inspiration, the book A Grammar of Graphics, and the main goal is to allow coders to distill complex data structure and express their desired graphical outcome in a concise manner instead of telling the computer every detail about what should happen. For example, you would say “colour my data by species” instead of “go through this data frame and plot any observations of species1 in blue, any observations of species2 in red, etc”. Thanks to this functional way of interfaces with data, various plot elements interact seamlessly, publication-quality plots can be created with minimal amounts of adjustment and tweaking.\nggplot2 graphics are built step by step by adding new elements, or layers. Adding layers in this fashion allows for extensive flexibility and customization of plots. To build a ggplot, we need to:\n\nUse the ggplot() function and bind the plot to a specific data frame using the data argument\n\nRemember, if the arguments are provided in the right order then the names of the arguments can be omitted.\n\nDefine aesthetics (aes), by selecting the variables to be plotted and the variables to define the presentation such as plotting size, shape colour, etc.\n\n\nggplot(surveys_subset, aes(x = weight, y = hindfoot_length))\n\n\n\n\n\nAdd geoms – geometrical objects as a graphical representation of the data in the plot (points, lines, bars). ggplot2 offers many different geoms. We will use a few common ones today, including:\n\ngeom_point() for scatter plots, dot plots, etc.\ngeom_line() for trend lines, time-series, etc.\ngeom_histogram() for histograms\n\n\nTo add a geom to the plot use + operator. Because we have two continuous variables, let’s use geom_point() first:\n\nggplot(surveys_subset, aes(x = weight, y = hindfoot_length)) +\n  geom_point()\n\nWarning: Removed 505 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNote: Notice that triangle-! warning sign above the plot? ggplot is telling you that it wasn’t able to plot all of your data. Typically this means that there are NAs in the data, or that some data points lie outside of the bounds of the axes. Can you figure what it is in this instance?\nThe + in the ggplot2 package is particularly useful because it allows you to modify existing ggplot objects. This means you can easily set up plot “templates” and conveniently explore different types of plots. The + sign used to add layers must be placed at the end of each line containing a layer. If, instead, the + sign is added in the line before the other layer, ggplot2 will not add the new layer and R will return an error message.\nThe above plot can be generated with code like this:\n\n# Assign plot to a variable\nsurveys_plot &lt;- ggplot(surveys_subset, aes(x = weight, y = hindfoot_length))\n\n# Draw the plot\nsurveys_plot + geom_point()\n\nWarning: Removed 505 rows containing missing values (`geom_point()`).\n\n\n\n\n\nAnything you put in the ggplot() function can be seen by any geom layers that you add (i.e., these are universal plot settings). This includes the x and y axis you set up in aes(). You can also specify aesthetics for a given geom independently of the aesthetics defined globally in the ggplot() function, which is particularly handy when you are building complex plots layering data from different data frames.\n\n7.2.1 Building plots iteratively\nBuilding plots with ggplot is typically an iterative process – we sequentially add more layers and options until we are satisfied. Typically, the process starts with defining the dataset we’ll use, laying the axes, and choosing a geom, as we just did:\n\nggplot(surveys_subset, aes(x = weight, y = hindfoot_length)) +\n    geom_point()\n\nWarning: Removed 505 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThen, we start modifying this plot to extract more information from it. For instance, we can add the argument for transparency (alpha) to reduce overplotting:\n\nggplot(data = surveys_subset, aes(x = weight, y = hindfoot_length)) +\n    geom_point(alpha = 0.2)\n\nWarning: Removed 505 rows containing missing values (`geom_point()`).\n\n\n\n\n\nBased on the hindfoot length and the weights, there appears to be 4 clusters in this data. Potentially, one of the categorical variables we have in the data could explain this pattern. Colouring the data points according to a categorical variable is an easy way to find out if there seems to be correlation. Let’s try colouring this points according to plot_type.\n\nggplot(surveys_subset, aes(x = weight, y = hindfoot_length, colour = plot_type)) +\n    geom_point(alpha = 0.2)\n\nWarning: Removed 505 rows containing missing values (`geom_point()`).\n\n\n\n\n\nIt seems like the type of plot the animal was captured on correlates well with some of these clusters, but there are still many that are quite mixed. Let’s try to do better! This time, the information about the data can provide some clues to which variable to look at. The plot above suggests that there might be 4 clusters, so a variable with 4 values is a good guess for what could explain the observed pattern in the scatter plot.\nBoth dplyr and ggplot2 are developed within “the tidyverse” and can use the pipes, but you may not be able to pipe in base R functions or functions from different packages.\n\nsurveys_subset %&gt;%\n    summarize_all(n_distinct) \n\n# A tibble: 1 × 14\n   ...1 record_id month   day  year plot_id species_id   sex hindfoot_length\n  &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt;           &lt;int&gt;\n1  4349      4349    12    31    26      24         37     3              44\n# ℹ 5 more variables: weight &lt;int&gt;, genus &lt;int&gt;, species &lt;int&gt;, taxa &lt;int&gt;,\n#   plot_type &lt;int&gt;\n\n# `n_distinct` is a function that counts unique values in a set of vectors\n\nRemember that there are still NA values here, that’s why there appears to be three sexes although there is only male and female. There are four taxa so that could be a good candidate, let’s see which those are.\n\nsurveys_subset %&gt;%\n    distinct(taxa)\n\n# A tibble: 4 × 1\n  taxa   \n  &lt;chr&gt;  \n1 Rodent \n2 Bird   \n3 Rabbit \n4 Reptile\n\n\nIt seems reasonable that these taxa contain animals different enough to have diverse weights and length of their feet. Lets use this categorical variable to colour the scatter plot.\n\nggplot(surveys_subset, aes(x = weight, y = hindfoot_length, colour = taxa)) +\n    geom_point(alpha = 0.2)\n\nWarning: Removed 505 rows containing missing values (`geom_point()`).\n\n\n\n\n\nOnly rodents? That was unexpected… Let’s check what’s going on.\n\nsurveys_subset %&gt;%\n    group_by(taxa) %&gt;%\n    tally()\n\n# A tibble: 4 × 2\n  taxa        n\n  &lt;chr&gt;   &lt;int&gt;\n1 Bird       57\n2 Rabbit      7\n3 Reptile     3\n4 Rodent   4282\n\n\nDefinitely mostly rodents in our data set…\n\nsurveys_subset %&gt;%\n    filter(!is.na(hindfoot_length)) %&gt;% # control by removing `!`\n    group_by(taxa) %&gt;%\n    tally()\n\n# A tibble: 1 × 2\n  taxa       n\n  &lt;chr&gt;  &lt;int&gt;\n1 Rodent  3929\n\n\n…and it turns out that only rodents have had their hindfeet measured! Rats.\nLet’s remove all animals that did not have their hindfeet measured, including those rodents that did not. We’ll also remove animals whose weights weren’t measured.\n\nsurveys_hf_wt &lt;- surveys_subset %&gt;%\n    filter(!is.na(hindfoot_length) & !is.na(weight))\n\nsurveys_hf_wt %&gt;%\n    summarize_all(n_distinct)\n\n# A tibble: 1 × 14\n   ...1 record_id month   day  year plot_id species_id   sex hindfoot_length\n  &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt;           &lt;int&gt;\n1  3844      3844    12    31    26      24         23     3              41\n# ℹ 5 more variables: weight &lt;int&gt;, genus &lt;int&gt;, species &lt;int&gt;, taxa &lt;int&gt;,\n#   plot_type &lt;int&gt;\n\n\nMaybe the genus of the animals can explain what we are seeing.\n\nggplot(surveys_hf_wt, aes(x = weight, y = hindfoot_length, colour = genus)) +\n    geom_point(alpha = 0.2)\n\n\n\n\nNow this looks good! There is a clear separation between different genus, but also significant spread within genus. For example, in the weight of the green Neotoma observations. There are also two clearly separate clusters that are both coloured in olive green (Dipodomys). Maybe separating the observations into different species would be better?\n\nggplot(surveys_hf_wt, aes(x = weight, y = hindfoot_length, colour = species)) +\n    geom_point(alpha = 0.2)\n\n\n\n\nGreat! Together with the genus plot, this definitely seem to explain most of the variation we see in the hindfoot length and weight measurements. It is still a bit messy as it appears like we have around four clusters of data points, maybe three colours of points that really stood out, but there are 21 species in the legend. Let’s investigate!\n\nsurveys_subset %&gt;%\n    filter(!is.na(hindfoot_length) & !is.na(weight)) %&gt;%\n    group_by(species) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n))\n\n# A tibble: 21 × 2\n   species          n\n   &lt;chr&gt;        &lt;int&gt;\n 1 merriami      1213\n 2 penicillatus   377\n 3 ordii          347\n 4 baileyi        346\n 5 megalotis      305\n 6 torridus       265\n 7 spectabilis    262\n 8 flavus         182\n 9 eremicus       151\n10 albigula       125\n# ℹ 11 more rows\n\n\nWhen we look at the number of observations for each species, it seems like most species are fairly well represented. However some have rarely been sampled, and there is a big drop from 104 to 19 observations. Let’s include only species with more than 100 observations and see what happens.\n\nsurveys_abun_species &lt;- surveys_subset %&gt;%\n    filter(!is.na(hindfoot_length) & !is.na(weight)) %&gt;%\n    group_by(species) %&gt;%\n    mutate(n = n()) %&gt;% # add count value to each row\n    filter(n &gt; 100) %&gt;%\n    select(-n)\n\nsurveys_abun_species\n\n# A tibble: 3,791 × 14\n# Groups:   species [12]\n    ...1 record_id month   day  year plot_id species_id sex   hindfoot_length\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n 1     4      3000     5    18  1980       2 NL         F                  31\n 2     5      5558     3    29  1982       2 NL         M                  33\n 3     6      6500     8    16  1982       2 NL         F                  33\n 4     7      8657    11    13  1983       2 NL         F                  32\n 5     8      9605     9    30  1984       2 NL         F                  32\n 6     9     11215    12     8  1985       2 NL         F                  32\n 7    10     11879     9     7  1986       2 NL         F                  32\n 8    11     12729     4    26  1987       2 NL         M                  32\n 9    12     13434     9    27  1987       2 NL         M                  33\n10    14     17230     2    25  1990       2 NL         M                  33\n# ℹ 3,781 more rows\n# ℹ 5 more variables: weight &lt;dbl&gt;, genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;,\n#   plot_type &lt;chr&gt;\n\n\nStill has 3791 observations, so only 558 observations were removed.\n\nggplot(surveys_abun_species, aes(x = weight, y = hindfoot_length, colour = species)) +\n    geom_point(alpha = 0.2)\n\n\n\n\nThe plot is now cleaner – there are less species and so less colours, and the clusters are now more distinct.\n\n7.2.1.1 Challenge\nCreate a scatter plot of hindfoot_length against species with the weight showing in different colours. Hint: If you are confused about which variable to put on the x-axis, we generally describe a plot by saying plot the y variable against the x variable.\n\n\n7.2.1.2 Take home challenge\nHow would you improve the readability of this plot?\nAnswer: using jitter"
  },
  {
    "objectID": "lec05-data-visualization.html#split-apply-combine-plot",
    "href": "lec05-data-visualization.html#split-apply-combine-plot",
    "title": "7  Data visualization in ggplot",
    "section": "7.3 Split-apply-combine… plot!",
    "text": "7.3 Split-apply-combine… plot!\nIn this section, we will learn how to work with dplyr and ggplot together. Aided by the pipes (%&gt;%), we can create a powerful data exploration workflow using these two packages.\nLet’s calculate number of counts per year for each species. First, we need to group the data and count records within each group:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n)) # Adding arrange just to compare with histogram\n\n# A tibble: 259 × 3\n# Groups:   year [26]\n    year species      n\n   &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;\n 1  2002 baileyi    109\n 2  1985 merriami    81\n 3  1997 merriami    74\n 4  2001 baileyi     68\n 5  1982 merriami    64\n 6  1983 merriami    63\n 7  2000 baileyi     61\n 8  1998 merriami    57\n 9  1987 merriami    56\n10  1995 merriami    55\n# ℹ 249 more rows\n\n\nWe could assign this table to a variable, and then pass that variable to ggplot().\n\nyearly_counts &lt;- surveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n))\n\nggplot(yearly_counts, aes(x = n)) +\n    geom_histogram()\n\n\n\n\nRemember that a histogram plots the number of observations based on a variable, so you only need to specify the x-axis in the ggplot() call.\nTo change up how the n variable is binned on the x-axis, you can adjust the number of bins in your plot!\n\nggplot(yearly_counts, aes(x = n)) +\n    geom_histogram(bins=10)\n\n\n\n\nCreating an intermediate variable would be preferable for time consuming calculations, because you would not want to do that operation every time you change the plot aesthetics.\nIf it is not a time consuming calculation or you would like the flexibility of changing the data summary and the plotting options in the same code chunk, you can pipe the output of your split-apply-combine operation to the plotting command:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = n)) +\n        geom_histogram()\n\n\n\n\nWe can perform a quick check that the plot corresponds to the table by colouring the histogram by species:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = n, fill = species)) + \n        geom_histogram()\n\n\n\n\nNote2: Here we are using fill to assign colours to species rather than colour. In general colour refers to the outline of points/bars or whatever it is you are plotting and fill refers to the colour that goes inside the point or bar. If you are confused, try switching out fill for colour to see what looks best!\nLet’s explore how the number of each genus varies over time. Longitudinal data can be visualized as a line plot with years on the x axis and counts on the y axis:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n)) +\n        geom_line()\n\n\n\n\nUnfortunately, this does not work because we plotted data for all the species together as one line. We need to tell ggplot to draw a line for each species by modifying the aesthetic function to include group = species:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n, group = species)) +\n        geom_line()\n\n\n\n\nWe will be able to distinguish species in the plot if we add colours (using colour also automatically groups the data):\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n, colour = species)) + # `colour` groups automatically\n        geom_line()"
  },
  {
    "objectID": "lec05-data-visualization.html#faceting",
    "href": "lec05-data-visualization.html#faceting",
    "title": "7  Data visualization in ggplot",
    "section": "7.4 Faceting",
    "text": "7.4 Faceting\nggplot has a special technique called faceting that allows the user to split one plot into multiple subplots based on a variable included in the dataset. This allows us to examine the trends associated with each grouping variable more closely. We will use it to make the plot above more readable:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n)) + \n        geom_line() +\n        facet_wrap(~species)\n\n\n\n\nNow we would like to split the line in each plot by the sex of each individual measured. To do that we need to make counts in the data frame grouped by year, species, and sex:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species, sex) %&gt;%\n    tally()\n\n# A tibble: 479 × 4\n# Groups:   year, species [259]\n    year species      sex       n\n   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n 1  1977 eremicus     M         1\n 2  1977 flavus       F         2\n 3  1977 flavus       M         2\n 4  1977 megalotis    F         1\n 5  1977 merriami     F        12\n 6  1977 merriami     M        11\n 7  1977 ordii        F         2\n 8  1977 ordii        M         1\n 9  1977 penicillatus F         1\n10  1977 spectabilis  F         2\n# ℹ 469 more rows\n\n\nWe can make the faceted plot by splitting further by sex using colour (within a single plot):\n\nsurveys_abun_species %&gt;%\n    group_by(year, species, sex) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n, colour = sex)) +\n        geom_line() +\n        facet_wrap(~species)\n\n\n\n\nThere are several observations where sex was not recorded. Let’s filter out those values.\n\nsurveys_abun_species %&gt;%\n    filter(!is.na(sex)) %&gt;%\n    group_by(year, species, sex) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n, color = sex)) +\n        geom_line() +\n        facet_wrap(~species)\n\n\n\n\nIt is possible to specify exactly which colors1 to use and to change the thickness of the lines to make the them easier to distinguish.\n\nsurveys_abun_species %&gt;%\n    filter(!is.na(sex)) %&gt;%\n    group_by(year, species, sex) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n, colour = sex)) +\n        geom_line(size = 1) +\n        scale_colour_manual(values = c(\"black\", \"orange\")) +\n        facet_wrap(~species) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nNot sure what colours would look good on your plot? The R Community got you covered! Check out these awesome color palettes where nice-looking color combos come predefined. We especially recommend the viridis color palettes. These palettes are not only pretty, they are specifically designed to be easier to read by those with colorblindness.\nTia hinted at a Studio Ghibli colour palette during the first class, so let’s use it! You’ll need to download the ghibli package and call library().\n\n# install.packages(\"ghibli\")\nlibrary(ghibli)\n\n# check out their palettes\n# display palettes w/ names\npar(mfrow=c(9,3))\nfor(i in names(ghibli_palettes)) print(ghibli_palette(i))\n\n\n\nsurveys_abun_species %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  group_by(year, species, sex) %&gt;%\n  tally() %&gt;%\n  ggplot(aes(x = year, y = n, color = sex)) +\n        geom_line(size = 1) +\n  # make sure to read the package documentation so that you know how it works!\n  scale_colour_ghibli_d(\"SpiritedMedium\", direction = -1) +\n  facet_wrap(~species)\n\n\n\n\nLastly, let’s change the x labels so that they don’t overlap, and remove the gray background so that the lines can stand out more. To customize the non-data components of the plot, we will pass some theme statements2 to ggplot.\n\nsurveys_abun_species %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  group_by(year, species, sex) %&gt;%\n  tally() %&gt;%\n  ggplot(aes(x = year, y = n, color = sex)) +\n  geom_line(size = 1) +\n  scale_colour_ghibli_d(\"SpiritedMedium\", direction = -1) +       \n  facet_wrap(~species) +\n  theme_bw() +\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(angle=30, hjust=1))\n\n\n\n\nI like to use the classic theme_classic because it also gets rid of grid lines\n\nsurveys_abun_species %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  group_by(year, species, sex) %&gt;%\n  tally() %&gt;%\n  ggplot(aes(x = year, y = n, colour = sex)) +\n  geom_line(size = 1) +\n  scale_colour_ghibli_d(\"SpiritedMedium\", direction = -1) +       \n  facet_wrap(~species) +\n  theme_classic() +\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(angle=30, hjust=1))\n\n\n\n\n\n7.4.0.1 Challenge\n\nRemember the histogram coloured according to each species? Starting from that code, how could we separate each species into its own subplot?\n\nUse the filtered data frame (surveys_abun_species) for these two questions.\n\nCreate a plot to determine to show changes in average weight over years. Which year was the average weight of the animals the highest?\nCreate a plot to show differences in yearly trends across species. Is the yearly trend the same for all species?"
  },
  {
    "objectID": "lec05-data-visualization.html#why-visualize",
    "href": "lec05-data-visualization.html#why-visualize",
    "title": "7  Data visualization in ggplot",
    "section": "7.5 Why visualize?",
    "text": "7.5 Why visualize?\nThe Datasaurus Dozen dataset is a handful of datasets that complement the dplyr package. Aside from functions, packages can also import objects.\n\nlibrary(tidyverse)\nlibrary(dplyr)\n\n# install.packages(\"datasauRus\")\nlibrary(datasauRus)\n\n\nsummary(datasaurus_dozen)\n\n   dataset                x               y           \n Length:1846        Min.   :15.56   Min.   : 0.01512  \n Class :character   1st Qu.:41.07   1st Qu.:22.56107  \n Mode  :character   Median :52.59   Median :47.59445  \n                    Mean   :54.27   Mean   :47.83510  \n                    3rd Qu.:67.28   3rd Qu.:71.81078  \n                    Max.   :98.29   Max.   :99.69468  \n\nhead(datasaurus_dozen)\n\n# A tibble: 6 × 3\n  dataset     x     y\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 dino     55.4  97.2\n2 dino     51.5  96.0\n3 dino     46.2  94.5\n4 dino     42.8  91.4\n5 dino     40.8  88.3\n6 dino     38.7  84.9\n\ntable(datasaurus_dozen$dataset)\n\n\n      away   bullseye     circle       dino       dots    h_lines high_lines \n       142        142        142        142        142        142        142 \nslant_down   slant_up       star    v_lines wide_lines    x_shape \n       142        142        142        142        142        142 \n\n\nThere are 13 different datasets in this one object. We will use tidyverse functions to take a look at the object, grouped by the datasets.\n\ndatasaurus_dozen %&gt;% \n    group_by(dataset) %&gt;% \n    summarize(\n      mean_x    = mean(x),\n      mean_y    = mean(y),\n      std_dev_x = sd(x),\n      std_dev_y = sd(y),\n      corr_x_y  = cor(x, y)\n    )\n\n# A tibble: 13 × 6\n   dataset    mean_x mean_y std_dev_x std_dev_y corr_x_y\n   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 away         54.3   47.8      16.8      26.9  -0.0641\n 2 bullseye     54.3   47.8      16.8      26.9  -0.0686\n 3 circle       54.3   47.8      16.8      26.9  -0.0683\n 4 dino         54.3   47.8      16.8      26.9  -0.0645\n 5 dots         54.3   47.8      16.8      26.9  -0.0603\n 6 h_lines      54.3   47.8      16.8      26.9  -0.0617\n 7 high_lines   54.3   47.8      16.8      26.9  -0.0685\n 8 slant_down   54.3   47.8      16.8      26.9  -0.0690\n 9 slant_up     54.3   47.8      16.8      26.9  -0.0686\n10 star         54.3   47.8      16.8      26.9  -0.0630\n11 v_lines      54.3   47.8      16.8      26.9  -0.0694\n12 wide_lines   54.3   47.8      16.8      26.9  -0.0666\n13 x_shape      54.3   47.8      16.8      26.9  -0.0656\n\n\nAll of the datasets have roughly the same mean and standard deviation along both the x and y axis.\nLet’s take a look at how the data looks in a graphical sense. We will use filter to extract the rows belonging to one dataset and then pipe that directly into a ggplot.\n\ndatasaurus_dozen %&gt;% \n  filter(dataset == \"circle\") %&gt;% \n  ggplot(aes(x=x, y=y)) +\n  geom_point()\n\n\n\n\nRemember that tidyverse’s data wranging packages use the pipe %&gt;% to move the previous output to the next line, where as ggplot uses the plus sign +\nTry editing the code above to display different datasets. Notice how different distributions of data can all give similar statistical summaries - so it’s always a good choice to visualize your data rather than relying on just numbers!\nIf we wanted to take a look at all of the datasets at once, we can also use the facet_wrap() function\n\ndatasaurus_dozen %&gt;% \n  #filter(dataset == \"circle\") %&gt;% remove filter \n  ggplot(aes(x=x, y=y, color = dataset)) + # Add color \n  geom_point() + \n  facet_wrap(~dataset, ncol = 3) + \n  theme_void() + \n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "lec05-data-visualization.html#generalizable-plots",
    "href": "lec05-data-visualization.html#generalizable-plots",
    "title": "7  Data visualization in ggplot",
    "section": "7.6 Generalizable plots",
    "text": "7.6 Generalizable plots\nRemember when we edited our code to test out multiple datasets in the datasaurus dozen object? Perhaps you copy and pasted the code several time and changed the column name? This is not optimal because if you need to change the code in one instance (for example changing the x-axis label), you’ll need to revisit ever instance that you copy and pasted to code to. This approach leads you vulnerable to errors when copy and pasting.\nOne way to make your code robust is to bring all the factors that need editing to the start of the data. This may seem cumbersome for such a simple example where we are only changing the dataset name, but we’ll return to this concept later with more complicated examples.\nLet’s grab the code we used to make one plot earlier and modify it to be more generalizable\n\nunique(datasaurus_dozen$dataset)\n\n [1] \"dino\"       \"away\"       \"h_lines\"    \"v_lines\"    \"x_shape\"   \n [6] \"star\"       \"high_lines\" \"dots\"       \"circle\"     \"bullseye\"  \n[11] \"slant_up\"   \"slant_down\" \"wide_lines\"\n\n\n\ndataset_name &lt;- \"dino\" # new addition\n\ndatasaurus_dozen %&gt;% \n  filter(dataset == dataset_name) %&gt;% # change object name \n  ggplot(aes(x=x, y=y)) +\n  geom_point()\n\n\n\n\nOnce we have converted our code to a generalized format, we can convert it into a more versatile custom function!\nRemember that curly brackets are used for inputting multiple lines of code. It is generally attached to the function that proceeds it.\n\ndino_plot &lt;- function(data_name) {\n  \n  datasaurus_dozen %&gt;% \n    filter(dataset == data_name) %&gt;% # change object name \n    ggplot(aes(x=x, y=y)) +\n    geom_point()\n}\n\ndino_plot(\"circle\")\n\n\n\ndino_plot(\"dino\")\n\n\n\ndino_plot(\"star\")"
  },
  {
    "objectID": "lec05-data-visualization.html#export-plots",
    "href": "lec05-data-visualization.html#export-plots",
    "title": "7  Data visualization in ggplot",
    "section": "7.7 Export plots",
    "text": "7.7 Export plots\nLet’s save our star-shaped plot.\n\ndino_star &lt;- datasaurus_dozen %&gt;% \n  filter(dataset == \"star\") %&gt;% \n  ggplot(aes(x=x, y=y)) +\n  geom_point()\n\nThere are multiple ways we can save a plot. Using a point-and-click method, you can display your plot in the Viewer and save straight from that panel. Run your ggplot object in the Console. Once it pops up in the Viewer, click “Export”, and save your plot! You can also readjust the width and height of your plot, so you can have a look at the size of it before saving it to any directory.\nAnother way to save a plot is by using R Graphics, which will save your plot to your working directory. Remember that you should be using relative paths!\n\ngetwd()\n\n[1] \"/Users/meteyuksel/eeb313website\"\n\nlibrary(here)\nhere()\n\n[1] \"/Users/meteyuksel/eeb313website\"\n\n\nUse the function png() to save your file. Make sure to run all three lines together, including dev.off(), which ensures that your graphics device is closed - otherwise, R will try to keep saving your figures!\n\npng(file = \"figures/dino_star1.png\", bg = \"transparent\")\ndino_star\ndev.off()\n\nquartz_off_screen \n                2 \n\n\nFinally, you can save your figures using ggsave(), from ggplot2.\n\nggsave(\"figures/dino_star2.png\", dino_star, width = 6, height = 8)\n\nParts of this lesson material were taken and modified from Data Carpentry under their CC-BY copyright license. See their lesson page for the original source."
  },
  {
    "objectID": "lec05-data-visualization.html#footnotes",
    "href": "lec05-data-visualization.html#footnotes",
    "title": "7  Data visualization in ggplot",
    "section": "",
    "text": "There are so many colors to chose from in R. Check out the R Color doc to find your gem.↩︎\nThe amount of control over various plot elements in ggplot is truly astonishing. Check out the complete list of themes here. Have fun!↩︎"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#lesson-preamble",
    "href": "lec06-exploratory-data-analysis.html#lesson-preamble",
    "title": "8  Exploratory data analysis",
    "section": "8.1 Lesson preamble:",
    "text": "8.1 Lesson preamble:\n\n8.1.1 Lesson objectives:\n\nImplications of (not) understanding your data\n\nHow did you collect your data?\nWhat are the properties of your data?\n\nExploring and asking questions about your data with graphing/visualization\nUsing insights from exploratory analysis to clean up data:\n\nDealing with unusual values/outliers\nDealing with missing values (NAs)\n\n\n8.1.2 Lesson outline:\nTotal lesson time: 2 hours\n\nData properties, initial predictions (15 min)\nPlotting and exploring data (45 min)\nDealing with unusual values (15 min)\nRe-connecting with our predictions (30 min)\nDealing with missing values (15 min)"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#introduction",
    "href": "lec06-exploratory-data-analysis.html#introduction",
    "title": "8  Exploratory data analysis",
    "section": "8.2 Introduction",
    "text": "8.2 Introduction\nExploratory data analysis is your exciting first look at your data! It’s a chance to develop a better understanding of the variables in your data set and the relationships between them. You can check your assumptions, find outliers, and possible errors. But THEN you’ll get to ask your questions! Yay!!\nYou need to understand your data you before you analyze it.\n\nWhat kind of data is it?\nWhat variation is present in my data?\nAre there any data points with values beyond the limits I anticipated?\nDo you notice any patterns?\n\nThe patterns you see can lead you to exciting new questions you may not have anticipated!"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#setup",
    "href": "lec06-exploratory-data-analysis.html#setup",
    "title": "8  Exploratory data analysis",
    "section": "8.3 Setup",
    "text": "8.3 Setup\nWe’ll use what you’ve learned in past lectures about summarizing and visualizing data with dplyr and ggplot to get to know some data!\n\nlibrary(tidyverse)\n\n\ndownload.file(\"https://uoftcoders.github.io/rcourse/data/pseudo.ara.busco\", \n              \"pseudo.ara.busco\")\ndownload.file(\"https://uoftcoders.github.io/rcourse/data/pseudo.LTRs\", \n              \"pseudo.LTRs\")\ndownload.file(\"https://uoftcoders.github.io/rcourse/data/pseudoMol_Kdist.txt\",\n              \"pseudoMol_Kdist.txt\")\n\nWe’re going to load the genomic data we have on the frequency of genes, the frequency of a type of repetitive element (LTRs stands for Long Terminal Repeat - there’s some more info on them coming up in the ‘predictions’ section), and the approximate evolutionary age of those repetitive elements.\n\ngeneDensity &lt;- read_tsv(\"pseudo.ara.busco\", \n                        col_names = c(\"chromosome\", \"start\", \"end\", \"winNum\", \n                                      \"numElements\", \"numBases\", \"winSize\", \n                                      \"density\"))\n\nRows: 48952 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): chromosome\ndbl (7): start, end, winNum, numElements, numBases, winSize, density\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nltrDensity &lt;- read_tsv(\"pseudo.LTRs\", \n                       col_names = c(\"chromosome\", \"start\", \"end\", \"winNum\", \n                                     \"numElements\", \"numBases\", \"winSize\", \n                                     \"density\"))\n\nRows: 48952 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): chromosome\ndbl (7): start, end, winNum, numElements, numBases, winSize, density\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nltrAge &lt;- read_tsv(\"pseudoMol_Kdist.txt\", col_names=TRUE)\n\nRows: 42339 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): chrom\ndbl (3): start, end, K2P\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe’re using “read_tsv” because the columns in this file are separated by tabs instead of commas or white space. The LTR age data (pseudoMol_Kdist.txt) already has column names, but the other two data sets need some more information"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#what-is-my-data-actually",
    "href": "lec06-exploratory-data-analysis.html#what-is-my-data-actually",
    "title": "8  Exploratory data analysis",
    "section": "8.4 What is my data, actually?",
    "text": "8.4 What is my data, actually?\nBefore we do anything else, we have to think about where this came from & whether the data is appropriate for the kinds of questions we might have.\nThis data describes a two of the genetic units (we’ll call them “elements”) that live in one plant genome: a set of highly conserved genes and one type of transposon (a “selfish” gene that makes copies of itself at the expense of its host genome). The chromosomes have been broken down into 1Mb pieces (“windows”) that overlap each other. In each window, we know the number and size (base pairs occupied) of the conserved genes and transposons.\n\n8.4.1 Predictions\nIt’s always good to lay out your hypotheses first. It can help you figure out how you need to assemble your data in order to test those predictions effectively.\n\nIn areas where gene density is high, LTR density is low\n\nLTRs are a type of transposable element, aka “genomic parasite”\n\nThey make copies of themselves at the expense of their host genome\nThey make up a large portion of plant genomes (can be &gt;40%!)\nThe host genome wants to prevent them from replicating\n\nCertain regions of a chromosome are more tightly wound up with histones\n\nThis makes them less accessible to molecular machinery\nIf polymerases aren’t likely to access a region, the region can’t be expressed\nIf a region is unexpressed, you don’t want genes there!!\nLTRs tend to accumulate in these regions\n\nMore accessible, active regions of a chromosome have higher gene content\n\nThese regions can be expressed effectively!\nLTRs that insert into these regions have a worse impact on the host\n\nOther factors like recombination rate and methylation also support this pattern\n\nIn areas where gene density is high, LTR age will be high (old, not transposing anymore)\n\nThere won’t be many new deleterious LTR insertions\n\nFew young LTRs\n\nThe LTRs that are present in those regions can’t have lethal effects\n\nIf they’re there, their effects are unlikely to have terrible effects on fitness\nSome transposable elements have been “domesticated” by their hosts\nThis all contributes to the likelihood that LTRs present can/have persisted\nLTRs present are more likely to be older\n\n\nThe sex chromosome (LG_X) will have higher LTR density\n\nLarger proportions of sex chromosomes are less accessible\nSex chromosomes experience lower rates of recombination relative to autosomes\n\nAlso correlated with higher transposon density and lower gene density\n\nThese trends are more true for non-recombining Y chromosomes than X chromosomes\n\nRecombination can occur between the two X chromosomes in females\n\n\n\n\n\n8.4.2 First Peek\nFirst, let’s just take a quick look at the gene density data set and ask ourselves what we’re dealing with. On a very basic level, what kind of variables do we have?\nWhat is one way to view a data frame?\n\n#head(geneDensity) # prints the first 6 rows\n#tail(geneDensity) #prints the last 6 rows\nglimpse(geneDensity) #prints number of rows and columns, column names, types, and several entries\n\nRows: 48,952\nColumns: 8\n$ chromosome  &lt;chr&gt; \"LG_N\", \"LG_N\", \"LG_N\", \"LG_N\", \"LG_N\", \"LG_N\", \"LG_N\", \"L…\n$ start       &lt;dbl&gt; 0, 20000, 40000, 60000, 80000, 100000, 120000, 140000, 160…\n$ end         &lt;dbl&gt; 1000000, 1020000, 1040000, 1060000, 1080000, 1100000, 1120…\n$ winNum      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ numElements &lt;dbl&gt; 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ numBases    &lt;dbl&gt; 2499, 2499, 10158, 11583, 11583, 11583, 11583, 11583, 1158…\n$ winSize     &lt;dbl&gt; 1e+06, 1e+06, 1e+06, 1e+06, 1e+06, 1e+06, 1e+06, 1e+06, 1e…\n$ density     &lt;dbl&gt; 0.002499, 0.002499, 0.010158, 0.011583, 0.011583, 0.011583…\n\n\nWhat are your first impressions of the data?\nWhich variables will be relevant for testing our predictions?\nSomething to note here: in this data set, there are a number of things that will be clearly"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#basic-variable-categories",
    "href": "lec06-exploratory-data-analysis.html#basic-variable-categories",
    "title": "8  Exploratory data analysis",
    "section": "8.5 Basic Variable Categories",
    "text": "8.5 Basic Variable Categories\nCommon variable types:\n\nIndependent vs dependent\nContinuous vs discrete\nQualitative: categorical/nominal, ranked/ordinal, dichotomous\nQuantitative: interval, ratio\n\nThis matters because the type of data tells us the appropriate way to visualize it:\n\nQualitative data: pie charts or bar charts\nQuantitative data: histograms, box plots"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#visualizing-your-data",
    "href": "lec06-exploratory-data-analysis.html#visualizing-your-data",
    "title": "8  Exploratory data analysis",
    "section": "8.6 Visualizing Your Data",
    "text": "8.6 Visualizing Your Data\n\n8.6.1 Describing Patterns in Histograms\nFor a given variable, you’re generally looking at the range of values and where the majority of the data lies. This gives you an idea of the distribution of your data. As you’re aware, many statistical tests make assumptions about the distribution of your input data - it’s very important to make note of the shapely properties of your data.\n\nAverage (mean, median, mode)\nRange (max, min)\nSkewness: how symmetrical is your data around the average?\n\nClassic bell curve has a skew of zero\nIf your data isn’t symmetrical, that can give you important info!\nSkewed distributions aren’t likely to be normally distributed\n\nKurtosis: how sharp is your central peak?\n\nIf your distribution is basically flat, its kurtosis is negative\nIf your distribution has a huge spike, its kurtosis will be positive\n\n\n\n\n8.6.2 Qualitative Data with Histograms\nHistograms are great for qualitative data because they visualize the number of times a given value appears in your data."
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#quantitative-data-with-histograms",
    "href": "lec06-exploratory-data-analysis.html#quantitative-data-with-histograms",
    "title": "8  Exploratory data analysis",
    "section": "8.7 Quantitative Data with Histograms",
    "text": "8.7 Quantitative Data with Histograms\nHistograms provide an important view into continuous data, providing that you tell ggplot how to group your data into discrete bins. Here, we can look at our data’s values for gene density. This density is a measurement of the number of base pairs in a 1Mb window that are part of a gene divided by the total number of base pairs in the window (1 000 000).\n\nggplot(geneDensity, aes(density)) +\n  geom_histogram(binwidth = 0.01) + \n  labs(title = \"Distribution of gene density values\",\n       x = \"Gene density\", y = \"Count (bin size = 0.01)\") # Adding labels helps!\n\n\n\n\nWhat are some words you’d use to describe this distribution?\nDoes this change\n\n8.7.1 Binning Quantitative Data\nWhen you’re subsetting continuous data into discrete bin widths, it’s important to try out different values because different bin sizes can give vastly different impressions of your data’s distribution.\n\nggplot(geneDensity, aes(density)) +\n  geom_histogram(binwidth = 0.001) +  # Teeny tiny bins\n  labs(title = \"Distribution of gene density values\",\n       x = \"Gene density\", y = \"Count (bin size = 0.001)\")\n\n\n\nggplot(geneDensity, aes(density)) +\n  geom_histogram(binwidth = 0.1) +  # Huge bins! (for this data)\n  labs(title = \"Distribution of gene density values\",\n       x = \"Gene density\", y = \"Count (bin size = 0.1)\")\n\n\n\n\nIt’s also interesting to see whether your data’s distribution is different for any of the categories you’re looking at. Is there greater variation in height among women vs humans as a whole? (Do be careful with this, because looking for patterns by poking your data into a bunch of different subsets will basically guarantee you’ll find a pattern, whether or not it’s biologically relevant.)\n\n\n8.7.2 Histogram for One Chromosome\nLet’s see whether the gene density on one of the autosomes (how about LG_2) fits the general pattern.\n(Based on our initial hypotheses, would you predict that it would?)\nIt is important to consider how your predictions may affect the way you filter your data, so be mindful about tweaking parameters (like bin width) to fit the output you expect!\n\ngeneDensity %&gt;%\n  filter(chromosome == \"LG_2\") %&gt;%\n  ggplot(aes(density)) +\n  geom_histogram(binwidth = 0.01) +\n  labs(title = \"Distribution of gene density values on LG_2\",\n       x = \"Gene density\", y = \"Count\")\n\n\n\n\nThe range for the x axis is much smaller! The maximum gene density here (~12%) is much smaller than the highest value in the full genome data set (~40/50%).\n(Why might this be?)\nOne of the aspects of your data that you can’t visualize well with a histogram is whether there are any values that exceed the limits you expected for your data."
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#scatterplots-box-plots",
    "href": "lec06-exploratory-data-analysis.html#scatterplots-box-plots",
    "title": "8  Exploratory data analysis",
    "section": "8.8 Scatterplots & Box plots",
    "text": "8.8 Scatterplots & Box plots\n\n8.8.1 More info! Less bias!\nWith quantitative data, we can get more information by looking at scatterplots and box plots. Not only are they immune to bin size bias, they can help us find outliers and let us make initial visual comparisons of averages across categories.\n\n\n8.8.2 Visualize raw data as a scatterplot\nWe know that “chromosome” is a categorical, independent variable appropriate for our X axis and that “density” is a continuous, dependent variable that will be appropriate for the Y.\n\nggplot(geneDensity, aes(x = chromosome, y = density)) +\n  geom_point() +\n  labs(title = \"Comparison of gene density across chromosomes\",\n       x = \"Chromosome\", y = \"Gene density\")\n\n\n\n\nAlready, we can see that there different maximum gene density values on each chromosome. Because the points are overlapping, it’s hard to evaluate what the average or skewness might be for any of the categories.\n\n\n8.8.3 Boxplots for better comparisons\nBecause boxplots display the median and quartile limits, it’s much easier to evaluate the properties of the distribution.\n\nggplot(geneDensity, aes(x = chromosome, y = density)) +\n  geom_boxplot() +\n  labs(title = \"Comparison of gene density across chromosomes\",\n       x = \"Chromosome\", y = \"Gene density\")\n\n\n\n\nThere’s definitely a value that jumps out immediately. It’s stretching the scale of the Y axis so that it’s hard to effectively compare the medians of each of the chromosomes.\nBefore we officially decide what to do with this outlier, we’ll visually set it aside for now by re-scaling our Y axis, which we’ve already learned how to do!\n\nggplot(geneDensity, aes(x = chromosome, y = density)) +\n  geom_boxplot() +\n  ylim(0, 0.125) + #other methods possible\n  labs(title = \"Comparison of gene density across chromosomes\",\n       x = \"Chromosome\", y = \"Gene density\")\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nLook at that handy warning! It lets us know that one value was thrown out: “removed 1 rows”. This view helps us to get a better general understanding of how this categorical “chromosome” value might relate to gene density. However! It’s important not to throw out data unless you have good reason to believe it doesn’t belong in the data set.\nBonus note: you can use the coord_cartesian function instead of ylim. It won’t warn you if any of your data points are beyond the limits of the axes, though.\n\n\n8.8.4 What about the other variables?\n\nThis data describes a few of the genetic “bits” (we generally call them “elements”) live in one plant genome. The chromosomes have been broken down into 1Mb pieces (“windows”) that overlap each other and the contents of each window have been averaged. We’ve got information on the density of conserved genes and one type of transposon for each window. Additionally, we have the evolutionary age for those transposons.\n\nAverage number of genes in bins along chromosomes.\nDefinitely more interesting to compare across the categories built into our data (here, chromosomes) to see how the gene density looks in each one separately. We can see whether the global pattern is present in each category. But how can we get all that info in one graph??\nFirst step is to ask ourselves what we currently have in our data. If our category for comparison is chromosome, what independent variables are shared among them that could facilitate comparison of the dependent gene density variable?\n\nhead(geneDensity)\n\n# A tibble: 6 × 8\n  chromosome  start     end winNum numElements numBases winSize density\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 LG_N            0 1000000      1           1     2499 1000000 0.00250\n2 LG_N        20000 1020000      2           1     2499 1000000 0.00250\n3 LG_N        40000 1040000      3           2    10158 1000000 0.0102 \n4 LG_N        60000 1060000      4           2    11583 1000000 0.0116 \n5 LG_N        80000 1080000      5           2    11583 1000000 0.0116 \n6 LG_N       100000 1100000      6           2    11583 1000000 0.0116 \n\n\nStart, end, and winNum would all be reasonable proxies for position along the chromosome.\n\ngeneDensity %&gt;%\n  filter(chromosome == \"LG_2\") %&gt;%\n  ggplot(aes(x = start, y = density)) +\n  geom_point() +\n  labs(title = \"Comparison of gene density along LG_2\",\n       x = \"Chromosomal position (bp)\", y = \"Gene density\")\n\n\n\n\nThis gives us an overview of how many of the conserved genes are found in which region of this LG_2 chromosome.\nTo be able to compare all the chromosomes at the same time, we can split our graph into “facets” so there’s one per chromosome, as you’ve learned how to in the last lecture.\n\nggplot(geneDensity, aes(x=start, y=density)) +\n  geom_point() +\n  labs(title=\"Comparison of gene density across chromosomes\",\n       x=\"Chromosomal position (bp)\", y=\"Gene density\") +\n  facet_wrap( vars(chromosome) )\n\n\n\n\nBecause not all of the chromosomes are the same length, the data appears more squished in some of the panels. We can adjust that by telling facet wrap to scale the X axis per-panel instead of globally.\nIf we want to be able to visually compare the densities across chromosomes, we should not allow the Y axis to scale freely. We can, however, set a limit for the Y axis values, as we’ve done before.\nUse different command for scaling the Y axis\n\nggplot(geneDensity, aes(x = start, y = density)) +\n  geom_point() +\n  coord_cartesian( ylim = c(0,0.13) ) +\n  labs(title = \"Comparison of gene density across chromosomes\",\n       x = \"Chromosomal position (bp)\", y = \"Gene density\") +\n  facet_wrap( vars(chromosome), scales = \"free_x\" )\n\n\n\n\nCool, eh?? The chromosomes have very different patterns! The range and distribution of values differs considerably!\nWhat are some reasons for gene density to change along a chromosome?\n\nCentromeres are mostly made up of repeats - very low gene content\n\nCentromeres can be in the middle or near one end of a chromosome\nWhere do you think the centromeres are in these chromosomes?\n\nCertain regions of a chromosome are more tightly wound up with histones\n\nMakes them less accessible to molecular machinery\nIf polymerases don’t reach a region, that region can’t be expressed\nIf a region is unexpressed, you don’t want genes there!\nCentromeres are generally one of these ‘inactive’ regions\n\nMore accessible, active regions of a chromosome have higher gene content\n\nThese regions are generally along chromosome arms\n\n\n\n\n8.8.5 Boxplot augmentations\nThere are a few additional things we can do that might make boxplots even more informative for our data.\n\nViolin plots - boxplots but with curves instead of boxes\nAdding a scatterplot behind the boxplot\nAdding “jitter” to scatterplots so the points are offset\n\nAdditionally, you can make the points more transparent (change the alpha value)\n\nYou can also add a trend line to help you visualize potential relationships\n\n\nggplot(geneDensity, aes(x = chromosome, y = density)) +\n  geom_point(alpha = 0.1, position = \"jitter\") +\n  geom_violin() + \n  labs(title = \"Comparison of gene density across chromosomes\",\n       x = \"Chromosome\", y = \"Gene density\")\n\n\n\n\nMaking the points more transparent gives us a better idea of what density values are most common. You can see this at the bottom of the graph, where the points don’t look transparent at all - so many data points!!"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#challenge",
    "href": "lec06-exploratory-data-analysis.html#challenge",
    "title": "8  Exploratory data analysis",
    "section": "8.9 Challenge!",
    "text": "8.9 Challenge!\nHow could you visualize the LTR data across chromosomes? Don’t forget to use axis labels.\nWhat is the range of LTR density for the LG_2 chromosome?\n\nggplot(ltrDensity, aes(x = chromosome, y = density)) +\n  geom_violin() + #boxplot also valid\n  geom_point(alpha = 0.01, position = \"jitter\") +\n  labs(title = \"Comparison of LTR density across Chromosomes\",\n       x = \"Chromosome\", y = \"LTR density\")\n\n\n\nltrDensity %&gt;%\n  group_by(chromosome) %&gt;%\n  summarize(mean = mean(density), median = median(density), \n            n = n(), max = max(density))\n\n# A tibble: 5 × 5\n  chromosome  mean median     n   max\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 LG_2       0.324  0.331 11234 0.488\n2 LG_4       0.303  0.320  6520 0.973\n3 LG_7       0.331  0.339 15938 0.503\n4 LG_N       0.318  0.332  8551 0.476\n5 LG_X       0.307  0.315  6709 0.477\n\n\nNow it’s time to start thinking about what to do with rebellious outliers!"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#outliers",
    "href": "lec06-exploratory-data-analysis.html#outliers",
    "title": "8  Exploratory data analysis",
    "section": "8.10 Outliers",
    "text": "8.10 Outliers\n\n8.10.1 But why are you like this?\nThere could be many reasons why your data has values that exceed the limits you expected it would have. It basically comes down to error, whether in your data or in the expectation you had for its limits. Consider error carefully.\n\nIncorrect prediction of what the limits should be\n\nMaybe your study system has different qualities than literature spp.\n\nSystematic error is predictable and affects a measurement’s accuracy\n\nIncorrectly calibrated lab equipment (pH meter, thermometer, etc.)\nGenomics - your gene annotation can be biased by repetitive elements\nCan be very difficult to compensate for this kind of error\n\nRandom error affects measurement precision (think significant figures)\n\nWriting down measurements incorrectly in your notes\nA lab scale can only weigh samples to a given decimal point\nSimple human fallibility when it comes to assessing measurements\nTake multiple measurements and average the results to compensate\n\nCommon sources\n\nThis (exploratory data analysis) is a great time to look for issues!\nError in previous analysis steps (code that produced LTR age estimates)\nErroneous assumptions about your sample sites or methods\n\nDon’t just throw out a point because you can ascribe it to error\n\nIMPORTANT NOTE: if you do end up removing any data, you MUST disclose the removal and your justification for the removal. Your reasons could be great, but you need to make sure your audience has access to those reasons.\nThis consideration of weird values brings up an interesting point: you’re doing these checks because your values are different than what you expected. It’s important to think about analytical ‘controls’ to look for potential errors even when your data looks the way you expect it to! Steps towards this can be as simple as sharing your code publicly.\n\n\n8.10.2 Let’s take a look!\nWe had that weird really high gene density value on the LG_X chromosome. Let’s look at what’s happening there.\n\nfilter(geneDensity, density &gt; 0.13)\n\n# A tibble: 1 × 8\n  chromosome     start       end winNum numElements numBases winSize density\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 LG_X       134160000 134165151   6709           1     2655    5151   0.515\n\n\nWhat do the other variables tell us about this data point?\nThis data point has a really high winNum, so it’s likely located near the end of the chromosome. But importantly, our windows are supposed to be 1Mb in size (1 000 000 value in the winSize column). The winSize value for this outlier is tiny in comparison!!\n\n\n8.10.3 Wholesome thoughts about your data\nAverages – how was your data collected & what biases might be inherent? The data I’m showing you is a pretty clear example of how important (and difficult) it is to understand what the variables mean in your data sets. What might cause outliers in the kind of data you’re interested in?\nThat last look showed us that it’s definitely very important to consider our data as a whole: to think not only about the variables relevant to our hypotheses, but the way in which it was collected (how that was reflected in the other properties of the data).\nSo. Let’s try to understand more about the interaction between gene density and window size in the rest of our data. Visualization time!\n\nggplot(geneDensity, aes(x = start,y = winSize, colour = chromosome)) +\n  geom_point() +\n  labs(title = \"Window sizes along the chromosome\",\n       x = \"Chromosomal position (bp)\", y = \"Window size (bp)\")\n\n\n\n\nIt looks like all of the chromosomes get this trail-off in window size near their ends. This is not what we expected!! All of the squish at the end is basically just error from a previous analysis.\n\n\n8.10.4 Challenge!\nCreate a category based on window size: density as either belonging in a “small window” or a “normal window”. We can create a new “winCategory” variable using mutate() and assign the value of “small” to windows with winSize less than 1Mb and “normal” to all the other windows (which we expect will have a value of 1Mb).\n\ngeneDensity2 &lt;- geneDensity %&gt;%\n  mutate( winCategory = case_when(winSize&lt;1000000 ~ \"small\",\n                                  TRUE ~ \"normal\") ) %&gt;%\n  group_by(winCategory, chromosome)\n\nsummarize(geneDensity2,\n          mean = mean(density), median = median(density), n = n(), \n          max = max(density), sd = sd(density), .groups = \"keep\")\n\n# A tibble: 10 × 7\n# Groups:   winCategory, chromosome [10]\n   winCategory chromosome    mean  median     n    max     sd\n   &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 normal      LG_2       0.0104  0.00481 11184 0.119  0.0168\n 2 normal      LG_4       0.0123  0.00730  6470 0.0900 0.0146\n 3 normal      LG_7       0.00909 0.00284 15888 0.123  0.0151\n 4 normal      LG_N       0.0111  0.00464  8501 0.0690 0.0143\n 5 normal      LG_X       0.0119  0.00449  6659 0.0960 0.0175\n 6 small       LG_2       0.0443  0.0472     50 0.0964 0.0299\n 7 small       LG_4       0       0          50 0      0     \n 8 small       LG_7       0.0443  0.0370     50 0.122  0.0233\n 9 small       LG_N       0       0          50 0      0     \n10 small       LG_X       0.0358  0.0226     50 0.515  0.0705\n\n\nWhat can we take away from this table?\nThe n values are considerably larger for the normal-sized windows group. LG_4 and LG_N had 0 gene density in their small windows but have some of the highest median gene densities in the normal-sized windows.\nThe standard deviation of the small windows is much higher. Is that what we would expect for that data category? Perhaps most importantly for our purposes, the mean and median are quite different. These smaller windows have considerably different values.\nWhat does this look like in an actual plot? This is going to take a bit of black magic in the form of two separate calls to geom_boxplot(). The first will use all the windows (setting it to colour values by ‘all’) and the second will actually create (and colour) different box plots based on their winCategory value.\n\nggplot(geneDensity2, aes(x = chromosome, y = density, colour = winCategory)) +\n  geom_boxplot( aes(x = chromosome, y = density, colour = \"all\") ) + \n  geom_boxplot() +\n  ylim(0, 0.125) +\n  labs(title=\"Visualizing gene density across window size and chromosome\",\n       x=\"Chromosome\", y=\"Gene density\", colour=\"Window\\nCategory\")\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\nRemoved 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nThe small window values seem quite different than the global gene density results!\nWhy do you think this might be? Looking back on the summaries, we can see that there aren’t many data points in the ‘small’ window category.\nIn conclusion!!\nThese small windows do seem to contain interesting information, but they are over-weighted given the amount of information they’re based on. Based on the analysis conducted to create the windows, it might be appropriate to discard the small windows on the ends of the chromosomes. Each windowed data point is weighted equally, even though these smaller windows contain less information, which creates a bias.\nWhat do you think is the most appropriate way to deal with this data?\nIs there a way to weight the gene density by window size?"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#so-what-does-this-mean-for-our-predictions",
    "href": "lec06-exploratory-data-analysis.html#so-what-does-this-mean-for-our-predictions",
    "title": "8  Exploratory data analysis",
    "section": "8.11 So what does this mean for our predictions?",
    "text": "8.11 So what does this mean for our predictions?\nRight. The reason we collected this data in the first place!\n\nIn areas where gene density is high, LTR density is low\nIn areas where gene density is high, LTR age will be high\nThe sex chromosome (LG_X) will have higher LTR density\n\nNote: preparing data for analysis is generally the most time-consuming part of any project. Establishing what you need to do with your data in order to test your hypotheses & thoroughly exploring your data and its properties are extremely useful steps in this process.\n\n8.11.1 How do we explore these questions?\nWe need to relate gene density, LTR density, and LTR age. Do we have this data?\nIs it currently in a form where we can make comparisons?\nBased on the properties of our gene and LTR density data sets, what are the shared “units”? Essentially, what are we trying to compare within and among each chromosome?\n\nhead(geneDensity)\n\n# A tibble: 6 × 8\n  chromosome  start     end winNum numElements numBases winSize density\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 LG_N            0 1000000      1           1     2499 1000000 0.00250\n2 LG_N        20000 1020000      2           1     2499 1000000 0.00250\n3 LG_N        40000 1040000      3           2    10158 1000000 0.0102 \n4 LG_N        60000 1060000      4           2    11583 1000000 0.0116 \n5 LG_N        80000 1080000      5           2    11583 1000000 0.0116 \n6 LG_N       100000 1100000      6           2    11583 1000000 0.0116 \n\nas_tibble(ltrDensity)\n\n# A tibble: 48,952 × 8\n   chromosome  start     end winNum numElements numBases winSize density\n   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 LG_N            0 1000000      1         396   258184 1000000   0.258\n 2 LG_N        20000 1020000      2         400   274748 1000000   0.275\n 3 LG_N        40000 1040000      3         390   271692 1000000   0.272\n 4 LG_N        60000 1060000      4         390   271540 1000000   0.272\n 5 LG_N        80000 1080000      5         386   268414 1000000   0.268\n 6 LG_N       100000 1100000      6         380   268519 1000000   0.269\n 7 LG_N       120000 1120000      7         396   285314 1000000   0.285\n 8 LG_N       140000 1140000      8         396   284053 1000000   0.284\n 9 LG_N       160000 1160000      9         396   270360 1000000   0.270\n10 LG_N       180000 1180000     10         388   267071 1000000   0.267\n# ℹ 48,942 more rows\n\n\nThe basic “unit” in this data is the 1Mb window. Because this is shared across the two data sets, we can use it to join them together. Excellent!\nWhat about the LTR age data set?\n\nas_tibble(ltrAge)\n\n# A tibble: 42,339 × 4\n   chrom    start      end     K2P\n   &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 LG_2  61204840 61212964 0.0202 \n 2 LG_2  61219360 61221366 0.0538 \n 3 LG_2  61236795 61241286 0.00514\n 4 LG_2  61241905 61249226 0.00100\n 5 LG_2  61268475 61269268 0.0808 \n 6 LG_2  61337049 61348048 0.00472\n 7 LG_2  61365365 61372328 0.0260 \n 8 LG_2  61382447 61389344 0.0617 \n 9 LG_2  61396331 61407449 0.00800\n10 LG_2  61413022 61414052 0.0984 \n# ℹ 42,329 more rows\n\n\nThis data was prepared differently, so it doesn’t have the same ‘window’ units. It does contain chromosomal position information, however, which we can use to make some preliminary comparisons.\n\n8.11.1.1 Actual wrangling\nWe’re also going to pull out only the variables we now know we’ll need (what’s shared among the data sets and what will be used to try and test our predictions), just because of how large this data frame will be. It’s not a good idea to do this before looking at all the variables together.\n\nsimpleGeneDensity &lt;- geneDensity %&gt;%\n  mutate(elementType = \"gene\") %&gt;%\n  select(chromosome, start, elementType, density)\n\nsimpleLTRdensity &lt;- ltrDensity %&gt;%\n  mutate(elementType = \"LTR\") %&gt;%\n  select(chromosome, start, elementType, density)\n\nhead(simpleLTRdensity)\n\n# A tibble: 6 × 4\n  chromosome  start elementType density\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 LG_N            0 LTR           0.258\n2 LG_N        20000 LTR           0.275\n3 LG_N        40000 LTR           0.272\n4 LG_N        60000 LTR           0.272\n5 LG_N        80000 LTR           0.268\n6 LG_N       100000 LTR           0.269\n\n\nAt this point, are these data “long” or “wide”? #throwback\n\n\n8.11.1.2 ? Knowledge Check Challenge\nJoin the two data sets (simpleLTRdensity and simpleGeneDensity) into one data frame called “densities”. As a bonus, try mutating the start variable so that it’s measured in 10kb increments instead of 1bp. This will just make our X axis labels are easier to interpret.\n\ndensities &lt;- full_join(simpleLTRdensity, simpleGeneDensity,\n                       by = c(\"chromosome\", \"start\", \"elementType\", \"density\")) %&gt;%\n  mutate(start = start / 10000) %&gt;%\n  group_by(chromosome, elementType)\n  \nhead(densities)\n\n# A tibble: 6 × 4\n# Groups:   chromosome, elementType [1]\n  chromosome start elementType density\n  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 LG_N           0 LTR           0.258\n2 LG_N           2 LTR           0.275\n3 LG_N           4 LTR           0.272\n4 LG_N           6 LTR           0.272\n5 LG_N           8 LTR           0.268\n6 LG_N          10 LTR           0.269\n\nrm(simpleGeneDensity, simpleLTRdensity)\n\nWe’ve got two independent categorical variables, an independent numerical variable, and a dependent numerical variable. It’s beautiful."
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#is-gene-density-high-when-ltr-density-is-low-hyp-1",
    "href": "lec06-exploratory-data-analysis.html#is-gene-density-high-when-ltr-density-is-low-hyp-1",
    "title": "8  Exploratory data analysis",
    "section": "8.12 Is gene density high when LTR density is low? (hyp #1)",
    "text": "8.12 Is gene density high when LTR density is low? (hyp #1)\nWhat variables do we want to plot?\n\nChromosome\nStart position (bp)\nElement type\nElement density\n\n\n\n8.12.0.1 Challenge\nOf the plot types we’ve used so far, what would you use to try and compare gene densities along the chromosomal positions on each chromosome?\n\nggplot(densities, aes(x = density, fill = elementType)) +\n  geom_histogram( binwidth = 0.03 ) +\n  facet_wrap( vars(chromosome), scales = \"free_y\" ) +\n  coord_cartesian(xlim = c(0,0.6)) +\n  labs(x = \"Element Density\", y = \"Count\", fill = \"Element\\nType\",\n       title = \"Element densities among chromosomes\")\n\n\n\n\nPoking at the histogram shows us some interesting things about differences in the frequencies of LTRs and genes. Gene values have extremely high kurtosis near 0. LG_4 may have the highest median/mode LTR density.\nThe Y axis can be free-scaled here because all of the counts are based on the size of their chromosome. We don’t want one chromosome to seem like it has a much higher LTR count just because it has more windows (greater n) than the other chromosomes.\nThis was an interesting plot, but it compares densities across chromosomes more than it looks at differences in LTR/gene patterns within chromosomes.\n\nggplot(densities, aes(x = start,y = density,colour = elementType)) +\n  geom_point(alpha = 0.3) +\n  facet_wrap( vars(chromosome), scales = \"free_x\" ) +\n  ylim(0, 0.5) +\n  labs(title = \"Element densities along chromosomes\",\n       x = \"Chromosomal position (10kb)\", y = \"Element density\",\n       colour = \"Element\\nType\")\n\nWarning: Removed 5 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThis looks like the kind of information we want! If we squint, we can almost see that increases in gene density seem to correlate with decreases in LTR density.\nIf you can remember how to add a smooth line to show broad patterns, this will be the easiest view.\n\nggplot(densities, aes(x = start,y = density,colour = elementType)) +\n  geom_smooth() +\n  ylim(0, 0.4) +\n  facet_wrap( vars(chromosome), scales = \"free_x\" ) +\n  labs(title = \"Element densities along chromosomes\",\n       x = \"Chromosomal position (10kb)\", y = \"Element density\",\n       colour = \"Element\\nType\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 3700 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 6 rows containing missing values (`geom_smooth()`).\n\n\n\n\n\nBroadly, we can see that when the LTR density plummets, gene density smudges upward.\n\n\n8.12.1 Is gene density high when LTR age is high? (hyp #2)\nLet’s take a look at the LTR age data.\n\nhead(ltrAge)\n\n# A tibble: 6 × 4\n  chrom    start      end     K2P\n  &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 LG_2  61204840 61212964 0.0202 \n2 LG_2  61219360 61221366 0.0538 \n3 LG_2  61236795 61241286 0.00514\n4 LG_2  61241905 61249226 0.00100\n5 LG_2  61268475 61269268 0.0808 \n6 LG_2  61337049 61348048 0.00472\n\n\nSome of the variable names are different (K2P is our measure of age) but are really familiar to the other data we’ve been analyzing (chrom instead of chromosome). Let’s see if the LTR age data looks anything like our gene and LTR density data.\n\nggplot(ltrAge, aes(x = start,y = K2P)) +\n  geom_point(alpha = 0.1) +\n  facet_wrap( vars(chrom), scales = \"free_x\" ) +\n  labs(title = \"LTR age along chromosomes\",\n       x = \"Chromosomal position\", y = \"LTR age (K2P)\")\n\n\n\n\nThese clouds of points are really hard to understand. We can try using geom_smooth to see if it can reveal what’s going on in these clouds. The one argument we’ll make note of right now is n, which tells geom_smooth how many points along the x it should be using to make its average. Because of how big our data is, we’ll give it a smaller value so it doesn’t take forever to plot.\n\nggplot(ltrAge, aes(x = start,y = K2P)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(n = 50) + # try different values (think histogram bin widths)\n  facet_wrap( vars(chrom), scales = \"free_x\" ) +\n  labs(title = \"LTR age along chromosomes\",\n       x = \"Chromosomal position\", y = \"LTR age (K2P)\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nWell, what can we take away from this visualization? Not much. It seems pretty clear that the average LTR is old (darkness at the bottom of the clouds). The few younger LTRs near the top of the plots might have useful information for us, given that LTR “reproduction” (transposition) is generally rare."
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#discretizing-our-ages",
    "href": "lec06-exploratory-data-analysis.html#discretizing-our-ages",
    "title": "8  Exploratory data analysis",
    "section": "8.13 Discretizing our ages",
    "text": "8.13 Discretizing our ages\nWe can try binning LTRs based on their age, to see if the youngest LTRs are able to be “born” in gene-dense regions. Note: don’t categorize numerical data like this without considering what information you’re losing!\n\nggplot(ltrAge, aes( x = start / 10000,\n                    y = K2P,\n                    colour = cut(K2P, 4,\n                               labels = c(\"youngest\", \"young\",\n                                          \"old\", \"oldest\")) \n                    )) +\n  geom_point(alpha = 0.5) +\n  #geom_smooth() + #just aren't enough points in the younger categories\n  facet_wrap( vars(chrom), scales = \"free_x\" ) +\n  labs(title = \"LTR age along chromosomes\",\n       x = \"Chromosomal position (10Mb)\", y = \"LTR age (K2P)\",\n       colour = \"Age Class\")\n\n\n\n\nThere are few young LTRs, but their positions match with the pattern of our overall TE density plot. The positions of the young LTRs aren’t close to gene-dense regions on the autosomes, though interestingly there are a few on the right-hand side of LG_X, which was the most gene-dense region of our sex chormosome!\nThat was cool. Definitely not a statistical test of correlations between LTR age and gene density, but the fact that sub-sections of this data behave quite differently is really interesting to see!\n\n8.13.1 Missing values in gene density comparison\nYou might remember that in last lecture, when you encountered missing values in your data, you could replace them with 0. If we want to put our LTR age data into the same kind of windows that the gene density data is in, we’re going to have some windows without any age data. Do you think that setting LTR age to 0 would be a good way to handle the windows with missing age data?\n\nwindowedAges &lt;- ltrAge %&gt;%\n  mutate(chromosome = chrom,            # get the \"chromosome\" col name to match\n         age = K2P,                     # might as well give a better name\n         winNum = floor(start/20000)) %&gt;% # bin the start (20Mb) & round \n  select(chromosome, winNum, age)\n  \ngenesPlusAges &lt;- geneDensity %&gt;%\n  select(\"chromosome\", \"winNum\", \"density\") %&gt;%\n  full_join(windowedAges, by = c(\"chromosome\", \"winNum\") )\n\nas_tibble(genesPlusAges)\n\n# A tibble: 57,617 × 4\n   chromosome winNum density      age\n   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 LG_N            1 0.00250  0.0122 \n 2 LG_N            2 0.00250  0.0508 \n 3 LG_N            3 0.0102   0.0181 \n 4 LG_N            4 0.0116   0.111  \n 5 LG_N            5 0.0116  NA      \n 6 LG_N            6 0.0116   0.0343 \n 7 LG_N            6 0.0116   0.00505\n 8 LG_N            7 0.0116   0.0277 \n 9 LG_N            8 0.0116   0.0318 \n10 LG_N            9 0.0116   0.0629 \n# ℹ 57,607 more rows\n\n\nJust for the record, this is not a great way to window data.\nBut! Look what happens when you change the join method! Got more missing data… Maybe it’ll be easier to understand if we plot it.\n\nfindMissing &lt;- genesPlusAges %&gt;%\n  mutate(noDense = is.na(density),\n         noAges = is.na(age))\n\nggplot(findMissing) +\n  geom_point( aes(x = winNum, y = density, colour = noAges) ) +\n  coord_cartesian( ylim = c(0,0.15) ) +\n  facet_wrap( vars(chromosome), scales = \"free_x\" ) +\n  labs(title = \"Missing gene density data\",\n       x = \"Window number\", y = \"Gene density\", colour = \"Missing ages!\")\n\nWarning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n\n\nggplot(findMissing) +\n  geom_point( aes(x = winNum, y = age, colour = noDense) ) +\n  facet_wrap( vars(chromosome), scales = \"free_x\" ) +\n  labs(title = \"Missing LTR age data\",\n       x = \"Window number\", y = \"LTR age\", colour = \"Missing densitiy!\")\n\nWarning: Removed 15278 rows containing missing values (`geom_point()`).\n\n\n\n\n\nOoph. We’re only missing gene density data for 3 windows, but we’re missing out on LTR age data for around 15k windows… Is there anything we can do about it?\nAssigning a value of 0 to our LTR ages is asserting we have different data than we actually have: that we know there’s an LTR in that position and that it’s an incredibly recent insertion. But what if we replace the missing age points with the mean LTR age?\n\nfindMissing &lt;- genesPlusAges %&gt;%\n  mutate(noDense = is.na(density),\n         noAges = is.na(age),\n         rplAge = replace_na( age,mean(age,na.rm=TRUE) ),\n         rplDensity = replace_na( density,mean(density,na.rm=TRUE) ))\n\nsum( is.na(findMissing$rplDensity) )\n\n[1] 0\n\nsum( is.na(findMissing$rplAge) )\n\n[1] 0\n\nggplot(findMissing) +\n  geom_point(alpha = 0.5,\n             aes(x = winNum, y = rplAge,\n                 colour = cut(rplAge, 3, labels = c(\"youngest\", \"middle\", \"oldest\")))) +\n  geom_col(aes(x = winNum, y = rplDensity)) +\n  ylim(c(0,0.2)) +\n  facet_wrap(vars(chromosome), scales = \"free_x\" ) +\n  labs(title = \"Gene density and LTR age along chromosomes\",\n       x = \"Chromosomal position (window number)\", y = \"Altered ages\",\n       colour = \"Age Class\")\n\nWarning: Removed 1 rows containing missing values (`position_stack()`).\n\n\nWarning: Removed 21 rows containing missing values (`geom_col()`).\n\n\n\n\n\nNo more warnings about missing data! Woo! We can feel whole again! On the other hand, replacing these values didn’t actually help us improve our understanding of our age-density prediction…\nWe should actually plot a window’s LTR age against its gene density! First, let’s look at the general pattern with the adjusted ages (when we replaced NA with the mean value for the variable).\n\nggplot(findMissing) +\n  geom_point(alpha = 0.5,\n             aes(x = rplDensity, y = rplAge,\n                 colour = cut(rplAge, 3, labels = c(\"youngest\", \"middle\", \"oldest\")))) +\n  coord_cartesian( xlim = c(0, 0.13) ) +    # being mindful of our outlier\n  facet_wrap( vars(chromosome), scales=\"free_x\" ) +\n  labs(title=\"Gene density vs LTR age post-adjustment\",\n       x=\"Gene density (adjusted)\", y=\"LTR age (adjusted)\",\n       colour=\"Age Class\")\n\n\n\n\nThat’s pretty dang cool! We do see a general downward trend - as gene density increases (going right along the X axis), LTR age tends to decrease! We can even see some interesting differences among the chromosomes.\nLG_7 is our biggest chromosome and it seems to have a lot of the young (blue) LTRs. Why do you think that might be? Generally, as chromosome length increases, recombination rate decreases. Recombination rate is one of the properties we know correlates with the accessibility of genomic regions. So, we might predict that longer chromosomes have a larger proportion of ‘inactive’ regions and a greater number of TEs.\nMoving on!\nLet’s judge our past selves! Let’s see the effects of our decision to replace the missing age data with an average. Was it foolish or as wise as the time we joined a dragon boat team?\n\nggplot(findMissing) +\n  # This time, we'll colour the points based on whether the age should be NA\n  geom_point(alpha=0.5, aes(x=rplDensity,y=rplAge, colour=noAges)) +\n  coord_cartesian( xlim=c(0,0.13) ) +    # being mindful of our outlier\n  facet_wrap( vars(chromosome), scales=\"free_x\" ) +\n  labs(title=\"Gene density vs LTR age post-adjustment\",\n       x=\"Gene density (adjusted)\", y=\"LTR age (adjusted)\",\n       colour=\"Agelessss\")\n\n\n\n\nNot too bad! Not mechanistically motivated, but it does show that replacing missing data with the value we would expect (basically the definition of the mean) can have fewer consequences than replacing it with 0 in some contexts."
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#does-the-sex-chromosome-lg_x-have-higher-ltr-density-hyp-3",
    "href": "lec06-exploratory-data-analysis.html#does-the-sex-chromosome-lg_x-have-higher-ltr-density-hyp-3",
    "title": "8  Exploratory data analysis",
    "section": "8.14 Does the sex chromosome (LG_X) have higher LTR density? (hyp #3)",
    "text": "8.14 Does the sex chromosome (LG_X) have higher LTR density? (hyp #3)\nWe want to compare LTR densities across chromosomes. How would you do this?\n\n8.14.0.1 Challenge!\nBox plot is a good base line. Ask for types of improvements that could be made to a basic box plot.\n\nggplot(ltrDensity, aes(x = chromosome, y = density)) +\n  geom_boxplot() +\n  labs(title = \"LTR densities among chromosomes\",\n       x = \"Chromosome\", y = \"LTR density\")\n\n\n\n\nWhat unusual things do you notice about this plot?\nDoes this initial exploration lead us to think that the sex chromosome, with its reduced rate of recombination, has accumulated more LTRs?\nBased on what we know about the gene density data, what would you suggest we might need to do with this LTR density data?\nThere is one extreme outlier again - let’s see what happens if we colour the data based on window size.\n\nltrDensity %&gt;%\n  ggplot( aes(x = chromosome, y = density, colour = winSize==1000000) ) +\n  geom_boxplot() +\n  ylim(c(0,0.6)) +\n  labs(title = \"LTR densities among chromosomes\",\n       x = \"Chromosome\", y = \"LTR density\", colour = \"Normal\\nWindow\\nSize\")\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nThere are fewer outliers, but the pattern remains the same. GOOD SIGN! :)"
  },
  {
    "objectID": "lec07-intro-to-stats.html#lesson-preamble",
    "href": "lec07-intro-to-stats.html#lesson-preamble",
    "title": "9  Introduction to inference",
    "section": "9.1 Lesson preamble",
    "text": "9.1 Lesson preamble\n\n9.1.1 Lesson objectives\n\nDevelop understanding of random variables, probability distributions, and likelihood.\nUnderstand how to simulate realizations of of a random variable.\nDevelop familiarity with maximum likelihood estimation and inference.\nBecome familiar with hypothesis testing: null and alternate hypotheses, test statistics, power, significance, p-values, and the duality of hypothesis tests and confidence intervals.\n\n9.1.2 Lesson outline\n\nRandom variables, probability distributions, and likelihood\n\nInterpretations of probability, sources of uncertainty\nDiscrete vs continuous RVs\nSimulating random variables in R\nMeans, variances, and other moments\n\nlikelihood estimation and inference\n\nUnderstanding the likelihood function\nMathematical and numerical optimization\n\nHypothesis testing: theory and examples\n\n\n\nlibrary(tidyverse)\ntheme_set(theme_bw())\n### see other themes here: https://ggplot2.tidyverse.org/reference/ggtheme.html"
  },
  {
    "objectID": "lec07-intro-to-stats.html#whats-chance-got-to-do-got-to-do-with-it",
    "href": "lec07-intro-to-stats.html#whats-chance-got-to-do-got-to-do-with-it",
    "title": "9  Introduction to inference",
    "section": "9.2 What’s chance got to do … got to do … with it?",
    "text": "9.2 What’s chance got to do … got to do … with it?\nStatistics is the study of decision making under uncertainty. Randomness that is inherit in many physical processes (especially those in ecology and evolution) makes it difficult to, given data, choose between alternative explanations for how the world works. But probability theory provides us with tools to combat this uncertainty and to come to principled, informed conclusions.\nIn this lecture, we will introduce key concepts in probability and statistics. These concepts (e.g., likelihood) form the backbone for future lectures and will be invaluable as you complete your projects."
  },
  {
    "objectID": "lec07-intro-to-stats.html#random-variables-and-probability-distributions",
    "href": "lec07-intro-to-stats.html#random-variables-and-probability-distributions",
    "title": "9  Introduction to inference",
    "section": "9.3 Random variables and probability distributions",
    "text": "9.3 Random variables and probability distributions\nConsider an experiment which gives rise to a set \\(\\Omega\\) of outcomes. This set of outcomes is called the sample space. The space of all events is formed by considering all possible combinations of outcomes (or some combinations but not all for technical reasons). Suppose the sample space is \\(\\Omega = \\{1,2,3,4\\}\\), so that the experiment might be rolling a four-sided die. One event is observing 2 or 3 upon rolling the die: this event is denoted \\(\\{2,3\\}\\). (This is only one of many such events!). Probability allows us to assign to each event (\\(\\{2,3\\}, \\{\\}, \\{1,2,3,4\\}\\), etc.) a value between zero and one capturing how likely that event is to occur under the experiment we have preformed. We also need certain conditions on this function to be met:\n\nThe probability of any event must be \\(\\geqslant 0\\). Negative probabilities make no sense!\nThe probability of the event \\(\\Omega\\) must be \\(=1\\). Something must have happened!\nCountably many mutually exclusive events \\(A_1,A_2,\\dots,\\) must satisfy the following:\n\n\\[\\Pr (A_1 \\text{ or } A_2 \\text{ or } A_3 \\text{ or } \\cdots) = \\sum_{i=1}^\\infty \\Pr (A_i)\\]\n\\(A_1,\\dots,A_n,\\dots\\) are mutually exclusive if \\(A_i, A_j\\) do not share outcomes for all \\(i \\neq j\\). (The events \\(A_1 = \\{1,2\\}\\) and \\(A_2 = \\{3,4\\}\\) are distinct, while the events \\(A_1 = \\{1,3\\}\\) and \\(A_2 = \\{3,4\\}\\) are not..)\nIn the case of rolling a four-sided die, the probability of all events can be calculated (due to the extra conditions) by specifying what the probability of each outcome. Suppose \\(\\Pr(\\{1\\}) = 0.2, \\Pr(\\{2\\}) = 0.3, \\Pr(\\{3\\}) = \\Pr(\\{4\\}) = 0.25\\), i.e., the die is not fair but not horribly biased to one side.\nA random variable is the (uncertain) outcome of a random experiment; more precisely, a RV is a function from the sample space to the real numbers. In the above example, the experiment is the roll of a four-sided die and the outcomes are 1,2,3, and 4. The random variable, which we call \\(X\\), has an associated probability distribution: \\(\\Pr(X = 1) = 0.2, \\Pr(X = 2) = 0.3, \\Pr(X = 3) = \\Pr(X = 4) = 0.25\\). Capital letters are often used to denote random variables and lower case letters to denote the values they can assume (i.e., elements of their range). We can use this probability distribution to simulate realizations (draws) of \\(X\\), which correspond to repeatedly and independently preforming the experiment:\n\nN &lt;- 100 # = number of experiments (realizations)\nrealizations_N_die_rolls &lt;- sample(c(1,2,3,4), \n                                   size = N, \n                                   prob = c(0.2,0.3,0.25,0.25), \n                                   replace = T)\nrealizations_N_die_rolls\n\n  [1] 4 2 2 1 3 1 1 2 1 2 3 2 1 4 4 2 4 2 1 3 4 2 4 1 3 4 4 1 1 2 1 4 1 1 3 3 3\n [38] 1 4 1 3 4 4 2 2 2 4 4 2 3 4 2 2 3 2 1 2 1 1 2 4 2 1 4 3 4 2 2 4 3 2 3 1 3\n [75] 3 3 3 1 3 3 2 1 1 1 3 4 3 2 3 1 1 4 2 4 1 2 1 4 1 2\n\nrealizations_N_die_rolls &lt;- data.frame(value = realizations_N_die_rolls)\nggplot(realizations_N_die_rolls, \n       aes(x = value, y = ..count../N)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis example gives rise to a probability distribution on the set \\(\\{1,2,3,4\\}\\) and, in fact, all of its subsets. Discrete probability distributions like this show up quite often, but so do continuous ones.\nFor example, a uniform distribution assigns equal weight to all real numbers between, say, \\(0\\) and \\(1\\). If \\(X \\sim \\text{Uniform}(0,1)\\), i.e., \\(X\\) is a random variable with a uniform distribution, then \\(\\Pr(a_1 &lt; X &lt; b_1) = (b_1 - a_1).\\). We can simulate realizations of a uniform random variable using\n\nN &lt;- 10000\nrealizations_N_uniform &lt;- runif(n = N, min = 0, max = 1)\n\nrealizations_N_uniform &lt;- data.frame(value = realizations_N_uniform)\nggplot(realizations_N_uniform, aes(x = value, y = ..count../N)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nIn general, the distribution of a discrete random variable \\(X\\) (which models an experiment with a countable number of outcomes \\(1,2,3.\\dots\\)) is completely determined by the probabilities \\(\\Pr(X = k)\\). These probabilities form the probability mass function for \\(X\\), i.e., a function which returns how much “mass” is assigned to each outcome. The probabilities of more complex events can be formed by taking sums.\nThe below code chunk simulates \\(n\\) realizations of a random variable that is 0 with probability \\(1-p\\) and 1 with probability \\(p\\). Such a random variable is said to have a Bernoulli distribution, or be Bernoulli.\n\nbernouli &lt;- function(p, n){\n  return(sample(c(0,1), size = n, prob = c(1-p, p), replace = T))\n}\n\nbernouli(0.1, 100) ### 100 realizations of a Bernouli(p = 0.1) RV\n\n  [1] 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1\n\n\nThe distribution of a continuous random variable \\(Y\\) is defined by a function \\(f(y|\\theta)\\) called its probability density. The probability of an event \\(A\\) (for example, observing a number between 0.3 and 0.7) is\n\\[\\Pr(Y \\in A) = \\int_A f(y|\\theta) \\text{d} y.\\]\nFor example, if \\(Y \\sim \\text{Uniform}(a,b)\\),\n\\[\\Pr(a &lt; Y &lt; b) = \\int_a^b \\frac{1}{b-a} \\text{d} y = \\frac{1}{b-a}(b-a) = 1.\\]\nImportantly, it is convenient to use a common notation for the distribution of discrete and continuous random variables. Since sums are special kinds of integrals, we write \\(X \\sim f(x|\\theta)\\) even if \\(X\\) is discrete. A list of important probability distribution functions can be found here."
  },
  {
    "objectID": "lec07-intro-to-stats.html#means-variances-and-other-moments",
    "href": "lec07-intro-to-stats.html#means-variances-and-other-moments",
    "title": "9  Introduction to inference",
    "section": "9.4 Means, variances, and other moments",
    "text": "9.4 Means, variances, and other moments\nRandom variables model outcomes from uncertain experiments, and their properties can tell us about what to expect (or how confident we are that certain outcomes will be realized). One way to characterize the behavior of a RV is by considering its mean, variance, and higher moments.\nThe mean (or expected value) of \\(X \\sim f(x|\\theta)\\) is a measure of central tendency:\n\\[E(X) = \\int_\\Omega f(x|\\theta) x \\text{d} x.\\]\nThe mean of a random variable with a Uniform(0,\\(n\\)) distribution is\n\\[\\int_0^n \\frac{1}{n} x \\text{d} x = \\frac{1}{n} \\int_0^n x \\text{d} x = \\frac{1}{n} \\frac{x^2}{2} \\bigg\\lvert_0^n = \\frac{1}{n} \\frac{n^2}{2} = \\frac{n}{2}.\\]\nLet \\(E(X) = \\mu\\). The variance of \\(X \\sim f(x|\\theta)\\) is \\(V(X) = E[(X-\\mu)^2] = E(X^2) - \\mu^2.\\) Variance provides a measure of how far \\(X\\) is, on average, from its mean.\n\n9.4.1 Higher moments\nThe mean and the variance turn out to be two (very important) measures of central tendency and spread, respectively. Higher ``moments” of a random variable \\(X\\) characterize the skewness and other properties of \\(X\\); these moments are given by \\(E(X^n)\\) for \\(n = 1,2,\\dots\\)."
  },
  {
    "objectID": "lec07-intro-to-stats.html#simulating-important-random-variables",
    "href": "lec07-intro-to-stats.html#simulating-important-random-variables",
    "title": "9  Introduction to inference",
    "section": "9.5 Simulating important random variables",
    "text": "9.5 Simulating important random variables\nWe will return to this topic in a later lecture, but simulation of random variables turns out to be an important tool in scientific computing, which allows us to tackle problems that would be difficult to solve otherwise.\nIn R, simulating realizations from well-known random variables is very easy.\n\nrunif(n = 10, min = -1, max = 1) \n\n [1]  0.22629179 -0.39119896 -0.17492544  0.15724293 -0.03510705 -0.82557388\n [7] -0.18895382  0.21607572  0.46452211 -0.75917345\n\n## simulates n = 10 realizations of a Uniform(-1,1) RV\n\nrnorm(n = 100, mean = 0, sd = 1) \n\n  [1]  1.2102904071 -1.3773350414  1.2692060569 -2.5346441637  0.0930678788\n  [6] -0.0006833914  1.4443417712  0.7868880789 -0.5258848409  0.0526918184\n [11]  1.3155591070 -1.4142328548  2.1824746503 -0.3634801920  1.7808878536\n [16]  0.3774999416 -0.2535686796 -1.5422837078 -0.6313908910 -2.1074696140\n [21]  0.2518842693  0.1094523344  0.1646994496 -0.2638074059  0.7668268522\n [26] -0.0668139217 -0.9597338691  0.3032689162  1.2753290597  0.1613875890\n [31] -1.2578792104 -1.8625342619 -0.1497923768 -1.1633750277 -0.3213457372\n [36] -0.4454443280 -0.9175225786 -0.0881146399 -1.7460771348 -0.1438579814\n [41]  1.7574966278  0.4488787203  0.2026099033 -0.1880505390 -1.0271877996\n [46] -0.5822660978  0.0493817013  1.1532429804  0.4639943008 -1.0402973018\n [51]  0.4778934413  2.6573461365  0.1857555015 -0.9680731163 -0.0203424312\n [56]  1.0367119661  0.5765527667 -1.5071447164  0.4945208998  2.1115055740\n [61]  1.9323851353  0.6986422738  0.7845778027 -0.4619172903  3.2748080099\n [66] -0.2538495250  0.3012037179 -0.1622611564  0.8367527854 -0.3302552537\n [71]  0.3308310159  0.5206449144  1.3887840435 -1.4552422964 -1.2017399687\n [76] -0.2290382531  0.9294548526  0.0331371701  1.1498993302  0.9928514026\n [81]  1.0297567129 -0.2583417603 -0.4275943547  0.7021605737 -1.1872457401\n [86] -1.6072429688 -0.7414762891  0.6867245097  0.7145692914  1.5237917494\n [91] -0.1102546015 -0.1508391447 -1.7913795969  2.8804156409 -0.8609229690\n [96] -0.2616467165 -0.3444324496  0.4062154003  0.3628236262 -0.2546986433\n\n## simulates n = 100 realizations of a Normal(mean = 0, variance = 1) RV\n## sd stands for standard deviation, and is the sqrt of the variance\n\nThe syntax for simulating from well-known random variables (distributions) is r followed by the name of the distribution. For example, if \\(X \\sim \\text{Exponential}(\\lambda = 3)\\), then we can simulate realizations of \\(X\\) by calling rexp() and specifying rate = 3. The same syntax works for discrete random variables. If \\(X \\sim \\text{Poisson}(\\lambda = 0.1)\\), then rpois(lambda = 0.1) does the trick!\nImportantly, this also provides a means to simulate from RVs with probability distributions that we cannot write down easily. For example, if \\(X \\sim \\text{Normal}(0,1)\\), then we can simulate from \\(X^2 + 6\\) as follows:\n\nX &lt;- rnorm(n = 20, mean = 0, sd = 1) \nX^2 + 6\n\n [1]  9.000266  6.063133  6.033870  8.004691  6.840319  6.049393  6.972648\n [8]  7.542659  7.250806  6.451892  6.018609  6.021553  6.716698  6.019027\n[15]  6.298371  6.001588  6.103019  6.675709  6.214515 11.138219"
  },
  {
    "objectID": "lec07-intro-to-stats.html#all-about-likelihood",
    "href": "lec07-intro-to-stats.html#all-about-likelihood",
    "title": "9  Introduction to inference",
    "section": "9.6 All about likelihood!",
    "text": "9.6 All about likelihood!\nSo far we have seen:\n\nRandom variables model experiments with uncertain outcomes and come in many flavors. They are the main workhorse of probability and statistics.\nFunctions in base R allow us to conveniently and easily simulation realizations (i.e., draws) of many random variables. When the probability distribution of a random variable may be hard to write down, transformations of other random variables may provide a means to generate random numbers.\nThe mean, variance, and higher moments are useful summaries of the central tenancy of and variability in a random experiment or process.\n\nNow, we turn our attention to the central problem of statistics: determining what processes and parameters gave rise to data (estimation), and quantifying uncertainty in those estimates (inference). Estimation and inference based on the likelihood function is the basis/foundation for most statistical procedures used in the sciences (including Analysis of Variance, fitting mixed models).\n\n9.6.1 The likelihood function\nThe idea is as follows. Given data \\(X_1,X_2,\\dots,X_n \\sim f(x|\\theta)\\), we want to estimate \\(\\theta\\), i.e., to determine what parameters were mostly likely to have given rise to the data (under the assumption \\(f\\) models the data generating process). We do this by maximizing the likelihood function\n\\[L(X_1,\\dots,X_n|\\theta) = f(X_1|\\theta) \\cdots f(X_n|\\theta) = \\prod_{i=1}^n f(X_i|\\theta),\\]\nwhich is formed the assumption \\(X_1,\\dots,X_n\\) are independent. (There are methods that accommodate for dependent data, but we will not get into them here.) The likelihood function is a function of the parameters \\(\\theta\\), but not of the data. It contains all of the information in the data about the parameters. The likelihood is formed by simply plugging in the data into the probability distribution function from which they jointly arose. When the data are independent, the joint probability distribution function is the product of the individual distribution functions.\nViewed as a function of \\(\\theta\\), the likelihood tells us how likely each set of parameter values is to have given rise to the data. The set of parameter values which jointly maximize \\(L\\) (i.e., have the highest likelihood of generating the observed data) is called the maximum likelihood estimator \\(\\hat{\\theta}_{\\text{MLE}}\\).\n\n\n9.6.2 An example of maximum likelihood estimation\nSuppose \\(X_1, X_2, \\dots, X_n \\sim \\text{Exponential}(\\lambda)\\). The probability distribution for an Exponential random variable is \\(f(x|\\lambda) = \\lambda e^{-\\lambda x}\\), where \\(x \\geqslant 0\\). The likelihood function for \\(\\lambda\\) is\n\\[L(X_1,\\dots,X_n|\\lambda) = \\prod_{i=1}^n \\lambda e^{-\\lambda X_i} = \\lambda^n e^{-\\lambda (X_1+\\dots+X_n)}.\\]\nThis appears difficult to maximize, but it turns out that it is quite easy if we take the log of the likelihood. The point at which the likelihood is greatest is not changed under this transformation (since log is an increasing function). The log-likelihood is a bit easier to deal with, and has some nice statistical properties.\nWith Exponential(\\(\\lambda\\)) data, the log-likelihood function is\n\\[\\ln L(X_1,\\dots,X_n|\\lambda) =  \\ln \\lambda^n e^{-\\lambda (X_1+\\dots+X_n)} = n \\ln \\lambda - \\lambda (X_1+\\cdots + X_n).\\]\nNow, it is possible to maximize this function with respect to \\(\\lambda\\) without too much hassle. Taking the derivative with respect to \\(\\lambda\\), setting it equal to zero, and solving for \\(\\lambda\\), one has\n\\[\\hat{\\lambda}_{\\text{MLE}} = \\frac{n}{(X_1+\\dots+X_n)} = \\frac{1}{\\overline{X}}.\\]\nThis shows that the “best” (maximum likelihood) estimator of \\(\\lambda\\) is the inverse of the sample mean! Importantly, the estimator is a function of the data and is thus random—this means the estimator has a distribution that is set by the distribution of the data.\nTo see this, we can do a quick simulation:\n\nexponential_MLE &lt;- function(n = 1000, lambda = 5){\n  data &lt;- rexp(n, rate = lambda)\n  return(1/mean(data)) ### this is our maximum likelihood estimator\n}\n\nexponential_MLE()\n\n[1] 4.826571\n\nestimator &lt;- c()\nN &lt;- 1000 ### number of n=100 exponential draws\n\nfor (i in 1:N){\n  estimator[i] &lt;- exponential_MLE(n = 1000)\n}\n\nestimator &lt;- data.frame(value = estimator)\n\nggplot(estimator, aes(x = value)) +  geom_histogram(aes(y = ..density..), \n                                                    fill = \"gray\", alpha = 0.5) + \n  geom_vline(xintercept = mean(estimator$value), color = \"red\") +\n  geom_vline(xintercept = mean(estimator$value) - sd(estimator$value), \n             color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = mean(estimator$value) + sd(estimator$value), \n             color = \"red\", linetype = \"dashed\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nImportantly, the distribution of \\(\\hat{\\lambda}_{\\text{MLE}}\\) is approximately normal for large \\(n\\), and has mean equal to the true value of \\(\\lambda\\). When R returns a confidence interval, it often makes use of the asymptotic distribution of the maximum likelihood estimator (even when the assumption of a large sample size is not satified).\n\n\n9.6.3 An aside: numerical evaluation of the likelihood\nOften, it is not possible to use calculus to maximize the likelihood function. Sometimes it is not even possible to write down a closed form for the likelihood! In this case, we can use numerical methods to evaluate and maximize the likelihood. The following code chunk illustrates how to do this when the data arise from a mixture of two Normal distributions: \\(X \\sim \\text{Normal}(\\mu,\\sigma^2)\\) with probability \\(p\\) and \\(X \\sim \\text{Normal}(\\nu,\\tau^2)\\) with probability \\(1-p\\). The probability distribution function for this data is\n\\[f(x|\\mu,\\sigma^2,\\nu,\\tau^2) = p \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-(x-\\mu)^2/2\\sigma^2} + (1-p) \\frac{1}{\\sqrt{2\\pi \\tau^2}} e^{-(x-\\nu)^2/2\\tau^2}.\\]\nIn the below code chunk, \\(p = 0.1\\), \\(\\mu = 0, \\nu = 10, \\sigma = 1, \\tau = 10\\). We assume that we know \\(p\\) and \\(\\sigma^2\\), and use numerical methods to evaluate the likelihood function across different combinations of parameters. The MLEs for \\(\\mu, \\nu,\\) and \\(\\tau\\) is found by determining where the log-likelihood assumes a maximum.\n\ndata &lt;- c(rnorm(n = 0.1*1000, mean = 0, sd = 1),\n          rnorm(n = 0.9*1000, mean = 10, sd = 10))\n\nreturn_LL_at_given_set_parameters &lt;- function(parameters){\n  # parameters is a vector with values at which evaluate the likelihood\n  # contains value for normal means and standard deviations\n  \n  likelihood_each_observation &lt;- \n    0.1*dnorm(data, mean = parameters$mean1, sd = parameters$sd1) +\n    0.9*dnorm(data, mean = parameters$mean2, sd = parameters$sd2) \n  # this is the prob. dist of the data, but will look different depending on the application\n  \n  LL_at_given_theta &lt;- sum(log((likelihood_each_observation)))\n  return(LL_at_given_theta)\n}\n\nLL &lt;- c()\nvalues_to_test &lt;- expand.grid(mean1 = c(-0.5,-0.1,0,0.1,0.5),\n                              mean2 = seq(-20,20,by=0.5),\n                              sd1 = 1, sd2 = seq(5,15,by=0.1))\n\nfor (i in 1:nrow(values_to_test)){\n  parameters &lt;- as.data.frame(values_to_test[i,])\n  LL[i] &lt;- return_LL_at_given_set_parameters(parameters)\n}\n\nlog_likelihood &lt;- \n  as.data.frame(cbind(LL, values_to_test)) %&gt;% subset(is.finite(LL))\n\n### returns which parameters gave rise to max likelihood\nMLE &lt;- log_likelihood %&gt;% subset(LL == max(LL)); MLE\n\n             LL mean1 mean2 sd1  sd2\n21364 -3686.122   0.1    10   1 10.2\n\nlog_likelihood %&gt;% subset(mean1 == MLE$mean1 & \n                         mean2 == MLE$mean2 & sd1 == MLE$sd1) %&gt;%\nggplot(aes(x = sd2, y = LL)) + geom_line() +\n  geom_vline(xintercept = MLE$sd2, color = \"red\") + \n  labs(x = \"tau\", \n       y = \"log-likelihood (fixing all other parameters at their MLEs)\")\n\n\n\n## this shows the *profile* likelihood, a slice of the full likelihood\n## mu, sigma, and nu are fixed at their MLEs"
  },
  {
    "objectID": "lec07-intro-to-stats.html#hypothesis-testing-theory",
    "href": "lec07-intro-to-stats.html#hypothesis-testing-theory",
    "title": "9  Introduction to inference",
    "section": "9.7 Hypothesis testing: theory",
    "text": "9.7 Hypothesis testing: theory\nOften, the objective of a study is not to estimate a parameter but to decide which of two (or more) contradictory claims about the parameter is consistent with the data. This part of statistics is called hypothesis testing. Hypothesis testing is intimately connected to the construction and interpretation of confidence intervals which quantify estimation uncertainty.\n\n9.7.1 Null and alternative hypotheses\nA statistical hypothesis is an assertion about the values of one or more parameters, or the form of a probability distribution that is used to model the data.\nTwo contradictory hypotheses of the first kind are\n\\[H_0: \\theta = \\theta_0\\] \\[H_1: \\theta \\neq \\theta_0\\]\nThe first hypothesis is called the null hypothesis and may correspond to an expectation we have about the parameter (from, e.g., prior data). The second hypothesis is called the alternative hypothesis. The data are used to make a principle conclusion about if the null hypothesis is consistent with the data; if so, we reject the alternative hypothesis and, if not, we reject the null hypothesis as an explanation for the data generative process. In the following sections, we will describe the process for conducting such a hypothesis test.\n\n\n9.7.2 Test statistics\nSuppose we have data \\(X_1,\\dots,X_n \\sim f(x|\\theta)\\) and wish to test the above hypotheses, i.e., to decide if \\(\\theta \\neq \\theta_0\\). We do this by constructing a test statistic, i.e., function of the data, and assessing if the realized value of statistic is consistent with its distribution under the null hypothesis. Is the value of the statistic, at some level of significance, different from what we would expect if the null hypothesis were true?\nMany choices of test statistic are possible, but the likelihood ratio is one that is commonly used:\n\\[\\lambda_{LR} = -2 (\\ln L(\\theta_0)-\\ln L(\\hat{\\theta}_{\\text{MLE}})).\\]\nThe statistic is based on the likelihood function, and its asymptotic distribution (as the sample size becomes large) under the null hypothesis is known. \\(\\lambda_{LR}\\) has an approximate \\(\\chi^2\\) distribution under \\(H_0\\). (Importantly, one must know the approximate distribution of a test statistic to preform a hypothesis test.) Given the value and distribution of our test statistic under the null hypothesis, we can determine which two competing hypothesis is consistent with the data.\n\n\n9.7.3 How do we do this?\nTo decide between the null and alternative hypothesis, given a test statistic and its distribution under \\(H_0\\), we must specify a significance level \\(\\alpha\\). The significance level measures how likely we are to reject the null hypothesis, given that it is true: \\(\\alpha = P(\\text{reject } H_0 | H_0)\\). The significance level is chosen before data collection, and is typically set to 0.05 or smaller. By a similar token, the power of a statistical test is defined as the probability of rejecting the alternative hypothesis, given it is true: \\(1-\\beta = \\Pr(\\text{reject } H_0 | H_1).\\) Many factors affect the power of a test, but a test based on the likelihood ratio test statistic is the uniformly most powerful among all alternatives to test the above hypothesis. In general, one can preform (and it is best practice to preform!) analyses ahead of data collection to ensure power at a certain level.\nWe conduct a hypothesis test at significance level \\(\\alpha\\) as follows:\n\nState the null and alternative hypothesis and significance level \\(\\alpha\\).\nCollect data, possibly with knowledge of the sample size required to achieve a certain power.\nCalculate the realized value \\(s\\) of a test statistic \\(S\\), e.g., \\(\\lambda_{LR}\\). The test statistic must have a known distribution under the null hypothesis. The likelihood ratio has a \\(\\chi^2\\) distribution under \\(H_0\\) above.\nCompute the probability of observing the realized value of the test statistic or something more extreme, given the null hypothesis is true, i.e., \\(p = \\Pr(S &gt; s | H_0)\\). This probability is called a \\(p\\) value. If \\(p &lt; \\alpha\\), we reject the null hypothesis at significance level \\(\\alpha\\) and, if not, we fail to reject \\(H_0\\).\n\nTo illustrate how this works, we will return to the previous example. A test of \\(H_0: (\\mu,\\sigma,\\nu,\\tau) = (0,1,10,10)\\) vs \\(H_1: (\\mu,\\sigma,\\nu,\\tau) \\neq (0,1,10,10)\\) at significance level \\(\\alpha = 0.05\\) can be preformed as follows.\n\nlikelihood_at_null &lt;- log_likelihood %&gt;% \n  subset(mean1 == 0 & mean2 == 10 & sd1 == 1 & sd2 == 10)\n\ntest_statistic &lt;- -2*(likelihood_at_null$LL - MLE$LL) \ntest_statistic ### realized value of the likelihood ratio test statistic\n\n[1] 1.244292\n\npchisq(test_statistic, df = 1, lower.tail = FALSE)\n\n[1] 0.2646455\n\n### if p &lt; 0.05, then we reject H0; otherwise, we fail to reject H0\n\nSince we have designed \\(H_0\\) so that it is true (i.e., the values for the parameters are those used to generate the data), it should come as no surprise that we fail to reject the null hypothesis in favour of \\(H_1\\).\nIn contrast, consider the test \\(H_0: (\\mu,\\sigma,\\nu,\\tau) = (0,1,10,5)\\) vs \\(H_1: (\\mu,\\sigma,\\nu,\\tau) \\neq (0,1,10,5)\\). In this case, \\(p &lt; \\alpha\\), so that we have sufficient evidence to reject the null hypothesis.\n\nlikelihood_at_null &lt;- log_likelihood %&gt;% \n  subset(mean1 == 0 & mean2 == 10 & sd1 == 1 & sd2 == 5)\n\ntest_statistic &lt;- -2*(likelihood_at_null$LL - MLE$LL) \ntest_statistic ### realized value of the likelihood ratio test statistic\n\n[1] 1515.148\n\npchisq(test_statistic, df = 1, lower.tail = FALSE)\n\n[1] 0\n\n### if p &lt; 0.05, then we reject H0; otherwise, we fail to reject H0\n\n\n\n9.7.4 Confidence intervals\nOne way to think of a \\(100(1-\\alpha)\\%\\) confidence interval is as the set of values \\(\\theta_0\\) such that we fail to reject the hypothesis \\(\\theta = \\theta_0\\) at significance level \\(\\alpha\\). (There are other ways to define and understand confidence intervals but they are somewhat oblique.) Depending on the application, it may be more convenient to do a hypothesis test or to construct a confidence interval. Unlike hypothesis tests, confidence intervals quantify the uncertainty around point estimates like the MLE. There are many ways to construct confidence intervals, but below we show one that involves finding which \\(\\theta_0\\) are such that \\(\\lambda_{LR} -2(\\ln L(\\theta_0)-\\ln L(\\hat{\\theta}_{\\text{MLE}})) &lt; \\chi^2_c\\), where \\(\\chi^2_c\\) a cutoff based on \\(100(1-\\alpha)\\%\\)-ile for a \\(\\chi^2\\) distribution with one degree of freedom.\n\ncutoff &lt;- qchisq(0.95,df=1)/2 \n# cutoff for admissible values based on 95%-ile for a chi-squared dist df=1\n\nlog_likelihood %&gt;% subset(mean1 == MLE$mean1 & \n                         mean2 == MLE$mean2 & sd1 == MLE$sd1) -&gt; LL_plot\n\nLL_plot %&gt;% subset(abs(LL - MLE$LL) &lt; cutoff) -&gt; values_inconfidence_interval\n\nc(min(values_inconfidence_interval$sd2),\n  max(values_inconfidence_interval$sd2)) ### confidence interval!\n\n[1]  9.8 10.7\n\nLL_plot %&gt;% ggplot(aes(x = sd2, y = LL)) + geom_line() +\n  geom_vline(xintercept = MLE$sd2, color = \"red\") +\n  geom_vline(xintercept = min(values_inconfidence_interval$sd2), \n              color = \"red\", linetype = \"dashed\") +\n   geom_vline(xintercept = max(values_inconfidence_interval$sd2), \n              color = \"red\", linetype = \"dashed\") +\n  labs(x = \"tau\", y = \"log-likelihood (all other parameters at their MLEs)\")\n\n\n\n\nThe limits for the \\(95\\%\\) confidence interval for \\(\\tau\\) are given by the dashed red lines. As before, the maximum likelihood estimate for \\(\\tau\\) (with all other parameters fixed at their MLEs) is represented by the solid red line."
  },
  {
    "objectID": "lec07-intro-to-stats.html#the-t-test-as-a-special-case-of-the-lrt",
    "href": "lec07-intro-to-stats.html#the-t-test-as-a-special-case-of-the-lrt",
    "title": "9  Introduction to inference",
    "section": "9.8 The t-test as a special case of the LRT",
    "text": "9.8 The t-test as a special case of the LRT\nMost of the tests that are presented in introductory statistics courses are special cases of the likelihood ratio test, or approximations to the LRT. Consider the t-test: \\(H_0: \\mu = \\mu_0\\) vs \\(H_1: \\mu \\neq \\mu_0\\). The \\(t\\)-test assumes the data \\(X_1,\\dots,X_n\\) are independent and normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nUnder these assumptions, the likelihood ratio test statistic is of the form\n\\[\\lambda_{LR} = \\bigg( 1 + \\frac{t^2}{n-1} \\bigg)^{-n/2},\\]\nwhere\n\\[t = \\frac{\\sqrt{n} (\\overline{x} - \\mu_0)}{S}\\]\nand\n\\[S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\overline{x})^2.\\]\n\\(S^2\\) is an unbiased estimator of the population variance. (The ML estimator is biased, so it is corrected and the unbiased estimator is used in calculating the test statistic.) The corresponding LR is based on the statistic \\(t\\), which has an approximate \\(t\\) distribution with \\(n-1\\) degrees of freedom.\nA \\(t\\) test assesses if there is evidence to reject \\(H_0\\) based on the realized value of \\(t\\) and knowledge of its approximate (asymptotic) distribution. As before, this involves calculating the realized value of the statistic and determining the probability of observing data more extreme under the null hypothesis; if this value is \\(&lt; \\alpha\\), the null hypothesis is rejected at significance \\(\\alpha\\). Importantly, any test based on the statistic \\(t\\) can be formulated in terms of the likelihood ratio; in this way, the \\(t\\) test is a special case of the LRT.\nThe t.test function implements several variants of the \\(t\\)-test in R:\n\ndata &lt;- rnorm(100, mean = 0, sd = 1) # change mean to 0.25, 0.3\nt.test(data, mu = 0.25, conf.level = 0.01)\n\n\n    One Sample t-test\n\ndata:  data\nt = -3.4965, df = 99, p-value = 0.0007073\nalternative hypothesis: true mean is not equal to 0.25\n1 percent confidence interval:\n -0.1161813 -0.1135589\nsample estimates:\n mean of x \n-0.1148701 \n\n\n\n9.8.1 Tests of the equality of group means\nWith a one-sample \\(t\\)-test, you’re asking whether the mean of your sample differs significantly from a mean value that you expect it to have. You might want to use this if you have an expected population mean value from the literature. You may have even amassed your own dataset that you think is representative of a population’s true mean & you want to compare a newly collected sample. To determine if there are differences between group means, a two-sample \\(t\\)-test can be used:\n\nobs1 &lt;- rnorm(100, mean = 0, sd = 1)\nobs2 &lt;- rnorm(1000, mean = 1, sd = 1) # note difference in sample size\n\nt.test(obs1, obs2)\n\n\n    Welch Two Sample t-test\n\ndata:  obs1 and obs2\nt = -8.3887, df = 116.12, p-value = 1.344e-13\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.1929522 -0.7372308\nsample estimates:\n mean of x  mean of y \n0.01169377 0.97678528 \n\n### alternatively, one can use the following syntax:\n\ndata &lt;- as.data.frame(rbind(cbind(obs = obs1, ID = 1), \n                            cbind(obs = obs2, ID = 2)))\n\nt.test(obs~ID, data) ### the syntax will be the same for linear models!\n\n\n    Welch Two Sample t-test\n\ndata:  obs by ID\nt = -8.3887, df = 116.12, p-value = 1.344e-13\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -1.1929522 -0.7372308\nsample estimates:\nmean in group 1 mean in group 2 \n     0.01169377      0.97678528 \n\n# anova(lm(obs~ID, data))\n\nWhen the goal of an investigation is to test if mean values of a dependent variable are different between categories that were paired by design, a paired \\(t\\)-test is used. An example where this would be appropriate is an experiment in which Daphnia abundance was sampled before and after exposure to a pathogen.\nThe syntax for a paired \\(t\\)-test is as follows:\n\nt.test(obs1, obs2, paired=TRUE) ## sample sizes must be equal here\n\nIn the next assignment, you will be asked to apply content from this lecture, and to read some notes on other tests (chi square, permutation, etc.) that are commonly used in ecology and evolution. Be on the lookout for these notes, which will be on Quercus and eeb313.github.io soon."
  },
  {
    "objectID": "lec08-linear-models.html#lesson-preamble",
    "href": "lec08-linear-models.html#lesson-preamble",
    "title": "10  Linear models",
    "section": "10.1 Lesson preamble:",
    "text": "10.1 Lesson preamble:\n\n10.1.1 Lesson objectives\n\nUnderstand the logic of simple and multiple linear regression, including the assumptions that are placed on the data, parameters, and errors.\nUnderstand the meaning of regression coefficients and how they are estimated.\nLearn how confidence intervals and \\(p\\)-values associated to the regression coefficients are calculated and used to test hypotheses.\nUnderstand how to implement linear models (including ANOVA) in R.\nDevelop familiarity with generalized linear models and some important examples (logistic, Poisson, negative binomial regression).\n\n10.1.2 Lesson outline\n\nLinear models: theory and examples\n\nStructure and assumptions, including interpretation of the effects\nLikelihood-based estimation and inference\nTransformations\nDummy variables, interactions between covariates, etc.\nAnalysis of Variance\n\nGeneralized linear models\n\nNon-normal errors, link functions\nEstimation and inference: even more likelihood!\nLogistic regression\nPoisson, negative binomial regression\n\n\n\n\nlibrary(tidyverse)\nlibrary(car)\nsurveys_subset &lt;- read_csv(\"survey_subset.csv\")\ntheme_set(theme_bw())"
  },
  {
    "objectID": "lec08-linear-models.html#linear-models-why-we-care",
    "href": "lec08-linear-models.html#linear-models-why-we-care",
    "title": "10  Linear models",
    "section": "10.2 Linear models: why we care",
    "text": "10.2 Linear models: why we care\nLinear models are at the heart of statistical practice in the physical, life, and social sciences! Linear regression actually refers to a family of modeling approaches that attempt to learn how the mean and/or variance of a response variable \\(\\boldsymbol{y} = (y_1,\\dots,y_n)\\) depend on (linear) combinations of variables \\(\\boldsymbol{x}_i = (x_{i1},\\dots,x_{in})\\) called predictors. In this lecture, we will discuss various forms of the linear model and assumptions placed on the data to make estimation and inference of the relationships between variables tractable. We will see how the likelihood function forms the basis for this estimation/inference, and how extensions of multiple regression (generalized linear models and mixed models!) can be understood in a likelihood framework. Our goal will be to become familiar with how these models work and how they are fit to data in R."
  },
  {
    "objectID": "lec08-linear-models.html#theory-likelihood-estimation-and-inference-for-linear-models",
    "href": "lec08-linear-models.html#theory-likelihood-estimation-and-inference-for-linear-models",
    "title": "10  Linear models",
    "section": "10.3 Theory: likelihood estimation and inference for linear models",
    "text": "10.3 Theory: likelihood estimation and inference for linear models\nA linear model takes the form\n\\[ y_i = \\beta_0 + \\beta_1 x_{1i} + \\cdots + \\beta_p x_{pi} + \\varepsilon_{i}, \\]\nwhere \\(i=1,\\dots,n\\) correspond to observations of a random variable \\(Y\\) which we call the response. The goal of regression is to explain and to predict the behavior of the response variable by adding together effects of \\(p\\) covariates \\(x_1,\\dots,x_p\\), which are also called predictors. Importantly, the phrase “linear model” is somewhat deceptive in that the model can be used to describe many kinds of functional relationships in the data: for example, we could use \\(x_2 = x_1^2\\) to model higher order effects of \\(x_1\\) on the response. In general, the linear model above specifies how the realizations \\(y_1,\\dots,y_n\\) of a random variable \\(Y\\) depend on the additive effects of one or more non-random covariates/predictors. We will discuss random and mixed effects next class, but the machinery used to estimate the effect sizes \\(\\beta_1,\\dots,\\beta_p\\) and error variance are very similar to the model in which the \\(x\\)s are assumed to be fixed.\nTo make estimation and inference tractable, the errors \\(\\varepsilon_i\\) are assumed to be Normal with mean zero and variance \\(\\sigma_i^2\\). Equivalently, we could assume the data \\(y_1,\\dots,y_n\\) are 1) independent and 2) Normal with mean(s) \\(\\beta_0 + \\beta_1 x_{1i} + \\cdots + \\beta_p x_{pi}\\) and variance(s) \\(\\sigma_1^2,\\dots,\\sigma_n^2\\). Although the normality assumption is not strictly necessary (nor is the assumption of equal variance across observations), they are commonly made and allow us to easily estimate the effect sizes and error variance. Under the assumption of normality and constant error variance \\(\\sigma_1^2 = \\dots = \\sigma_n^2 = \\sigma^2\\), the likelihood can be written as\n\\[ L(\\beta_0,\\beta_1,\\dots,\\beta_p,\\sigma^2) = \\prod_{i=1}^n (2 \\pi \\sigma^2)^{-1/2} e^{-(y_i - \\beta_0 - \\beta_1 x_{1i} - \\dots - \\beta_p x_{pi})^2/2\\sigma^2}. \\]\nEstimators of the regression coefficients \\(\\beta_1,\\dots,\\beta_p\\) and \\(\\sigma^2\\) are found by maximizing the likelihood. In fact, the estimators found by maximum likelihood methods are exactly those which minimize the distance from the model \\(y = \\beta_0 + \\beta_1 x_{1} + \\dots + \\beta_p x_{p}\\) from the data. The line (and in higher dimensions, plane) given by this equation is such that the the sum of squared departures of the line from the data is as small is it can be. Other notions of distance turn out to give rise to interesting extensions of the linear model, such as ridge and LASSO regression, which are beyond the scope of the course.\n\n10.3.1 Simple linear regression: \\(p=1\\)\nSimple linear regression is a special case of the general linear model (above), and corresponds to the case there is only one predictor, i.e., \\(p=1\\).\n\n\n10.3.2 The matrix vesion of the general linear model\nWhat is often used in theory and practice is a matrix version of the above model:\n\\[ \\boldsymbol{y} = \\begin{bmatrix}\n     y_{1} \\\\\n     y_{2} \\\\\n     \\vdots \\\\\n     y_{n}\n     \\end{bmatrix} = \\begin{bmatrix}\n     1 & x_{11} & x_{21} & \\cdots & x_{p1} \\\\\n     1 & x_{12} & x_{22} & \\cdots & x_{p2} \\\\\n     \\vdots & \\vdots & \\vdots & & \\vdots \\\\\n     1 & x_{1n} & x_{2n} & \\cdots & x_{pn}\n     \\end{bmatrix} \\begin{bmatrix}\n     \\beta_{1} \\\\\n     \\beta_{2} \\\\\n     \\vdots \\\\\n     \\beta_{p}\n     \\end{bmatrix} + \\begin{bmatrix}\n     \\varepsilon_{1} \\\\\n     \\varepsilon_{2} \\\\\n     \\vdots \\\\\n     \\varepsilon_{n}\n     \\end{bmatrix} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}.\n  \\]\nHere, \\(\\boldsymbol{y} = (y_1,\\dots,y_n)'\\) is a vector of measurements for the response, \\(\\boldsymbol{x_i} = (x_{i1},\\dots,x_{in})'\\) is a vector of measurements for the \\(k\\)th predictor, and \\(\\boldsymbol{\\varepsilon} = (\\varepsilon_1,\\dots,\\varepsilon_n)'\\) is a vector of measurement errors. The \\('\\) symbol corresponds to transposition, which involves interchanging the rows and columns of a vector or matrix.1\nWhen all the errors are modeled as above (i.e., as Normal with constant variance and mean zero), the ML estimator for the vector of regression coefficients is \\(\\boldsymbol{\\hat{\\beta}_{\\text{MLE}}} = (\\boldsymbol{X'X})^{-1} \\boldsymbol{X'} \\boldsymbol{y}\\).\n\n10.3.2.1 What do the regression coefficients mean?\nThe interpretation of the regression coefficients is as follows: \\(\\beta_j\\) describes how much the response is expected to change, all else constant, if we increase \\(x_j\\) by exactly one unit. Importantly, the fitted regression coefficients measure the effect (slope) of increasing \\(x_1,\\dots,x_j\\) on the response under the assumption of normal data — this distinction between the theoretical and fitted coefficients is important to keep mind.\n\n\n10.3.2.2 Categorical predictors\nBefore we dive into implementing linear models in R, it is important to mention how the linear model accommodates discrete predictors like sex (or genotype, ID, race). To deal with categorical predictors, we define the model in terms of a baseline and to interpret the regression coefficients relative to this baseline. This involves coding “dummy variables” \\(x_1,\\dots,x_{k-1}\\) for all but one the values (\\(1,2,\\dots,k\\)) the predictor can take one, so that \\(x_{ji} = 1\\) for observations where the categorical variable is \\(=j\\) and \\(=0\\) otherwise."
  },
  {
    "objectID": "lec08-linear-models.html#practice-fitting-linear-models-with-lm",
    "href": "lec08-linear-models.html#practice-fitting-linear-models-with-lm",
    "title": "10  Linear models",
    "section": "10.4 Practice: fitting linear models with lm",
    "text": "10.4 Practice: fitting linear models with lm\nTo illustrate how regression models are fitted (via maximum likelihood) in R, we will use the survey dataset from a couple classes ago. We begin by regressing weight on hindfoot length:\n\nmodel &lt;- lm(weight ~ hindfoot_length, data=surveys_subset)\nsummary(model) # assuming normality, homogeneity of variance\n\n\nCall:\nlm(formula = weight ~ hindfoot_length, data = surveys_subset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-60.342 -13.126  -4.018   3.351 221.135 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -33.2179     1.3575  -24.47   &lt;2e-16 ***\nhindfoot_length   2.5651     0.0441   58.16   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.19 on 3842 degrees of freedom\n  (505 observations deleted due to missingness)\nMultiple R-squared:  0.4682,    Adjusted R-squared:  0.4681 \nF-statistic:  3383 on 1 and 3842 DF,  p-value: &lt; 2.2e-16\n\n\nR returns the following after fitting a linear model via lm():\n\nDescriptive statistics for the “residuals” \\(\\varepsilon_i = y_i - \\widehat{\\beta_0} - \\widehat{\\beta_1} x_i\\), which tell us about how much variability there is in the data relative to the linear model specified and fitted.\nThe regression coefficients minimizing the sum of squared departures from the data (i.e., the ML estimates) and \\(95\\%\\) confidence intervals for each. The CIs are expressed as standard errors, since the estimators have an approximate Normal distribution. A joint confidence region for the coefficients can also be found using, e.g., the LRT statistic.\nA suite of test statistics! The \\(t\\) statistics and their \\(p\\) values are associated to the test \\(H_0: \\beta_i = 0\\) vs \\(H_1: \\beta_i \\neq 0\\). Significance codes specify the level \\(\\alpha\\) at which we have evidence to reject the null hypothesis for each coefficient.\nMeasures of goodness-of-fit: the multiple \\(R^2\\) and the adjusted \\(R^2\\). These explain the proportion of variance that are explained by the model. The latter measures the proportion of variance explained by the linear model upon adjusting for sample size and \\(\\#\\) of predictors.\n\n\nggplot(surveys_subset, aes(x = hindfoot_length, y = weight)) + \n  geom_point(aes(color = as.factor(sex)), size = 2) +\n  stat_smooth(method = \"lm\", se = F, color = \"gray\") + labs(color = \"sex\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 505 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 505 rows containing missing values (`geom_point()`).\n\n\n\n\n### are the data normal?\n\nggplot(surveys_subset, aes(x = weight)) + geom_histogram() ## oh no!\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 316 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n10.4.1 Transformations\nOften, data are non-normal! This is an unfortunate fact of life. It is sometimes possible, however, to use the machinery of regression if there is a suitable transformation of the data which makes it normal, e.g., log(), sqrt(). Right-skewed data (like above) may be normalized using log or root transformations (e.g. square root, third-root, etc.), with greater roots required for increasingly right-skewed data. Left-skewed data could be normalized with power transformations (e.g. squared, 3rd power, etc.).\n\nggplot(surveys_subset, aes(x = log(weight))) + geom_histogram() ## better!\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 316 rows containing non-finite values (`stat_bin()`).\n\n\n\n\nmodel_logtrasnformed &lt;- lm(log(weight)~hindfoot_length, data=surveys_subset)\nsummary(model_logtrasnformed)\n\n\nCall:\nlm(formula = log(weight) ~ hindfoot_length, data = surveys_subset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8285 -0.2135 -0.0473  0.1488  1.9612 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     1.5655038  0.0203235   77.03   &lt;2e-16 ***\nhindfoot_length 0.0647424  0.0006603   98.06   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3922 on 3842 degrees of freedom\n  (505 observations deleted due to missingness)\nMultiple R-squared:  0.7145,    Adjusted R-squared:  0.7144 \nF-statistic:  9615 on 1 and 3842 DF,  p-value: &lt; 2.2e-16\n\n\nAs before, there is evidence to reject the null hypothesis that hindfoot length has no effect on weight (or, in this case, its log transformation) at significance level \\(\\alpha = 0.05\\). This is because the \\(p\\)-value that is associated to the coefficent of hindfoot length is \\(&lt; \\alpha\\). We can also use what is returned by lm() to predict what the response will be if we observe new data (hindfoot lengths).\n\nnew_hindfoot_length_obs &lt;- data.frame(hindfoot_length = seq(0,100,0.1))\npredicted_values &lt;- predict.lm(object = model_logtrasnformed, \n                               newdata = new_hindfoot_length_obs)\n\nggplot(cbind(logweight = predicted_values, new_hindfoot_length_obs), \n       aes(x = hindfoot_length, y = logweight)) + geom_line(size = 1, color = \"gray\") + \n  geom_point(data = surveys_subset, inherit.aes = F,  size = 2, \n             aes(x = hindfoot_length, y = log(weight), color = as.factor(sex))) + \n  labs(color = \"sex\", y = \"log(weight)\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 505 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n10.4.1.1 Challenge\nRegress weight on sex. Be sure to specify that sex is a factor in the call to lm(). What value of sex is used as a baseline? Is there a significant effect of sex on weight?\n\nmodel &lt;- lm(weight~as.factor(sex), data=surveys_subset)\nsummary(model) \n\n\nCall:\nlm(formula = weight ~ as.factor(sex), data = surveys_subset)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-38.14 -22.14  -6.99   5.01 227.86 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      42.9896     0.8403  51.160   &lt;2e-16 ***\nas.factor(sex)M  -0.8537     1.1651  -0.733    0.464    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 36.89 on 4014 degrees of freedom\n  (333 observations deleted due to missingness)\nMultiple R-squared:  0.0001337, Adjusted R-squared:  -0.0001154 \nF-statistic: 0.5368 on 1 and 4014 DF,  p-value: 0.4638\n\n\n\n\n\n10.4.2 Syntax for multiple regression, interactions\nOne can regress on several explanatory variables simultaneously as follows:\n\nmodel &lt;- lm(weight~hindfoot_length+as.factor(sex), data=surveys_subset)\nsummary(model, type = 3)\n\n\nCall:\nlm(formula = weight ~ hindfoot_length + as.factor(sex), data = surveys_subset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-58.736 -12.860  -3.985   3.519 222.859 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -31.55118    1.41051 -22.369  &lt; 2e-16 ***\nhindfoot_length   2.57297    0.04414  58.287  &lt; 2e-16 ***\nas.factor(sex)M  -3.64249    0.84599  -4.306 1.71e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.15 on 3832 degrees of freedom\n  (514 observations deleted due to missingness)\nMultiple R-squared:  0.4702,    Adjusted R-squared:  0.4699 \nF-statistic:  1700 on 2 and 3832 DF,  p-value: &lt; 2.2e-16\n\n\nInteractions between covariates are modeled by introducing additional covariates of the form \\(x_i x_j\\). An interaction occurs when an independent variable has a different effect on the outcome depending on the values of another independent variable. They are super important! To estimate the coefficients associated to an interaction, * is used in the call to lm:\n\nmodel &lt;- lm(weight~hindfoot_length*as.factor(sex), data=surveys_subset)\n\nWe interpret the coefficient of the interaction term as we do all other regression coefficients. Per unit change in \\(x_i x_j\\) (here, the interaction between hindfoot length and sex), the associated regression coefficient measures the change in the response (weight).\n\nsurveys_subset %&gt;% subset(! is.na(sex)) %&gt;%\n  ggplot(aes(x = hindfoot_length, y = weight, color = as.factor(sex))) + geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 285 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 285 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n10.4.2.1 Challenge\nIs there a significant interaction between hindfoot length and sex on weight?\n\n\n\n10.4.3 The relationship between linear models and ANOVA\nAnalysis of Variance (ANOVA) is a statistical technique that is used to analyze variation in observations within and between categories, and to test if two or more population means are equal. Importantly, ANOVA is mathematically equivalent to regression with covariates that are all categorical. This means the assumptions of linear regression apply when preforming an ANOVA.\nANOVA can be implemented in R as follows:\n\nmodel &lt;- lm(weight~as.factor(sex), data=surveys_subset)\nanova(model) # must wrap the anova() around a lm()/model\n\nAnalysis of Variance Table\n\nResponse: weight\n                 Df  Sum Sq Mean Sq F value Pr(&gt;F)\nas.factor(sex)    1     730  730.48  0.5368 0.4638\nResiduals      4014 5461759 1360.68               \n\n\nBriefly, ANOVA tests differences between means by decomposing the total sum of squares (variance in the observations) into the variance due to the level under investigation (e.g., sex) and its factors (M, F). The \\(F\\)-statistic for each level (excluding the residuals) is the mean square, divided by the residual mean square, and should be \\(\\sim 1\\) if the corresponding effects are \\(=0\\). We reject the null hypothesis (there is no differences in group means) for values of \\(F\\) that are inconsistent with its distribution under the null hypothesis; recall from last time that the \\(p\\)-value measures the probability of observing data more extreme than the calculated value of \\(F\\) from the data, under the null hypothesis (i.e., the factors at some level have no effect). If \\(p &lt; \\alpha\\), we reject the null hypothesis at significance level \\(\\alpha\\).\n\n10.4.3.1 ANOVA with two or more covariates\n\nmodel &lt;- lm(weight~hindfoot_length*as.factor(sex), data=surveys_subset)\n\n# wrap Anova(), not summary()\nAnova(model, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: weight\n                                Sum Sq   Df   F value  Pr(&gt;F)    \n(Intercept)                     203901    1  298.3134 &lt; 2e-16 ***\nhindfoot_length                1151284    1 1684.3645 &lt; 2e-16 ***\nas.factor(sex)                      48    1    0.0709 0.79010    \nhindfoot_length:as.factor(sex)    1951    1    2.8544 0.09121 .  \nResiduals                      2618537 3831                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# compare p-values with summary(), which are slightly different\nanova(model)\n\nAnalysis of Variance Table\n\nResponse: weight\n                                 Df  Sum Sq Mean Sq   F value    Pr(&gt;F)    \nhindfoot_length                   1 2312681 2312681 3383.5236 &lt; 2.2e-16 ***\nas.factor(sex)                    1   12677   12677   18.5473 1.699e-05 ***\nhindfoot_length:as.factor(sex)    1    1951    1951    2.8544   0.09121 .  \nResiduals                      3831 2618537     684                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInstead of wrapping summary() around our linear model object, we use the Anova() function from the cars package to test the significance of our predictors. A Type 1 ANOVA (sequential sum of squares) will test the main effect of variable A, followed by the main effect of variable B after the main effect of A, followed by the interaction effect of AB. Since it tests factor A without controlling for factor B, the order that you specify your independent variables will matter! A Type 2 ANOVA (hierarchical sum of squares) tests for the main effect of variable A and variable B, but it does not assume that there is an interaction. If you have a factorial design, you should not be using a Type 2 ANOVA. Finally, a Type 3 ANOVA (marginal sum of squares) tests for the presence of a main effect while controlling for the other main effect and interaction; that is, it tests the effect of variable A if all other variables are already considered. This approach is therefore valid in the presence of significant interactions.\nThe type of ANOVA you use will matter when you have more than one independent variable, and especially if you have unbalanced data. By default, R uses type II sums of squares. Above, we are use Type 3 sums of squares as we care about the interaction between hindfoot length and sex, and we don’t prioritize the effects of one variable.\n\n\n\n10.4.4 A fun application!\nIn quantitative genetics, regression is used to estimate the strength of directional, stabilizing, or disruptive selection on continuous traits (e.g., beak length) controlled by a large number of genes of small additive effect. Without getting into the weeds, the regression of relative fitness (or some proxy for fitness) on trait value provides an estimate of the selection differential \\(S\\), defined as the co-variance between fitness and the trait. The observation linear models could be used to estimate selection on on or more quantitative traits (possibly correlated) was operationalized in 1983 by Lande & Arnold. When trait measurements are normalized, the slope of the regression equals the strength of directional selection on the trait. Regression has been applied in a wide range of plant and animal taxa to estimate selection and, when there are multiple traits under investigation, the relative importance of direct vs indirect selection on evolution.\n\ngenerate_data_directionalselection &lt;- function(n = 100, values){\n  \n  trait_data &lt;- sort(sample(size = n, x = values, replace = T))\n  fitness_data &lt;- 1 - dexp(trait_data, rate = 1) + rnorm(n, mean = 0, sd = 0.05)\n  \n  return(\n    data.frame(\n      trait = trait_data, fitness = fitness_data/mean(fitness_data)\n      )\n  )\n}\n\ndata &lt;- generate_data_directionalselection(values = seq(0,3,0.01))\n\nmodel1 &lt;- lm(fitness~scale(trait), data)\n# scale function transforms trait data so that mean=0, variance=1\n\nsummary(model1) \n\n\nCall:\nlm(formula = fitness ~ scale(trait), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.34482 -0.10435  0.00363  0.09729  0.29445 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.00000    0.01425   70.16   &lt;2e-16 ***\nscale(trait)  0.32857    0.01433   22.94   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1425 on 98 degrees of freedom\nMultiple R-squared:  0.843, Adjusted R-squared:  0.8414 \nF-statistic: 526.1 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n### fitness changed by ~50% per sd change in trait value!\n\nggplot(data, aes(x = scale(trait), y = fitness)) + \n  geom_point(color = \"gray\") +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = F) +\n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), se = F,\n              color = \"red\") +\n  labs(x = \"standardized trait value\", y = \"relative fitness\")\n\n\n\nmodel2 &lt;- lm(fitness~scale(trait)+scale(trait^2), data)\nsummary(model2)\n\n\nCall:\nlm(formula = fitness ~ scale(trait) + scale(trait^2), data = data)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.161739 -0.051332  0.002518  0.055765  0.196206 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     1.000000   0.007906  126.48   &lt;2e-16 ***\nscale(trait)    0.853731   0.036167   23.61   &lt;2e-16 ***\nscale(trait^2) -0.538310   0.036167  -14.88   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07906 on 97 degrees of freedom\nMultiple R-squared:  0.9522,    Adjusted R-squared:  0.9512 \nF-statistic: 965.7 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nHere, the blue (red) regression line is the linear (quadratic) individual selection surface, estimated by regressing fitness on trait value (and its square) simulated under a model of directional selection.\n\n10.4.4.1 Other applications…\nRegression has also been used in quantitative genetics to estimate the hertiability of traits (i.e., the proportion of variance in the trait that is explained by the additive action of genes), and to preform Genome-Wide Association Studies (GWAS) and uncover the genetic basis of complex traits. One form of GWAS involves regressing phenotype (e.g., disease status) on the existence/non-existence of genetic differences (most often, SNPs) among individuals sampled, and identifying those which have a significant effect on phenotype."
  },
  {
    "objectID": "lec08-linear-models.html#generalized-linear-models-theory-and-examples",
    "href": "lec08-linear-models.html#generalized-linear-models-theory-and-examples",
    "title": "10  Linear models",
    "section": "10.5 Generalized linear models: theory and examples",
    "text": "10.5 Generalized linear models: theory and examples\nSo far we have seen how regression can be used to fit linear models when the distribution of the data is approximately normal or when the data can be transformed so that this assumption is not violated. What if we were, say, interested in a binary response (e.g., disease status) and how it changes with a continuous predictor (e.g., age)? In this case, one can use a special kind of linear model called logistic regression to estimate the additive effect of predictor(s) on the binary response. Generalized linear models (GLMs) are useful when the response has a non-normal distribution, and transformation of the data is undesirable or impossible. A GLM takes the form\n\\[E(\\boldsymbol{y}|\\boldsymbol{X}) = g^{-1}(\\boldsymbol{X} \\boldsymbol{\\beta}),\\]\nwhere \\(g(\\cdot)\\) is a smooth and invertible link function taking the conditional expectation on the LHS to the linear predictor on the RHS. The link function for distributions in the overdispersed exponential family (including the Normal, Gamma, Exponential, Poisson, and Multinomial) are known. GLMs with these data distributions can be implemented in R by calling glm() with the appropriate family specified:\n\nresult_binary &lt;- glm(as.factor(sex)~weight, \n                     family=binomial, \n                     data=surveys_subset)\n\nsummary(result_binary)\n\n\nCall:\nglm(formula = as.factor(sex) ~ weight, family = binomial, data = surveys_subset)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.222  -1.214   1.134   1.143   1.204  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  0.1074131  0.0482240   2.227   0.0259 *\nweight      -0.0006272  0.0008560  -0.733   0.4638  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5560.8  on 4015  degrees of freedom\nResidual deviance: 5560.3  on 4014  degrees of freedom\n  (333 observations deleted due to missingness)\nAIC: 5564.3\n\nNumber of Fisher Scoring iterations: 3\n\n\nAbove are estimates of coefficients of a regression of sex on weight, as well as some details about the procedure used to fit those parameters. All GLMs are fitted via maximum likelihood, using using numerical optimization methods like iteratively reweighted least squares.\nUsing the cannonical link function, logistic regression can be formulated as follows:\n\\[ \\text{logit}(p) = \\log\\frac{p}{1-p} = \\boldsymbol{X} \\boldsymbol{\\beta} \\iff p = e^{\\boldsymbol{X} \\boldsymbol{\\beta}} = e^{\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p}. \\]\n\\(\\text{logit}(p)\\) is called the log-odds, which can be thought of as a likelihood the response takes on the value one. Under logistic regression, the log-odds ratio is modeled as a linear combination in the predictors \\((= \\boldsymbol{X} \\boldsymbol{\\beta})\\). Since the regression coefficients appear only through the logit-transformed proportions, their interpretation is somewhat different under logistic regression and other GLMs. Importantly, changing \\(x_j\\) by one unit, all else constant, results in change \\(\\beta_j\\) to the link-transformed response. This is how the effect sizes are interpreted for GLMs like the one fitted above.\n\nsurveys_subset %&gt;% filter(! is.na(sex)) %&gt;%\n  mutate(ID = ifelse(sex == \"M\", 0, 1)) %&gt;%\n  ggplot(aes(x = weight, y = ID)) + geom_point() +\n  geom_smooth(method = \"glm\", \n              method.args = list(family = \"binomial\"),\n              se = T\n              )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 104 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 104 rows containing missing values (`geom_point()`).\n\n\n\n\n\nHere are some common types of response variables and their corresponding distributions:\n\nCount data (positive integers only): Poisson distribution\nOver-dispersed count data (when the count data is more spread out than “expected”): negative binomial distribution\nBinary data (two discrete categories): binomial distribution\n\n\n10.5.1 Returning to the disease-age example…\nBelow we simulate binary disease data (0,1) for patients of several ages, assuming the the log-odds of disease is a linear function of age. We then fit a logistic regression to this data to determine the effect of age on disease risk. Next, we write a function to do a power analysis. That is, we determine the sample size that is required so that simulating the age-disease data repeatedly we identify a significant effect of age on disease status (i.e., reject the null hypothesis) at level \\(\\alpha = 0.01\\) at least \\(99\\%\\) of the time.\n\ndata_generator &lt;- function(sample_size = 100){\n  age &lt;- sample(size = sample_size, x = seq(0,100,0.1), replace = T)\n  linear_predictor &lt;- 0.8*scale(age)\n  prob &lt;- 1/(1+exp(-linear_predictor))\n  \n  disease_status &lt;- c()\n  \n  for (i in 1:length(prob)){\n  disease_status[i] &lt;- rbinom(n = 1, size = 1, prob = prob[i])\n  }\n  \n  return(data.frame(age = age, disease_status = disease_status))\n}\n\ndata &lt;- data_generator()\n\ndata %&gt;% pivot_longer(! age) %&gt;% \n  ggplot(aes(x = age, y = value)) + geom_point() + \n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\")\n              ) + labs(y = \"prob. of disease (i.e., disease status =1)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nmodel &lt;- glm(disease_status~scale(age), family = binomial, data = data)\nsummary(model)\n\n\nCall:\nglm(formula = disease_status ~ scale(age), family = binomial, \n    data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.8483  -1.0799   0.6374   1.0345   1.6268  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   0.1436     0.2126   0.675   0.4995   \nscale(age)    0.7149     0.2290   3.122   0.0018 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.27  on 99  degrees of freedom\nResidual deviance: 127.22  on 98  degrees of freedom\nAIC: 131.22\n\nNumber of Fisher Scoring iterations: 4\n\npower_analysis_function &lt;- function(sample_size){\n  \n  sims &lt;- 1000\n  pvalues &lt;- c()\n  for (i in 1:sims){\n  data &lt;- data_generator(sample_size)\n  model &lt;- glm(disease_status~scale(age), family = binomial, data = data)\n  pvalues[i] &lt;- summary(model)$coefficients[2,4]\n  }\n  \n  power_estimate &lt;- length(which(pvalues &lt; 0.01))/length(pvalues)\n  \n  return(power_estimate)\n}\n\nsample_sizes &lt;- seq(100,200,10); power_estimates &lt;- c()\n\nfor (i in 1:length(sample_sizes)){\n  power_estimates[i] &lt;- power_analysis_function(sample_size = sample_sizes[i])\n}\n\nknitr::kable(cbind(n = sample_sizes, power = power_estimates))\n\n\n\n\nn\npower\n\n\n\n\n100\n0.878\n\n\n110\n0.919\n\n\n120\n0.943\n\n\n130\n0.958\n\n\n140\n0.971\n\n\n150\n0.980\n\n\n160\n0.980\n\n\n170\n0.989\n\n\n180\n0.993\n\n\n190\n0.995\n\n\n200\n0.997"
  },
  {
    "objectID": "lec08-linear-models.html#footnotes",
    "href": "lec08-linear-models.html#footnotes",
    "title": "10  Linear models",
    "section": "",
    "text": "In R, the function t() can be used to transpose a vector or matrix.↩︎"
  },
  {
    "objectID": "lec09-mixed-effects-models.html#lesson-preamble",
    "href": "lec09-mixed-effects-models.html#lesson-preamble",
    "title": "11  Linear mixed models",
    "section": "11.1 Lesson preamble",
    "text": "11.1 Lesson preamble\n\n11.1.1 Learning objectives\n\nUnderstand the structure of linear mixed models.\nUnderstand how linear mixed models are fitted via maximum liklihood.\nUnderstand the differences between fixed and random effects.\nApply random effects models to nested experimental data.\nApply mixed models to data of Fitzpatrick et al.\n\n11.1.2 Lesson outline\n\nDescribe the structure of a linear mixed model, the form of the likelihood function, and how fixed/random effects are estimated. Discuss how variance components can be estimated.\nFamiliarize ourselves with the RIKZ data.\nPerform standard linear regression on the RIKZ data. Check the assumptions of multiple regression, including the independence of observations.\nExplore ways to overcome this violation without the use of mixed effects modeling.\nApply random intercept, random intercept and slope, and random-effects only models to RIKZ data. Discuss the differences between these models.\nExplore to how use mixed-effects models for more deeply nested data.\nExplore differences between nested and crossed random effects.\n\n11.1.3 Required packages\n\ntidyverse\nggalt\nlme4\nlmerTest\nggalt\nMuMIn\nsjmisc"
  },
  {
    "objectID": "lec09-mixed-effects-models.html#why-linear-mixed-models",
    "href": "lec09-mixed-effects-models.html#why-linear-mixed-models",
    "title": "11  Linear mixed models",
    "section": "11.2 Why linear mixed models?",
    "text": "11.2 Why linear mixed models?\nLast class we discussed how to apply linear models to data (e.g., linear regression, ANOVA, etc.) to understand the relationship between predictor (i.e., independent) and response (i.e., dependent) variables. As a reminder, the usual assumptions are:\n\nNormality of the errors\nHomogeneity of error variances\nIndependence of observations\n\nAlthough one can model the distribution of the data differently (e.g., using transformations or GLMMs), serious violations of independence and equality of error variances can pose problems and result in biased parameter estimates and \\(p\\)-values. Additionally, ecological and evolutionary data are often very messy, with a lot of noise, unequal sample sizes, and missing data. Thankfully, linear mixed effects models provide us with an estimation and inference framework which alleviates violations of these assumptions. Mixed effects models also allow us to better understand the sources of variation in the data; in quantitative genetics, for example, the goal of many analyses is to understand how much variation in a trait is additive, and this involves estimating what is called the additive genetic variance using a mixed effects model. Like linear models of other kinds, mixed effects models can be fit via maximum likelihood."
  },
  {
    "objectID": "lec09-mixed-effects-models.html#fitting-lmms",
    "href": "lec09-mixed-effects-models.html#fitting-lmms",
    "title": "11  Linear mixed models",
    "section": "11.3 Fitting LMMs",
    "text": "11.3 Fitting LMMs\nA linear mixed effects model is of the form\n\\[\\boldsymbol{y}_i = \\boldsymbol{X}_i \\boldsymbol{\\beta} + \\boldsymbol{Z}_i \\boldsymbol{b}_i + \\boldsymbol{\\varepsilon}_i,\\]\nwhere \\(\\boldsymbol{y}_i\\) is the \\(n_i \\times 1\\) response vector for observations in the \\(i\\)th group (\\(i = 1,\\dots,M\\)). \\(\\boldsymbol{X}_i\\) is an \\(n_i \\times p\\) matrix of fixed effects for the observations in group \\(i\\), and \\(\\boldsymbol{\\beta}\\) is a \\(1 \\times p\\) vector of fixed effects. Similarly, \\(\\boldsymbol{Z}\\) is a \\(n_i \\times q\\) matrix of \\(q\\) random effects associated to the observations in group \\(i\\), and \\(\\boldsymbol{b}_i\\) is a \\(q \\times 1\\) vector of random coefficients for group \\(i\\). This is called the Laird-Ware form of the linear mixed model.\nA more compact way to write an LMM is as follows:\n\\[\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{Z} \\boldsymbol{b} + \\boldsymbol{\\varepsilon}.\\]\nThe random effect coefficients \\(b_{i1},\\dots,b_{iq}\\) for group \\(i\\) are assumed to be \\(\\text{Normal}(0,\\psi_i^2)\\). Furthermore, \\(\\text{Cov}(b_i,b_{i'}) = \\psi_{ii'}\\) so that the random effects need not be independent of each other. In the more compact notation, \\(\\boldsymbol{b} \\sim \\text{Normal}(0,\\boldsymbol{\\Psi}).\\) Other distributions can be used to model variability in the random effects, but the choice of a Normal distribution has advantages which we will not get into here. In a similar spirit, to make estimation and inference of LMMs tractable, the errors \\(\\boldsymbol{\\varepsilon}\\) are assumed to be \\(\\text{Normal}(0,\\boldsymbol{\\Lambda})\\), where \\(\\boldsymbol{\\Lambda}\\) is the matrix of error variances (on the diagonal) and co-variances (on the off-diagonal).\nImportantly, we interested in random effects insofar as they can provide information and help form inferences about the distribution of response measurements at different levels. Because the random effects are unobserved, we must first estimate the fixed effects. The assumptions we have made about the random effects and error variances imply \\(\\boldsymbol{y}|\\boldsymbol{b} \\sim \\text{Normal}(\\boldsymbol{X} \\boldsymbol{\\beta}, \\boldsymbol{\\Sigma})\\), where \\(\\boldsymbol{\\Sigma} = \\boldsymbol{Z} \\boldsymbol{\\Psi} \\boldsymbol{Z}' + \\boldsymbol{\\Lambda}\\). Conditional on the random effects, the data are independent. The log-likelihood for the data is\n\\[\\frac{1}{2} \\ln |\\boldsymbol{\\Sigma}| - \\frac{1}{2} (\\boldsymbol{y}-\\boldsymbol{X} \\boldsymbol{\\beta})'  \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{y}-\\boldsymbol{X} \\boldsymbol{\\beta}).\\]\n\n11.3.1 Estimating fixed, random effects with \\(\\boldsymbol{\\Sigma}\\) known\nMaximizing the log-likelihood with respect to \\(\\boldsymbol{\\beta}\\) gives \\(\\boldsymbol{\\hat{\\beta}_{\\text{MLE}}} = (\\boldsymbol{X}' \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{X})^{-1} \\boldsymbol{X}' \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{y}.\\) This, as in standard regression theory, is the best linear unbiased estimator (BLUE) of the fixed effects. In addition to the estimation of fixed effects, prediction1 of random effects is often of interest. The best linear unbiased predictor (BLUP) of the random effects is \\(\\boldsymbol{\\hat{u}} = \\boldsymbol{\\Psi}\\boldsymbol{Z}'(\\boldsymbol{Z} \\boldsymbol{\\Psi} \\boldsymbol{Z}'+\\boldsymbol{\\Lambda})^{-1}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\hat{\\beta}})\\).\n\nNote: the ML estimator of the random effects is biased, so Restricted ML Estimation (REML) is often used. It’s generally good to use REML when you are interested in the magnitude of the random effects variances, but do not do this when comparing models with different fixed effects.\n\n\n11.3.2 Estimation and inference when \\(\\boldsymbol{\\Sigma}\\) is unknown\nIn the preceding discussion, we have assumed the variance-covariance matrices \\(\\boldsymbol{\\Psi},\\boldsymbol{\\Lambda}\\). This is rarely the case. Sometimes, modelers will specify variance-covariance matrices of a certain form (so that there are not so many parameters the model becomes impossible to fit). Otherwise, it is advisable to use methods that estimate the variance components from the data. In today’s lecture, we will have to estimate varience components to get a sense of how much random effects matter and shape variation in the data. The math that goes into correctly estimating varience components is quite complicated, but we’ll mention that an algorithm known as expectation-maximization can often be used.\n\n\n11.3.3 On the difference between fixed and random effects\nOne of the most tricky things about mixed effects modeling is deciding what co-variates are “fixed” and which are “random” — sometimes, a co-variate can be both! The meaning of “fixed” and “random” can, ironically, be variable depending on in what context they are used, and who they are used by. As a rule of thumb, a random effect is one which is drawn from a population of effects and can be used to combine information from different groups to learn about the properties (e.g, the variance) of that distribution.\nFor more on this, see\n\nthis thread on the difference between fixed and random effects\nthis page of linear mixed modeling questions and answers, maintained by Dr. Ben Bolker\nthis paper by Bolker et al."
  },
  {
    "objectID": "lec09-mixed-effects-models.html#on-to-data-analysis",
    "href": "lec09-mixed-effects-models.html#on-to-data-analysis",
    "title": "11  Linear mixed models",
    "section": "11.4 On to data analysis!",
    "text": "11.4 On to data analysis!\n\n11.4.1 The RIKZ dataset\nThroughout the lecture, we will be making use of the RIKZ dataset. The data are as follow: for each of 9 intertidal areas (denoted ‘Beaches’), the researchers sampled five sites (denoted ‘Sites’) and at each site they measured abiotic variables and the diversity of macro-fauna (e.g. aquatic invertebrates). Here, species richness refers to the total number of species found at a given site while NAP (i.e. Normal Amsterdams Peil) refers to the height of the sampling location relative to the mean sea level and represents the amount of food available for birds, etc. The data are described in more detail in Zuur et al. (2009).\n\nrikz_data &lt;- read_csv(\"rikz_data.csv\")\nhead(rikz_data)\n\n# A tibble: 6 × 6\n   ...1 Richness Exposure    NAP Beach  Site\n  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1       11       10  0.045     1     1\n2     2       10       10 -1.04      1     2\n3     3       13       10 -1.34      1     3\n4     4       11       10  0.616     1     4\n5     5       10       10 -0.684     1     5\n6     6        8        8  1.19      2     1\n\n\nThe question we’ll try to answer with this data is: What is the influence of NAP on species richness?\n\n\n\nDiagrammatic representation of the RIKZ dataset (modified from Zuur et al. (2009), Chapter 5)\n\n\n\n11.4.1.1 Challenge\nNow that you are experts at exploring data, work in groups to answer the following questions and get familiar with this dataset!\n\nHow many columns of data are there? How many rows of data? Are all the data columns classified correctly?\nWhat do the distributions of NAP and Richness look like? Take note of if these distributions are symmetric, and whether or not there are any outliers.\nMake a plot of the relationship between NAP and Richness. How would you describe this relationship?\n\n\n# question 1\nstr(rikz_data)\n\nspc_tbl_ [45 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ ...1    : num [1:45] 1 2 3 4 5 6 7 8 9 10 ...\n $ Richness: num [1:45] 11 10 13 11 10 8 9 8 19 17 ...\n $ Exposure: num [1:45] 10 10 10 10 10 8 8 8 8 8 ...\n $ NAP     : num [1:45] 0.045 -1.036 -1.336 0.616 -0.684 ...\n $ Beach   : num [1:45] 1 1 1 1 1 2 2 2 2 2 ...\n $ Site    : num [1:45] 1 2 3 4 5 1 2 3 4 5 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   ...1 = col_double(),\n  ..   Richness = col_double(),\n  ..   Exposure = col_double(),\n  ..   NAP = col_double(),\n  ..   Beach = col_double(),\n  ..   Site = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nhead(rikz_data)\n\n# A tibble: 6 × 6\n   ...1 Richness Exposure    NAP Beach  Site\n  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1       11       10  0.045     1     1\n2     2       10       10 -1.04      1     2\n3     3       13       10 -1.34      1     3\n4     4       11       10  0.616     1     4\n5     5       10       10 -0.684     1     5\n6     6        8        8  1.19      2     1\n\nis.factor(rikz_data$Beach)\n\n[1] FALSE\n\nrikz_data &lt;- rikz_data %&gt;% mutate(Beach = as.factor(Beach))\n\n# we can see that the data contains 45 rows (observations). \n# as expected, observations were taken across 9 beaches, each with 5 sites. \n# we have encoded 'Beach' as a factor...\n# this will facilitate plotting and its use as a random effect downstream.\n\n\n# question 2\nrikz_data %&gt;% select(\"NAP\", \"Richness\", \"Beach\") %&gt;% \n  pivot_longer(1:2) %&gt;% ggplot(aes(x = value)) + geom_histogram() + \n  facet_wrap(~name, scales = \"free_x\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n# question 3\nggplot(rikz_data, aes(x = NAP, y = Richness)) + geom_point(size = 2) +\n    geom_smooth(method = \"lm\", se = F, color = \"gray\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n11.4.2 Limitations of simple linear models\nLet’s go ahead and perform a linear regression to examine the relationship between species richness and NAP, pooling data across all beaches to see if it matches our expectation based on the graph.\n\nmodel &lt;- lm(Richness~NAP, data=rikz_data)\nsummary(model)\n\n\nCall:\nlm(formula = Richness ~ NAP, data = rikz_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0675 -2.7607 -0.8029  1.3534 13.8723 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.6857     0.6578  10.164 5.25e-13 ***\nNAP          -2.8669     0.6307  -4.545 4.42e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.16 on 43 degrees of freedom\nMultiple R-squared:  0.3245,    Adjusted R-squared:  0.3088 \nF-statistic: 20.66 on 1 and 43 DF,  p-value: 4.418e-05\n\n\nIt appears there is a significant relationship between NAP and species richness! Next we assess if the assumptions (equality of variances, normality) of the linear model are met. Thankfully, plot() applied to a linear model does all of diagnostic work for us.\n\npar(mfrow=c(2,2)) # look at all 4 plots together\nplot(model)\n\n\n\n\nThe residual plot suggests that the homogeneity assumption is violated (increasing variance in the residuals with increasing fitted values). Similarly, the QQ plot suggests non-normality (points falling off of the dotted line). The scale-location plot shows us that the assumption of equal variance between groups (homoscedasticity) is violated, since the residuals are not spread equally along predictors, but instead the range of residuals is wider as the x-axis increases. Finally, residuals vs. leverage helps to identify influential data points on the model (e.g., outliers, but also non-outlier points that don’t follow the trend) – look for points with a high Cook’s distance, outside the dashed line(s) on the top-right or bottom-right. Lucky for us, there are no highly-influential points; in fact, we can barely see the Cook’s distance lines.\n\n11.4.2.1 Transformations to the rescue?\nLet’s try a third root transformation to see if that helps:\n\nmodel_3rd_root_transform &lt;- lm(Richness^(1/3)~NAP, data=rikz_data)\npar(mfrow=c(2,2)) # look at all 4 plots together                                 \nplot(model_3rd_root_transform)\n\n\n\n\nA third-root transformation of the response variable (i.e. Richness) seems to alleviate our three problems. Nonetheless, for the analyses in this section, we will ignore violations of these assumptions and carry out the rest of the analysis with un-transformed data for the purpose of highlighting the fact that these data violate yet a key assumption, independence of observations.\n\n\n\n11.4.3 Non-independence of observations\nThe species richness data come from multiple sites within multiple beaches. While each beach may be independent, sites within a beach are likely to have similar species richness due simply to their proximity within the same beach. In other words, observations among sites within a beach are not independent. Another way of saying this is that the data are nested. Nesting in this sense is a product of the experimental design (i.e., we chose to sample 5 sites within each beach) and not necessarily of the data itself. Other types of nested data include: sampling the same individual pre- and post-treatment or sampling them multiple times (i.e., repeated measures), or sampling multiple tissues from the same individuals.\n\n\n11.4.4 Ways to deal with non-independence in the data\n\nOne way to account for the non-independence of observations would be to run a separate analysis for each beach. In this case, each analysis only has five observations to work with, and we have to run multiple tests. This means we run the risk of obtaining spuriously significant results by chance.\n\n\nmodels &lt;- NULL\n\nfor (i in 1:9){\n  data &lt;- rikz_data %&gt;% subset(Beach == i)\n  models[[i]] &lt;- cbind(\n    summary(lm(Richness~NAP, data = data))$coefficients[,c(1,4)],\n    Beach = i)\n}\n\ndo.call(rbind, models)\n\n              Estimate     Pr(&gt;|t|) Beach\n(Intercept) 10.8218944 0.0006886412     1\nNAP         -0.3718279 0.6938082209     1\n(Intercept) 13.3456944 0.0054952211     2\nNAP         -4.1752712 0.1282157989     2\n(Intercept)  3.4007021 0.0035446596     3\nNAP         -1.7553529 0.0362969533     3\n(Intercept)  3.0877160 0.0005459215     4\nNAP         -1.2485766 0.0061310867     4\n(Intercept) 12.7828276 0.0173369425     5\nNAP         -8.9001779 0.0483908629     5\n(Intercept)  4.3246341 0.0008653329     6\nNAP         -1.3885120 0.0147940222     6\n(Intercept)  3.5206265 0.0079312538     7\nNAP         -1.5176126 0.0579432596     7\n(Intercept)  4.9514552 0.0022220969     8\nNAP         -1.8930665 0.0170191039     8\n(Intercept)  6.2950533 0.0447894638     9\nNAP         -2.9675304 0.1850319458     9\n\n\n\nAlternatively, one could account for the fact observations are non-independent by including a term for each beach in the model, and estimate a separate effect for each level.\n\nIn this case, adding terms to account for between-beach differences did not change our interpretation of the association between NAP and species richness (which is still negative and significant). However, the inclusion of additional terms in this way will sometimes change the estimated effect of other terms in the model and alter their interpretation. The question we need to ask ourselves here is: Do we really care about differences between beaches represented in the data? These beaches were a random subset of all beaches that could have been chosen — their effects on the response are best modeled as random variables.\n\nmodel_beaches &lt;- lm(Richness ~ NAP + Beach, data = rikz_data)\nsummary(model_beaches)\n\n\nCall:\nlm(formula = Richness ~ NAP + Beach, data = rikz_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8518 -1.5188 -0.1376  0.7905 11.8384 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.8059     1.3895   7.057 3.22e-08 ***\nNAP          -2.4928     0.5023  -4.963 1.79e-05 ***\nBeach2        3.0781     1.9720   1.561  0.12755    \nBeach3       -6.4049     1.9503  -3.284  0.00233 ** \nBeach4       -6.0329     2.0033  -3.011  0.00480 ** \nBeach5       -0.8983     2.0105  -0.447  0.65778    \nBeach6       -5.2231     1.9682  -2.654  0.01189 *  \nBeach7       -5.4367     2.0506  -2.651  0.01196 *  \nBeach8       -4.5530     1.9972  -2.280  0.02883 *  \nBeach9       -3.7820     2.0060  -1.885  0.06770 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.06 on 35 degrees of freedom\nMultiple R-squared:  0.7025,    Adjusted R-squared:  0.626 \nF-statistic: 9.183 on 9 and 35 DF,  p-value: 5.645e-07\n\n\n\n\n11.4.5 Random intercept model\nTo estimate the variance among beaches and account for the non-independence of sites within beaches, we include Beach as a random effect in our model. NAP remains a fixed effect. As such, we model a separate \\(y\\)-intercept (i.e. Richness at NAP both = 0) for each beach and estimate the variance around this intercept. A small variance means that variances between beaches are small whereas a large variance means the opposite. We can run mixed-effects models using the lmer function from the lme4 R package and obtain parameter estimates using the lmerTest package. The question we are now asking is: What is the influence of NAP on species richness, accounting for the non-independence of sites within each beach?\n\nlibrary(lme4)\nlibrary(lmerTest)\n\n# Random intercept model with NAP as fixed effect and Beach as random effect\nmixed_model_IntOnly &lt;- lmer(Richness~NAP+(1|Beach), data=rikz_data, REML=FALSE)\nsummary(mixed_model_IntOnly)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: Richness ~ NAP + (1 | Beach)\n   Data: rikz_data\n\n     AIC      BIC   logLik deviance df.resid \n   249.8    257.1   -120.9    241.8       41 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.4258 -0.5010 -0.1791  0.2452  4.0452 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Beach    (Intercept) 7.507    2.740   \n Residual             9.111    3.018   \nNumber of obs: 45, groups:  Beach, 9\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)   6.5844     1.0321  9.4303   6.380 0.000104 ***\nNAP          -2.5757     0.4873 38.2433  -5.285 5.34e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n    (Intr)\nNAP -0.164\n\n\nThe (1|Beach) is the random effect term, where the 1 denotes this is a random-intercept model and the term on the right of the | is a nominal variable (or factor) to be used as the random effect. lmer returns log-likelihood values for the model, as well as information theoretic criteria (AIC, which stands for Akaike information criterion) used to evaluate the goodness-of-fit of the model and preform model selection.\nlmer also returns an estimated variance for the random effects. For this data, the variance associated with the effect of beach is 7.507. By dividing the variance of beach by the total variance, we see that differences between beaches account for (7.507/(7.507+9.111))*100=45% of the residual variance (i.e., variance left over after accounting for the fixed effects) in the model. Note the total variance explained by random effects (i.e., the sum of the variance components from all the random effects, including the residuals). This allows us to calculate the importance of each random effect relative to each another (as measured by how much of the residual variance it explains), which is crucial to note when there is more than one random effect.\nWe can visualize the fitted values for this model as follows:\n\nrikz_data &lt;- rikz_data %&gt;% mutate(fit_InterceptOnly=predict(mixed_model_IntOnly))\n# uses model to predict Richness at NAP\n\nggplot(rikz_data, aes(x=NAP, y=Richness, colour=Beach)) +\n  \n  # fixed effect regression line (read values off of model output above)\n  geom_abline(aes(intercept=6.5844, slope=-2.5757), linewidth=2) +\n  \n  # fitted values (i.e., regression lines) for each beach\n  geom_line(aes(y=fit_InterceptOnly), linewidth=1) +\n  geom_point(size=3)\n\n\n\n\nThe thick black line corresponds to the fitted values associated with the fixed-effect component of the model. The thin coloured lines correspond to the fitted values estimated for each beach. As you can see, they all have separate intercepts, as expected. As the estimated variance of the random effect increases, these lines would become more spread out around the thick black line. If the variance was \\(=0\\), all the coloured lines would coincide with the thick black line.\n\n\n11.4.6 Random intercept-slope model\nThe model above allows the intercept for each beach to vary around the population-level intercept. This means that each beach may differ in species richness, but how that respond to NAP remains the same across beaches. However, what if beaches don’t only vary in their mean richness, but the richness on each beach also varies in its response to NAP?\nIn a standard regression, this would amount to including NAP, Beach and interaction effects in the model. Of course, including such fixed-effects here would consume way too many degrees of freedom and we already decided we don’t really care about differences between beaches per se. Thankfully, we can still allow beaches to vary in the response to NAP using a random intercept-slope model.\nWe can fit the random intercept-slope model to these data using the code below.\n\nmixed_model_IntSlope &lt;- lmer(Richness~NAP+(1+NAP|Beach), data=rikz_data, REML=FALSE)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(mixed_model_IntSlope)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: Richness ~ NAP + (1 + NAP | Beach)\n   Data: rikz_data\n\n     AIC      BIC   logLik deviance df.resid \n   246.7    257.5   -117.3    234.7       39 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.7985 -0.3418 -0.1827  0.1749  3.1389 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n Beach    (Intercept) 10.949   3.309         \n          NAP          2.502   1.582    -1.00\n Residual              7.174   2.678         \nNumber of obs: 45, groups:  Beach, 9\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)   6.5818     1.1883  8.8936   5.539 0.000377 ***\nNAP          -2.8293     0.6849  7.9217  -4.131 0.003366 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n    (Intr)\nNAP -0.810\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nThe above model now allows both the intercept and slope of the relationship between Richness and NAP to vary across beaches. The only difference here is the additional variance component in the random effects, which estimates the variance in slopes across beaches. It also includes a Corr term, which estimates the correlation between the intercept and slope variances. A correlation of -1, for example, would imply that beaches with larger intercepts also have more steeply negative slopes, as can be seen in the figure below.\n\nrikz_data$fit_IntSlope &lt;- predict(mixed_model_IntSlope)\n\nggplot(rikz_data, aes(x=NAP, y=Richness, colour=Beach)) +\n    geom_abline(aes(intercept=6.582, slope=-2.829), linewidth=2) +\n    geom_line(aes(y=fit_IntSlope), linewidth=1) +\n    geom_point(size=3)\n\n\n\n\n\n\n11.4.7 Random effects only model\nNote that it is not always necessary to specify fixed effects, in the same way that it is not always necessary to specify random effects. For example, we could run the following model which would allow us to solely estimate the variation in species richness between beaches. Here, 1 is used as a placeholder to indicate no fixed effects are fitted.\n\nmixed_model_NoFix &lt;- lmer(Richness~1+(1|Beach), data=rikz_data, REML=TRUE)\n### note we are using REML because we are not interested in the fixed effects\n### varience component estimation is more accurate with REML\nsummary(mixed_model_NoFix)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: Richness ~ 1 + (1 | Beach)\n   Data: rikz_data\n\nREML criterion at convergence: 261.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.7797 -0.5070 -0.0980  0.2547  3.8063 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Beach    (Intercept) 10.48    3.237   \n Residual             15.51    3.938   \nNumber of obs: 45, groups:  Beach, 9\n\nFixed effects:\n            Estimate Std. Error    df t value Pr(&gt;|t|)   \n(Intercept)    5.689      1.228 8.000   4.631  0.00169 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nrikz_data$fit_NoFix &lt;- predict(mixed_model_NoFix)\n\nggplot(rikz_data, aes(x=NAP, y=Richness, colour=Beach)) +\n    geom_abline(aes(intercept=6.582, slope=-2.829), linewidth=2) +\n    geom_line(aes(y=fit_NoFix), linewidth=1) +\n    geom_point(size=3)\n\n\n\n\n\n\n11.4.8 Inspecting model fit\nIn previous lectures we emphasized how regression models seek the “best fit model” (that is: the one that maximizes the liklihood of the observed data under some parametric model, e.g., normal errors, for the generative process). But we need to be careful that the “best fit” might not be a “good fit” after all! To inspect model fit, we look to the coefficient of determination, or \\(R^2\\), which represents the percentage of variance in your data that is explained by your model. In simple linear models, the \\(R^2\\) value is rather straightforward and included in the summary output. In a mixed model, this is slightly more complicated, as we now have to think about about variance explained by the fixed effects as well as random effects. Here, we introduce an extension to the concept of \\(R^2\\):\n\nConditional \\(R^2\\) (\\(R_c^2\\)): Variance explained by both fixed and random effects\nMarginal \\(R^2\\) (\\(R_m^2\\)): Variance explained by fixed effects only\n\nAnd of course R has handy functions to help us calculate these:\n\nlibrary(MuMIn)\n\nr.squaredGLMM(mixed_model_IntOnly)\n\n          R2m      R2c\n[1,] 0.282994 0.606889\n\nr.squaredGLMM(mixed_model_IntSlope)\n\n           R2m       R2c\n[1,] 0.3150674 0.7143823\n\n\nWe see that \\(R_c^2\\) is a lot larger than \\(R_m^2\\) in both cases, meaning that most of the variance in our data is actually captured by our random effects (i.e., due to differences between beaches) – good thing we included them! In general, \\(R_m^2\\) of \\(\\sim 10%\\) is considered a good day in field ecology. Since NAP explained \\(\\sim 30\\%\\) of the variance in species richness in both cases, we can conclude that the models are performing well!\nNow that we have specified these models and see that they both perform reasonably well, how do we know which to choose? After all, they all provide slightly different estimates for the effects of NAP and P-values.\nOur model structure should be informed by our hypothesis of “reality”, and this provides one way to choose between models. But in cases where we are unsure, or trying to compete different hypotheses (e.g., trying to figure out whether the species richness and NAP relationship differ by beach), then just knowing that both models fit well isn’t good enough anymore — we need a way to determine which model is a better fit to the data, controlling for the number of parameters fitted. We will come back to this question in the next lecture when discussing model selection."
  },
  {
    "objectID": "lec09-mixed-effects-models.html#deeply-nested-and-crossed-effects",
    "href": "lec09-mixed-effects-models.html#deeply-nested-and-crossed-effects",
    "title": "11  Linear mixed models",
    "section": "11.5 Deeply nested and crossed effects",
    "text": "11.5 Deeply nested and crossed effects\nHave a look back at the original diagram showing the layout of the RIKZ data and the dataset itself. Every site within each beach is associated with only one observation for each of the variables (e.g. species richness). As such, we used mixed-effects modeling to account for the variance among these five observations within each of the five beaches. But what about if each of those sites additionally included multiple samples (e.g. measurements of morphological traits of multiple individuals of a species), as in the diagram below?. We would the need to account for the variance both within sites and within beaches.\n\n\n\nDiagrammatic representation of what the RIKZ data would look like if it were more deeply nested (i.e., if each site had multiple samples taken)\n\n\nThankfully, lmer allows us to do this quite easily. To account for variation within sites and within beaches, we would need to modify our existing random effect term. We would write this as (1|Beach/Site), which means “Site nested within beach”. This expands to — and can also be written as (1|Beach) + (1|Beach:Site). Thus, we are modeling a separate intercept for every beach and for every site within every beach.\nIt is worth emphasizing that the ‘Site 1’ from ‘Beach 1’ is not the same as ‘Site 1’ from ‘Beach 2’ and this was true of the original data as well. These sites are distinct; despite carrying the same label, they are occurring on distinct beaches and are thus not the same site. This is what makes the data nested.\nAn alternative design to this would be crossed effects, in which any site could be observed on any beach (sometimes, this is called “multiple membership”). If all sites were observed on all beaches, this would be a fully crossed effect whereas if some sites were observed on only some beaches, this would be a partially crossed effect. If every site occurred on every beach, then we would code this as (1|Beach) + (1|Site). Crossed effects are shown in the diagram below. This may sound confusing; the easiest way to think about it is that if the data are not nested, they are crossed.\n\n\n\nDiagrammatic representation of what the RIKZ dataset would look like if it were fully crossed (i.e., if every site occurred on every beach)\n\n\n\n11.5.1 Example\nWe will illustrate deeply nested and crossed random effects in a single model using data from a greenhouse experiment conducted at UTM. The researchers were interested in understanding whether soil microbes collected within populations of invasive species can help a second generation of invasive plants perform better. This is called plant-soil feedbacks (PSF), in which plants influence the soil that they grow in, which in turn benefits future generations of the same species. The researchers were especially interested to see if PSF can occur even soils collected from the edge of the species’ range, at which the invader is still in the process of spreading.\nTo do this, soil was collected from 14 sites in a subarctic region. These 14 sites were found in 3 main location: the town, the airport, and the regional studies centre, which was about 20 km away. At each site, soil was collected from within an existing population of invasive Linaria vulgaris (common toadflax) and about 2 meters away outside of the population. Then, soil was left live to preserve microbes, or sterilized. This produced four treatments:\n\nLive invaded soil (microbes from invaded soils present)\nSterile invaded soil\nLive uninvaded soil (microbes from uninvaded soils present)\nSterile uninvaded soil\n\nThe authors collected L. vulgaris seeds from two sources, germinated them and measured their initial height before transplant into soil treatments. In the greenhouse, L. vulgaris was randomly assigned to a soil treatment and individually planted, with soil coming from one of 14 sites, found in one of 3 locations. Plants were grown in soil treatments for 6 weeks, and harvested plant material at the end of the experiment to weigh, and biomass (g) was recorded for each plant (\\(n = 224\\)).\nIs this nested or crossed? Or both? Soil was collected only from one site, and each site was only part of one location (sites nested within location). Given that sites could contain soil that is more similar to soil within a location (site effect), and the same is true for location (location effect), this non-independence needs to be accounted for. However, seeds from both seed sources (1 and 2) were planted in soils from all sites, and seeds from one source may be more similar to each other than seeds from the second source.\nThe code below provides a useful way of examining the data to assess whether terms are nested or crossed and then fits a mixed-effects model to the biomass data. Note that this data is currently in the process of manuscript submission - to protect privacy, some data have been redacted and modified2.\nThe question we are interested in here is:\nDoes invasion status, sterilization, or the interaction influence growth of L. vulgaris plants?\nLoad in the data with this code:\n\nbiomass &lt;- read_csv(\"biomass.csv\", col_names = TRUE)\n\nLet’s quickly clean up and inspect this data.\n\nbiomass &lt;- biomass %&gt;% \n  mutate(invaded_status = as.factor(invaded_status),\n         sterile_status = as.factor(sterile_status),\n         site_number = as.numeric(site_number)) %&gt;% \n    filter(!is.na(biomass))\n\nglimpse(biomass)\n\nRows: 224\nColumns: 9\n$ ID             &lt;dbl&gt; 2, 17, 23, 46, 78, 93, 101, 106, 129, 157, 159, 163, 17…\n$ replicate      &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 1, 1, 1…\n$ site_number    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2…\n$ invaded_status &lt;fct&gt; uninvaded, invaded, invaded, uninvaded, invaded, uninva…\n$ sterile_status &lt;fct&gt; sterile, sterile, live, live, sterile, live, live, ster…\n$ location       &lt;chr&gt; \"town\", \"town\", \"town\", \"town\", \"town\", \"town\", \"town\",…\n$ source         &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1…\n$ initial_height &lt;dbl&gt; 1.145, 0.610, 0.660, 0.510, 0.680, 0.620, 0.660, 1.020,…\n$ biomass        &lt;dbl&gt; 0.23665, 0.25730, 0.16635, 0.01900, 0.16855, 0.00075, 0…\n\nhead(biomass)\n\n# A tibble: 6 × 9\n     ID replicate site_number invaded_status sterile_status location source\n  &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;          &lt;fct&gt;          &lt;chr&gt;     &lt;dbl&gt;\n1     2         1           1 uninvaded      sterile        town          1\n2    17         1           1 invaded        sterile        town          1\n3    23         1           1 invaded        live           town          1\n4    46         1           1 uninvaded      live           town          1\n5    78         2           1 invaded        sterile        town          2\n6    93         2           1 uninvaded      live           town          2\n# ℹ 2 more variables: initial_height &lt;dbl&gt;, biomass &lt;dbl&gt;\n\n\nNow let’s have a look at how sites break down across the locations. We can do this using dplyr or cross-tabulation.\n\nbiomass %&gt;% \n  group_by(site_number, location) %&gt;%\n  tally() %&gt;%\n  spread(site_number, n)\n\n# A tibble: 3 × 15\n  location   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`\n  &lt;chr&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 airport     NA    NA    16    NA    NA    NA    NA    NA    16    16    16\n2 centre      NA    NA    NA    16    16    NA    NA    NA    NA    NA    NA\n3 town        16    16    NA    NA    NA    16    16    16    NA    NA    NA\n# ℹ 3 more variables: `12` &lt;int&gt;, `13` &lt;int&gt;, `14` &lt;int&gt;\n\n# cross tabulation\nxtabs(~ site_number + location, biomass)\n\n           location\nsite_number airport centre town\n         1        0      0   16\n         2        0      0   16\n         3       16      0    0\n         4        0     16    0\n         5        0     16    0\n         6        0      0   16\n         7        0      0   16\n         8        0      0   16\n         9       16      0    0\n         10      16      0    0\n         11      16      0    0\n         12      16      0    0\n         13      16      0    0\n         14       0      0   16\n\n\nAs you can see above, each site occurs in only one location. This is an indication that the data are nested (i.e., site is nested within location).\nNow, let’s look at how seeds from the two different sources were planted into soil from sites:\n\nbiomass %&gt;% \n  group_by(site_number, source) %&gt;%\n  tally() %&gt;%\n  spread(source, n)\n\n# A tibble: 14 × 3\n# Groups:   site_number [14]\n   site_number   `1`   `2`\n         &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n 1           1     8     8\n 2           2     8     8\n 3           3     8     8\n 4           4     8     8\n 5           5     8     8\n 6           6     8     8\n 7           7     8     8\n 8           8     8     8\n 9           9     8     8\n10          10     8     8\n11          11     8     8\n12          12     8     8\n13          13     8     8\n14          14     8     8\n\nxtabs(~ site_number + source, biomass)\n\n           source\nsite_number 1 2\n         1  8 8\n         2  8 8\n         3  8 8\n         4  8 8\n         5  8 8\n         6  8 8\n         7  8 8\n         8  8 8\n         9  8 8\n         10 8 8\n         11 8 8\n         12 8 8\n         13 8 8\n         14 8 8\n\n\nIn this case, we see that seeds from both sources occur on every site equally. In other words, seed source and site are fully crossed.\nRemember also that we collected initial height data, as seeds will vary in their growth ability. Let’s now look at variation in initial height across the sites.\n\nbiomass %&gt;% \n  group_by(site_number, location) %&gt;%\n  ggplot(aes(x = site_number, y = initial_height, colour = location)) +\n  geom_point(position = \"jitter\") +\n  stat_summary(fun = \"mean\", colour = \"black\") # add mean\n\nWarning: Removed 14 rows containing missing values (`geom_segment()`).\n\n\n\n\n\nContinuous variables should not be treated as a random effect. This is because it makes no sense to ask what the variance is across a continuous variable. Instead, what you are doing is forcing a continuous variable to be used as a categorical effect, and estimating a random intercept for each value of the continuous variable. So, if we were to make initial_height as a random effect, we would be estimating an intercept and/or slope for each level of initial_height, which is not informative. Continuous variables are often modeled as fixed effects if we believe they are important to the model due to the methods or data collection. In this case, initial heights of plants did vary, from 1.6 cm to 0.2 cm, so we are choosing to include it in our model.\nLet’s go ahead and fit our model. Note that this is a random intercept model, not a random slope-intercept model (i.e., the slope is the same throughout sites).\n\nbiomass_model &lt;- lmer(biomass ~ invaded_status * sterile_status +\n                        initial_height + # initial height as a fixed effect\n                        (1|source) + # seed source fully crossed\n                        (1|location/site_number), # site nested within location\n                      data = biomass)\nsummary(biomass_model)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: biomass ~ invaded_status * sterile_status + initial_height +  \n    (1 | source) + (1 | location/site_number)\n   Data: biomass\n\nREML criterion at convergence: -409.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.7256 -0.6464 -0.1958  0.4352  7.9139 \n\nRandom effects:\n Groups               Name        Variance  Std.Dev.\n site_number:location (Intercept) 0.0001069 0.01034 \n location             (Intercept) 0.0001071 0.01035 \n source               (Intercept) 0.0001893 0.01376 \n Residual                         0.0080960 0.08998 \nNumber of obs: 224, groups:  site_number:location, 14; location, 3; source, 2\n\nFixed effects:\n                                               Estimate Std. Error        df\n(Intercept)                                    -0.01735    0.02637  21.30047\ninvaded_statusuninvaded                         0.04866    0.01701 205.67953\nsterile_statussterile                           0.04770    0.01701 205.67177\ninitial_height                                  0.11666    0.02499 216.68478\ninvaded_statusuninvaded:sterile_statussterile  -0.07576    0.02410 206.42001\n                                              t value Pr(&gt;|t|)    \n(Intercept)                                    -0.658  0.51767    \ninvaded_statusuninvaded                         2.860  0.00467 ** \nsterile_statussterile                           2.804  0.00553 ** \ninitial_height                                  4.669 5.31e-06 ***\ninvaded_statusuninvaded:sterile_statussterile  -3.144  0.00191 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) invdd_ strl_s intl_h\ninvdd_sttsn -0.306                     \nstrl_sttsst -0.314  0.491              \ninitil_hght -0.769 -0.015 -0.003       \ninvdd_stt:_  0.185 -0.707 -0.707  0.051\n\n\nNotice we are now estimating 3 random-effect variance components. First, we are estimating the variance among seed sources, which explains the most residual variance among the random effects (\\(=0.0001893\\)). We also estimate the variance between locations (\\(=0.0001071\\)) and sites (\\(=0.0001069\\)). Note that our random effects explained very little of the residual variance; for example, seed source only explained 2.2% of the variance after accounting for fixed effects.\nFrom the fixed effects, we see that the initial height of plants was a strong predictor of the final plant biomass. Additionally, the soil sterilization treatment (sterile_status) and the invasion history (invasion_status) had a significant effect on final biomass of the plant. In fact, the effect of invasion depended on the sterilization treatment (invaded_status:sterile_status interaction). While we will not go into the details of these results here3, this example serves to illustrate how nested and crossed random effects are encoded.\n\n\n11.5.2 Checking nested versus crossed data\nYou can use the sjmisc package to check whether groups are nested, fully crossed, or cross-classified (partially crossed). The recommendation is that you inspect your data first, and make sure you understand the nested and/or crossed design, and then check using the following functions:\n\nlibrary(sjmisc)\n\n\nis_nested(biomass$site_number, biomass$location) # returns \"TRUE\" if nested\n\n'f1' is nested within 'f2'\n\n\n[1] TRUE\n\nis_crossed(biomass$source, biomass$location) # returns \"TRUE\" if fully crossed\n\n[1] TRUE\n\nis_cross_classified(biomass$source, biomass$location) # returns \"TRUE\" if partially crossed\n\n[1] FALSE\n\n\nIt is very important to understand the experimental, study, or survey design, because this and this alone determines the random effects that should be included."
  },
  {
    "objectID": "lec09-mixed-effects-models.html#additional-readings",
    "href": "lec09-mixed-effects-models.html#additional-readings",
    "title": "11  Linear mixed models",
    "section": "11.6 Additional readings",
    "text": "11.6 Additional readings\n\nZuur, A. et al. 2009. Mixed effects models and extensions in ecology with R. Springer\nDouglas Bates, Martin Mächler, Ben Bolker, and Steve Walker. 2015. Fitting linear mixed-effects models using lme4. Journal of Statistical Software 67: 1-48.\nFitzpatrick, C. R., Mustafa, Z., and Viliunas, J. Soil microbes alter plant fitness under competition and drought. Journal of Evolutionary Biology 32: 438-450.\nThis Stack Exchange post reagrding crossed vs. nested random effects\nThis blog post on the use of fixed vs. random effects\nThis article on the use of LMMs in quantitative genetics"
  },
  {
    "objectID": "lec09-mixed-effects-models.html#footnotes",
    "href": "lec09-mixed-effects-models.html#footnotes",
    "title": "11  Linear mixed models",
    "section": "",
    "text": "To emphasize the fact the random effects come from a population (distribution) of such effects, this term is used.↩︎\nVicki is happy to talk more about this research if you are interested!↩︎\nPlant-soil feedbacks can be complicated!↩︎"
  },
  {
    "objectID": "lec10-model-selection.html#lesson-preamble",
    "href": "lec10-model-selection.html#lesson-preamble",
    "title": "12  Model selection",
    "section": "12.1 Lesson preamble",
    "text": "12.1 Lesson preamble\n\n12.1.1 Learning objectives\n\nUnderstand the difference between likelihood estimation/inference and model selection\nDevelop familiarity with common model selection approaches\nUnderstand intuition for and the use of common information theoretic model selection criteria\nPerform model selection on the RIKZ data from last class\n\n12.1.2 Lesson outline\n\nBrief intro to model selection (20 mins)\nUnderstanding information theoretic criteria (20 mins)\nModel selection of RIKZ dataset (40 mins)\nModel dredging and averaging (30 mins)\n\n12.1.3 Required packages\n\ntidyverse\nlme4\nlmerTest\nMuMIn\nglmnet"
  },
  {
    "objectID": "lec10-model-selection.html#model-selection-theory",
    "href": "lec10-model-selection.html#model-selection-theory",
    "title": "12  Model selection",
    "section": "12.2 Model selection: theory",
    "text": "12.2 Model selection: theory\nStatistics is about making decisions under uncertainty. Estimation involves deciding what parameter values were most likely to (under a model of the distribution of the data and, in some circumstances, any prior knowledge about the parameters) have given rise to that data. Inference involves quantifying the uncertain around our estimates using confidence intervals. As we have seen, however, to any confidence interval there is an associated hypothesis test. Inference and hypothesis testing, then, involve deciding if a particular set of parameter values could have plausibly given rise to the data.\nIt should come as no surprise, since both estimation and inference involve decision making, that in statistics we often interested in deciding if the models we build are appropriate descriptions of how the world works (given the data we have and use to fit those models), or what among a set of candidate models is the “best”. This practice is called model selection and is the focus of our lecture today.\n\n12.2.1 Examples\n\nHypothesis testing is a kind of model selection. For example, for data \\(x_1,\\dots,x_n \\sim f(x|\\theta)\\), testing \\(H_0 \\colon \\theta = \\theta_0\\) vs \\(H_1 \\colon \\theta \\neq \\theta_0\\) is equivalent to choosing between models \\(\\mathcal{M}_0 \\colon f(x|\\theta)\\) and \\(\\mathcal{M}_1 \\colon f(x|\\theta_0)\\).\nSuppose we regress \\(y\\) on the 1st, 2nd, \\(\\dots\\), \\(p\\)th powers of a covariate \\(x\\):\n\n\\[y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_p x^p.\\]\nThis gives rise to a sequence of models \\(\\mathcal{M}_1,\\mathcal{M}_2,\\dots,\\mathcal{M}_p\\) of the data generative process. Which model is the best description of the data? Although the full model is more flexible in that it has more parameters than, say, the model in which all second and higher order terms are \\(=0\\), it is more prone to overfitting. Choosing between a sequence of nested linear models like this is a classic model selection problem.\n\nSuppose we would like to model the relationship between expression (i.e., the number of transcripts produced) of each coding gene in the human genome and, say, height. If there are \\(p\\) genes for which we have measurements and only \\(n \\ll p\\) observations, it is not possible to fit a linear model of the form\n\n\\[y_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_p x_{pi}.\\]\n(The reason is that there are infinitely many solutions to the likelihood maximization problem.) In this case, we might want to select a subset of the covariates which best explain the available data. Of \\(x_1,\\dots,x_p\\), which are most informative? This too is a kind of model selection problem. We will discuss methods to do “regularized” regression and feature selection (e.g., ridge and LASSO regression) today."
  },
  {
    "objectID": "lec10-model-selection.html#common-model-selection-techniques",
    "href": "lec10-model-selection.html#common-model-selection-techniques",
    "title": "12  Model selection",
    "section": "12.3 Common model selection techniques",
    "text": "12.3 Common model selection techniques\n\nAIC and related methods (BIC, \\(C_p\\))\nCross validation\nRegularized regression (ridge, LASSO, elastic net)\nStepwise regression\n\n\n12.3.1 The Akaike information criterion (AIC)\nSuppose we have data \\(y_1,\\dots,y_n\\) that are drawn from a distribution \\(f\\) and a set of candidate models\n\\[\\mathcal{M}_j = \\{p_j(y|\\theta_j)\\}.\\]\nIt is possible under this setup to find the maximum likelihood estimators for each of the candidate models; it is, however, difficult to compare these models in that the parameters underlying each model might not match (i.e., the models may not be nested). The Akaike information criterion overcomes this issue, and despite having a lengthy and complicated derivation, is a metric which balances two things: 1) the goodness-of-fit of the model and 2) the number of parameters fitted.\nThe intuition and formula for the AIC is as follows. If \\(\\hat{\\theta}_{j,\\text{MLE}}\\) is the MLE for \\(\\theta\\) under model \\(\\mathcal{M}_j\\), then we can measure the distance between the ground truth (i.e., distribution \\(f\\) of the data) and fitted model \\(\\hat{p}_j = p_j(y|\\hat{\\theta}_{j,\\text{MLE}})\\) using a metric called the Kullback-Leibler divergence:\n\\[D_{KL}(p, \\hat{p}_j) = \\int p(y) \\log p(y) \\text{d} y - \\int p(y) \\log \\hat{p_j}(y) \\text{d} y. \\]\nMinimizing the Kullback-Leibler divergence (distance) between the ground truth and density \\(j\\) is a principled way to preform model selection, and forms the basis for the AIC. Note that minimizing only involves the second integral, and we can estimate the integral with an average\n\\[\\frac{1}{n} \\log L(\\hat{\\theta}_{j,\\text{MLE}}) = \\frac{1}{n} \\sum_{i=1}^n \\log p(y_i|\\hat{\\theta}_{j,\\text{MLE}})\\]\nImportantly, AIC corrects for the fact this is an unbiased estimator of the divergence by adding \\(d_j/n\\), where \\(d_j = \\text{dim}(\\mathcal{M}_j)\\). This term is what penalizes models with a large number of parameters. So,\n\\[\\text{AIC} = - 2 \\log L(\\hat{\\theta}_{j,\\text{MLE}}) + 2 d_j.\\]\nNotice we have multiplied the preceding quantities by \\(-2n\\) to get the above expression; this does not change anything, and is largely for historical reasons. Based on the AIC expression, it is clear 1) the higher the likelihood, the lower the AIC; 2) introducing more parameters into the model without changing the likelihood results in a greater value for the AIC. The balance between goodness-of-fit (likelihood) and the number of parameters (the potential to overfit) is what AIC tries to optimize in choosing between candidate models. As we have shown here, that balance is struck by minimizing the distance between candidate models and the ground truth while correcting for bias introduced by having models of different dimensions.\n\n\n12.3.2 \\(\\text{AIC}_c\\) for small sample sizes\nIt is sometimes convenient to, when sample sizes are small (\\(n &lt; 40\\) is a commonly used rule-of-thumb) use the following metric to choose between candidate models:\n\\[\\text{AIC}_c = \\text{AIC} + \\frac{2d_j(d_j+1)}{n-d_j-1}\\]"
  },
  {
    "objectID": "lec10-model-selection.html#stepwise-and-regularized-regression",
    "href": "lec10-model-selection.html#stepwise-and-regularized-regression",
    "title": "12  Model selection",
    "section": "12.4 Stepwise and regularized regression",
    "text": "12.4 Stepwise and regularized regression\nWe will return to model selection via AIC later in the lecture, but in the meantime we will discuss two kinds of feature selection techniques that can help us choose between covariates in a linear model."
  },
  {
    "objectID": "lec10-model-selection.html#stepwise-regressionand-why-you-should-not-use-it.",
    "href": "lec10-model-selection.html#stepwise-regressionand-why-you-should-not-use-it.",
    "title": "12  Model selection",
    "section": "12.5 Stepwise regression…and why you should not use it.",
    "text": "12.5 Stepwise regression…and why you should not use it.\nStepwise selection involves iteratively adding and removing predictors according to some criterion (e.g., variance explained). The steps for forward stepwise regression are as follows:\n\nLet \\(\\mathcal{M}_0\\) be a null model, i.e., one with no predictors.\nFor \\(k=0,1,\\dots,p-1\\), fit all models which add one predictor to those in model \\(\\mathcal{M}_k\\).\nChoose the best among the \\(p-k\\) models in step (2) and call it model \\(\\mathcal{M}_{k+1}\\). This is done according to some criterion. We might, e.g., choose the model with the greatest \\(R^2\\).\nSelect from models \\(\\mathcal{M}_0,\\dots,\\mathcal{M}_p\\) using, for example, AIC.\n\nWe, however, urge you to AVOID stepwise selection and to be critical of analyses which use it. No statistics can be a substitute for good theory which is informative about the predictors which have predictive power because they are biologically meaningful. When choosing between models, careful consideration of the what predictors are informative and why is key.\nFor more on the problems with automated stepwise selection methods, see here."
  },
  {
    "objectID": "lec10-model-selection.html#regularized-regression",
    "href": "lec10-model-selection.html#regularized-regression",
    "title": "12  Model selection",
    "section": "12.6 Regularized regression",
    "text": "12.6 Regularized regression\nA more principled approach to linear model selection is so-called regularized regression. These approaches constrain the least squares problem (i.e., maximizing the likelihood function with respect to the regression coefficients) in some way. These constraints help “pull” the regression coefficients to zero and filter the features which are most informative about the response. The induced sparsity (i.e., many of the regression coefficients being \\(=0\\)) is ideal in that it helps improve the interpretability and predictive power of the model.\nDifferent types of constraint “regularize” the regression coefficients in different ways. Ridge regression fits the coefficients of a linear model \\(\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\) subject to \\(||\\boldsymbol{\\beta}||_2^2 = \\beta_0^2 + \\beta_2^2 + \\cdots + \\beta_p^2 \\leqslant t.\\) The LASSO fits the same model, but subject to \\(||\\boldsymbol{\\beta}||_1 = |\\beta_0| + |\\beta_2| + \\cdots + |\\beta_p| \\leqslant t.\\) While ridge regression pulls coefficients close to zero, LASSO forces the fitted model to be sparse (i.e., most coefficients are exactly zero).\nThere are several packages which fit models subject to the ridge and LASSO constraints in R. The below example shows how to implement LASSO regression for simulated data using the function glmnet. (The syntax is similar for ridge and other forms of regularized regression.)\n\np &lt;- 7; n &lt;- 200\nbeta &lt;- c(-10,0,2.5,5,10,25,50)\nx &lt;- matrix(rnorm(n*p, mean = 0, sd = 0.1), nrow = n, ncol = p)\ny &lt;- x %*% beta + rnorm(n, mean = 0, sd = 1)\n\ndata &lt;- as.data.frame(cbind(y = y, x = x))\ncolnames(data) &lt;- c(\"y\", paste(\"x\", 1:p, sep=\"\"))\n\nfit_lasso &lt;- glmnet(x, y, alpha = 1) # alpha = 0 gives ridge constraint\nplot(fit_lasso, xvar = \"lambda\", label = TRUE)\n\n\n\n# solution path shows how size of fitted coefficients changes as a function of the penalty\n# increasing lambda and t both result in more stringent constraint on the coeffs\n\nHow to choose \\(\\lambda\\) (or, equivalently, \\(t\\))? Extreme values force the regression coefficients to be all \\(=0\\), but small values may give rise to an over-saturated model with too many predictors. There are a couple ways to decide what constraint is best to use, but cross-validation is quite popular. This involves splitting the data into “training” and “test” data and determining for which value of \\(\\lambda\\) the model minimizes, say, the mean squared error. The following implements so-called leave-one-out cross-validation:\n\nlambda_values &lt;- exp(seq(log(0.001), log(1000), length = 100))\ncross_val_lasso &lt;- cv.glmnet(x, y, alpha = 1, nfolds = 10, lambda = lambda_values)\nplot(cross_val_lasso)\n\n\n\nlasso_predictions &lt;- predict(cross_val_lasso, \n                             s = cross_val_lasso$lambda.min,\n                             type = \"coefficients\")\nlasso_VS_truth &lt;- matrix(cbind(beta, lasso = lasso_predictions[-1]), ncol = 2)\ncolnames(lasso_VS_truth) &lt;- c(\"true coeff\",\"fitted coeff\")\nrownames(lasso_VS_truth) &lt;- NULL\n\nknitr::kable(lasso_VS_truth)\n\n\n\n\ntrue coeff\nfitted coeff\n\n\n\n\n-10.0\n-8.8955045\n\n\n0.0\n-0.8041639\n\n\n2.5\n3.3456484\n\n\n5.0\n5.9103302\n\n\n10.0\n11.4068502\n\n\n25.0\n27.3250980\n\n\n50.0\n49.4104262\n\n\n\n\n\nIt appears the value of \\(\\lambda\\) that is ideal for the problem at hand is so small that LASSO regression returns the same coefficients as un-constrained least squares, i.e., the penalty does not affect estimation of the coefficients too much! The data we have used here are cherry-picked to illustrate how the method works with relative ease. Implementation of the LASSO is a bit trickier with more features. However, when you need to preform feature selection on a large number of covariates and have limited data (\\(p\\) large), a combination of cross-validation and LASSO regression is the way to go."
  },
  {
    "objectID": "lec10-model-selection.html#model-selection-for-lmms",
    "href": "lec10-model-selection.html#model-selection-for-lmms",
    "title": "12  Model selection",
    "section": "12.7 Model selection for LMMs",
    "text": "12.7 Model selection for LMMs\nModel selection for mixed models is even more complicated, due to the addition of random effects. A procedure that is commonly used to preform selection of LMMs is as follows:\n\nCreate a saturated model that includes all fixed effects (and their interactions1) and random effects you wish to test.\nOptimize the random-effect structure by comparing models with the same saturated fixed effects structure but differing random effect structures. These models should be fitted using Restricted Maximum Likelihood (i.e., REML = TRUE), since varience component estimation will be biased otherwise. The optimal random effect structure is the one that provides the lowest AIC. Do not remove random effects if they are included to account for non-independence in your data. Skip this step if random effects must be included given the hierarchical structure (i.e., nestedness) in the data.\nOptimize the fixed-effect structure by comparing models with the same optimized (or necessary) random effects but with differing fixed effect structures. These models should be fitted with Maximum Likelihood (i.e., REML = FALSE) to prevent biased fixed-effect parameter estimates. Models can be selected on the basis of AIC (the lower the better), by comparing nested models using a likelihood ratio test (LRT), or both.\nFit the final model with optimized fixed and random effects using REML and interpret your results.\n\nWe can apply this procedure to the RIKZ data as follows.\n\nrikz_data &lt;- read_csv(\"rikz_data.csv\")\n\n\n12.7.1 Step 1: Create saturated model\nLet’s recreate the random intercept and random intercept-slope models we created last class, and add in Exposure and the interaction between NAP and Exposure as additional fixed effects.\n\n# Set `REML=TRUE` in anticipation of step 2 \nmixed_model_IntOnly &lt;- lmer(Richness~NAP*Exposure+(1|Beach), REML=TRUE, data=rikz_data)\n\nmixed_model_IntSlope &lt;- lmer(Richness~NAP*Exposure+(1+NAP|Beach), REML=TRUE, data=rikz_data)\n\n\n\n12.7.2 Step 2: Optimize random-effect structure\nTo optimize the random effects, we compare mixed_model_IntSlope with mixed_model_IntOnly. This will determine whether including a random slope for each beach improves the fit of the model to the observed data, and we couldn’t decide otherwise because there isn’t any prior research on whether we should expect the species richness and NAP relationship to differ by beach. We are NOT testing the mixed_model_IntOnly model against one in which there are no random effects since including a random intercept for each beach is required to account for the non-independence in the data.\nLet’s get the \\(\\text{AIC}_c\\) for our two models are given by:\n\nAICc(mixed_model_IntOnly, mixed_model_IntSlope)\n\n                     df     AICc\nmixed_model_IntOnly   6 235.2327\nmixed_model_IntSlope  8 237.2527\n\n\nBased on the above, the random intercept only model is a better fit to the data (\\(\\text{AIC}_c\\) is lower by 2.0157 units2). The optimal random-effect structure is thus one that includes only a random intercept for each beach but not a random slope.\n\n\n12.7.3 Step 3: Optimize the fixed effect structure\nWe now need to refit the model with the optimal random-effect structure using ML and compare different fixed effect structures. We will construct a series of models that captures all possible combinations of fixed effects terms in our saturated model in order to find the best one. Let’s fit these models below and check their \\(\\text{AIC}_c\\)s. Don’t forget to include a model that doesn’t have any fixed effects!\n\n# Full model with both fixed effects and their interaction\nmixed_model_IntOnly_Full &lt;- lmer(Richness~NAP*Exposure+(1|Beach), \n                                 REML=F, data=rikz_data)\n\n# No interaction\nmixed_model_IntOnly_NoInter &lt;- lmer(Richness~NAP+Exposure+(1|Beach), \n                                    REML=F, data=rikz_data)\n\n# No interaction or main effect of exposure\nmixed_model_IntOnly_NAP &lt;- lmer(Richness~NAP+(1|Beach), REML=F, data=rikz_data)\n\n# No interaction or main effect of NAP\nmixed_model_IntOnly_Exp &lt;- lmer(Richness~Exposure+(1|Beach), REML=F, data=rikz_data)\n\n# No fixed effects\nmixed_model_IntOnly_NoFix &lt;- lmer(Richness~1+(1|Beach), REML=F, data=rikz_data)\n\nAICc(mixed_model_IntOnly_Full, mixed_model_IntOnly_NoInter,\n    mixed_model_IntOnly_NAP, mixed_model_IntOnly_Exp,\n    mixed_model_IntOnly_NoFix)\n\n                            df     AICc\nmixed_model_IntOnly_Full     6 236.5947\nmixed_model_IntOnly_NoInter  5 238.1467\nmixed_model_IntOnly_NAP      4 250.8291\nmixed_model_IntOnly_Exp      4 261.7996\nmixed_model_IntOnly_NoFix    3 269.8889\n\n\nWow, that was a lot of typing! And we only have two predictors plus an interaction! We can see that this can easily get out of hand when the model structure becomes more complex. But fear not! Obviously there is a package with a function that deals with this exact situation. Our hero today is the MuMIn package and the dredge function, which automates this dropping-of-terms process and summarizes and outputs all of the model results in one table. It is highly flexible, you can customize the criterion used (AIC, BIC, etc.), how the output is reported, what’s included in the output (e.g., do you want F-stats and R2 to be included?), whether some terms should be represented in all models, and even only include some terms in models if other terms are included (a.k.a. dependency chain). So handy!!! In fact, soooooooooo handy, it is the perfect tool for \\(p\\)-hacking… but … we can trust one another, right???\n\n# Argument `na.action=\"na.fail\"` is required for `dredge` to run\nmixed_model_IntOnly_Full &lt;- lmer(Richness~NAP*Exposure+(1|Beach), REML=F, \n                                 data=rikz_data, na.action=\"na.fail\")\n\nmixed_model_dredge &lt;- dredge(mixed_model_IntOnly_Full, rank=AICc)\nmixed_model_dredge\n\nGlobal model call: lmer(formula = Richness ~ NAP * Exposure + (1 | Beach), data = rikz_data, \n    REML = F, na.action = \"na.fail\")\n---\nModel selection table \n   (Int)    Exp     NAP Exp:NAP df   logLik  AICc delta weight\n8 40.740 -3.340 -13.710   1.072  6 -111.192 236.6  0.00  0.684\n4 37.290 -2.999  -2.725          5 -113.304 238.1  1.55  0.315\n3  6.584         -2.576          4 -120.915 250.8 14.23  0.001\n2 37.860 -3.147                  4 -126.400 261.8 25.20  0.000\n1  5.689                         3 -131.652 269.9 33.29  0.000\nModels ranked by AICc(x) \nRandom terms (all models): \n  1 | Beach\n\n\nAnd voila! SO. MUCH. FASTER. And look! Isn’t this table just wonderful???\nBased on the output above, it looks like the model that includes NAP, Exposure, and their interaction provides the overall best fit to the data. However, this model is indistinguishable from the model without the interaction term (\\(\\Delta \\text{AIC}_c=1.552\\)).\nLet’s try and see if we can resolve this using a likelihood ratio test (LRT), which we learned about in the introductory statistics lecture. Recall that the LR test statistic has an approximate \\(\\chi^2\\) distribution with degrees of freedom equal to the difference in dimensional between nested models.\n\nanova(mixed_model_IntOnly_Full, mixed_model_IntOnly_NoInter)\n\nData: rikz_data\nModels:\nmixed_model_IntOnly_NoInter: Richness ~ NAP + Exposure + (1 | Beach)\nmixed_model_IntOnly_Full: Richness ~ NAP * Exposure + (1 | Beach)\n                            npar    AIC    BIC  logLik deviance Chisq Df\nmixed_model_IntOnly_NoInter    5 236.61 245.64 -113.30   226.61         \nmixed_model_IntOnly_Full       6 234.38 245.22 -111.19   222.38 4.224  1\n                            Pr(&gt;Chisq)  \nmixed_model_IntOnly_NoInter             \nmixed_model_IntOnly_Full       0.03986 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAh-Ha! With \\(p\\)-value of 0.040, we reject the null hypothesis (i.e., the simpler model is a better description of the data) in favor of alternative hypothesis (the more complex model!).\n\n\n12.7.4 Step 4: Interpret model output\nFinally, let’s re-fit the best-fitting model via REML:\n\nmixed_model_IntOnly_Full2 &lt;- lmer(Richness~NAP*Exposure+(1|Beach), \n                                  REML=T, data=rikz_data)\nsummary(mixed_model_IntOnly_Full2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: Richness ~ NAP * Exposure + (1 | Beach)\n   Data: rikz_data\n\nREML criterion at convergence: 221\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.4138 -0.4394 -0.1063  0.1511  4.3635 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Beach    (Intercept) 0.3063   0.5534  \n Residual             8.7447   2.9571  \nNumber of obs: 45, groups:  Beach, 9\n\nFixed effects:\n             Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)   40.7149     5.6164   7.9362   7.249 9.17e-05 ***\nNAP          -13.5864     5.4298  36.5654  -2.502 0.016947 *  \nExposure      -3.3385     0.5485   7.9972  -6.087 0.000294 ***\nNAP:Exposure   1.0625     0.5278  36.6843   2.013 0.051504 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) NAP    Exposr\nNAP         -0.300              \nExposure    -0.996  0.301       \nNAP:Exposur  0.302 -0.997 -0.306\n\n\nWe see that increasing both NAP and Exposure results in a significant decrease in species richness (\\(p&lt;0.05\\)). There is also a marginally insignificant interaction between NAP and Exposure which we won’t read too much into (\\(p=0.051\\)!). Finally, while Beach is included in our model as a random effect, notice how little variation is attributed to differences between beaches relative to the model we ran in lecture 7 (~45% of residual variance vs. approx. 0). The only difference is that our current model includes Exposure as a fixed effect. This suggests that much of the variation between beaches in lecture 7 was likely attributable to differences in exposure, which is now being captured by the fixed effects."
  },
  {
    "objectID": "lec10-model-selection.html#model-averaging",
    "href": "lec10-model-selection.html#model-averaging",
    "title": "12  Model selection",
    "section": "12.8 Model averaging",
    "text": "12.8 Model averaging\nIn Assignment 4, you will use data from Santangelo et al. (2019) who were interested in understanding how insect herbivores and plant defenses influence the expression of plant floral traits. While that was one component of the study, the main question was whether herbivores, pollinators, and plant defenses alter the shape and strength of natural selection on plant floral traits. In other words, which of these 3 agents of selection (plant defenses, herbivores, or pollinators) are most important in driving the evolution of floral traits in plants?\nThe motivation for that experiment actually came a few year prior, in 2016, when Thompson and Johnson published an experiment examining how plant defenses alter natural selection on plant floral traits. They found some interesting patterns but it was unclear whether these were being driven by the plant’s interactions with herbivores, pollinators, or both. This was because they didn’t directly manipulate these agents: pollination was not quantified in their experiment and herbivore damage was measured observationally and thus these results were correlative. However, their experimental data provides a prime use of model selection in ecology and evolution.\nThe data consists of 140 observations (rows). Each row in the dataset corresponds to the mean trait value of one plant genotype (they had replicates for each genotype but took the average across these replicates) grown in a common garden. They measured 8 traits and quantified the total mass of seeds produced by plants as a measure of absolute fitness. Genotypes were either “cyanogenic” (i.e., containing plant defenses) or were “acyanogenic” (i.e., lacking plant defenses). In addition, they quantified the amount of herbivore damage (i.e., percent leaf area lost) on each plant twice throughout the growing season, although here we will only focus on the effects of plant defenses and avoid their herbivore damage measurements. We are going to estimate the strength of selection on each trait (controlling for correlations between traits due to the pleiotropic action of genes) and assess whether plant defenses alter the strength or direction of natural selection imposed on these traits. We developed the tools to do this in a previous lecture, where we learned that estimating different kinds of selection can be done by regressing fitness on standardized trait values (i.e., mean of 0 and standard deviation of 1).\nLet’s start by loading in the data.\n\n\nRows: 140\nColumns: 38\n$ Genotype     &lt;dbl&gt; 2, 4, 5, 6, 8, 9, 10, 11, 13, 15, 16, 17, 18, 19, 21, 22,…\n$ cyanide      &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, …\n$ linamarin    &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, …\n$ linamarase   &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, …\n$ VegBiomass   &lt;dbl&gt; 1.16840, 4.42100, 0.79500, 4.44740, 1.17575, 2.70160, 2.4…\n$ LatSpread    &lt;dbl&gt; 283.500, 995.400, 298.000, 1019.450, 271.000, 887.800, 76…\n$ FrstFlwr     &lt;dbl&gt; 2.200000, 11.250000, 7.666667, 32.000000, 8.500000, 6.800…\n$ BnrLgth      &lt;dbl&gt; 6.326250, 6.472500, 5.712500, 5.422500, 5.565000, 5.88625…\n$ BnrWdt       &lt;dbl&gt; 3.568750, 3.700000, 2.961250, 3.222500, 3.350000, 3.29375…\n$ InflHt       &lt;dbl&gt; 23.21750, 20.62000, 20.09500, 25.13500, 14.93000, 19.7116…\n$ InflWdt      &lt;dbl&gt; 22.67250, 17.72833, 21.44250, 25.01000, 15.37000, 19.3916…\n$ InflNum      &lt;dbl&gt; 4.20, 4.00, 3.00, 1.00, 1.75, 3.40, 2.20, 1.50, 1.80, 12.…\n$ FlwrCt       &lt;dbl&gt; 32.75000, 29.50000, 21.50000, 59.25000, 56.00000, 35.4444…\n$ SeedBm       &lt;dbl&gt; 0.030000, 0.038625, 0.004375, 0.014960, 0.025875, 0.03092…\n$ AvgDMG1      &lt;dbl&gt; 43.26000, 21.95000, 43.17500, 21.72000, 30.16250, 27.4000…\n$ AvgDMG2      &lt;dbl&gt; 21.3700, 17.5500, 24.4500, 15.9200, 37.3375, 20.2400, 22.…\n$ VegGrowth    &lt;dbl&gt; -1.12177796, 0.88565970, -1.23438613, 0.92433034, -1.1345…\n$ RFSeed       &lt;dbl&gt; 0.9282121, 1.1950731, 0.1353643, 0.4628684, 0.8005829, 0.…\n$ FrstFlwr.T   &lt;dbl&gt; 1.483240, 3.354102, 2.768875, 5.656854, 2.915476, 2.60768…\n$ InflHt.T     &lt;dbl&gt; 539.0523, 425.1844, 403.8090, 631.7682, 222.9049, 388.549…\n$ SeedBm.T     &lt;dbl&gt; 0.17320508, 0.19653244, 0.06614378, 0.12231108, 0.1608570…\n$ InflNum.T    &lt;dbl&gt; 1.6486586, 1.6094379, 1.3862944, 0.6931472, 1.0116009, 1.…\n$ AvgDmg1.T    &lt;dbl&gt; 6.577233, 4.685083, 6.570769, 4.660472, 5.492040, 5.23450…\n$ AvgDmg2.T    &lt;dbl&gt; 4.622770, 4.189272, 4.944694, 3.989987, 6.110442, 4.49888…\n$ VegGrowth.T  &lt;dbl&gt; 0.7926046, 1.6234715, 0.7180626, 1.6353380, 0.7845251, 1.…\n$ VegBiomass.T &lt;dbl&gt; 1.0809255, 2.1026174, 0.8916277, 2.1088860, 1.0843201, 1.…\n$ LatSpread.T  &lt;dbl&gt; 16.83746, 31.54996, 17.26268, 31.92883, 16.46208, 29.7959…\n$ FlwrCt.S     &lt;dbl&gt; -0.51866068, -0.84578217, -1.65100431, 2.14863763, 1.8215…\n$ BnrLgth.S    &lt;dbl&gt; 0.98570003, 1.28769651, -0.28165272, -0.88048334, -0.5862…\n$ BnrWdt.S     &lt;dbl&gt; 0.91616830, 1.41223922, -1.37993138, -0.39251403, 0.08938…\n$ InflHt.S     &lt;dbl&gt; 0.7190552, -0.3496320, -0.5502468, 1.5892241, -2.2480905,…\n$ InflWdt.S    &lt;dbl&gt; 0.83894602, -0.80610308, 0.42969396, 1.61669129, -1.59078…\n$ FrstFlwr.S   &lt;dbl&gt; -1.54639903, 0.34224409, -0.24854531, 2.66688196, -0.1005…\n$ SeedBm.S     &lt;dbl&gt; 0.08114206, 0.44458767, -1.58689750, -0.71179816, -0.1112…\n$ InflNum.S    &lt;dbl&gt; 0.94431501, 0.85825229, 0.36860437, -1.15238099, -0.45359…\n$ AvgDmg1.S    &lt;dbl&gt; 0.66702301, -1.29690435, 0.66031291, -1.32244855, -0.4593…\n$ AvgDmg2.S    &lt;dbl&gt; -0.29425351, -0.87159469, 0.13449201, -1.13700594, 1.6870…\n$ VegGrowth.S  &lt;dbl&gt; -1.18918210, 0.91239380, -1.37772698, 0.94240864, -1.2096…\n\n\nWe will now generate the global model. Remember, this should be a saturated model with all of the fixed effects and their interactions. We are including the presence of hydrogen cyanide (HCN, cyanide in model below), all standardized traits and the trait by HCN interactions as fixed effects in this model. There are no random effects in this model so we can go ahead and use lm().\n\n# Create saturated model\nGTSelnModel.HCN &lt;- lm(RFSeed ~ VegGrowth.S*cyanide + BnrLgth.S*cyanide +\n                          BnrWdt.S*cyanide + FrstFlwr.S*cyanide + \n                          InflNum.S*cyanide + FlwrCt.S*cyanide + \n                          InflWdt.S*cyanide + InflHt.S*cyanide,\n                      data = Thompson_data)\n\nNext, we will perform our model selection based on AICc (due to low sample sizes). We automate this process using the dredge() function from the MuMIn package. We warned earlier that this dredging approach has been criticized. However, in this case it’s reasonable: we know from work in other systems that all of these traits could conceivably experience selection, and we know that that selection could vary due to plant defenses. In other words, all terms in this model represent biologically real hypotheses. Let’s go ahead and dredge.\n\nGTSelnModel.HCN &lt;- lm(RFSeed ~ VegGrowth.S*cyanide + BnrLgth.S*cyanide + \n                          BnrWdt.S*cyanide + FrstFlwr.S*cyanide + \n                          InflNum.S*cyanide + FlwrCt.S*cyanide + \n                          InflWdt.S*cyanide + InflHt.S*cyanide,\n                      data = Thompson_data, \n                      na.action=\"na.fail\")\n\nGTmodel_dredge &lt;- dredge(GTSelnModel.HCN,\n                         beta = F, # or \"none\" corresponds to unstandardized coefficients\n                         evaluate = T, # evaluate/rank models\n                         rank = AICc) # rank function\n\nLet’s have a look at the first few lines returned by dredge(). Let’s also print out how many models were compared.\n\nhead(GTmodel_dredge)\n\nGlobal model call: lm(formula = RFSeed ~ VegGrowth.S * cyanide + BnrLgth.S * cyanide + \n    BnrWdt.S * cyanide + FrstFlwr.S * cyanide + InflNum.S * cyanide + \n    FlwrCt.S * cyanide + InflWdt.S * cyanide + InflHt.S * cyanide, \n    data = Thompson_data, na.action = \"na.fail\")\n---\nModel selection table \n       (Int)  BnL.S    BnW.S     cyn  FlC.S    FrF.S  InN.S   VgG.S BnL.S:cyn\n3920  0.9783 0.2297 -0.09086 0.04292 0.1671          0.4958 0.07497   -0.3245\n3664  0.9668 0.2597 -0.10380 0.05271 0.1756          0.5099           -0.3513\n2894  0.9895 0.1861          0.02887 0.1721          0.4916 0.09900   -0.1837\n69456 0.9809 0.2151 -0.08489 0.04193 0.1624          0.4992 0.10790   -0.3086\n3936  0.9825 0.2238 -0.09101 0.04230 0.1687 -0.02619 0.4860 0.08231   -0.3215\n2910  0.9943 0.1794          0.02828 0.1739 -0.02989 0.4806 0.10700   -0.1834\n      BnW.S:cyn cyn:FlC.S cyn:VgG.S df  logLik  AICc delta weight\n3920     0.2273    0.2328           11 -85.102 194.3  0.00  0.254\n3664     0.2633    0.2437           10 -86.353 194.4  0.14  0.237\n2894               0.2200            9 -87.650 194.7  0.42  0.206\n69456    0.2391    0.2521   -0.0917 12 -84.648 195.8  1.49  0.121\n3936     0.2238    0.2311           12 -84.880 196.2  1.95  0.096\n2910               0.2181           10 -87.370 196.4  2.18  0.086\nModels ranked by AICc(x) \n\nnrow(GTmodel_dredge)\n\n[1] 6817\n\n\nThe output tells us the original model and then provides a rather large table with many rows and columns. The rows in this case are different models with different combinations of predictors (\\(n = 6,817\\) models). The columns are the different terms from our model, which dredge() has abbreviated. The numbers in the cells are the estimates (i.e. beta coefficients) for each term that is present in the model; blank cells mean that term was not included in the model. The last 5 columns are important: they give us the degrees of freedom for the model (a function of the number of terms in the model), the log-likelihood of the model, the AIC score, the delta AIC, and the AIC weights. The \\(\\Delta AIC\\) is the difference between the AIC score of a model and the AIC score of the top model. The weight can be thought of as the probability that the model is the best model given the candidate set included in the model selection procedure.\nWe can retrieve the top model using the get.models() function and specifying that we want to top model using the subset argument. We need to further subset this output since it returns a list.\n\ntop_model &lt;- get.models(GTmodel_dredge, subset = 1)[[1]]\nsummary(top_model)\n\n\nCall:\nlm(formula = RFSeed ~ BnrLgth.S + BnrWdt.S + cyanide + FlwrCt.S + \n    InflNum.S + VegGrowth.S + BnrLgth.S:cyanide + BnrWdt.S:cyanide + \n    cyanide:FlwrCt.S + 1, data = Thompson_data, na.action = \"na.fail\")\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.20717 -0.24783 -0.02084  0.26346  1.25135 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.97834    0.05049  19.376  &lt; 2e-16 ***\nBnrLgth.S          0.22967    0.05807   3.955 0.000125 ***\nBnrWdt.S          -0.09086    0.05758  -1.578 0.116972    \ncyanide            0.04292    0.08416   0.510 0.610918    \nFlwrCt.S           0.16709    0.05256   3.179 0.001845 ** \nInflNum.S          0.49578    0.04865  10.191  &lt; 2e-16 ***\nVegGrowth.S        0.07497    0.04898   1.531 0.128237    \nBnrLgth.S:cyanide -0.32449    0.11197  -2.898 0.004409 ** \nBnrWdt.S:cyanide   0.22730    0.10567   2.151 0.033313 *  \ncyanide:FlwrCt.S   0.23284    0.09003   2.586 0.010806 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4612 on 130 degrees of freedom\nMultiple R-squared:  0.6198,    Adjusted R-squared:  0.5935 \nF-statistic: 23.55 on 9 and 130 DF,  p-value: &lt; 2.2e-16\n\n\nBut how much evidence do we actually have that this is the best model? We have over 6,000 models, so it’s unlikely that only one model explains the data. From the dredge output we can see there is little difference in the AIC and weights of the first few models.\nIs there really much of a difference between two models who’s AIC differ by only 0.14 points? How do we decide which model(s) to interpret? Statisticians have thought about this problem and it turns out that models with delta AIC (or other criterion) less than 2 are considered to be just as good as the top model and thus we shouldn’t just discount them. In fact, the top 5 models all have AICc scores within 2 units of the top model.\nWe could use likelihood ratio test(s) to try to test if more complex models are better descriptions of the data. Alternatively, we could use the weights: if a model has weight greater or equal to 95% then it is likely to be the top model. We can generate a “credibility” set consisting of all models whose cumulative sum of AIC weights is 0.95. In any case, the point is that we have no good reason to exclude models other than the top one when the next models after it are likely to be just as good.\nTo get around this, we will perform what’s called model averaging. This allows us to average the parameter estimates across multiple models that are performing equally well, and avoids the issue of model uncertainty. Let’s do this below by averaging all models with a delta \\(\\text{AIC}_c \\leqslant 2\\).\n\nsummary(model.avg(GTmodel_dredge, subset = delta &lt;= 2))\n\n\nCall:\nmodel.avg(object = GTmodel_dredge, subset = delta &lt;= 2)\n\nComponent model call: \nlm(formula = RFSeed ~ &lt;5 unique rhs&gt;, data = Thompson_data, na.action = \n     na.fail)\n\nComponent models: \n                      df logLik   AICc delta weight\n1+2+3+4+6+7+8+9+10    11 -85.10 194.27  0.00   0.28\n1+2+3+4+6+8+9+10      10 -86.35 194.41  0.14   0.26\n1+3+4+6+7+8+10         9 -87.65 194.68  0.42   0.23\n1+2+3+4+6+7+8+9+10+11 12 -84.65 195.75  1.49   0.13\n1+2+3+4+5+6+7+8+9+10  12 -84.88 196.22  1.95   0.10\n\nTerm codes: \n          BnrLgth.S            BnrWdt.S             cyanide            FlwrCt.S \n                  1                   2                   3                   4 \n         FrstFlwr.S           InflNum.S         VegGrowth.S   BnrLgth.S:cyanide \n                  5                   6                   7                   8 \n   BnrWdt.S:cyanide    cyanide:FlwrCt.S cyanide:VegGrowth.S \n                  9                  10                  11 \n\nModel-averaged coefficients:  \n(full average) \n                     Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n(Intercept)          0.978654   0.051169    0.051637  18.953  &lt; 2e-16 ***\nBnrLgth.S            0.225054   0.062200    0.062681   3.590  0.00033 ***\nBnrWdt.S            -0.072942   0.064461    0.064835   1.125  0.26058    \ncyanide              0.042088   0.084783    0.085570   0.492  0.62282    \nFlwrCt.S             0.169975   0.052873    0.053364   3.185  0.00145 ** \nInflNum.S            0.497923   0.049505    0.049957   9.967  &lt; 2e-16 ***\nVegGrowth.S          0.066116   0.060037    0.060342   1.096  0.27321    \nBnrLgth.S:cyanide   -0.297258   0.124060    0.124926   2.379  0.01734 *  \nBnrWdt.S:cyanide     0.186530   0.137538    0.138125   1.350  0.17687    \ncyanide:FlwrCt.S     0.235112   0.091214    0.092057   2.554  0.01065 *  \ncyanide:VegGrowth.S -0.012129   0.047872    0.048135   0.252  0.80106    \nFrstFlwr.S          -0.002749   0.015496    0.015604   0.176  0.86018    \n \n(conditional average) \n                    Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n(Intercept)          0.97865    0.05117     0.05164  18.953  &lt; 2e-16 ***\nBnrLgth.S            0.22505    0.06220     0.06268   3.590  0.00033 ***\nBnrWdt.S            -0.09420    0.05800     0.05853   1.609  0.10753    \ncyanide              0.04209    0.08478     0.08557   0.492  0.62282    \nFlwrCt.S             0.16997    0.05287     0.05336   3.185  0.00145 ** \nInflNum.S            0.49792    0.04951     0.04996   9.967  &lt; 2e-16 ***\nVegGrowth.S          0.08921    0.05295     0.05341   1.670  0.09487 .  \nBnrLgth.S:cyanide   -0.29726    0.12406     0.12493   2.379  0.01734 *  \nBnrWdt.S:cyanide     0.24090    0.10646     0.10743   2.242  0.02494 *  \ncyanide:FlwrCt.S     0.23511    0.09121     0.09206   2.554  0.01065 *  \ncyanide:VegGrowth.S -0.09170    0.10015     0.10110   0.907  0.36438    \nFrstFlwr.S          -0.02619    0.04092     0.04131   0.634  0.52601    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe first part of the output breaks down which terms are part of which models and provides descriptive statistics for these models. Parameter estimates from the model averaging are also returned. Note there are two sets of estimates: the full coefficients set terms to 0 if they are not included in the model while averaging, whereas the conditional coefficients ignores the predictors entirely. The full coefficients are thus more conservative and it is best practice to interpret these.\nTo get a sense of what’s going on, a visual might be helpful. (But there are 6000 regressions to deal with! However will we plot them all??) Although the code is admittedly somewhat clunky3, following chunk plots how predicted fitness (RFSeed) changes as a function of one of standardized trait (BnrLgth.S) value for the top 100 models (in gray) and using the full coefficients estimated after averaging the top five models, i.e., those with \\(\\Delta \\text{AIC}_c &lt; 2\\) (in black). As expected, the averaged model lives somewhere between the top models. Given the slope of the averaged model, it seems the top fitting models are ones which predict a very strong relationship between fitness and the focal trait.\n\n### can ignore this code! focus on the plot.\n\nBnrLgth.S_predictions &lt;- seq(min(Thompson_data$BnrLgth.S),\n                               max(Thompson_data$BnrLgth.S), \n                               length=100)\n\npredct_values &lt;- data.frame(BnrLgth.S = BnrLgth.S_predictions,\n                       VegGrowth.S = 0, cyanide = 0, BnrWdt.S = 0, FrstFlwr.S = 0, \n                       InflNum.S = 0, FlwrCt.S = 0, InflWdt.S = 0, InflHt.S = 0)\n# predicted values...only varying BnrLgth.S, all other covariates set = 0 for plotting purposes\n\npredictions &lt;- NULL\n\nfor (i in 1:100){\n  model &lt;- get.models(GTmodel_dredge, subset = i)\n  cbind(RFSeed_predicted = predict(model[[1]], newdata = predct_values), \n        BnrLgth.S = BnrLgth.S_predictions,\n        index = i) -&gt; predictions[[i]]\n}\n# only top 100 models\n\npredictions_all &lt;- do.call(rbind, predictions)\npredictions_ave &lt;- data.frame(BnrLgth.S = BnrLgth.S_predictions,\n                              RFSeed_predicted = 0.9786538 + 0.225054*BnrLgth.S_predictions)\n\nas.data.frame(predictions_all) %&gt;% \n  ggplot() +\n  geom_line(aes(x = BnrLgth.S, y = RFSeed_predicted, group = as.factor(index)),\n            alpha = 0.5, color = \"lightgray\") +\n  geom_line(data = predictions_ave, inherit.aes = F,\n            aes(x = BnrLgth.S, y = RFSeed_predicted), color = \"black\", size = 2)\n\n\n\n# note: all other trait values are =0 for the purpose of visualization"
  },
  {
    "objectID": "lec10-model-selection.html#summaryand-some-words-of-caution",
    "href": "lec10-model-selection.html#summaryand-some-words-of-caution",
    "title": "12  Model selection",
    "section": "12.9 Summary…and some words of caution",
    "text": "12.9 Summary…and some words of caution\n\nAs we have seen, many problems can be formulated in terms of choosing between candidate models or deciding if a given model is an appropriate description of the data. Model selection is important and active area of statistics, and is complementary to the statistics we have so far discussed in the course.\nThe tools to preform model selection will depend on the application. It is often difficult (or impossible) to fit linear models to high dimensional data, so tools like ridge and LASSO regression are very useful in that they 1) get the job done and 2) can be used to preform feature selection.\nModel selection depends on the set set of models considered. You can’t identify a model as being the “best” fit to the data if you exclude it from the analysis.\nStatistical significance and biological significance are different things!\nStepwise selection is problematic. Don’t do it. It gives rise to biased regression coefficients, \\(p\\) values, etc. and provides little insight into how the world works.\nDO *clap* NOT *clap* USE *clap* dredge *clap* TO *clap* \\(p\\) *clap* HACK *clap*."
  },
  {
    "objectID": "lec10-model-selection.html#references-and-additional-reading",
    "href": "lec10-model-selection.html#references-and-additional-reading",
    "title": "12  Model selection",
    "section": "12.10 References and additional reading",
    "text": "12.10 References and additional reading\n\nJohnson & Omland 2004. Model selection in ecology and evolution. Trends in Ecology and Evolution.\nBurnham & Anderson 2002. Model Selection and Multimodel Inference (2nd ed.).\nZuur et al. 2009. Mixed Effects Models and Extensions in Ecology with R."
  },
  {
    "objectID": "lec10-model-selection.html#footnotes",
    "href": "lec10-model-selection.html#footnotes",
    "title": "12  Model selection",
    "section": "",
    "text": "Note that models that include interaction terms will automatically also include the interacting terms as independent predictors. In other words, you cannot have a model that contain only the interaction term but not the independent effects.↩︎\nThese AICc score are close! It barely made our predetermined cut off of 2 units. This is a very good example of why it is important to decide on a “recipe” for model selection before we do anything. Let’s pretend we are very attached to the random slope. We see that the AICc scores are really close! We might think to ourselves “you know what, maybe 2 units is too stringent, let’s set the cut off at 3 instead” and then proceed to happily not-reject the intercept-slope model because it’s indistinguishable from the intercept-only model. Of course, there is nothing stopping anybody from just swapping out the recipe under the table half way through without telling anyone and just pretending it was like that all along.↩︎\nWhatever works! :P↩︎"
  },
  {
    "objectID": "lec11-multivariate-stats.html",
    "href": "lec11-multivariate-stats.html",
    "title": "13  Multivariate statistics",
    "section": "",
    "text": "14 Multivariate Statistics"
  },
  {
    "objectID": "lec11-multivariate-stats.html#lesson-intro",
    "href": "lec11-multivariate-stats.html#lesson-intro",
    "title": "13  Multivariate statistics",
    "section": "14.1 Lesson intro",
    "text": "14.1 Lesson intro\nMost things in the natural world are inter-related and correlated, where many trends and observations tend to change in a predictable way together. This is personally one of the things that I find exciting about biology–the sheer complexity of interactions in natural systems.\nBut this also makes our lives as scientists challenging, because we then have to distill, what is sometimes called a “causal soup”, into variables of interest and then figure out a meaningful analysis that can tease apart which effects came from where.\nAnd this is where it gets complicated—how do we accurately represent the true complex and multidimensional nature of ecological systems without breaking our brains?\nCue multivariate statistics.\nWe use multivariate techniques to reduce complex situations into more manageable ones and to deal with the fact that patterns are correlated with other patterns, which are in turn driven by a number of interacting ecological processes.\nUp until now we’ve been working with univariate methods – when there is a single response variable is of interest, or when we have multiple predictors but we are mostly interested in their independent effects on the response.\nIn contrast, multivariate statistics is a family of statistical tools used to to handle situations where the predictor and/or the response variables come in sets, allowing us to distill through some of that correlated (and sometimes redundant) information and to analyze these variables simultaneously, as they should be.\nUnfortunately (?), it is not within the scope of this class (or even this course!) to teach all of this. Multivariate statistics area large discipline in their own right. They are a flexible tool with a lot of variations - which can make it challenging to choose the correct technique for an analysis.\nToday, we will focus on a couple of the most common techniques that you are likely to come across in the papers you read and/or use in your own work. This is a quick overview and we necessarily have to have to skip over some of the details—if want to know more or have a specific question related to data, we are happy provide readings or to chat during office hours.\n\n14.1.1 A bit of theory1\nMultivariate statistics roughly split into unconstrained vs constrained ordination techniques.\nUnconstrained ordinations have no a priori groupings or relationships in the variables or observations. Common examples include: PCA, PCoA, cluster analysis, correspondence analysis (CA), factor analysis and NMDS. We will focus on this broad category today.\nConstrained ordinations specify a priori groupings. Examples include discriminant analysis, canonical variate analysis, canonical correlation analysis, and redundancy analysis.\nUnconstrained ordinations generally work by the extraction of the eigenvectors of an association matrix. For example, imagine your data containing n objects and p variables. The n objects can be represented as a cluster of points in the p-dimensional space. This cluster is generally not spheroid: it is elongated in some directions and flattened in others. These directions are not necessarily aligned with a single dimension (= a single variable) of the multidimensional space.\nThe direction where the cluster is most elongated corresponds to the direction of largest variance of the cluster and this is the first axis that an ordination will extract. The next axis to be extracted is the second most important in variance, provided that it is orthogonal (linearly independent, uncorrelated) to the first one.\nThe process continues until all axes have been computed. When there are a few major structures in the data (gradients or groups) and the method has been efficient at extracting them, then the few first axes contain most of the useful information, i.e. they have extracted most of the variance of the data. In that case, the distances among sites in the projection in reduced space are relatively similar to the distances among objects in the multidimensional space.\nHow the methods do this specifically varies and depends on the types of data and specific methods you are using.\nFor more details see this chapter in Borcard, D., Gillet, F. and Legendre, P. 2011. Numerical Ecology with R."
  },
  {
    "objectID": "lec11-multivariate-stats.html#lets-practice-toy-example",
    "href": "lec11-multivariate-stats.html#lets-practice-toy-example",
    "title": "13  Multivariate statistics",
    "section": "14.2 Let’s practice: toy example",
    "text": "14.2 Let’s practice: toy example\nFor this section we will be using soil chemistry data from the vegan library.\n\ndata(varechem)\nhead(varechem)\n\n      N    P     K    Ca    Mg    S    Al   Fe    Mn   Zn  Mo Baresoil Humdepth\n18 19.8 42.1 139.9 519.4  90.0 32.3  39.0 40.9  58.1  4.5 0.3     43.9      2.2\n15 13.4 39.1 167.3 356.7  70.7 35.2  88.1 39.0  52.4  5.4 0.3     23.6      2.2\n24 20.2 67.7 207.1 973.3 209.1 58.1 138.0 35.4  32.1 16.8 0.8     21.2      2.0\n27 20.6 60.8 233.7 834.0 127.2 40.7  15.4  4.4 132.0 10.7 0.2     18.7      2.9\n23 23.8 54.5 180.6 777.0 125.8 39.5  24.2  3.0  50.1  6.6 0.3     46.0      3.0\n19 22.8 40.9 171.4 691.8 151.4 40.8 104.8 17.6  43.6  9.1 0.4     40.5      3.8\n    pH\n18 2.7\n15 2.8\n24 3.0\n27 2.8\n23 2.7\n19 2.7\n\nstr(varechem)\n\n'data.frame':   24 obs. of  14 variables:\n $ N       : num  19.8 13.4 20.2 20.6 23.8 22.8 26.6 24.2 29.8 28.1 ...\n $ P       : num  42.1 39.1 67.7 60.8 54.5 40.9 36.7 31 73.5 40.5 ...\n $ K       : num  140 167 207 234 181 ...\n $ Ca      : num  519 357 973 834 777 ...\n $ Mg      : num  90 70.7 209.1 127.2 125.8 ...\n $ S       : num  32.3 35.2 58.1 40.7 39.5 40.8 33.8 27.1 42.5 60.2 ...\n $ Al      : num  39 88.1 138 15.4 24.2 ...\n $ Fe      : num  40.9 39 35.4 4.4 3 ...\n $ Mn      : num  58.1 52.4 32.1 132 50.1 ...\n $ Zn      : num  4.5 5.4 16.8 10.7 6.6 9.1 7.4 5.2 9.3 9.1 ...\n $ Mo      : num  0.3 0.3 0.8 0.2 0.3 0.4 0.3 0.3 0.3 0.5 ...\n $ Baresoil: num  43.9 23.6 21.2 18.7 46 40.5 23 29.8 17.6 29.9 ...\n $ Humdepth: num  2.2 2.2 2 2.9 3 3.8 2.8 2 3 2.2 ...\n $ pH      : num  2.7 2.8 3 2.8 2.7 2.7 2.8 2.8 2.8 2.8 ...\n\n\nThis data frame is composed of 24 rows that correspond to different sites and 14 columns, which correspond to the different chemicals measured at each site.\n\n14.2.1 A note on distance matrices\nMany multivariate methods rely on being able to represent your observations as being more or less similar to each other (similarities or distances). This makes, because if you’re trying to group observations by variance along an axis, you need to know how far apart each observation is from other observations.\nThis is easy to do (and visualize) with quantitative data—a distance is just the numeric difference between the observations along an axis. To deal with negative numbers and multiple dimensions, rather than subtracting one value the other, we instead compute euclidean distances.\nFor visualization, let’s compute the euclidean distances between the first four soil chemicals. We will use the vegdist function in vegan. vegdist is handy because it allows us to specify which type of distance matrix we want to use.\n\n?vegdist\n# let's just look at the first column\neuclidean.distance &lt;- vegdist(varechem[, 1:4], method=\"euclidean\") \n# lets see it \neuclidean.distance\n\n          18        15        24        27        23        19        22\n15 165.14239                                                            \n24 459.56128 618.58209                                                  \n27 328.81899 482.43858 141.98528                                        \n23 261.12068 420.92078 198.55261  78.22110                              \n19 175.28391 335.26172 285.02937 156.53428  86.77350                    \n22 221.62195 382.15759 239.50061 116.61587  43.40369  47.14149          \n16 125.38142  49.65350 583.95530 450.65908 385.46067 299.21572 345.65371\n28 260.84978 404.51356 231.11534  90.72365  86.64825 110.39909  96.51135\n13 175.40339 235.66141 446.46377 304.81017 271.65128 207.69502 243.84187\n14  10.92932 157.07342 465.96998 334.10115 267.45923 181.30130 227.78744\n20 231.32127 386.96504 232.30166  98.16649  42.52905  61.98032  39.88233\n25 129.45443 292.09529 330.43671 203.54086 132.25891  48.99612  93.83017\n7  337.13646 191.42586 796.46802 664.90879 598.04244 512.08316 558.00082\n5  296.18195 171.77805 752.47104 624.68037 554.89965 469.70461 514.65909\n6  272.41973 144.90680 730.14008 601.23737 532.27095 446.78778 492.18765\n3  307.97131 167.57619 766.90027 636.19769 568.62795 482.93683 528.74291\n4   87.65546 172.76415 456.04597 316.57096 263.35787 183.75062 229.19383\n2   87.45988  80.33941 539.16396 403.86099 341.29395 255.84222 302.76968\n9   24.71882 148.66563 480.77366 351.15263 282.65819 197.46881 243.57432\n12  50.88271 121.96975 509.56721 379.21342 311.23155 225.19911 271.19956\n10 205.77395 357.02368 263.83591 125.84288  76.71760  52.82717  59.88447\n11 653.73125 814.06559 196.79954 336.95884 393.54222 479.37130 432.96563\n21  51.98279 143.46616 500.87685 373.94623 303.28060 218.05641 262.75243\n          16        28        13        14        20        25         7\n15                                                                      \n24                                                                      \n27                                                                      \n23                                                                      \n19                                                                      \n22                                                                      \n16                                                                      \n28 376.81408                                                            \n13 228.66139 217.27572                                                  \n14 118.15198 264.60605 169.54852                                        \n20 353.81686  59.52411 230.46991 236.84586                              \n25 254.51902 149.74198 194.73007 136.46575 106.65711                    \n7  214.70470 590.73971 423.76340 331.17554 567.57318 466.05208          \n5  181.39942 554.78769 404.46266 291.45274 526.91462 422.83496  62.65477\n6  156.05073 530.56984 378.72901 267.48510 503.62712 400.06362  73.67035\n3  187.20286 562.92360 401.51605 302.37391 538.61420 436.49603  31.85750\n4  154.15687 234.16757  95.94514  82.45684 224.87883 150.10506 362.41522\n2   52.55578 327.48820 181.37481  79.01114 307.61853 212.39995 263.62223\n9  107.37337 283.60679 191.48940  26.21927 253.52177 150.59459 316.16703\n12  78.15120 310.71511 200.32099  46.29903 281.94744 179.42770 287.37355\n10 326.07708  62.24942 197.40995 210.59713  36.10637  89.77878 540.37336\n11 778.43029 425.20842 638.57443 660.35430 428.65991 524.50748 989.92165\n21  96.48300 309.98103 217.30067  51.80463 276.74176 171.62628 297.57023\n           5         6         3         4         2         9        12\n15                                                                      \n24                                                                      \n27                                                                      \n23                                                                      \n19                                                                      \n22                                                                      \n16                                                                      \n28                                                                      \n13                                                                      \n14                                                                      \n20                                                                      \n25                                                                      \n7                                                                       \n5                                                                       \n6   29.48525                                                            \n3   38.15573  44.10873                                                  \n4  334.10919 308.22925 336.35840                                        \n2  232.42560 206.79284 236.51364 101.98505                              \n9  274.31192 250.47305 286.59810 100.16816  74.62433                    \n12 246.28329 222.22664 258.28515 113.13218  55.33643  32.30681          \n10 501.74134 477.63498 511.88009 192.37204 278.54732 228.16376 256.08832\n11 944.08465 922.41863 960.16075 652.44735 734.72664 674.65552 703.12318\n21 252.26522 229.82385 267.82042 130.32041  81.19384  35.54786  26.96294\n          10        11\n15                    \n24                    \n27                    \n23                    \n19                    \n22                    \n16                    \n28                    \n13                    \n14                    \n20                    \n25                    \n7                     \n5                     \n6                     \n3                     \n4                     \n2                     \n9                     \n12                    \n10                    \n11 460.16602          \n21 253.41513 692.92554\n\n\nNote that the output here is an object of class “distance” that shows all the pairwise euclidean distances between all cells in the data.\nThinking about distances are more challenging to do if you have some data that is categorical (what is the similarity along an axis of apples to oranges?). Luckily there are different types of distance matrices that can handle different types of data.\nAnother really common type of distance matrix is the Gower distance, which can handle categorical and mixed data, and so is very useful. For more detail on distance matrices read this.\nOkay, now we’re ready to do some statistics!!\n\n\n14.2.2 PCA\nOne of the most common (unconstrained) ordination techniques used in EEB is the Principle components analysis (PCA). A PCA seeks to find a new coordinate system (PC axes) that best accounts for the variation in a multivariate data space, thus reducing dimensionality in the process but still accounting for as much of the original variation as possible. Critically, a PCA assumes a linear relationship between your variables, can only take quantitative data, and requires more rows than columns in the data itself.\nThe procedure to conduct a PCA is as follows:\n\nImagine a 14-dimensional object with each of our soil measurements as an axis (k=14).\nPlot our sites (n=24) in this 14-D space, and then find the Euclidean distance between those points (do not despair, R will do the actual calculations for us).\n\nLet’s visualize this (but in less dimensions because we are human). One way to do this is with a pairs plot in base R, which plots the relationships between all variables (columns) as a scatter plot.\n\npairs(varechem[, 1:4]) #only the first 4 columns for simplicity\n\n\n\n\nWhich pairs of chemicals are tightly related to each other? Which are not closely related?\n\nAfter you have the distances between points, we can use procedures that are similar (in concept) to least squares to find a “best-fit line” through our data points in multidimensional space (again, R will take care of all this). This will be our first principle component (PC) axis and it explains the most variance in our data.\nFind another best-fit line through the data under the condition that it has to be orthogonal to the first axes (i.e., at a 90° angle, meaning that these two axes are as unrelated to each other as possible). This will be our second PC axis. See here for visual PCA axis.\nThis process continues until we have k PC axes.\n\nOkay, let’s work through some of the details in an example.\nWe will use the function prcomp. At its most basic, prcomp takes your data and then you specify if you want to scale = TRUE/FALSE. The scale command specifies whether you want to run the pca on a correlation matrix or a covariance matrix. This sounds complex but it boils down to a simple idea. Choosing scale = TRUE means you are selecting a correlation matrix which standardizes your data to have unit variance ie puts all your variables on the same scale. Choose this option if your data contains multiple types of units.\n\nvarechem.pca &lt;- prcomp(varechem, scale. = TRUE)\n\nPhew, that wasn’t too bad. Let’s take a look at our varechem.pca object\n\nsummary(varechem.pca) # this is ugly becuase of the linebreaks, look at in console, it's easier\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.2785 1.7869 1.2983 1.03392 0.90332 0.84013 0.66061\nProportion of Variance 0.3708 0.2281 0.1204 0.07636 0.05829 0.05042 0.03117\nCumulative Proportion  0.3708 0.5989 0.7193 0.79564 0.85392 0.90434 0.93551\n                           PC8    PC9    PC10    PC11    PC12   PC13    PC14\nStandard deviation     0.60732 0.4132 0.38669 0.29199 0.26430 0.1872 0.15367\nProportion of Variance 0.02635 0.0122 0.01068 0.00609 0.00499 0.0025 0.00169\nCumulative Proportion  0.96185 0.9740 0.98473 0.99082 0.99581 0.9983 1.00000\n\n\nThis gives us a table summary of variance explained by each axes. R even did the math for us and showed cumulative variance explained as we move to the higher dimensions, so convenient! We see that the first two PC axes already explained close to 60% of the variation of the data. But it doesn’t give us any information about our variables or sites!\nLet’s have a look at this object in another way.\n\nstr(varechem.pca)\n\nList of 5\n $ sdev    : num [1:14] 2.278 1.787 1.298 1.034 0.903 ...\n $ rotation: num [1:14, 1:14] 0.0946 -0.3559 -0.3632 -0.3598 -0.3523 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:14] \"N\" \"P\" \"K\" \"Ca\" ...\n  .. ..$ : chr [1:14] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n $ center  : Named num [1:14] 22.4 45.1 162.9 569.7 87.5 ...\n  ..- attr(*, \"names\")= chr [1:14] \"N\" \"P\" \"K\" \"Ca\" ...\n $ scale   : Named num [1:14] 5.53 14.95 64.84 243.58 41.01 ...\n  ..- attr(*, \"names\")= chr [1:14] \"N\" \"P\" \"K\" \"Ca\" ...\n $ x       : num [1:24, 1:14] 0.132 0.528 -3.963 -3.241 -1.837 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:24] \"18\" \"15\" \"24\" \"27\" ...\n  .. ..$ : chr [1:14] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n - attr(*, \"class\")= chr \"prcomp\"\n\n\nWe see that this object is structured as a five-item list. We mostly care about two of these items:\n\nLoadings – appropriately named rotation.\nSite scores – lazily named x.\n\nTo extract items from a list, use the $ notation.\n\nvarechem.pca$x # this gives us the particular location (ie the coordinates) of each of our sites in the multidimensional space formed by the PC axes\n\n           PC1         PC2          PC3         PC4          PC5         PC6\n18  0.13222328  1.70101442 -0.279475522 -0.47325693  0.103821307 -0.97013110\n15  0.52756644  0.75652573  0.004377408 -1.24113026  1.202410202 -0.87748856\n24 -3.96343087 -2.54549515 -0.017887116 -1.01474228 -1.896445495  1.65855241\n27 -3.24105345  1.03869050  1.093443151  0.74721229  1.122790249  0.14867036\n23 -1.83673667  1.74473867 -0.298946184 -0.06989374 -0.804034232 -0.76167251\n19 -1.96510029  1.49265376 -1.119483753 -0.37139226 -1.332867226 -0.59222144\n22 -0.78264065  1.74589273  0.277038549  0.64096723 -0.079834902  0.23366653\n16  1.61990524  1.54627261 -0.182700904 -0.30489556  0.009249015 -0.02669483\n28 -2.93871880  0.90850144  0.336742959  1.89138943  0.861994408  0.57878720\n13 -1.61179969 -1.72776197 -1.851617499  1.61763757 -0.161168189 -0.99674051\n14 -0.10849087  1.23231788 -1.057894796 -0.85576997 -0.235369165  0.24258363\n20 -2.41827770  0.41983066 -1.013756555  0.72805394 -0.288590298  0.31608364\n25 -0.88868934  1.82326518  1.044623907  0.75063513  1.242461532  0.45818828\n7   3.88231709 -0.44736574 -0.728450291  1.01917052 -0.786719915 -0.24475511\n5   3.63433625  1.21449816  1.339689070  0.83264806 -0.499185602  2.33214424\n6   3.37347840  0.07290487 -0.358467072 -1.02764414  0.555584417  0.44585427\n3   4.25544857 -1.67410775 -0.504746514  1.63909051 -0.558770476 -0.18623480\n4  -0.68902667 -3.34189696 -2.563741690 -0.71125297  0.968989199  0.43945828\n2   1.53878447 -2.46012329 -0.294009378  0.85232874  0.817340772 -0.72299099\n9   0.08438668 -1.40485249 -0.514479851 -1.58312456  0.733411043  0.78911410\n12  1.23674421  0.53360601  0.271442076 -1.49529509  0.586950936  0.06898806\n10 -0.85106814 -1.93158072  2.455295189 -0.60099298  0.837723132 -0.46303287\n11 -0.25592697 -3.08135124  3.665611461 -0.10586883 -1.117079105 -1.05247280\n21  1.26576946  2.38382268  0.297393354 -0.86387385 -1.282661605 -0.81765547\n            PC7         PC8         PC9        PC10        PC11         PC12\n18 -0.412107828  0.10430066  0.89379861 -0.19515377  0.15181987  0.446288151\n15  0.140335713  0.35570552  0.18415914 -0.32702920 -0.26463780  0.216772715\n24  0.584842783  0.19856484  0.59859756 -0.34214482 -0.03469025  0.310027914\n27  0.577453010 -0.34781468 -0.05752049 -0.59425418  0.07314193  0.063378029\n23  0.030023854 -0.13218763  0.51649591  0.70693203  0.03233029  0.054332723\n19  1.238669139 -0.34158207 -0.31889053 -0.05364217 -0.16861193 -0.476514833\n22  0.539332177 -0.05889938 -0.51844860  0.03006888  0.45835909  0.245481007\n16 -0.373476941  0.78419427 -0.06115766  0.48740293  0.23745992  0.059594599\n28  0.267720379 -0.27061746  0.09308865  0.99318331 -0.21893369  0.097099274\n13  0.028618132  1.41746150 -0.78794716 -0.41248879 -0.06593851  0.293911543\n14  0.117482816  0.34746586 -0.11334200  0.29864105  0.21053275 -0.319591277\n20 -1.808311684 -0.48521666  0.13191310 -0.29763388 -0.11048490 -0.198738243\n25 -0.296177146 -0.70116183 -0.08870626 -0.56597254  0.03863706 -0.175523779\n7   0.003896015 -0.07322921 -0.07022198  0.03076471 -0.32299944  0.093391136\n5  -0.372775208  1.06173977  0.03575899 -0.10222536  0.04588475 -0.150519119\n6   0.437849242 -0.66318161 -0.02922027 -0.12788259  0.20774098  0.368308444\n3   0.473990937 -1.15101958  0.18069717  0.01462754 -0.33603215  0.155223171\n4  -1.092945035 -0.43319856 -0.16366937  0.32388296  0.31092450 -0.016558671\n2   0.764129828  0.52251461  0.87005247 -0.21039984  0.45544416 -0.635306833\n9   0.313163663 -0.40901417 -0.38867009  0.02167642 -0.40521219 -0.212953448\n12  0.473997752  0.13222574 -0.44070463  0.20907903  0.10370214  0.071014379\n10 -0.259576328  0.80876134  0.14111906  0.27398425 -0.67108522 -0.072769123\n11 -0.473633258 -0.53811517 -0.46308153  0.16295082  0.47248237  0.001778163\n21 -0.902502014 -0.12769610 -0.14410013 -0.32436679 -0.19983373 -0.218125923\n           PC13        PC14\n18  0.180710086 -0.14511706\n15 -0.264956749  0.10969811\n24 -0.084404758  0.05320728\n27 -0.169590445 -0.18408979\n23  0.215246883 -0.07599179\n19 -0.109832641 -0.17267154\n22  0.344342190  0.09944716\n16 -0.068810220  0.04176308\n28 -0.286368123  0.10527341\n13  0.019257591  0.08668395\n14 -0.001286276  0.11266438\n20  0.218426950  0.21618583\n25  0.079312882 -0.09860196\n7   0.116566189 -0.41080566\n5  -0.068070865 -0.09223078\n6  -0.078853433  0.10186206\n3  -0.036095816  0.23450059\n4  -0.232369489 -0.25165355\n2   0.038260431  0.10380174\n9   0.379813850  0.05207296\n12  0.041195274  0.05293037\n10  0.132399959 -0.03854731\n11 -0.044201689 -0.01970105\n21 -0.320691780  0.11931958\n\nvarechem.pca$rotation # this tells us which of our soil chemicals are contributing most strongly to each PC axis (ie which variables are being summarized by each newly created PC axis)\n\n                 PC1         PC2         PC3         PC4          PC5\nN         0.09462709  0.12435859 -0.16204826  0.79318367 -0.351576656\nP        -0.35589258 -0.20731069  0.09086948  0.08242977  0.219919629\nK        -0.36318854 -0.16172669 -0.04259682  0.21401371  0.176886144\nCa       -0.35975485 -0.08457237  0.32644437 -0.01826328 -0.236121676\nMg       -0.35228219 -0.09776116  0.08467641 -0.03784222 -0.486392688\nS        -0.33246862 -0.29668916 -0.21512714 -0.01410909  0.070039686\nAl        0.11552207 -0.48048232 -0.26540100  0.07767669  0.005266422\nFe        0.20299687 -0.38301396 -0.15691339  0.31769008 -0.022858769\nMn       -0.28756460  0.19318776  0.08921960  0.37885343  0.489948050\nZn       -0.34603172 -0.16198093  0.04178581 -0.05229606 -0.150267479\nMo       -0.04154531 -0.22863241 -0.58319905 -0.23286801 -0.029395588\nBaresoil -0.16349721  0.31570370 -0.27274616 -0.06312303 -0.407367062\nHumdepth -0.23531857  0.33736134 -0.28352933  0.02541797 -0.046678588\npH        0.16613845 -0.32216601  0.45164891  0.06492250 -0.259486172\n                 PC6          PC7          PC8          PC9        PC10\nN         0.31876043 -0.020445294  0.106079920 -0.029711941  0.25325314\nP         0.08346185 -0.202993610 -0.291937970  0.529497898  0.48691417\nK        -0.30156737 -0.083967201  0.345810459 -0.347935071  0.19105757\nCa       -0.10026235 -0.056467690 -0.241640431 -0.132902952  0.15581868\nMg       -0.11335320  0.422483826  0.002896128  0.185900569 -0.18858761\nS        -0.09229889 -0.001743922  0.271322793 -0.041707742 -0.01394397\nAl       -0.20467312 -0.090888045 -0.037652261 -0.325530088 -0.02736515\nFe       -0.33950608  0.197580497 -0.192886023  0.424437201 -0.32652805\nMn        0.07123733 -0.136543653 -0.274959999 -0.080205160 -0.54341919\nZn        0.54306692  0.032492006  0.238361969 -0.005992104 -0.36679941\nMo        0.41761742 -0.120157357 -0.352212047 -0.082184113  0.02092970\nBaresoil -0.32655495 -0.662017409 -0.006567573  0.150678212 -0.20591182\nHumdepth -0.16741045  0.425702015 -0.465061249 -0.315633320  0.12595591\npH        0.05330910 -0.262135909 -0.374832351 -0.349371603 -0.07947962\n                 PC11        PC12        PC13         PC14\nN        -0.005103292  0.03375847  0.12866179 -0.023577121\nP        -0.212772662 -0.15369608 -0.16014570 -0.146803748\nK         0.118589614  0.28135388 -0.50806589  0.191151759\nCa        0.669374213 -0.03834446  0.36929697  0.035843646\nMg       -0.239307771  0.42332112 -0.05980040 -0.345891242\nS        -0.378600375 -0.07818118  0.63006839  0.345755661\nAl        0.018994101 -0.30010097  0.04257464 -0.654805487\nFe        0.238879497 -0.09798831 -0.12533928  0.354078026\nMn       -0.035024496  0.18554028  0.13661147 -0.181857434\nZn        0.104956310 -0.48566331 -0.29628189  0.070169749\nMo        0.159957373  0.43567172 -0.03761246  0.117855258\nBaresoil -0.053374989 -0.10187147 -0.07709019  0.007445089\nHumdepth -0.168085863 -0.37001287 -0.13026298  0.156358945\npH       -0.406958380  0.04733921 -0.12013250  0.265236656\n\n\nWe can interpret these outputs directly, specifically the rotations.\n\nvarechem.pca$rotation[ ,1:4] # let's just look examine the first 4 PC axes\n\n                 PC1         PC2         PC3         PC4\nN         0.09462709  0.12435859 -0.16204826  0.79318367\nP        -0.35589258 -0.20731069  0.09086948  0.08242977\nK        -0.36318854 -0.16172669 -0.04259682  0.21401371\nCa       -0.35975485 -0.08457237  0.32644437 -0.01826328\nMg       -0.35228219 -0.09776116  0.08467641 -0.03784222\nS        -0.33246862 -0.29668916 -0.21512714 -0.01410909\nAl        0.11552207 -0.48048232 -0.26540100  0.07767669\nFe        0.20299687 -0.38301396 -0.15691339  0.31769008\nMn       -0.28756460  0.19318776  0.08921960  0.37885343\nZn       -0.34603172 -0.16198093  0.04178581 -0.05229606\nMo       -0.04154531 -0.22863241 -0.58319905 -0.23286801\nBaresoil -0.16349721  0.31570370 -0.27274616 -0.06312303\nHumdepth -0.23531857  0.33736134 -0.28352933  0.02541797\npH        0.16613845 -0.32216601  0.45164891  0.06492250\n\n\nThe way we interpret this is by looking, within each PC axis, for the variables that have the largest absolute values. These are the variables that are contributing variation to this axis.\nFor example, PC1 sets up an axis of increasing concentrations of P, K, Ca, Mg, S, Zn. PC2 is driven largely by Al, Baresoil, Humdepth and pH. Note that the positive and negative numbers indicate a positive and negative relationship (eg along PC1 P and Fe are negatively correlated, sites that have high P tend to have low Fe and vice versa).\nThis is much, much easier to see visually. We can do this with the base R function biplot, which is a bit ugly.\n\nbiplot(varechem.pca)\n\n\n\n\nOn this figure, there are 2 sets of information. Each site is plotted (these are the numbers). As well, the variable loadings are represented as arrows and tell us how each variable is contributing to each axis. The clustering of sites along the axes (and rotations) tell us which sites are similar in terms of their chemical composition. Eg, we can see that sites 19, 20, 27 and 28 all have a similar chemical composition. We can also see clearly that along PC2 sites with low pH also tend to have low Al, while they have high Humdepth, Baresoil and Mn. Future analyses might relate the soil characteristics of these sites to other environmental data (history of land use, pollution etc)\nWe can do this with the function autoplot in the ggfortify package, which takes advantage of ggplot style plotting.\n\nautoplot(varechem.pca) +\n  theme_classic()\n\n\n\n\nThe simplest version of autoplot displays less information than our biplot function. On this figure we can see that we are plotting PC1 by PC2 and each point on the figure is each of the rows aka sites in our data. autoplot handily also shows us the proportion of variance explained by each of these PC axes. Now lets add the loadings.\n\n# Remember to tell R where the original data is coming from! \n# Also, `autoplot` will refuse to understand you if you spell \"colour\" the non-Canadian way\nautoplot(varechem.pca, loadings=TRUE, loadings.label=TRUE, \n         loadings.colour=\"grey30\", \n         data=varechem) +\n  theme_classic()\n\n\n\n\nBut do we make biplot for each of our PC axes??? Nope, this would take too long and each subsequent axis explains less of the variation in our data… ie it becomes less important. As well, we often perform PCA to simplify our data from many variables to few.\nSo the next step is to pick how many axes show meaningful information.\nWe can do this with a scree plot. This shows us the amount of variation explained by each axis. As we know, each axis explains subsequently less variation. We use scree plots to help us determine at which point the axes stop adding much new information. We can do this visually or quantitatively. We will just go over the visual method as determining which axes to keep is pretty subjective and depends on your system and question of interest.\n\nplot(varechem.pca)\n\n\n\n\nEach bar is a PC axis, starting from PC 1. In a scree plot, what you want to look for is a big jump between subsequent axes. Its subjective, but based on this scree plot I would probably be inclined to keep the first 2 or 3 PC axes. As well, we can look at the cumulative proportion of explained variance:\n\nsummary(varechem.pca)$importance[,1:6]\n\n                            PC1      PC2      PC3      PC4      PC5      PC6\nStandard deviation     2.278498 1.786853 1.298271 1.033922 0.903324 0.840125\nProportion of Variance 0.370830 0.228060 0.120390 0.076360 0.058290 0.050420\nCumulative Proportion  0.370830 0.598890 0.719280 0.795640 0.853920 0.904340\n\n\nAnd we can see that by PC3 we have explained ~72% of the variation in the data, which is not bad (not great either, but not bad). By the time we get to PC4, each individual axis is explaining less than 10% of variation in the data, so adding them, and conversely excluding them, isn’t going to have a large effect.\n*Note 1: there are many functions across many libraries for running a pca: prcomp, princomp, rda, dudi.rda. Each method makes a slightly different choice about how the underlying math is performed. prcomp and rda are both very common choices.\n*Note 2: for those of you working with phylogenetic data: you may want to perform a phylogenetic PCA (pPCA). The function phyl.pca is available in package phytools.\nFor further readings see:\n\nPCA: a visual explanation\nPCA explained using a metaphor with wine, dinner, grandmothers, and animationsPCoA\n\n\n\n14.2.3 PCoA\nA PCoA (principal coordinates analysis) is a more flexible form of a PCA that can take any type of distance matrix. Remember that distance matrices allow you to express differences between your data points so by specifying the type of distance matrix, so this means that PCoA can handle a mix of data types—which is very handy if you have both categorical and numeric data. Note that your choice of distance matrix can have a large effect on the conclusions you draw in your analysis–make sure to read the literature and learn what best practices there are for you specific data types.\nYou interpret the output of a PCoA the same way as a PCA. Common functions are cmdscale in base R, wcmdscale in vegan, and pcoa in ape.\nIn sum, both PCA and PCoA preserve the distances between your data points–for this reason they are commonly called “rotations”–these methods do not change the underlying relationships in your data, they just group variation in data across many dimensions and then make a nice tidy output that our brain’s can interpret."
  },
  {
    "objectID": "lec11-multivariate-stats.html#lets-practice-real-life-example",
    "href": "lec11-multivariate-stats.html#lets-practice-real-life-example",
    "title": "13  Multivariate statistics",
    "section": "14.3 Let’s practice: real life example",
    "text": "14.3 Let’s practice: real life example\nFor this section of the class, we will try to apply what we’ve learned above on a real world data set. We’ll be walking through the analysis and the choices we make and then at each section, I will give you a minute or so to try and either a. shout out the answer or b. write the code.\nWe are working with a cool dataset that was collected by Vicki as part of her PhD work outside of Churchill, Manitoba. Vicki generally studies invasive species in the tundra and the data we are working with today is part of a larger project that investigates the effect of soil on plant invasions.\nThe key question we are trying to answer today is if soil chemicals, which we expect to vary across a landscape according to human presence and land use, can predict whether or not invasive species are present.\nLet’s read in our data and take a look at it!\n\nsoil.data &lt;- read_csv(\"soil.data.csv\")\n\nRows: 28 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): sample_id, invaded_status, location, notes\ndbl (7): site_number, latitude, longitude, P, C, N, avg_pH\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(soil.data) \n\n# A tibble: 6 × 11\n  sample_id site_number invaded_status location latitude longitude     P     C\n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1A                  1 invaded        town         58.8     -94.2 95.2   9.92\n2 1B                  1 uninvaded      town         58.8     -94.2 42.4   5.47\n3 6A                  2 invaded        town         58.8     -94.2  4.63  2.22\n4 6B                  2 uninvaded      town         58.8     -94.2  8.31  1.74\n5 7A                  3 invaded        town         58.8     -94.2 33.6   5.49\n6 7B                  3 uninvaded      town         58.8     -94.2 77.2   4.93\n# ℹ 3 more variables: N &lt;dbl&gt;, avg_pH &lt;dbl&gt;, notes &lt;chr&gt;\n\nstr(soil.data)\n\nspc_tbl_ [28 × 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ sample_id     : chr [1:28] \"1A\" \"1B\" \"6A\" \"6B\" ...\n $ site_number   : num [1:28] 1 1 2 2 3 3 4 4 5 5 ...\n $ invaded_status: chr [1:28] \"invaded\" \"uninvaded\" \"invaded\" \"uninvaded\" ...\n $ location      : chr [1:28] \"town\" \"town\" \"town\" \"town\" ...\n $ latitude      : num [1:28] 58.8 58.8 58.8 58.8 58.8 ...\n $ longitude     : num [1:28] -94.2 -94.2 -94.2 -94.2 -94.2 ...\n $ P             : num [1:28] 95.2 42.4 4.63 8.31 33.6 77.2 12.1 22.5 7.55 11.4 ...\n $ C             : num [1:28] 9.92 5.47 2.22 1.74 5.49 4.93 4.4 7.7 6.07 6.86 ...\n $ N             : num [1:28] 0.52 0.12 0 0 0.1 0.2 0.11 0.35 0.04 0.07 ...\n $ avg_pH        : num [1:28] 7.57 7.45 7.56 7.5 7.46 ...\n $ notes         : chr [1:28] NA NA \"total N &lt; 0.02\" \"total N &lt; 0.02\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   sample_id = col_character(),\n  ..   site_number = col_double(),\n  ..   invaded_status = col_character(),\n  ..   location = col_character(),\n  ..   latitude = col_double(),\n  ..   longitude = col_double(),\n  ..   P = col_double(),\n  ..   C = col_double(),\n  ..   N = col_double(),\n  ..   avg_pH = col_double(),\n  ..   notes = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nOkay, so in this data, we have columns that contain our environmental data: the concentration of various chemicals in the soil, if there are invasive species (categorical, two levels, invaded/uninvaded), and then a bunch of spatial information that relates each site to a broader geographic region ( 3 levels, town, airport, CNSC). We may expect that each of these broader regions will have a particular soil chemical composition, which then should (according to our question) affect the presence of invasive species.\nMore formally then, we are testing for a relationship between location, P, C, N, and avg_pH (our explanatory variables) and invaded_status (our response variable).\nBefore we get started, let’s do some data organization.\n\nnames(soil.data)\n\n [1] \"sample_id\"      \"site_number\"    \"invaded_status\" \"location\"      \n [5] \"latitude\"       \"longitude\"      \"P\"              \"C\"             \n [9] \"N\"              \"avg_pH\"         \"notes\"         \n\n# convert response into 1s and 0s\nsoil.data$binary.invaded_status &lt;- ifelse(soil.data$invaded_status==\"invaded\", 1, 0)\n\n# make sure that location is a factor\nsoil.data$location &lt;- as.factor(soil.data$location)\n\n\n14.3.1 Multicollinearity\nThe first thing we need to do is assess the relationships between our explanatory variables (location, P, C, N, avg_pH) and check for multicollinearity.\nMulticollinearity refers to a situation in which two or more explanatory variables are highly linearly related (See here for a mathematical explanation). This can wreck havoc in an analysis because collinear predictors generate unstable and unreliable regression coefficients that have large standard errors.\nSo the first step in this analysis is to visualize all our environmental data and assess whether any variables are highly correlated with each other (ie are collinear).\n\n14.3.1.1 Correlations\nThere are a couple of ways of assessing collinearity of our predictor variables. One common way you may have heard of is to calculate the correlation between the variables. We do this using cor and can visualize the relationships using pairs. Note that this method only works for numeric data (ie it can’t handle categorical variables).\n\npairs(soil.data[,7:10])\n\n\n\ncor(soil.data[,7:10])\n\n                P          C          N     avg_pH\nP       1.0000000  0.5090295  0.6482000 -0.4091076\nC       0.5090295  1.0000000  0.9086144 -0.4722516\nN       0.6482000  0.9086144  1.0000000 -0.5681717\navg_pH -0.4091076 -0.4722516 -0.5681717  1.0000000\n\n\nWe can see from this that C and N are highly related. When we look at the correlations, this linked relationship is confirmed. As a rule of thumb, ecologists generally report an r&gt;=0.70 as being collinear. Note that this threshold is a bit arbitrary and different sub-disciplines have different traditions.\nBased off this output, we could choose to drop either C or N from the analysis as they will explain similar variation in the data. Generally, we would choose to drop whichever variable we were less interested in scientifically.\n\n\n14.3.1.2 VIF = Variance Inflation Factor\nThe other common way of assessing collinearity is with the VIF. This method can handle both categorical and numeric data.\nThe VIF is calculated for each predictor by doing a linear regression of that predictor on all the other predictors, and then obtaining the R2 from that regression. The VIF is just 1/(1-R2). This is why it’s called the variance inflation factor - it estimates how much the variance of a coefficient is “inflated” due to linear dependence with other predictors.\nVIF’s start at 1 and have no upper limit. In general:\n\nVIF = 1 means no correlation between predictor and any other predictor variables in model\n1 &lt; VIF &lt; 5 suggests moderate collinearity\nVIF &gt; 5 means severe collinearity\n\nNote that there are some cases in which high VIF values can be ignored, for example for polynomial terms (see here for a detailed discussion).\nVIF is used on model rather than on the raw predictors. So let’s fit our model and include location (town vs airport) and see if any of our predictors are collinear. A VIF above 3 (or also commonly 5) indicate high collinearity.\nNote that for 28 observations this model is also very over fit (too many predictor terms to get general model parameters), but let’s ignore that for now.\n\nmodel_collinear &lt;- glm(binary.invaded_status ~ location + P + C + N + avg_pH, \n                      data=soil.data, \n                      family=\"binomial\")\ncar::vif(model_collinear)\n\n             GVIF Df GVIF^(1/(2*Df))\nlocation 1.466368  2        1.100426\nP        2.441326  1        1.562474\nC        5.081174  1        2.254146\nN        8.102951  1        2.846568\navg_pH   1.462175  1        1.209204\n\n\nWe have multiple levels in our categorical predictor (airport/town/CNSC) and so we are returning GVIF which are produce better estimates when the degrees of freedom &gt; 1. We interpret GVIF the same was as VIF and apply the same rule of thumb.\nWhich variables are collinear? Does this match our conclusion from using cor() above?\nJust as above, we can see the C and N are collinear and have a high VIF. Generally, best practice is to drop the term with the highest VIF and rerun, and keep going, dropping terms until all VIF are below the threshold for collinearity.\n\nmodel_collinear_dropN &lt;- glm(binary.invaded_status ~ location + P + C + avg_pH, \n                      data=soil.data, \n                      family=\"binomial\")\nvif(model_collinear_dropN) \n\n             GVIF Df GVIF^(1/(2*Df))\nlocation 1.315957  2        1.071052\nP        1.964871  1        1.401739\nC        1.635915  1        1.279029\navg_pH   1.306870  1        1.143184\n\n\nHow does VIF change if drop N? How does the output change? What can we conclude from this?\nNote that dropping variables due to collinearity isn’t p-hacking. In your methods, you would report all the predictors and your method of checking for collinearity, and what threshold you use, and which variables you exclude.\nOkay, so what happens though if N is actually our variable of interest and we’ve just gotten rid of it from the model??? Do we panic??\nNo! Luckily we’ve just learned about multivariate techniques for grouping correlated data into common axes of variation.\n\n\n14.3.1.3 PCA\nWhich method might we use on this data?\nPCA is also useful in this case for exploring how these different soil measurements are organized across sites.\nShould we use a covariance or correlation matrix?\n\n# pca on data\n# each row is a site\nnames(soil.data)\n\n [1] \"sample_id\"             \"site_number\"           \"invaded_status\"       \n [4] \"location\"              \"latitude\"              \"longitude\"            \n [7] \"P\"                     \"C\"                     \"N\"                    \n[10] \"avg_pH\"                \"notes\"                 \"binary.invaded_status\"\n\n# make sure to select columns that you want to include in the PCA!\npca.soil &lt;- prcomp(soil.data[, c(7:10)], scale. = TRUE) # use corr bc multiple units\n\nNow let’s look at the output.\n\n# look at importance of components\nplot(pca.soil)\n\n\n\n\nHow many axes should we keep?\n\n# let's look at loadings\npca.soil$rotation\n\n              PC1       PC2        PC3        PC4\nP      -0.4561476 0.3487729 -0.8033374  0.1579108\nC      -0.5326177 0.2372107  0.5269634  0.6183519\nN      -0.5713178 0.1657433  0.2459331 -0.7652725\navg_pH  0.4264314 0.8914133  0.1283571 -0.0840417\n\n\nSo how would we interpret this? Let’s start with PC1.\nNow let’s visualize, it’ll be much easier to interpret! We’re just going to use autoplot here, but remember the base plot function biplot also works.\n\nautoplot(pca.soil, loadings=TRUE, loadings.label=TRUE, \n         loadings.colour = \"grey30\",  # changes the colour of the arrows\n         data = soil.data, \n         colour=\"location\", # colour obs by location\n         loadings.label.hjust = 1.2) + # moves loading labels so easier to read\n  theme_classic() +  # make fancier (removes box)\n  scale_color_manual(values= wes_palette(\"FantasticFox1\", n = 3))  # pretty colours!\n\n\n\n\nWhat jumps out to you about this figure? What are some of the things this tells us?\n\nPC1 explains most of variation and is doing most of heavy lifting\nP, C, and N are all highly correlated together (arrows of similar length and pointing in same direction)\npH is relatively uncorrelated (orthogonal to P, C, N)\nP, C, N load strongly on PC1, setting up an axis of more to less concentration\npH loads mostly (but not exclusively) on PC2\nIf you squint maybe the different locations are grouped in PC space… but more likely it looks like location doesn’t explain where the data (sites) fall out in relation to P, C, N, and pH (ah well, this is what happens when we work with real data!)\n\nLet’s run our same logistic model again to test if soil characteristics explain the relationship between if a site and invasive species or not. This time, instead of using all the soil data, we will use PC axes.\nBut first, some (more) data organization.\n\n# see each observation and it's location along PC axes\npca.soil$x\n\n              PC1         PC2         PC3          PC4\n [1,] -0.23126104  0.77648694 -0.84121494  0.235136523\n [2,]  0.84406257 -0.50103455 -0.34039831  0.114929541\n [3,]  1.88826355 -0.50887992  0.19959381 -0.253290787\n [4,]  1.75535290 -0.76347862  0.04570655 -0.258661129\n [5,]  0.99177246 -0.52426046 -0.15590610  0.098831472\n [6,]  0.53969902  0.08152624 -1.03403889  0.085747726\n [7,]  1.66916888 -0.03427478  0.30698806 -0.171959835\n [8,]  0.80122422 -0.31463880  0.37058135 -0.043269362\n [9,]  1.93351386  0.56837626  0.59677085 -0.005952411\n[10,]  1.63224314  0.29434028  0.54303874  0.082469913\n[11,] -2.71397334  0.75174581 -0.24500050 -0.299725322\n[12,] -0.03236594  0.14566698 -0.31835184  0.157973860\n[13,] -0.84681650 -1.12969032 -1.02413728  0.043433577\n[14,]  0.63779987 -0.13876400 -0.37249968 -0.141932555\n[15,] -2.10740961  0.03617329 -0.86306690  0.051402935\n[16,] -2.36369664  0.96422202 -0.37958188 -0.279045169\n[17,] -2.97841280 -0.28704561  0.51688116 -0.282026062\n[18,] -1.56857576  0.86351100 -1.57655687  0.058062467\n[19,] -0.87738490  0.53086067  0.92362157  1.051842622\n[20,]  0.13763151  0.16316485  0.27107051  0.304104023\n[21,] -4.08292912 -0.12691283  2.11374890 -0.146981087\n[22,] -0.03084636 -0.65130088  0.49727778 -0.033064882\n[23,]  0.18445264 -0.13967958 -0.06872346  0.014528613\n[24,]  1.72798063  0.92802356  0.10157757 -0.193211086\n[25,]  0.99564140  0.86534818  0.45856629 -0.223903218\n[26,] -0.74210843 -2.72156445 -0.10274029  0.116180707\n[27,]  0.75530840  0.17215662 -0.36418195 -0.039898398\n[28,]  2.08166538  0.69992209  0.74097578 -0.041722676\n\n# bind the observations to the data, note that `R` keeps everything in the same order, so this is easy\nsoil.data$PC1 &lt;- pca.soil$x[ ,1]  # pull PC1\nsoil.data$PC2 &lt;- pca.soil$x[ ,2]  # pull PC2\nhead(soil.data)\n\n# A tibble: 6 × 14\n  sample_id site_number invaded_status location latitude longitude     P     C\n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;          &lt;fct&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1A                  1 invaded        town         58.8     -94.2 95.2   9.92\n2 1B                  1 uninvaded      town         58.8     -94.2 42.4   5.47\n3 6A                  2 invaded        town         58.8     -94.2  4.63  2.22\n4 6B                  2 uninvaded      town         58.8     -94.2  8.31  1.74\n5 7A                  3 invaded        town         58.8     -94.2 33.6   5.49\n6 7B                  3 uninvaded      town         58.8     -94.2 77.2   4.93\n# ℹ 6 more variables: N &lt;dbl&gt;, avg_pH &lt;dbl&gt;, notes &lt;chr&gt;,\n#   binary.invaded_status &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;\n\n\n\n\n14.3.1.4 Modelling\nOkay, now let’s run the model.\n\nmodel_PCA &lt;- glm(binary.invaded_status ~ location + PC1, \n                      data = soil.data, \n                      family = \"binomial\")\nvif(model_PCA) # nice, this is what we want to see\n\n             GVIF Df GVIF^(1/(2*Df))\nlocation 1.136794  2        1.032572\nPC1      1.136794  1        1.066205\n\n\n\nsummary(model_PCA)\n\n\nCall:\nglm(formula = binary.invaded_status ~ location + PC1, family = \"binomial\", \n    data = soil.data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4490  -1.1062  -0.1187   1.1441   1.5497  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)   -0.1850     0.6195  -0.299    0.765\nlocationCNSC   0.4488     1.2323   0.364    0.716\nlocationtown   0.2988     0.8845   0.338    0.736\nPC1           -0.3401     0.2685  -1.267    0.205\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 38.816  on 27  degrees of freedom\nResidual deviance: 37.037  on 24  degrees of freedom\nAIC: 45.037\n\nNumber of Fisher Scoring iterations: 4\n\n\nAh well, it looks like there’s no relationship between soil characteristics and invasion.\nWe can actually see this very clearly if we just plot our data.\n\nggplot(soil.data, aes(PC1, binary.invaded_status, colour=location)) +\n  geom_point(size=3, alpha=.8) +\n  theme_classic() +  \n  scale_color_manual(values= wes_palette(\"FantasticFox1\", n = 3))  +\n  labs(y=\"Invasion success\")"
  },
  {
    "objectID": "lec11-multivariate-stats.html#footnotes",
    "href": "lec11-multivariate-stats.html#footnotes",
    "title": "13  Multivariate statistics",
    "section": "",
    "text": "This explanation was largely lifted verbatim from Borcard, D., Gillet, F. and Legendre, P. 2011. Numerical Ecology with R. Chapter 5, page 116.↩︎"
  },
  {
    "objectID": "lec12-command-line_git.html#lesson-preamble",
    "href": "lec12-command-line_git.html#lesson-preamble",
    "title": "14  Command line & Git(Hub)",
    "section": "14.1 Lesson preamble",
    "text": "14.1 Lesson preamble\n\n14.1.1 Lesson objectives\n\nDevelop familiarity with the logic of the command line, including how to\n\nNavigate between directories\nCreate, copy, move, delete and compare files and directories\nSearch, edit, and read content from files\nChange file permissions\nInstall, update and remove packages\nMove files and directories via scp\n\nUnderstand how to run R code from the command line.\nLearn how Git(Hub) works and the logic of version control\n\nDevelop proficiency using basic Git commands\nDevelop understanding of the Git workflow and recommended practices\nPractice using Git commands at the command line\n\n\n14.1.2 Lesson outline\n\nIntroduction to the command line and where/why/when we use it (10 mins)\nA tour through different command line tools (30 mins)\nRunning R on the command line (10 mins)\nCollaborating with GitHub with Command Line (40 min)\nWorkflows and recommended practices (10 mins)"
  },
  {
    "objectID": "lec12-command-line_git.html#why-use-the-command-line",
    "href": "lec12-command-line_git.html#why-use-the-command-line",
    "title": "14  Command line & Git(Hub)",
    "section": "14.2 Why use the command line?",
    "text": "14.2 Why use the command line?\nIncreasing amounts of data in biology (especially genomic data stored in databases like the UK Biobank and GenBank) are transforming the field. These data have allowed us to ask and answer questions that would otherwise be impossible to address, and have motivated extensive cross-talk between biology, computer science, mathematics, and statistics. To analyze large-scale datasets in a way that is efficient, robust, reproducible, and scale-able, researchers turn to the command line. The command line interface allows researches to pass commands to the shell.\nSome advantages of working are the commend line are:\n\nEfficient parallelization: where it would not be possible to preform certain calculations due to time and memory limitations, the command line allows us to interface with computing clusters which can run many calculations or processes simultaneously. For example, efficient parallelization can allow a user to run many simulations of a model (each run corresponding to a different combination of parameter values) simultaneously. This can cut run times from months to seconds.\nControl over data: the command line allows users to view, parse, transform, and transfer large files (such as genome sequences) efficiently. Such control is simply not possible with a GUI like RStudio.\nAutomation, reproducibility, and computational ease: shell commands automate tasks which would otherwise be 1) computationally expensive, 2) error-prone, and 3) emotionally taxing.1 Shell commands can be scripted, shared, version-controlled, and made to run at certain times. (For example, shell scripts designed to scrape the web for data are often made to run during off-peak hours.) Because the command line makes automating complex tasks possible, software used to analyze large data is often written for use on the command line, with no equivalent in languages like R and Python.\n\nAll that said, shell commands appear quite scary. Indeed, the command line is unforgiving–cryptic error messages abound when incorrect commands are entered. One can do serious damage without understanding the basis of the shell. The goal of this lecture is to provide you all with knowledge of 1) the logic of the command line, 2) important shell commands, 3) how to run R scripts from the command line, and 4) how to use the command line to interface with GitHub. The first half of the lecture will focus on the shell, and the second half on Git."
  },
  {
    "objectID": "lec12-command-line_git.html#logic-of-the-command-line",
    "href": "lec12-command-line_git.html#logic-of-the-command-line",
    "title": "14  Command line & Git(Hub)",
    "section": "14.3 Logic of the command line",
    "text": "14.3 Logic of the command line\nThe command line interface takes commands of the following form:\n\ncommand param1 param2 param3 … paramN\n\nwhere param1, param2, \\(\\dots,\\) and paramN are parameters provided by the user. This is a simplification, but one can get by thinking of commands as having this form, with the command line interpreter providing an interface between what is entered and the machine. Commands can range in what they do (navigation between directories, editing of files and of file permissions, etc.) but will not do what they are intended to if the relevant parameters are not specified, or if files which are called by the command are not accessible."
  },
  {
    "objectID": "lec12-command-line_git.html#where-are-we",
    "href": "lec12-command-line_git.html#where-are-we",
    "title": "14  Command line & Git(Hub)",
    "section": "14.4 Where are we?",
    "text": "14.4 Where are we?\nFor this portion of the lecture, we will move to the terminal on Mete’s machine. The commands that we execute there are given and described below:\n\ncal # returns this month's calender\ndate # returns today's date\npwd # print working directory\nls # returns contents of the working directory\nls -l # returns contents of wd and information about those contents\n\n\nmkdir newdirectory # makes new directory called newdirectory\ncd newdirectory # change directory to newdirectory\ntouch emptytextfile.txt # make new .txt file with name emptytextfile\nless emptytextfile.txt # view emptytextfile.txt\nq # stop viewing emptytextfile.txt\ntouch emptytextfile2.txt\nnano emptytextfile2.txt # edit emptytextfile2.txt\nmv emptytextfile2.txt file_nolongerempty.txt \n# make new file called file_nolongerempty.txt from emptytextfile2.txt\nless file_nolongerempty.txt\ncp file_nolongerempty.txt file_nolongerempty_todelete.txt\n# copy file_nolongerempty.txt and make new file called file_nolongerempty_todelete.txt\nrm file_nolongerempty_todelete.txt # delete file_nolongerempty_todelete.txt\n\n\ncd .. # go back to preview working directory\nmkdir newdirectory2\ncp -r newdirectory2 newdirectory3 # copy directory\nrm -r newdirectory3 # remove directory that was just made\n\n\ndiff -qr newdirectory/ newdirectory2/ \n# assess and return differences between directories specified\n# r recursively searches subdirectories, q reports 'briefly', when files differ\n# -arq is also valid, treats all files as text\n  \ncd newdirectory\ndiff file_nolongerempty.txt emptytextfile.txt \n# assess and return differences between files specified\n# -w ignores white space"
  },
  {
    "objectID": "lec12-command-line_git.html#all-about-files-permissions",
    "href": "lec12-command-line_git.html#all-about-files-permissions",
    "title": "14  Command line & Git(Hub)",
    "section": "14.5 All about files permissions",
    "text": "14.5 All about files permissions\nTo see what the permissions of a specific file are, one can use the command ls followed by -l (for all files within a directory) or -la for specific files within that directory.\n\nls -la file_nolongerempty.txt\n# multiple instances of r, w, and x reflect different levels of ownership\n# r = can read the file, can ls the directory\n# w = can write the file, can modify the directory's contents\n# x = can execute the file, can cd to the directory\n\nFor example, the rw that appears first in -rw-r--r-- indicates the owner (user) can can read and write to the file but can’t execute it as a program. The r that appears next indicates group members can read the file. drwxr-xr-x indicates group members can view as well as enter the directory.\nThe command cmod for “change mode” allows one to modify file permissions.\n\nchmod a+r file_nolongerempty.txt\n# a stands for all (default, so can be omitted), +r = add read permission\n# g = group, o = other, u = user\n# - = remove access, = sets exact access\nchmod go-rw file_nolongerempty.txt # removes group read and write permissions\n\nchmod also acts on directories but requires -R arguement. chmod -R o+x would grant execution permissions for other users to a directory and its subdirectories.\nFor more about the chmod command (e.g., specifying the entire state of a file or directories permissions by providing the command numbers rather than combinations of r,x,w), one can run the command man chmod. The man command displays the manual page for a particular command.\nTo see what a shell command does, we also reccomend you check out https://explainshell.com."
  },
  {
    "objectID": "lec12-command-line_git.html#apt-sudo-get-and-all-that-jazz",
    "href": "lec12-command-line_git.html#apt-sudo-get-and-all-that-jazz",
    "title": "14  Command line & Git(Hub)",
    "section": "14.6 apt, sudo, get, and all that jazz",
    "text": "14.6 apt, sudo, get, and all that jazz\nAn important but unfortunately tricky part of using the command line is using the utility apt (or one of its cousins) to install, update, remove, or otherwise manage packages on a Linux distribution. More infromation how apt works and can be used can be found at https://ubuntu.com/server/docs/package-management. Since there are some steps to get apt to work on a Mac, Mete will illustrate how to do package installation using the free and open source package manage Homebrew.\nOne can install Homebrew by running\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\nbrew can be used to install packages the machine does not have. For example, running\n\nbrew install wget\n\ninstalls the package wget, a free package for retrieving files using HTTP, HTTPS, FTP and FTPS. Homebrew does this by running Ruby and Git, which you can see for yourseld by entering the following command:\n\nbrew edit wget\n\nHomebrew also allows users to create their own packages, has very through documentation, and even facilitates the installation of new fonts and non-open source software.\n\nbrew install --cask firefox\n\nA list of packages that are installed can be found by running\n\nbrew list\nbrew deps --tree --installed # illustrates dependencies\n\nA particular piece of software can be found by using brew search:\n\nbrew search google\n\nbrew install will install software such as the highly useful Basic Local Alignment Search Tool (BLAST):\n\nbrew install blast\n\nAlong these lines, it is important to mention that one sometimes will run into issues installing packages using brew or apt. To get around this, a very handy (but at times dangerous) command to have ready is sudo. For example, trying to run shutdown, Mete gets an error. But if we run\n\nsudo shutdown\n\none can run … just about anything. sudo stands for superuser do."
  },
  {
    "objectID": "lec12-command-line_git.html#running-an-r-script-from-the-command-line",
    "href": "lec12-command-line_git.html#running-an-r-script-from-the-command-line",
    "title": "14  Command line & Git(Hub)",
    "section": "14.7 Running an R script from the command line!",
    "text": "14.7 Running an R script from the command line!\nSo far we have seen how to\n\nNavigate between directories\nCreate, copy, move, delete and compare files and directories\nEdit and read content from files\nChange file permissions\nInstall, update and remove packages\n\nNow we will discuss how to execute code (an R script) from the command line. We will then take the .txt that is returned, search it, edit, and move the edited file to another directory via the scp command.\nThe script we will run is as follows:\n\nsetwd(\"/Users/meteyuksel\")\ngetwd()\nlist.files()\n\nfile_nolongerempty &lt;- as.vector(read.table(\"file_nolongerempty.txt\", \n                                           sep = '\\t', header=F)\n                                )\n\nnames(file_nolongerempty) &lt;- NULL\nclass(file_nolongerempty)\nprint(file_nolongerempty)\n\nfile_nolongerempty[[1]][1]\n\nvec &lt;- c(pi)\noptions(digits = 15)\n\nfor (i in 1:20){\n  vec[i+1] &lt;- sqrt(vec[i])\n}\n\nprint(vec)\n\nwrite.csv(vec, \"squarerootsofpi.csv\")\n\nTo do this, we create a .R file called script_with_descriptive_name.R in the directory newdirectory by writing a new file of that name, and editing the file by copy/pasting the code within the preceding chunk.\n\n14.7.1 Challenge\nWhat commands are needed to do this, and to check the permissions of the program?\n\ntouch script_with_descriptive_name.R\nnano script_with_descriptive_name.R\nls -la script_with_descriptive_name.R\n\nTo run the .R script from the command line, we simply execute the command\n\nRscript script_with_descriptive_name.R\n\nFor other languages, the syntax varies slightly. In Python, we would write python script.py."
  },
  {
    "objectID": "lec12-command-line_git.html#moving-files-securely",
    "href": "lec12-command-line_git.html#moving-files-securely",
    "title": "14  Command line & Git(Hub)",
    "section": "14.8 Moving files securely…",
    "text": "14.8 Moving files securely…\nNow, to conclude this section, we will move the file to squarerootsofpi.csv that was created by running the R script script_with_descriptive_name.R in this newdirectory to a remote machine Mete uses for his research. We do this as follows:\n\nscp -r ~/squarerootsofpi.csv myuksel@graham.computecanada.ca:\n\nscp stands for secure transfer and is a very powerful tool to move feels between machines."
  },
  {
    "objectID": "lec12-command-line_git.html#intro-to-high-preformence-cluster-computing",
    "href": "lec12-command-line_git.html#intro-to-high-preformence-cluster-computing",
    "title": "14  Command line & Git(Hub)",
    "section": "14.9 Intro to high preformence (cluster) computing",
    "text": "14.9 Intro to high preformence (cluster) computing\nAlthough there is still so much left to learn about the command line, we will conclude our exploration of all it has to offer with a quick introduction to high preference (cluster) computing. We will see that, upon interfacing with the remote machine Mete uses for his research (Graham, hosted by Compute Canada), that the squarerootsofpi.csv file has been deposited in his home directory.\nIndeed, ssh (for secure shell) allows us to securely access the machine through Mete’s account:\n\nssh -Y myuksel@graham.computecanada.ca\nls ### good enough, there is squarerootsofpi.csv! incredible!\n\nOn machines like Graham, one has to use the command line to navigate between directories, write and edit files, execute programs, etc. The below chunk creates two text files in the home directory, concatenates them with the command cat, and stores the output in a new file newfile.txt using the pipe command &gt;.\n\ntouch file1.txt\nnano file1.txt ### write its as easy 1,2,3 and save\ntouch file2.txt\nnano file2.txt ### write you and me and save\ncat file1 file2 &gt; newfile\n# &gt;&gt; overwrites the destination file\n\nWe can run code in R using Rscript as before, or by entering an R session:\n\nmodule load gcc/9.3.0 r/4.0.2\n\nFinally, we can automate what code we would like to run by writing and executing a SLURM script with the extension .sh. SLURM is a scheduler for large, shared compuiting resources like Graham. A simple script that schedules a job to run when the appropriate resources are avaiable (depending on, e.g., how frequently one has run jobs in the recent past) is as follows:\n\n#!/bin/bash\n#SBATCH --account=def-someacct   # replace this with your own account\n#SBATCH --nodes=1                # number of node MUST be 1\n#SBATCH --ntasks-per-node=32     # specifies the number (=64) of cpus to use\n#SBATCH --mem-per-cpu=2000M      # memory; default unit is megabytes\n#SBATCH --time=0-00:15           # time (DD-HH:MM)\nmodule load gcc/9.3.0 r/4.0.2\n\nRscript computation.R\n\nSuppose the name for this SLURM file is job.sh. Running sbatch job.sh will execute the commands of this script line-by-line. The intermediate lines specify the account which is running the job, the memory that ought to be allocated to the job, the maximum time the job should run for, and the number of cores that should be used if computations are parallelized (i.e., are instructred to be sent to and run on separate CPUs) within the R script. This is an example of a bash script, i.e., a sequence of commands that are executed in the shell line-by-line. These scripts are a powerful way to automate complex tasks involving large files."
  },
  {
    "objectID": "lec12-command-line_git.html#reproducible-science-and-collaboration-with-git-and-github",
    "href": "lec12-command-line_git.html#reproducible-science-and-collaboration-with-git-and-github",
    "title": "14  Command line & Git(Hub)",
    "section": "14.10 Reproducible science and collaboration with Git and GitHub",
    "text": "14.10 Reproducible science and collaboration with Git and GitHub\n\n14.10.1 Intro to Version Control using Git\nGit is a version control system that tracks changes in files. Although it is primarily used for software development, Git can be used to track changes in any files, and is an indispensable tool for collaborative projects. Using Git, we effectively create different versions of our files and track who has made what changes. The complete history of the changes for a particular project and their metadata make up a repository.\nTo sync the repository across different computers and to collaborate with others, Git is often used via GitHub, a web-based service that hosts Git repositories. In the spirit of “working open” and ensuring scientific reproducibility, it has also become increasingly common for scientists to upload scripts and related files to GitHub for others to use.\n\n\n14.10.2 Intro to GitHub\nGitHub allows for easy use of Git for collaborative purposes using a primarily point-and-click interface, in addition to providing a web-based hosting service for Git repositories (or “repos”). If you have not already made a GitHub account, do so now here.\nA new repository can be made by clicking on the + in the top right of the page and selecting “New repository”. For now, however, navigate to your provided group repo. All members of the group have been given admin access to a pre-made repository in the 2023-GroupX projects repo. All groups have been provided with existing repositories, but a new repository can be made by clicking on the + in the top right of the page and selecting “New repository”. For now, however, navigate to your provided group repo."
  },
  {
    "objectID": "lec12-command-line_git.html#version-control-with-git",
    "href": "lec12-command-line_git.html#version-control-with-git",
    "title": "14  Command line & Git(Hub)",
    "section": "14.11 Version Control with Git",
    "text": "14.11 Version Control with Git\n\n14.11.1 Setup and Installation\nGit is primarily used on the command line. The implementation of the command line that we’ll be using is known as a “bash interpreter”, or “shell”. While bash interpreters are natively available on Mac and Linux operating systems, Windows users may have to externally install a bash shell.\nTo install Git on a Mac, you will have to install Xcode, which can be downloaded from the Mac App Store. Be warned: Xcode is a very large download (approximately 6 GB)! Install Xcode and Git should be ready for use. (Note: most students will have already installed Xcode already for some R packages we used earlier in the course. If you’re not sure whether this is the case, run the git --version command described below)\nTo install Git (and Bash) on Windows, download Git for Windows from its website. At the time of writing, 2.19.1 is the most recent version. Download the exe file to get started. Git for Windows provides a program called Git Bash, which provides a bash interpreter that can also run Git commands.\nTo install Git for Linux, use the preferred package manager for your distribution and follow the instructions listed here.\nTo test whether Git has successfully been installed, open a bash interpreter, type:\n\ngit --version\n\nand hit Enter. If the interpreter returns a version number, Git has been installed.\n\n\n14.11.2 Getting started with Git\nFirst, we have to tell Git who we are. This is especially important when collaborating with Git and keeping track of who did what! Keep in mind that this only has to be done the first time we’re using Git.\nThis is done with the git config command. When using Git, all commands begin with git, followed by more specific subcommands.\n\ngit config --global user.name \"My Name\"\ngit config --global user.email \"myemail@example.com\"\n\nFinally, the following command can be used to review Git configurations:\n\ngit config --list\n\n\n\n14.11.3 Cloning local from main\nAfter configuring our name and email, we are ready for version control!\nFirst, we need to initialize our local repository, so we need to tell Git where to store our files on our computer. At the same time, we can also connect the two repositories: the local one on your computer, and the remote one on GitHub. We do both by making the GitHub repository a remote for the local repository.\nFirst, head to your group’s repo on GitHub, and click on the green “Clone or download” button on the right side of the page. This yields a link to your fork. Copy this link to your clipboard.\nOn the command line, run\n\ngit clone [repo link]\n\nwith the link in place of [repo link]. This process, known as cloning, will create a new folder in your current working directory that contains the contents of your GitHub folder. This also initializes your local repo.\nEnter this new folder with cd and type git status to make sure the repo has been cloned properly. git status should output that the branch is even with origin/main, indicating that it is currently the same as the current state of your fork.\n\n\n14.11.4 Adding and commiting files\nFor your projects, you will be making edits to your .Rmd file in RStudio, or you will be writing your report in text editors. For today, let’s use the file we created earlier and commit it.\nThe “untracked files” message means that there’s a file in the directory that Git isn’t keeping track of. We can tell Git to track a file using git add:\n\ngit add draft-yn.txt\n\nGit now knows that it’s supposed to keep track of mars.txt, but it hasn’t recorded these changes as a commit yet. To get it to do that, we need to run one more command:\ngit commit -m \"intro about my project\nWhen we run git commit, Git takes everything we have told it to save by using git add and stores a copy permanently inside the special .git directory. This permanent copy is called a commit (or revision) and its short identifier is a series of letters and numbers. Each commit has a unique identifier.\nWe use the -m flag to write a message that describes our edits specifically.\n\n\n\nPictured: how add and commit saves our work\n\n\n\n\n14.11.5 Pushing changes\nNow, let’s push our changes from the local repo to the main repo.\nWe can push our changes:\n\ngit push origin main\n\nThe first time you run the push subcommand, you may get a prompt asking you to enter your GitHub username and password. If you are entering your password and nothing pops up, don’t worry! Your keystrokes are being recognized, although there is no visual cue for it.\nNow, running git status shows us that our local repo is up-to- date with origin/main.\nIf we navigate to GitHub, we now see that we have our updates in the main repo, and there is a comment “intro about my project” associated with the commit.\nGit commit history is a directed acyclic graph (or DAG), which means that every single commit always has at least one “parent” commit (the previous commit(s) in the history), and any individual commit can have multiple “children”. This history can be traced back through the “lineage” or “ancestry”.\n\n\n14.11.6 Pulling changes\nWhen other group members add to the shared repo, you have to make sure those edits have been incorporated into your repo before making new changes of your own. This ensures that there aren’t any conflicts within files, wherein your edits clash with someone else’s if one of you is working with an earlier version of the file.\nTo remain up to date, navigate to the local copy of your repo.\nFirst, you have to fetch the new changes that are in the shared origin repo.\n\ngit fetch origin\n\nOnce the edits have been downloaded from the origin, merge them into your local main repo:\n\ngit merge origin/main\n\nNote that the git pull command combines two other commands, git fetch and git merge.\n\ngit pull\n\nThis will download any changes your group members may have made and update your local versions of your fork accordingly.\nYour local copy is now even with the shared repo!\nBefore starting any edits of your own, it’s usually a good idea to start off by checking to see whether anything’s been added to the main repo and, if needed, pulling those changes\n\n\n14.11.7 Summary\nBasic Git commands:\n\ngit init (or git clone)\ngit status\ngit add\ngit commit\ngit push\ngit log\ngit checkout\ngit help\n\nThe “GitHub flow” (so far):\n\nMake sure your local is up to date using git pull\nMake your edits\nAdd your edits with git add\nCommit your edits with an informative commit message\nPush your edits\nRepeat\n\n\n\n\nPictured: The Git workflow, from Cloud Studio"
  },
  {
    "objectID": "lec12-command-line_git.html#github-issues",
    "href": "lec12-command-line_git.html#github-issues",
    "title": "14  Command line & Git(Hub)",
    "section": "14.12 GitHub Issues",
    "text": "14.12 GitHub Issues\nFinally, each repo on GitHub also has an Issues tab at the top of the page. Here, you and your group can create posts regarding the content of the repo that highlight issues with code or serve as to-do lists to manage outstanding tasks with.\nAlthough issues aren’t needed for any of the steps we discussed above, it can be useful to create a roadmap of your project with them and\nassign group members to specific tasks if need be."
  },
  {
    "objectID": "lec12-command-line_git.html#best-practices-for-collaboration-extra",
    "href": "lec12-command-line_git.html#best-practices-for-collaboration-extra",
    "title": "14  Command line & Git(Hub)",
    "section": "14.13 Best practices for collaboration (extra)",
    "text": "14.13 Best practices for collaboration (extra)\nUsually, there are several other steps involved during collaboration. At this point, we are able to push our edits from our local machine straight to the final version on the GitHub repo. This can be dangerous if no one else is checking over your code, especially if every team member has direct access to change the contents of the main repo, and all of your project files. These additional steps include using forks and branches.\n\n14.13.1 Creating a Fork\nThe repos that have already been created can be thought of as “main repos”, which will contain the “primary” version of the repo at any given point in time. However, instead of directly uploading and editing files right within this main repo itself, usually collaborators will be begin by forking the repo. When a given user forks a repo, GitHub creates a user-specific copy of the repo and all its files in a remote location.\nHaving a forked copy means that the developer who performs the fork will have complete control over the newly copied codebase. Developers who contributed to the Git repository that was forked will have no knowledge of the newly forked repo. Previous contributors will have no means with which they can contribute to or synchronize with the Git fork unless the developer who performed the fork operation provides access to them.\nSome famous examples of Git forks include:\n\nFire OS for Kindle Fire, which is a fork of Android\nLibreOffice, which is a fork of OpenOffice\nUbuntu, which is a fork of Debian\nMariaDB, from MySQL\n\n\n\n\nPictured: Git Clone verus Git Fork, from TheServerSide.\n\n\nIf you want to create a fork to have your version of your group’s repo on GitHub, navigate to the repo’s page and click on the “Fork” button at the top right of the page. Following a brief load screen, GitHub will redirect you to your new, forked repo.\nYou’ll notice that on the top left of this repo page, the repo’s name will be “[your username] / 2023-GroupX”, as opposed to “EEB313 / 2023-GroupX”. Furthermore, GitHub will indicate that this is a fork right underneath said repo name (“forked from eeb313-[year]/ [repo name]”).\n\n\n14.13.2 Setting up your remotes\nNow we connect the three repositories: the local repo on your computer, the forked repo on GitHub, and the main group remote repo on GitHub. We do this by making the GitHub repository a remote for the local repository.\nTo get your fork up to date with the main repo, you next have to add a remote linking to the main repo. Head to your group’s repo and once again click on “Clone or download” to grab its link. Then, using the main repo link, run:\n\ngit remote add upstream [repo link]\n\nThen, we have to update our forked repo. Earlier, since we didn’t have a forked repo, we only had a link between our local computer version of the folder and the shared version. When we cloned the shared version, we created that remote link. Now, we have changed that shared repo as the upstream remote, and we have a fork that acts as an intermediate step. Let’s change our fork to become the main remote.\nRun:\n\ngit remote -v\n\nto get a list of existing remotes. This should return four links, two of which are labelled origin and two of which are labelled upstream. At this point, they should be the same link. We need to update our main remote by removing it and adding our fork link.\nTo remove our origin remote, run:\n\ngit remote rm origin\n\nThen, let’s add our fork:\n\ngit remote add origin [repo link]\n\nNow, when you run :\n\ngit remote -v\n\nyou should find two links for origin and two links for upstream. These two links are used to fetch from upstream, and one to push from main to upstream. The links for origin are your main forked repo on your own GitHub, while the links for upstream are the EEB313-2023-GroupX repo.\n\n\n14.13.3 Syncing your fork\nNext, you have to fetch the new changes that are in the shared repo.\n\ngit fetch upstream\n\nOnce the edits have been downloaded from upstream, merge them into your local repo:\n\ngit merge upstream/main\n\nYour local copy is now even with the main repo! Finally, push these changes to the GitHub version of your fork (origin) from your main local repo.\n\ngit push origin main\n\nNow the GitHub version of the fork is all synced up, ready for your next batch of edits, and eventually another pull request!\n\n\n14.13.4 Making edits\nEvery time you make edits to your local files, first make sure you are first syncing any changes from upstream to your local to your origin (fork).\nThen, go ahead and make your edits! After you are done your batch of editing, you can add and commit those changes, still using git add and git commit.\nThese commits do not go to the upstream EEB313 repo, but instead end up in your forked repo. In order to contribute your changes from your fork to upstream, you will need to make a Pull Request (PR).\n\n\n14.13.5 Pull requests\nAfter a commit has been made, head to your fork. GitHub will have noticed that there are new edits that you can contribute. Click “Contribute” and “Open Pull Request”. A PR is GitHub’s way of neatly packaging one or more commits that have been made into a single request, and then you can merge said commits into upstream. In our case, a PR is essentially you saying: “Here are all the edits I’ve made. Have a look, and add them to main if you think they’re ready to go.”\nFollowing a pull request pening, GitHub takes you to the “Pull Requests” tab of the repo and prompts you to write about your pull request (i.e., describe the changes you’re attempting to merge). Here, you can (and should) explain the changes you’ve made for your collaborators, so that they know what to look for and review. Be specific and detailed to save your group members’ time – it’s a good idea to start off your pull request message with an overall summary (“adding dplyr code to clean dataset”) followed by a point-form list of what changes have been made, if necessary.\nOnce the pull request has been made, GitHub will list both your message and your commit messages below. Clicking on any of these commits opens up a new page highlighting the changes made in that specific commit. You also have the option of merging the pull request yourself – but don’t do this! When collaborating, always have someone else review and merge your pull request.\nIf all does not look good, your team members can add messages below, and tag others using the @ symbol, similarly to most social networks. If more changes are needed before the pull request is ready to merge, any new commits you make to the main branch on your fork will automatically be added on to the pull request. This way, you can incorporate any changes or fixes suggested by your team members simply by continuing to work in your fork until your changes are ready to merge. For line-specific edits, if a file is opened up (i.e., by clicking on one of the commits), clicking on the + button that appears when hovering over a line number will allow you or a group member to add a comment specifically attached to that line. This can be useful when pointing out typos, for instance, among other things.\n\n\n14.13.6 Creating a branch\nA branch is used to isolate development work without affecting other branches in the repository. Each repository has one default branch, and can have multiple other branches. Branches allow you to develop features, fix bugs, or safely experiment with new ideas in a contained area of your repository.\nThe main branch should be thought of as the actual current state of your project – branches are meant, by design, to be temporary, and exist only to facilitate edits and experimental work while avoiding any risk of breaking the original codebase. Branches can keep experimental work separate; for example, you can create a separate branch from your main branch so any trials or bugs only exist in your branch.\nBranches are simply a named pointer to a commit in the DAG, which automatically move forward as commits are made. Divergent commits (two commits with the same parent) could be considered “virtual” branches. Since they are simply pointers, branches in Git are very lightweight.\nYou always create a branch from an existing branch, typically, the default branch of your repository. You can then work on this new branch in isolation from changes that other people are making to the repository. A branch you create to build a feature is commonly referred to as a feature branch or topic branch.\n\n\n\nPictured: GitHub branches.\n\n\nTo create a new branch, run:\n\ngit branch &lt;branch-name&gt;\n\nFor example:\n\ngit branch new-feature\n\nThe repository history remains unchanged. Then, we need to record all new commits to that branch.\n\ngit checkout new-feature\n\nNote that you can combine branch creation and checkout by using only one command:\n\ngit checkout -b new-feature\n\nYou should see the confirmation: Switched to a new branch \"new-feature\"\nMake your edits, and commit them to your branch using git add and git commit. When you are ready to merge your new feature to your local main branch, head back to your main branch:\n\ngit checkout main\n\nAnd merge your new features from the new-feature branch:\n\ngit merge new-feature\n\nNow you can delete your branch, since it has been successfully merged:\n\ngit branch -d new-feature\n\nThis is a common workflow for short-lived topic branches that are used more as an isolated development than an organizational tool for longer-running features.\nNote that this is all happening locally. You now push your changes from your local main to upstream or origin. Don’t forget to create a pull request from your fork to the upstream EEB313 repo if you are using forks!\nHowever, you are also able to create a new branch and push that local branch to upstream or origin. To do this, make sure you are in your new branch:\n\ngit checkout new-feature\n\nThen, git add and git commit as you normally would. This time, instead of merging your new branch to your local main, git push to push them to your forked repo or upstream.\nNow, running git status shows us that our new-feature branch on our local repo is up-to- date with origin/main.\nIf we navigate to GitHub, we now see that we have a new branch in our forked repo (and remember that there should a comment associated with the commit)/\nNow, you can submit a pull request for the rest of your team to check on the repo. Once they review the changes, they can merge the PR, and the final version will show up on the upstream repo!\nOnce a pull request has been merged into the main repo, the new-feature branch (or whatever you have named your branch) isn’t needed anymore. Because of this, GitHub will immediately prompt you to delete the this branch as soon as the merge has been completed right in the PR.\nTo list the branches that are currently being used, use this for local branches:\n\ngit branch\ngit branch --list\n\nThe git branch command lets you create, list, rename, and delete branches. It doesn’t let you switch between branches or put a forked history back together again. For this reason, git branch is tightly integrated with the git checkout and git merge commands.\nstill need to udpate branch, pull requests, and syncing origin/upstream\n\n\n14.13.7 TL;DR\nThe general collaborative workflow is as follows:\n\nFirst, create a branch of the main repo.\nMake edits in the branch. These could involve adding/deleting lines of code or even adding/removing entire files. Keep in mind that the branch is separate from the main codebase, so don’t worry too much about deleting things or making large changes.\nOnce you have made your changes in this branch, submit what’s known as a pull request (PR) from this edited branch to main. A PR neatly packages all the edits that have been made in your branch for review by other members of your group.\nOnce your changes have been approved, merge the PR. It’s good practice to have group members merge your PRs instead of doing it yourself.\n\nAlthough this process may seem a bit laborious, using this method (also known as the “GitHub flow”) minimizes chances of error and ensures that all code is reviewed by at least one other person. Understanding how and why this process works is key to collaborative work in software development and the like, and is used by all sorts of open source projects on GitHub (including dplyr itself!)"
  },
  {
    "objectID": "lec12-command-line_git.html#git-terminology",
    "href": "lec12-command-line_git.html#git-terminology",
    "title": "14  Command line & Git(Hub)",
    "section": "14.14 Git Terminology",
    "text": "14.14 Git Terminology\n\nrepository (short form: repo): a storage area for a project containing all the files for the project and the history of all the changes made to those files\nlocal copy: the version of file stored on your own computer\nremote copy: the version of a file stored outside of your own computer, for example stored on an external server, perhaps at GitHub. Remotes are referenced by nicknames, e.g., origin or upstream.\nbranch: a named series of commits. The default branch that you download is usually called gh-pages or main. Creating a new branch makes a parallel version of a repository where changes can be made that affect the new branch only and not the original (base) version of the repository. New branches are often used to test changes or new ideas, which can later be merged with the base branch. Moving from one branch to another is called checking out a new branch.\nfork (GitHub-specific term): to copy a repository from another user and store it under your own GitHub account. Can also refer to the copied repo itself.\ngh-pages (GitHub-specific term): stands for “GitHub Pages”. This is often the default branch in repositories. Branches called gh-pages can be published as webpages hosted by GitHub.\norigin: the main remote repository you want to download files from or compare local changes you have made to. When you’ve forked a repository, your origin is your new copy of the repository in your account.\nupstream: the original repository you made your fork from. Both origin and upstream are remote repositories.\ncommit: to save changes in your working directory to your local repository\npush: send committed changes you have made on your local computer to a remote repository. For a change to show up on GitHub, the committed changes must be pushed from your computer to the remote repository.\npull: download changes from a remote repository to your local version of the same repository. This is useful when other people have made changes to a shared project, and you want to download (pull) the changes from the shared remote repository to your own computer.\npull request (GitHub-specific term, abbreviated as “PR”): send proposed changes from a specific version of a repository back to the main version of a repository to be considered for incorporation by the people maintaining the repository (the maintainers). You are requesting that the maintainers pull your changes into their repository.\n\nAdditional resources:\n\nA visual demonstration of the GitHub flow.\nA useful Git command cheat sheet.\nGuide to a good Git Workflow\n\n\n\n\nPictured: the GitHub workflow."
  },
  {
    "objectID": "lec12-command-line_git.html#footnotes",
    "href": "lec12-command-line_git.html#footnotes",
    "title": "14  Command line & Git(Hub)",
    "section": "",
    "text": "Trust us on this!↩︎"
  },
  {
    "objectID": "lec14-mathematical-models.html#lesson-preamble",
    "href": "lec14-mathematical-models.html#lesson-preamble",
    "title": "15  Mathematical modeling I",
    "section": "15.1 Lesson preamble",
    "text": "15.1 Lesson preamble\n\n15.1.1 Lesson objectives\n\nDevelop familiarity with mathematical models and why/where they are used in EEB\nDevelop understanding of important models in EEB, including\n\nThe (geometric) growth of a population with constant birth and death rates\nExponential growth of a population\nThe spread of an infectious disease in a susceptible population\nCompetition between types, competitive exclusion\nThe Wright-Fisher model of allele frequency change at a single, non-recombining locus\n\nConceptually understand equilibria and their stability\nBecome familiar with the ways approximation is used in modeling complex systems\nBecome familiar with how mathematical models can be fit to data\n\n\n\n15.1.2 Lesson outline\n\nWhy model?\nGeometric and exponential growth\nFitting models to the data\nSIR model: when does an epidemic grow?\nEquilibria and stability\nBack to the SIR model\nCompetition and competitive exclusion\nMutation-selection balance\nThe Wright-Fisher model: predictions for evolution via drift"
  },
  {
    "objectID": "lec14-mathematical-models.html#why-model",
    "href": "lec14-mathematical-models.html#why-model",
    "title": "15  Mathematical modeling I",
    "section": "15.2 Why model?",
    "text": "15.2 Why model?\nMathematical models are descriptions of biological processes which describe idealized situations and can help us build intuition and understanding for how those processes may unfold in nature. They help us\n\nclarify assumptions we may have about how complex systems work\nbuild understanding of how complex systems behave and why\ngenerate predictions and hypothesis\ntest predictions and hypothesis based on analysis of the model (or fitting the model to data)\ndetermine what data is needed to learn about something (of if it is possible to, given the data that is available, reliably infer parameters of interest)\ndetermine in what ways certain kinds of data may be problematic or biased\n\nIn this lecture, we will discuss six models: (1) a deterministic, discrete-time model of geometric growth of a population with constant per-capita rates of birth and death. (2) A model of exponential growth, the continuous-time equivalent of the geometric growth model. (3) A Susceptible-Infected-Recovered (SIR) model in which interactions between variables shape the transient and long-run the dynamics of the system. (4) A deterministic, continuous-time model of competitive exclusion between individuals of two types (e.g., different species, individuals of the same species carrying different alleles, etc.). (5) An extension of the previous model which maintains diversity via recurrent mutation between types. (6) The Wright-Fisher model, a discrete-time, stochastic model allele frequency change at a single, non-recombining locus. The Wright-Fisher model captures how genetic drift (without mutation, selection, migration, etc.) alone shapes patterns of genetic diversity through time and in the long-run."
  },
  {
    "objectID": "lec14-mathematical-models.html#geometric-growth",
    "href": "lec14-mathematical-models.html#geometric-growth",
    "title": "15  Mathematical modeling I",
    "section": "15.3 Geometric growth",
    "text": "15.3 Geometric growth\nLet \\(N_t\\) be the number of individuals at times \\(t=0,1,2,3,\\dots\\). Suppose we know \\(N_0\\), the initial number of individuals in the population. Assume\n\nFrom one time to the next, every individual reproduces at per-captia rate \\(b\\)\nFrom one time to the next, every individual dies at per-catpia rate \\(d\\)\nThe population is closed: there is no immigration and no emigration\nThere is no stochasticity: individuals do not, by chance, give birth to more/fewer offspring\n\nIn this case, we can write describe the population size at generation \\(t+1\\) in terms of the population size at the previous generation by keeping track of everything that could unfold: the birth of new individuals. The governing equation would be\n\\[N_{t+1} = N_t + b N_t - d N_t = (1+b-d)N_t, \\]\nwhere \\(b N_t\\) is the number of individuals born into the population and \\(d N_t\\) is the number of individuals who leave the population because they die. Births and deaths modify the previous population size such that, if \\(b &gt; d\\), then the population will grow. If \\(b &lt; d\\), the population will decline in size. If \\(b = d\\), then the population will stay constant in size. This model is unique insofar as it has a closed form solution, i.e., we know the state of the system at any time in the future given knowledge of its state at time zero:\n\\[N_1 = (1+b-d) N_0\\] \\[N_2 = (1+b-d) N_1 = (1+b-d)(1+b-d) N_0\\] \\[N_3 = (1+b-d) N_2 = (1+b-d) (1+b-d)^2 N_0 = (1+b-d)^3 N_0\\]\nIn general, \\(N_t = (1+b-d)^t N_0\\) for all \\(t\\).\nWhen \\(b&gt;d\\), the population is said to grow geometrically, in that the growth rate \\(1+b+d\\) is being raised to a power equal to the number of timesteps (e.g., generations) that have passed. Moreover, it is sometimes convenient in discrete time models like this to describe the dynamics in terms of how variables change from one time to the next. In the case of the geometric growth model, we can express the dynamics in terms of change in absolute population size\n\\[\\Delta N_t = N_{t+1} - N_t = (1+b-d) N_t - N_t = (b-d) N_t.\\]\nIn population ecology, \\((b-d)\\) is called the intrinsic rate of increase or decrease, and often denoted \\(r\\). If positive, the population grows in a given generation. If not, it becomes smaller."
  },
  {
    "objectID": "lec14-mathematical-models.html#exponential-growth",
    "href": "lec14-mathematical-models.html#exponential-growth",
    "title": "15  Mathematical modeling I",
    "section": "15.4 Exponential growth",
    "text": "15.4 Exponential growth\nContinuous time analogues of discrete time equations are often written as differential equations. If we assume the times \\(t=0,1,2,3,\\dots\\), we can replace the difference in the previous equation with a derivative to describe the dynamics of a population changing in continuous time:\n\\[\\frac{d N}{dt} = (b-d) N = r N\\]\nsubject to \\(N(0) = N_0\\). Like the geometric growth equation, we can solve this equation exactly. Diving through by \\(N\\) and using properties of derivatives from calculus, one has\n\\[\\frac{1}{N} \\frac{d N}{d t} = \\frac{d \\ln N}{d t} = r.\\]\nThis equation tells us that, in the continuous time version of the growth model above, the per-capita (i.e., logarithmic) rate of increase in the population size is constant. Integrating and applying the initial condition, one can show \\(N(t) = N_0 e^{r t}\\). If \\(r &gt; 0\\), i.e., \\(b &gt; d\\), then the population grows exponentially; if \\(r &lt; 0\\), the population will decay exponentially in size to zero. The trajectory of the population is completely determined in a deterministic model like this one by the parameters (i.e., the intrinsic growth rate) and initial conditions."
  },
  {
    "objectID": "lec14-mathematical-models.html#fitting-simple-models-to-data",
    "href": "lec14-mathematical-models.html#fitting-simple-models-to-data",
    "title": "15  Mathematical modeling I",
    "section": "15.5 Fitting (simple) models to data",
    "text": "15.5 Fitting (simple) models to data\nSuppose we measure the size of a population through time, and believe that the assumptions we have made (i.e., constant birth and death rates) are reasonable for the system.\nThe steps to fit a mathematical model to the data are as follow:\n\nDetermine the distribution of data around the solution of the model (difference or differential equation). A common convention is that the data are normally distributed around the solution to the model for a given initial condition and set of parameter values. If \\(Y_i\\) is the measurement at time \\(t_i\\) and \\(X_i\\) is the state of the system (e.g., the real population size or the true values of more variables, which influence each other and the dynamics), then this amounts to specifying the distribution of \\(Y_i\\) given \\(X_i\\):\n\n\\[f_{Y_i|X_i}(y_i|\\text{parameters})\\]\n\nWrite down and (numerically) maximize the log-likelihood function:\n\n\\[\\ln L(\\text{parameters}|\\text{data } y_1,\\dots,y_n) = \\sum_{i=1}^n \\ln f_{Y_i|X_i}(y_i|\\text{parameters}).\\]\nWhen the distribution of the measurements around the solution to the system (model) is normal with some variance (and mean equal to the model solution), maximizing the likelihood is equivalent to minimizing the sum of squared departures of the solution from the data.\nWe will return to fitting mathematical models to data next lecture."
  },
  {
    "objectID": "lec14-mathematical-models.html#the-sir-model",
    "href": "lec14-mathematical-models.html#the-sir-model",
    "title": "15  Mathematical modeling I",
    "section": "15.6 The SIR model",
    "text": "15.6 The SIR model\nThe Susceptible-Infected-Recovered (SIR) model describes how different processes (e.g., transmission, recovery, virulence, loss of immunity, demography) shape how a pathogen spreads in a population through time and if it is able to persist in the long-run. The model tracks the number of susceptible \\(S\\), infected \\(I\\), and recovered \\(R\\) individuals in continuous time.\nAssuming that the population size is constant (\\(N = S + I + R\\) does not change), susceptible individuals acquire the infection at rate \\(\\beta\\), and infected individuals recover at rate \\(\\gamma\\), the model equations are given by:\n\\[\\frac{dS}{dt} = - \\beta SI\\] \\[\\frac{dI}{dt} = \\beta SI - \\gamma I\\] \\[\\frac{dR}{dt} = \\gamma I\\]\nSince the population size is constant, one of these is not actually needed. Focusing in on the \\(I\\) equation, however, we notice that is of the same form as the geometric and exponential growth models: we simply track the flux in and out of the compartment due to, in the case of \\(I\\), infection and recovery.\n\n15.6.1 Challenge\n\nWhat are the assumptions of the model?\nWhat terms are missing?\nWhat is the model trying to capture?\nWhat dynamics and long run behavior might we expect of a disease that has been introduced to a largely susceptible population?"
  },
  {
    "objectID": "lec14-mathematical-models.html#when-will-the-epidemic-grow",
    "href": "lec14-mathematical-models.html#when-will-the-epidemic-grow",
    "title": "15  Mathematical modeling I",
    "section": "15.7 When will the epidemic grow?",
    "text": "15.7 When will the epidemic grow?\nTo determine if an epidemic that begins from the rare introduction of a pathogen into a closed population will spread, we can use the SIR equations together with a somewhat sneaky approximation. For the epidemic to grow, we need\n\\[\\frac{d I}{dt} = \\beta S I - \\gamma I &gt; 0\\]\nAt the beginning of the outbreak, \\(I \\approx 1\\) and so \\(S \\approx N\\) because no individuals have immunity to the disease under investigation. So the condition for the disease to spread is\n\\[\\frac{d I}{dt} = \\beta N - \\gamma &gt; 0 \\iff R_0 = \\frac{\\beta N}{\\gamma} &gt; 1.\\]\nThe quantity \\(R_0\\) measures the average number of secondary infections from a single infectious individual in an otherwise susceptible population. This measure of reproductive success or fitness is also common in population ecology. Intuitively, if the pathogen is unable to infect \\(&gt;1\\) individual on average when the susceptible pool has not yet been depleted (i.e., all individuals are susceptible), it will fail to spread. If \\(R_0 &gt; 1\\), then the disease will spread. Importantly, \\(R_0\\) does not provide a quantitative measure of how quickly this happens, and researchers in the field have proposed using other metrics (like the intrinsic growth rate of the pathogen) to measure the speed of pathogen invasion in a susceptible population."
  },
  {
    "objectID": "lec14-mathematical-models.html#what-happens-in-the-long-run",
    "href": "lec14-mathematical-models.html#what-happens-in-the-long-run",
    "title": "15  Mathematical modeling I",
    "section": "15.8 What happens in the long run?",
    "text": "15.8 What happens in the long run?\nTo understand the long-run behavior of a dynamical system (i.e., where key variables will go after enough time has passed), modelers will typically search for equilibria and characterize their stability properties. Intuitively, an equilibrium (or steady-state) is a point such that if the system starts there, it stays there. The stability of an equilibrium is determined by the behavior of the system (model) if perturbed.\nWhat are the equilibria of the SIR model above? They are exactly the points such that\n\\[\\frac{d S}{dt} = \\frac{d I}{d t} = \\frac{d R}{d t} = 0,\\] \ni.e., the variables of interest (number susceptible, infected, recovered) do not change. There are two such equillirbia, which we can find by solving for \\(S,I\\) in\n\\[\\frac{d S}{dt} = - \\beta S I = 0\\] \\[\\frac{d I}{dt} = \\beta S I - \\gamma I = 0.\\] \nThe two equilibria are \\((S^*,I^*,R^*) = (N,0,0)\\), which corresponds to the situation in which the disease is absent from the population (i.e., the disease-free equilibrium) and\n\\[ S^* = \\frac{\\gamma}{\\beta} = \\frac{N}{R_0}, \\hspace{12pt} I^* = 0, \\hspace{12pt} R^* = N - S^* = N (1- \\frac{1}{R_0}).\\] \nThe second equilibrium corresponds to the situation in which the disease has taken hold, but depleted the susceptible pool: although a fraction of individuals have experienced infection and recovered, a positive fraction of individuals did not acquire the disease. In other words, the disease burned itself out by depleting the resource pool (i.e., the number of susceptibles) such that some “resources” are still left.\nBut there are there are two equilibria — where will the system go? One way to answer this question is to determine the stability of each equilibrium point. Intuitively, a stable equilibrium is one such small perturbations from the equilibrium shrink the system returns to the equilibrium in question. An unstable equilibrium is one such small perturbations grow, leaving the system to go somewhere else. (There is, in fact, a third case that can arise. If the perturbation does not shrink or grow, the equilibrium is said to be neutrally stable.) The details are beyond the scope of this course but it turns out that it is enough to look at the linear behavior of the system about the equilibrium to characterize stability. To gauge local stability of an equilibrium \\(x^* = (x_1^*,\\dots,x_n^*)\\) for a system of differential equations\n\\[\\frac{d x_i}{dt} = f_i(x_1,\\dots,x_n)\\]\n\nTake derivatives of of \\(f_i\\) with respect to all \\(x_j\\)s.\nPut these derivatives into a matrix called the Jacobian, whose columns correspond to the derivatives \\(\\partial f_i/\\partial x_1, \\dots, \\partial f_n/\\partial x_1\\).\nEvaluate the Jacobian at \\(x^*\\).\nDetermine the eigenvalues of the Jacobian at \\(x^*\\).\nIf the eigenvalues have strictly negative real part, then \\(x^*\\) is stable.\n\n\n15.8.1 A stability analysis of the disease-free equilibrium\nHere is how the local stability analysis would work in the case of the SIR model with \\((S^*,I^*) = (N,0)\\). Remember we only need two equations to describe the dynamics of the population, since the population is of constant size. Using the notation above,\n\\[\\frac{dS}{dt} = -\\beta S I = f_1(S,I)\\] \\[\\frac{dI}{dt} = \\beta S I - \\gamma I = f_2(S,I).\\] \nThe Jacobian \\(J\\) is formed by taking partial derivatives of \\(f_1,f_2\\) with respect to the state variables. (One follows the same procedure for systems with more variables.)\n\\[J = \\begin{pmatrix} \\partial f_1/\\partial S & \\partial f_1/\\partial I \\\\\n\\partial f_2/\\partial S & \\partial f_2/\\partial I \\end{pmatrix}\n= \\begin{pmatrix} - \\beta I & - \\beta S \\\\\n\\beta I & \\beta S - \\gamma \\end{pmatrix}\\]\nEvaluating \\(J\\) at the disease-free equilibrium and solving for the eigenvalues, one can show that a necessary and sufficient condition for the disease-free equilibrium to be stable is that\n\\[\\beta N - \\gamma &lt; 0 \\iff R_0 &lt; 1.\\] \nThis should make sense as, in our previous analysis, we showed that for an epidemic to grow from a small introduction (perturbation), we need \\(R_0 &gt; 1\\). Fewer than one secondary infection from a single infectious individual in an otherwise susceptible population will ensure the stability of the disease-free steady-state since the pathogen will be unable to sustain transmission!"
  },
  {
    "objectID": "lec14-mathematical-models.html#competition-competitive-exclusion-and-mutation-to-maintain-diversity",
    "href": "lec14-mathematical-models.html#competition-competitive-exclusion-and-mutation-to-maintain-diversity",
    "title": "15  Mathematical modeling I",
    "section": "15.9 Competition, competitive exclusion, and mutation to maintain diversity!",
    "text": "15.9 Competition, competitive exclusion, and mutation to maintain diversity!\nNow, we go about deriving from a set of ecological equations a model for the evolution of a population which consists of individuals carrying one of two alleles at a locus. \\(n_i\\) will denote the number of individuals carrying allele \\(i=1,2\\). If individuals of both types are exponentially growing at rates \\(r_1,r_2\\), a model of the population dynamics is given by\n\\[\\frac{d n_i}{dt} = r_i n_i.\\]\nSuppose there is also mutation which converts allele \\(i\\) to \\(j\\) at rate \\(\\mu_{ij}\\). The model equations would then include a second term to account for changes in each type due to mutation:\n\\[\\frac{d n_i}{dt} = r_i n_i - \\mu_{ij} n_i + \\mu_{ji} n_j.\\] \nConsider the fraction of individuals carrying allele 1, i.e., \\(p = n_1/(n_1+n_2)\\). An equation for this new variable can be found by using the quotient rule, substituting, and simplifying so that the equation is in terms of \\(p\\):\n\\[\\begin{align*}\n\\frac{dp}{dt} &= \\frac{d (\\frac{n_1}{n_1+n_2})}{dt} \\\\\n&= \\frac{(n_1+n_2) \\frac{d n_1}{dt} - n_1 \\frac{d(n_1+n_2)}{dt}}{(n_1+n_2)^2} \\\\\n&= \\frac{1}{n_1+n_2} (r_1 n_1 - \\mu_{12} n_1 + \\mu_{21} n_2) - \\frac{n_1(r_1 n_1 + r_2 n_2)}{(n_1+n_2)^2} \\\\\n&= r_1 p - \\mu_{12} p + \\mu_{21} (1-p) - p(r_1 p + r_2 (1-p)) \\\\\n&= (r_1-r_2) p(1-p) - \\mu_{12} p + \\mu_{21} (1-p) \\\\\n\\end{align*}\\]\nThe terms in the equation can be interpreted as follow. (1) The first term describes the change in allele 1 frequency due to selection imposed by differences in intrinsic growth. If \\(r_1 &gt; r_2\\), then allele 1 grows in frequency due to its competitive advantage. If \\(r_1 &lt; r_2\\), then allele 1 is purged from the population by purifying selection. In this case, and without mutation, individuals carrying allele 2 take over the population in a process called competitive exclusion. (2) The second and third terms describe mutation from type 1 to type 2 and type 2 to 1, respectively. With mutation rates that are both \\(&gt;0\\), competitive exclusion cannot happen because individuals change type at rates \\(\\mu_{12},\\mu_{21}\\). The behavior of the system and the long-run predictions will depend on the relative balance between selection and mutation.\nTo understand where selection and mutation take the system, we can preform an approximation. This is common in modeling: either by assuming certain parameters are small, or that certain processes unfold faster than others (e.g., a separation of ecological and evolutionary timescales), we can learn what complex systems do and disentangle the reasons why. The key takeaway is that problems that would otherwise be difficult to solve can be addressed via approximation.\nHere, we will assume \\(s = r_1 - r_2 &lt; 0\\) and preform an approximation when strong selection and weak mutation. (A similar approximation can be applied in the case mutation is strong and selection is weak.) If selection is much stronger than selection, i.e., \\(|s| \\gg \\mu_{12}, \\mu_{21}\\), then we expect allele 1 to be rare. Ignoring terms like the allele frequency squared, allele frequency times mutation rate, etc. (which are expected to be smaller still!), we arrive at an approximate equation for allele frequency change:\n\\[\\frac{d p}{dt} = s p - \\mu_{21} + \\text{small stuff}.\\] \nThe approximate equilibrium allele frequency is \\(p^* = \\mu_{21}/s\\). Indeed, one can solve the model equations numerically (as we will do next class!) to see that this approximation is very good when the assumptions are met, and is decent still when the assumptions begin to break down."
  },
  {
    "objectID": "lec14-mathematical-models.html#the-wright-fisher-model-evolution-under-drift",
    "href": "lec14-mathematical-models.html#the-wright-fisher-model-evolution-under-drift",
    "title": "15  Mathematical modeling I",
    "section": "15.10 The Wright-Fisher model: evolution under drift",
    "text": "15.10 The Wright-Fisher model: evolution under drift\nTo conclude this modeling lecture, we will talk about one of the most important models in evolutionary genetics: the Wright-Fisher model, which in its simplest form describes evolution under genetic drift at a neutral, non-recombining locus in a haploid population of constant size \\(N\\). The model makes many assumptions, including the above, that there is no mutation, no migration, and generations are discrete and non-overlapping. Even with these assumptions, the Wright Fisher model is surprisingly general in the predictions is makes in its backward-time properties (i.e., the ways in which it or its approximations can be used to make inferences from present-day patterns of variation).\nSimply put, in the Wright Fisher model, individuals carry one of two alleles at a locus under consideration: \\(A_1\\) or \\(A_2\\). Each generation, the number of individuals (children) that carry each allele is determined by the number of individuals (parents) in the previous generations with the allele. In the WF model, if we use \\(X_n\\) to denote the number of \\(A_1\\) individuals at generation \\(n\\), then \\(X_{n+1}|X_n \\sim \\text{Binomial}(N,X_n/N)\\). With this setup, the number of individuals with allele \\(A_1\\) is a draw from a Binomial distribution with \\(N\\) “trials” (children) and success probability equal to the frequency of the allele among the parents. In other words, children choose their parents uniformly at random!\n\n\n\nRealization of the Wright Fisher model in which one type fixes after a few generations.\n\n\nThe WF model can be used to make predictions about the probability that a given allele fixes (i.e., that all individuals have it at some point in the future, and stuff of the sort), but it most importantly formalizes how we expect genetic drift to operate in populations that are not subject to other evolutionary forces. It, for example, is used to justify the otherwise verbal and imprecise model that drift is stronger in smaller populations. We will see more on this model and how it is simulated in the next lecture."
  },
  {
    "objectID": "lec14-mathematical-models.html#summary",
    "href": "lec14-mathematical-models.html#summary",
    "title": "15  Mathematical modeling I",
    "section": "15.11 Summary",
    "text": "15.11 Summary\nModeling is an indispensable tool in EEB, and a very creative endeavor! Here we have covered a lot of ground: geometric and exponential growth models in population ecology, an eco-evolutionary model of the balance between mutation and selection, the SIR model for the spread of an infectious disease, and the WF model of allele frequency change under drift. Next class will focus on simulating these models in R, and trying to use them to learn about biological processes unfolding across scales!"
  },
  {
    "objectID": "lec14-mathematical-models.html#resources",
    "href": "lec14-mathematical-models.html#resources",
    "title": "15  Mathematical modeling I",
    "section": "15.12 Resources",
    "text": "15.12 Resources\n\nOtto, S. & Day, T. (2007). A Biologist’s Guide to Mathematical Modeling in Ecology and Evolution.\nNuismer, S. (2017) Introduction to Coevolutionary Theory."
  },
  {
    "objectID": "lec15-mathematical-models-2.html#lesson-preamble",
    "href": "lec15-mathematical-models-2.html#lesson-preamble",
    "title": "16  Mathematical modeling II",
    "section": "16.1 Lesson preamble",
    "text": "16.1 Lesson preamble\n\n16.1.1 Lesson objectives\n\nDevelop familiarity with important mathematical models in EEB\nDevelop familiarity with how to fit mathematical models to data\nUnderstand how to numerically solve systems of differential equations in R\nUnderstand how to simulate simple stochastic models in R\nPractice solving systems of ODEs and stochastic simulation\n\n\n\n16.1.2 Lesson outline\n\nUsing deSolve to solve systems of differential equations\nFitting models to data: a case study in the SIR model\nSimulation of the Wright-Fisher model\n\n\n\nrequire(tidyverse)\nrequire(pbmcapply)\nrequire(deSolve)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "lec15-mathematical-models-2.html#using-desolve-to-solve-systems-of-differential-equations",
    "href": "lec15-mathematical-models-2.html#using-desolve-to-solve-systems-of-differential-equations",
    "title": "16  Mathematical modeling II",
    "section": "16.2 Using deSolve to solve systems of differential equations",
    "text": "16.2 Using deSolve to solve systems of differential equations\nLast class we discussed how to formulate and analyze (by finding equilibria and determining their stability properties) systems of differential equations of the form\n\\[\\frac{d x_i}{dt} = f_i(x_1,\\dots,x_n|\\theta),\\] \nwhere \\(x_1,\\dots,x_n\\) are variables of interest (e.g., the number of susceptible and infected individuals in a population) and \\(\\theta\\) is the set of all parameters (e.g., the transmission and recovery rates) which shape the dynamics and long-run behavior of the system (e.g., if the disease successfully spreads in the population).\nIn this class we will discuss how to numerically solve systems differential equations using the package deSolve. Numerical analysis of differential equations is a big, active area of research in mathematics but we will not concern ourselves with the details of how these methods ensure convergence to the true solution to a given system of differential equations. (It is enough to recognize that approximating each derivative with a difference quotient will provide a means to iteratively update variables through time.)\nTo illustrate how to solve differential equations using deSolve, we will consider the following system, which describes how a pathogen spreads in a population of hosts that are born and die at per-capita rate \\(\\mu\\); this ensures that the population size is constant. We assume that all individuals are born susceptible to the disease, and that immunity is life-long. Transmission occurs at rate \\(\\beta\\) and recovery at rate \\(\\gamma\\). Finally, we include a term to capture disease-induced mortality (i.e., virulence).\nThe model is as follows:\n\\[\\frac{dS}{dt} = \\mu N - \\beta S I - \\mu S\\] \\[\\frac{dI}{dt} = \\beta S I - (\\mu+\\gamma+v) I\\] \\[\\frac{dR}{dt} = \\gamma I - \\mu R\\]\nSince \\(dN/dt = 0\\), the population size is constant and we can (as we did last class) ignore the \\(R\\) equation (since \\(R = N - S - I\\)). Setting the previous equations to zero and solving, one has that there are two equilibria: the disease-free equilibrium \\((S^*, I^*) = (N,0)\\), and the endemic equilibrium\n\\[(S^*,I^*) = (\\frac{\\mu+\\gamma+v}{\\beta}, \\frac{\\mu N}{\\mu+\\gamma + v}-\\frac{\\mu}{\\beta}).\\] \nThe endemic equilibrium is stable (and the disease-free equilibrium is unstable) whenever the basic reproductive number of the disease agent\n\\[R_0 = \\frac{\\beta N}{\\mu+\\gamma+v}\\]\nis \\(&gt;1\\), i.e., in an otherwise susceptible population a single infectious individual infects more than one individual on average. Notice how demography, recovery, and virulence all reduce the reproductive value of the pathogen; intuitively, this is because an infected individual can die or recover before transmitting.\nBelow we solve the system and plot the dynamics for a specific set of parameter values.\n\nN &lt;- 100 # population size\n\n# define parameters\ngamma &lt;- 1/14  # mean infectious period = 14 days\nv &lt;- 1/14  # mean time before individual dies to due disease = 14 days\nbeta &lt;- 0.01  # transmission rate\nmu &lt;- 1/200 # average life time of individual = 200 days\n\nR0 &lt;- beta*N/(mu+gamma+v); R0\n\n[1] 6.763285\n\n# put parameter values into vector params\nparams &lt;- c(mu = mu, gamma = gamma, beta = beta, v = v)\n\ninitialI &lt;- 10\n\nstate &lt;- c(S=N-initialI, I=initialI) # define initial conditions\n\n# define times to save\ntimes &lt;- seq(0, 500, 0.01)\n\n# define the model!\nsir &lt;- function(time, state, params){\n  with(as.list(c(state,params)),{\n    \n    dS &lt;- mu*N - beta*S*I - mu*S\n    dI &lt;- beta*S*I - gamma*I - v*I - mu*I\n    \n    return(list(c(dS, dI)))\n  })\n}\n\n# numerically integrate equations!\nout &lt;- as.data.frame(ode(state, times, sir, params))\n\nout %&gt;% pivot_longer(! time) %&gt;% \n  ggplot(aes(x = time, y = value, color = name)) + \n  geom_line()\n\n\n\n\n\n16.2.1 Challenge\nDetermine at what time does the number of infected individuals peaks.\n\nout[which.max(out$I),]\n\n    time        S        I\n482 4.81 14.74886 59.10662\n\n\nA common way to visualize the behavior of a mathematical model is to plot the variables (in the previous case, \\(S\\) and \\(I\\)) against each other in phase space. Doing this, one can determine how the variables jointly influence each other through time, and if/how they enter an equilibrium or limit cycle.\n\n## plot phase plane\nplot(I~S, out, type=\"l\")\n\n\n\n\nBased on the phase portrait of the system, the number of susceptible and infected individuals spiral into the endemic equilibrium."
  },
  {
    "objectID": "lec15-mathematical-models-2.html#fitting-the-sir-model-to-data",
    "href": "lec15-mathematical-models-2.html#fitting-the-sir-model-to-data",
    "title": "16  Mathematical modeling II",
    "section": "16.3 Fitting the SIR model to data",
    "text": "16.3 Fitting the SIR model to data\nWe will now try to fit the above SIR model to (crudely) simulated case count data. We will assume \\(\\mu = v = 0\\), i.e., no births or deaths occur over the sampling period.\n\nread_csv(\"meas.csv\") -&gt; meas\n\nRows: 103 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): time, reports\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nggplot(meas, aes(x = time, y = reports)) + geom_line() + xlab(\"day\")\n\n\n\n\nTo fit the model to the data using ML, we will follow the steps outlined in the previous lecture.\n\n16.3.1 Step 1: determine the distribution of the data\nWe will assume the case count data are an imperfect and random sample of the true incidence. There are a couple ways to do this, but we will suppose the cases have a binomial distribution: \\(\\text{reports}_i \\sim \\text{Binomial}(\\kappa I(t_i),p)\\). In other words, a fraction \\(\\kappa\\) of infected individuals at an observation time are tested and the test comes up positive with probability \\(p\\). (Keep in mind there are other ways to specify the distribution of the data around the solution of the model!)\n\n\n16.3.2 Step 2: maximize the likelihood function\nWe will now write a function to solve the differential equation above for a fixed set of parameter values and evaluate the likelihood of the case count data for a set of parameters. Finally, looping over combinations of parameters we will determine where the likelihood assumes a maximum (i.e., the MLE).\n\ninitialI &lt;- 10\nstate &lt;- c(S=N-initialI, I=initialI) # define initial conditions\ntimes &lt;- seq(0, 75, 0.1)\n\nparams &lt;- expand.grid(mu = 0, \n                     gamma = seq(0.06,0.08,0.005),\n                     beta = seq(0.003,0.006,0.001),\n                     v = 0,\n                     p = seq(0.5,0.9,0.1),\n                     kappa = seq(0.4,1,0.1)\n                     )\n# many parameters fixed for convenience, but in general could / often should be fitted!\n\nreturn_LL_at_specific_combo_params &lt;- function(params_to_use){\n  \n  reporting_times &lt;- meas$time\n  \n  out &lt;- \n    as.data.frame(ode(state, times, sir, params_to_use)) %&gt;%\n    subset(time %in% reporting_times) \n  # solve model for particular set of parameters\n  # keep variables only at observation times\n  \n  LL &lt;- c()\n  \n  for (i in 1:length(reporting_times)){\n    LL[i] &lt;- dbinom(meas$reports[i], size = round(params_to_use$kappa*out$I[i]), \n                    prob = params_to_use$p)\n  }\n  # calculate likelihood of data at observation times\n  # based on assumption data are Normal around the solution at a given time\n  \n  LogLik &lt;- sum(log(LL))\n  # calculate log-likelihood of parameters given ALL data\n  \n  return(data.frame(params_to_use, LogLik = LogLik))\n}\n\nLogLikihoods &lt;- NULL\noutALL &lt;- NULL\n\nfor (i in 1:nrow(params)){\n  LogLikihoods &lt;- rbind(LogLikihoods, \n                        return_LL_at_specific_combo_params(params[i,])\n                        )\n  outALL[[i]] &lt;- data.frame(ode(state, times, sir, params[i,1:4]), index = i)\n}\n\nMLE &lt;- LogLikihoods %&gt;% subset(is.finite(LogLik)) %&gt;% \n  subset(LogLik == max(LogLik)); MLE\n\n    mu gamma  beta v   p kappa  LogLik\n573  0  0.07 0.005 0 0.8   0.9 -199.63\n\noutALL &lt;- do.call(rbind, outALL)\n\n# what does the solution at the MLE look like compared to the data?\nbest_solution &lt;- outALL %&gt;% \n  subset(index == which(LogLikihoods$LogLik == max(LogLikihoods$LogLik))) %&gt;%\n  group_by(time) %&gt;%\n  mutate(expected_measurement = MLE$kappa*MLE$p*I)\n\nbest_solution %&gt;% ggplot() + \n  geom_line(aes(x = time, y = expected_measurement), color = \"black\") +\n  geom_point(data = meas, aes(x = time, y = reports), size = 2) +\n  geom_line(aes(x = time, y = I), color = \"red\")\n\n\n\n\nThat’s a pretty great match to the data. It is rare to see fits this good or have sampling as frequent in this example, but the underlying methodology is still quite powerful!"
  },
  {
    "objectID": "lec15-mathematical-models-2.html#simulation-of-the-wright-fisher-model",
    "href": "lec15-mathematical-models-2.html#simulation-of-the-wright-fisher-model",
    "title": "16  Mathematical modeling II",
    "section": "16.4 Simulation of the Wright-Fisher model",
    "text": "16.4 Simulation of the Wright-Fisher model\nRecall that the Wright-Fisher model describes how a the frequency of an allele at a single, non-recombining locus changes due to genetic drift. To simulate realizations from the neutral WF model, we need to specify 1) the population size, 2) the initial frequency of the \\(A_1\\) allele, 3) the number of generations over which evolution will unfold, and 4) the number of replicate simulations. The below chunk does this using the fact \\(X_{n+1}|X_n \\sim \\text{Binomial}(N,X_n/N)\\), i.e., the number of \\(A_1\\) individuals in a given generation is a draw of a Binomial random variable with \\(N\\) trials and success probability \\(X_n/N\\). The function rbinom() is used in the simulation to randomly sample alleles.\n\n# data.frame to be filled\nwf_df &lt;- data.frame()\n\nsizes &lt;- c(50, 100, 500, 1000, 5000) # effective population sizes\nstarting_p &lt;- c(.01, .1, .25, .5, .8) # starting allele frequencies\nn_gen &lt;- 100 # number of generations\nn_reps &lt;- 50 # number of replicate simulations\n\nfor(N in sizes){\n  for(p in starting_p){\n    p0 &lt;- p\n    for(j in 1:n_gen){\n      X &lt;- rbinom(n_reps, N, p)\n      p &lt;- X/N\n      rows &lt;- data.frame(replicate = 1:n_reps, N = rep(N, n_reps), \n                         gen = rep(j, n_reps), p0 = rep(p0, n_reps), p = p)\n      wf_df &lt;- bind_rows(wf_df, rows)\n    }\n  }\n}\n\nhead(wf_df)\n\n  replicate  N gen   p0    p\n1         1 50   1 0.01 0.00\n2         2 50   1 0.01 0.00\n3         3 50   1 0.01 0.00\n4         4 50   1 0.01 0.00\n5         5 50   1 0.01 0.00\n6         6 50   1 0.01 0.02\n\n# plot!\n\nggplot(wf_df, aes(x = gen, y = p, group = replicate, color = as.factor(N))) +\n  geom_path(alpha = .5) + facet_grid(N ~ p0) + \n  theme(legend.position = \"none\") +\n  labs(x = \"generation\", y = \"frequency of allele A\",\n  title = \"Realizations of the Wright-Fisher model with no mutation and no selection\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.ticks.x = element_blank(), axis.text.x = element_blank(),\n        axis.ticks.y = element_blank(), axis.text.y = element_blank())"
  },
  {
    "objectID": "lec15-mathematical-models-2.html#adding-mutation-and-selection-to-the-wright-fisher-model",
    "href": "lec15-mathematical-models-2.html#adding-mutation-and-selection-to-the-wright-fisher-model",
    "title": "16  Mathematical modeling II",
    "section": "16.5 Adding mutation and selection to the Wright-Fisher model",
    "text": "16.5 Adding mutation and selection to the Wright-Fisher model\nIn the model above, there is no mutation or selection. To add those forces into our model of evolution at a single locus, it is not too difficult. As before, we will assume population size is constant and \\(=N\\), and denote by \\(X_n\\) the number of \\(A_1\\) individuals in generation \\(n\\).\nSuppose during reproduction \\(A_1\\) individuals produce \\(A_2\\) children with probability \\(u_{12}\\) and \\(A_2\\) individuals to \\(A_1\\) children with probability \\(u_{21}\\). In this case, the probability that an individual in a given generation has an \\(A_1\\) parent (i.e., that that child “chooses” an \\(A_1\\) parent) is given by\n\\[p_i = (1-u_{12}) \\frac{i}{N} + u_{21} \\frac{N-i}{N}.\\]\nThe first term describes the situation where an \\(A_1\\) parent gives rise to an \\(A_1\\) child (no mutation). The second term describes the situation where an \\(A_2\\) parent gives rise to an \\(A_1\\) offspring (i.e., there is mutation). In this case, the Wright-Fisher model is similar to the neutral case, except in that \\(X_{n+1}|X_n \\sim \\text{Binomial}(N,p_i)\\). To form the next generation, we sample from the parents and with probability \\(p_i\\) a given child has allele \\(A_1\\).\nWith selection and mutation, the form of the model is similar except in that we need another parameter. Assuming \\(A_1\\) has selective advantage \\(s\\) (i.e., produces \\(s\\) more copies than the alternative allele), then the probability that an individual in given generation has an \\(A_1\\) parent is\n\\[p_i = (1-u_{12}) \\frac{(1+s) i}{N+is} + u_{21} \\bigg( 1 - \\frac{(1+s) i}{N+is} \\bigg).\\] \nThe first term corresponds to the situation where an \\(A_1\\) parent is chosen and no mutation occurs, and the second term to the case an \\(A_2\\) parent is chosen and mutation does occur. Below we simulate from the model across a range of mutation rates, effective population sizes, and selective coefficients, using the function pbmclappy() to efficiently parallelize the simulations. Since the replicates are independent of each other, we can send simulations to different cores to reduce the run time of a program.\n\nWF_model_mutation_and_selection &lt;- function(k){\n  \n  number_A1_individuals &lt;- c(initial_number)\n  \n  for(i in 2:(generations+1))   {\n    p &lt;- (1-mu12)*(number_A1_individuals[i-1]*(1+s)/(number_A1_individuals[i-1]*s+N)) + \n      mu21*(1 - (number_A1_individuals[i-1]*(1+s)/(number_A1_individuals[i-1]*s+N)))\n    number_A1_individuals[i] &lt;- rbinom(n = 1, size = N, prob = p)\n  }\n  \n  return(number_A1_individuals/N) \n  ### returns fraction of the population that is A_1 at generations 1,2,...\n}\n\nrealizations &lt;- 50 # number of realizations of the WF process\ngenerations &lt;- 100 # number of generations\n\nparams &lt;- as.data.frame(expand.grid(s = c(0,0.001,0.01, 0.05, 0.1), \n                                    mu12 = 1e-05, mu21 = 1e-05, \n                                    N = c(10, 100, 1000, 10000)) %&gt;% \n                          mutate(initial_number = 0.1*N))\n\ndat &lt;- NULL\n\nfor (i in 1:nrow(params)){\n  \n  s &lt;- params$s[i]\n  mu12 &lt;- params$mu12[i]\n  mu21 &lt;- params$mu21[i]\n  N &lt;- params$N[i]\n  initial_number &lt;- params$initial_number[i]\n  \n  data &lt;- do.call(cbind, pbmclapply(X = 1:realizations, \n                                    FUN = WF_model_mutation_and_selection, \n                                    mc.cores = detectCores()-1))\n  \n  colnames(data) &lt;- 1:realizations\n  \n  dat[[i]] &lt;- as.data.frame(cbind(data, generation = 1:(generations+1), \n                                  s = s, mu12 = mu12, mu21 = mu21, N = N, \n                                  initial_number = initial_number))\n}\n\ndat_new &lt;- as.data.frame(do.call(rbind, dat)) %&gt;% \n  mutate(initial_freq_mutant = initial_number/N)\n\nggplot(pivot_longer(dat_new, 1:realizations), \n       aes(x = generation, y = value, group = name, color = as.factor(s))) + \n  geom_line(alpha = 0.5) + facet_grid(s~N) +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"none\", \n        axis.ticks.x = element_blank(), axis.ticks.y = element_blank(),\n        axis.text.x = element_blank(), axis.text.y = element_blank()) +\n  labs(x = \"generation\", y = \"frequency of A1 (with selective advantage)\",\n       title = \"Realizations of the Wright-Fisher model with mutation+selection\")\n\n\n\n\nAre there any patterns or general features of the above simulations? What might we be able to conclude about evolution at a single locus with mutation, selection, and drift based on these simulations?"
  },
  {
    "objectID": "assignment-01.html",
    "href": "assignment-01.html",
    "title": "17  Assignment 01",
    "section": "",
    "text": "Vectors (2 marks)\n\nCreate a vector v with all integers 0-20, and a vector w with every third integer in the same range. (0.25 marks)\nHow much longer is vector v compared with w? (0.25 marks)\nCreate a new vector, v_square, with the square of elements at indices 3, 6, 7, 10, and 15 from the variable v. Hint: Use indexing rather than a for loop. (0.5 marks)\nCalculate the mean and median of the first four values from v_square. (0.5 marks)\nCreate a logical (or boolean) vector v_bool to indicate which vector v_square elements are bigger than 30. How many values are over 30? Hint: In R, TRUE = 1, and FALSE = 0, so you can use simple arithmetic to find this out. (0.5 marks)\n\nFunctions (2 marks)\n\nWrite a function that calculates the mean of the last two elements of any numeric vector. Test this function with the v and v_square vectors.\n\nLoops (2 marks)\n\nTurn the given data frame into an exponent table, a table that shows you results of raising one number (the base, rows) to the power of another (the exponent, columns). See an example here. Use this data frame to find 5 to the power 9. Hint: use a nested for-loop to fill your data frame\n\nexp_table &lt;- data.frame(matrix(ncol=10, nrow=10))\n\n\nData frames (2 marks)\n\nThere are many built-in data frames in R, which you can find more details about these online. What are the column names of the built-in dataframe beaver1? How many observations (rows) and variables (columns) are there? (0.5 marks)\nDisplay both the first 6 and last 6 rows of this data frame. Show how to do so with both indexing as well as specialized functions. (0.5 marks)\nWhat is the difference in mean body temperature of the beaver when it was inside versus outside of the retreat? (0.5 marks)\nHow much did the body temperature of the beaver fluctuate (i.e., range) from 9:00-10:00 AM on the first day of the study? (0.5 marks)"
  },
  {
    "objectID": "assignment-02.html#footnotes",
    "href": "assignment-02.html#footnotes",
    "title": "18  Assignment 02",
    "section": "",
    "text": "Olofsson J, te Beest M, Ericson L (2013) Complex biotic interactions drive long-term vegetation dynamics in a subarctic ecosystem. Philosophical Transactions of the Royal Society B 368(1624): 20120486. https://dx.doi.org/10.1098/rstb.2012.0486↩︎"
  },
  {
    "objectID": "assignment-03.html#footnotes",
    "href": "assignment-03.html#footnotes",
    "title": "19  Assignment 03",
    "section": "",
    "text": "Recall that a confidence interval for a parameter can be constructed as follows: 1) evaluate the likelihood at the MLEs for all other parameters. This returns a slice of the likelihood function as a function of the parameter of interest. 2) Determine what values of the focal parameter \\(\\theta\\) are such that \\(\\lambda_{LR} -2(\\ln L(\\theta)-\\ln L(\\hat{\\theta}_{\\text{MLE}})) &lt; \\chi^2_c\\), where \\(\\chi^2_c\\) is cutoff for admissible values based on \\(100(1-\\alpha)\\%\\)-ile for a \\(\\chi^2\\) distribution with one degree of freedom. Hint: modify the code from class to do this.↩︎"
  },
  {
    "objectID": "assignment-04.html",
    "href": "assignment-04.html",
    "title": "20  Assignment 04",
    "section": "",
    "text": "To submit this assignment, upload the full document to Quercus, including the original questions, your code, and the output. Submit your assignment as a knitted .pdf.\n\nLinear models (4 marks)\n\nRun the following code to load the CO2 dataset.\n\nlibrary(tidyverse)\nlibrary(car)\nlibrary(lsmeans)\n\nco2_df &lt;- as_tibble(as.matrix(CO2)) %&gt;% \n  mutate(conc = as.integer(conc),\n         uptake = as.numeric(uptake))\n\n\nLook through the help documentation (?CO2) to understand what each variable means and what units they have; make sure you are including units in all subsequent answers (and all future assignments!). Construct a linear model to test the following null hypothesis, and state your conclusion in plain English (remember to refer to the hypothesis being tested) (1 mark)\n\nH0: Ambient carbon dioxide concentrations does not influence a plant’s CO2 uptake rates.\n\nCheck the assumptions of this model (normality and homogeneity of variance), and explain your conclusion in plain English. (1 mark)\nPredict how much CO2 plants would uptake if you observed an atmospheric concentration of 2,450 mL/L? (0.25 marks)\nLet’s say now we want to further investigate whether this relationship (between ambient CO2 concentration and uptake) differ by plant origin. Construction a linear model to test this, and explain your conclusion in plain English. (0.75 mark)\nExplain, in your own words, how to interpret the regression coefficients from part d. Hint: there are three of them! (0.25)\nPredict how much CO2 plants would uptake if you observed an atmospheric concentration of 2,450 mL/L, and a plant origin of Mississippi? (0.25 marks)\nVisualize what you found in part (d) by plotting values estimated from the model. (0.5 marks)\n\n\nLinear mixed-effects models (4 marks).\n\nSantangelo et al. (2018) were interested in understanding how plant defenses, herbivores, and pollinators influence the expression of plant floral traits (e.g. flower size). Their experiment had 3 treatments, each with 2 levels: Plant defense (2 levels: defended vs. undefended), herbivory (2 levels: reduced vs. ambient) and pollination (2 levels: open vs. supplemental). These treatments were fully crossed for a total of 8 treatment combinations. In each treatment combination, they grew 4 individuals from each of 50 plant genotypes for a total of 800 plants (8 treatment combinations x 50 genotypes x 2 individuals per genotype). Plants were grown in a common garden at the Koffler Scientific Reserve (UofT’s field research station) and 6 floral traits were measured on all plants throughout the summer. We will analyze how the treatments influenced one of these traits in this exercise.\nRun the code chunk below to download the data, which includes only a subset of the columns from the full dataset:\n\ndownload.file(\"https://uoftcoders.github.io/rcourse/data/Santangelo_JEB_2018.csv\", \"Santangelo_JEB_2018.csv\")\n\n\nplant_data &lt;- read_csv(\"Santangelo_JEB_2018.csv\", col_names = TRUE)\n\nRows: 792 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Genotype, Pollination, Herbivory, HCN, Block\ndbl (6): Flower.date, Avg.Bnr.Wdth, Avg.Bnr.Ht, Biomass.plant, Num.flwrs, To...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(plant_data)\n\nRows: 792\nColumns: 11\n$ Genotype      &lt;chr&gt; \"173-1\", \"173-1\", \"173-1\", \"173-1\", \"173-1\", \"173-1\", \"1…\n$ Pollination   &lt;chr&gt; \"Open\", \"Open\", \"Open\", \"Supp\", \"Supp\", \"Supp\", \"Open\", …\n$ Herbivory     &lt;chr&gt; \"Reduced\", \"Ambient\", \"Reduced\", \"Ambient\", \"Ambient\", \"…\n$ HCN           &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", …\n$ Flower.date   &lt;dbl&gt; 86, 23, 12, 25, 16, 38, 22, 61, 32, 25, 30, 33, 54, 23, …\n$ Avg.Bnr.Wdth  &lt;dbl&gt; 3.32, NA, 3.02, 3.14, 2.76, NA, 3.11, 3.07, 3.16, 3.16, …\n$ Avg.Bnr.Ht    &lt;dbl&gt; 6.50, NA, 7.02, 6.03, 5.20, NA, 5.69, 6.18, 6.04, 5.94, …\n$ Biomass.plant &lt;dbl&gt; 9.27, 8.34, 7.74, 15.76, 4.16, 7.75, 48.67, 10.32, 28.80…\n$ Num.flwrs     &lt;dbl&gt; 52.5, 67.0, 57.5, NA, NA, 47.5, 60.0, 58.5, 66.5, 64.5, …\n$ Total.Inf     &lt;dbl&gt; 4, 1, 21, 9, 0, 15, 41, 19, 17, 23, 129, 46, 35, 35, 22,…\n$ Block         &lt;chr&gt; \"E\", \"A\", \"E\", \"A\", \"A\", \"A\", \"D\", \"F\", \"C\", \"C\", \"A\", \"…\n\nhead(plant_data)\n\n# A tibble: 6 × 11\n  Genotype Pollination Herbivory HCN   Flower.date Avg.Bnr.Wdth Avg.Bnr.Ht\n  &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 173-1    Open        Reduced   Yes            86         3.32       6.5 \n2 173-1    Open        Ambient   Yes            23        NA         NA   \n3 173-1    Open        Reduced   Yes            12         3.02       7.02\n4 173-1    Supp        Ambient   Yes            25         3.14       6.03\n5 173-1    Supp        Ambient   Yes            16         2.76       5.2 \n6 173-1    Supp        Ambient   Yes            38        NA         NA   \n# ℹ 4 more variables: Biomass.plant &lt;dbl&gt;, Num.flwrs &lt;dbl&gt;, Total.Inf &lt;dbl&gt;,\n#   Block &lt;chr&gt;\n\n\nYou can see that the data contain 792 observations (i.e., plants, 8 died during the experiment). There are 50 genotypes across 3 treatments: Herbivory, Pollination, and HCN (i.e., hydrogen cyanide, a plant defense). There are 6 plant floral traits: Number of days to first flower, banner petal length, banner petal width, plant biomass, number of flowers, and number of inflorescences. Finally, since plants that are closer in space in the common garden may have similar trait expression due to more similar environments, the authors included 6 spatial “blocks” to account for this environmental variation (i.e. Plant from block A “share” an environment and those from block B “share” an environment, etc.). Also keep in mind that each treatment combination contains 4 individuals of each genotype, which are likely to have similar trait expression due simply to shared genetics.\n\nUse the lme4 and lmerTest R packages to run a linear mixed-effects model examining how herbivores (Herbivory), pollinators (Pollination), plant defenses (HCN) and all interactions influences the length of banner petals (Avg.Bnr.Wdth) produced by plants. Make sure to account for variation due to spatial block, and random variation due to Genotype, but only allow the intercept to vary. You only need to specify the model for this part of the question. (1 mark)\nSummarize (i.e., get the output) the model that you ran in part (a). Did any of the treatments have a significant effect on banner petal length? If so, which ones, and how did it effect banner petal length? Make a statement for each significant main effects in the model (i.e., not interactions). (1 mark)\nPlot the mean banner width for the interaction between HCN, herbivory, and pollination. Include 1 standard error around the mean, and avoid overlapping points in the figure. (1 mark)\n\nHint: to visualize a three-way interaction, you can either 1) plot one of the predictors on the x axis, and then differentiate the other two with different colors and shapes, 2) use facet_wrap to isolate one of the predictors.\n\nAfter accounting for the fixed effects, what percentage of the variation in banner petal width was explained by each of the random effects in the model? (1 mark)"
  },
  {
    "objectID": "assignment-05.html",
    "href": "assignment-05.html",
    "title": "21  Assignment 05",
    "section": "",
    "text": "To submit this assignment, upload the full document to Quercus, including the original questions, your code, and the output. Submit your assignment as a knitted .pdf.\nLoad these packages before you start:\n\nlibrary(car)\nlibrary(ggfortify)\nlibrary(tidyverse)\n\n1. In this exercise, we will be analyzing floral trait data using a base R data set iris. (4 marks)\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nThis data set consists of 4 floral trait measurements (sepal length and width, and petal length and width) for 3 species of iris.\n\nCalculate the correlations between each variable and produce the pairs plot showing the relationships. Which pairs of variables show high collinearity? (0.75 mark)\nPerform a principle components analysis on the floral traits in the iris data set (use a correlation matrix). Visualize your result in a biplot. (1 mark)\n\nNOTE: the select function in dplyr clashes with a function with the same name in the raster package. Be sure to specify to R which package you are calling the function from in your answer.\n\nTake a look at the trait loadings (rotation), and explain the loadings for PC1 and PC2 in plain English. (0.75 mark)\nColour the individual entities in your biplot by species. Explain the floral characteristics of the species “virginica” based on this plot. (1 mark)\nBased off of the cumulative proportion of variance explained by each axis or the scree plot, which axis (axes) would you retain for downstream analyses and why? (0.5 mark)\n\n2. In this exercise, you will create and clone a new GitHub repo onto your machine (call it assignment5_folder), use the command line to run an R script in this directory, modify the files that are returned by the R script, and push the results to remote repo.\n\nGo to your GitHub (i.e., to github.com/[yourusername]). Make a new repo called assignment5_folder. (0.25 mark)\nClone your GitHub repo onto your machine. (0.25 mark)\nMake an R script that uses a function to write a .csv file (named assignment5_file.csv) with two columns. The first column should be named x and should contain all numbers between 0 and 100. The second column should be named y and contain draws of a Normal random variable with mean 0 and variance 0.001. In your call to write.csv, be sure to specify row.names=FALSE. (1 mark)\nUse the command line to modify assignment5_file.csv so that the first entry in the x column is equal to 7 instead of 1. In your answer, specify what commands you ran (and in what order) to access and change the file. (1 mark)\nAdd, commit, and push the changes to the assignment5_folder repo to GitHub. As before, in your answer, specify what commands you ran (and in what order) to do this. (1 mark)\nProvide one reason we use the command line, and one reason we use GitHub. (0.25 marks each)"
  },
  {
    "objectID": "challenge-assignment.html#footnotes",
    "href": "challenge-assignment.html#footnotes",
    "title": "22  Challenge assignment",
    "section": "",
    "text": "Recall that the rules of the WF model are very simple. The population size is constant, generations are discrete and non-overlapping, and children choose their parents (regardless of type) at random. This means that the number of \\(A_1\\) individuals in generation \\(n+1\\) is Binomial(\\(N\\),\\(p_{n}\\)) where \\(p_n\\) is the frequency of \\(A_1\\)s in the previous generation.↩︎\nTo make inference under the model tractable, it is actually an approximation to the discrete WF model is used, in which it is assumed the \\(N\\) is large and the per-individual mutation rate is small.↩︎\nThis is a special kind of generalized linear model! The Poisson distribution models count data, like the number of satellite males at a beach.↩︎"
  },
  {
    "objectID": "projects.html#option-1-hypothesis-driven-project",
    "href": "projects.html#option-1-hypothesis-driven-project",
    "title": "23  Project description",
    "section": "23.1 Option 1: Hypothesis-driven project",
    "text": "23.1 Option 1: Hypothesis-driven project\nGroups will formulate their own hypotheses based on their interests within ecology and evolution. Groups will test predictions borne out of their hypotheses with reproducible and quantitative analysis techniques (e.g., ANOVA). If your group has an idea for statistical analyses that are beyond the scope of the course, please let us know. We are happy to support any groups who want to learn new tools, but expect that these groups are ready to learn how these tools work on their own; we hope to equip you with enough understanding to learn new things independently. Finally, the work must be original – while we may be repurposing data, we will not be simply redoing analyses. Keep in mind also that any work you do as part of this course may not be submitted for credit in another course (such as a fourth-year research project) and vice versa. While you may not submit your work for this course for credit in another course, you are welcome to publish or present your work in an academic setting.\nA note about community/citizen science websites: since the data is community-controlled, it may not always be research quality. There may be incorrect species IDs, inaccurate geolocations or time of observations, or discrepancies in protocols. When working with community science data, make sure that the data is cleaned and wrangled so that it is reliable. Quality control is a good first step when working with data, as simple errors can exist in any dataset.\n\n23.1.1 What is a hypotheses? What is a prediction?\nA hypothesis is a testable and falsifiable statement that offers a possible explanation of a phenomenon based on background knowledge, preliminary observations, or logic.\nE.g., Primary productivity is an important driver of mammal species richness.\nA prediction is based on a hypothesis. It is meant to describe what will happen in a specific situation, such as during an experiment, if the hypothesis is correct.\nE.g., If primary productivity is an important driver of mammal species richness, then more mammalian species would be found in sites with more plant biomass (proxy for primary productivity) compared with sites with less plant growth."
  },
  {
    "objectID": "projects.html#option-2-modeling",
    "href": "projects.html#option-2-modeling",
    "title": "23  Project description",
    "section": "23.2 Option 2: Modeling",
    "text": "23.2 Option 2: Modeling\nGroups will develop a mathematical model to answer a question in ecology and/or evolution they find interesting. There are many reasons to develop models: they help clarify assumptions, generate predictions, nullify hypotheses, provide mechanistic explanations for observed data, and help us know what kinds of data to look for. New models almost always build on existing and well-studied ones (e.g., the Lotka-Volterra model). The fact models are simplifying representations of the real world is by design! The goal of building a model is to identify the key features that make a process interesting, represent the process mathematically (and, in doing so, clarify what assumptions are being made!), characterize the behavior of the model, and from this characterization draw conclusions about how the process being modeled works. Characterization of a model can involve mathematical analysis, simulation, and confrontation with data.\nThe key steps in this project are to 1) identify an interesting question in ecology or evolution, 2) develop (and likely revise) a model to address that question, 3) characterize the behavior of the model, and 4) draw biological conclusions from the model and its characterization.\nIf you are interested in modeling, let Vicki and Mete know as soon possible!"
  },
  {
    "objectID": "projects.html#option-3-simulation-study",
    "href": "projects.html#option-3-simulation-study",
    "title": "23  Project description",
    "section": "23.3 Option 3: Simulation study",
    "text": "23.3 Option 3: Simulation study\nSimilar to Option 1, groups that do a simulation study will formulate hypotheses and use reproducible and quantitative analysis techniques to test predictions borne out of those hypotheses. The difference is that students will simulate their own data, instead of using an existing dataset. One reason to do a simulation study is to see what kind of data would be needed to test a hypothesis in the field, e.g., how much data would be needed to find a significant association between response and predictor variables.\nIf you are interested in doing a simulation study, let Vicki and Mete know as soon possible!"
  },
  {
    "objectID": "projects.html#finding-a-topic",
    "href": "projects.html#finding-a-topic",
    "title": "23  Project description",
    "section": "23.4 Finding a topic",
    "text": "23.4 Finding a topic\nHere are some discussion questions to help you and your group work towards a research topic and set of hypotheses and predictions:\n\nWhat is a paper you read recently that you found really interesting?\nWhat is your favorite EEB course so far? Why did you like it?\nThinking about EEB professors, was there anyone whose work you are particularly interested in?\nBrowse through some recent issues of broad scope EEB journals such as Trends in Ecology and Evolution and Annual Review of Ecology, Evolution, and Systematics. Any articles catching your eyes?\nCheck out this paper. Any of those questions spark your interest?"
  },
  {
    "objectID": "projects.html#project-timeline-and-deliverables",
    "href": "projects.html#project-timeline-and-deliverables",
    "title": "23  Project description",
    "section": "23.5 Project timeline and deliverables",
    "text": "23.5 Project timeline and deliverables\nAs instructors, we are here to help your group work towards a project idea that you are excited about! We have included multiple check points and small assignments throughout the semester for you to get feedback on your project ideas and ask us questions.\n\n23.5.1 Project proposal\nDue Oct 3rd, worth 4% of final grade\nGood research takes time! The purpose of the proposal is to get your group started on this process early on so that you will have sufficient time to do your project justice. This will also serve as official documentation of your project development process. Your projects will likely evolve over time, and there can be many reasons for this. For instance, as you explore your data, you might be inspired to ask different questions, or you may need to refine your hypotheses due to limitations in the data. All of these are fine, in fact, it happens all the time in real research settings.\nInclude the following information in your proposal:\n\nOption 1: your hypotheses and predictions (point form or short paragraph) and data source (short paragraph). Include a citation, a brief description of how the data was collection, and which section of the dataset you plan to use in your analysis (e.g., which columns).\nOption 2: a question you want to answer using a mathematical model (short paragraph describing the problem and the value modeling may add). Be sure to include a description of the variables that you may want to track and the kind of model you envision using.\nOption 3: same as 1, except with a description of how to simulate the data.\n\n\n\n23.5.2 Mid-project update\nDue Nov 2nd, worth 6% of final grade\nThe purpose of the mid-project update is to ensure you are on track with your projects. By now, you should have completed your exploratory data analyses, modeling, or simulation. You should have also solidified your hypotheses, predictions, and analyses plan. Essentially, you should be ready to write the Methods section of your report!\nIncluded the following information in your mid-project review:\n\nOptions 1 and 3:\n\n\nYour hypotheses and predictions (point form or short paragraph). If these differ from the ones in your proposal, explain clearly the rationale for the change.\nA detailed description of your data (a paragraph), including how the data was collected or simulated, along with any manipulation(s) you performed to get your data ready for the analysis.\nYour analysis plan (a paragraph): describe the statistical test(s) that you will use to test each prediction, including how you will validate the assumptions of each test.\n\n\nOption 2:\n\n\nA detailed description of the question you want to answer, any previous work (modeling and otherwise), the model you have built to answer this question, and your modeling assumptions.\nDetailed descriptions of the model analysis and biological interpretations of the results so far.\nYour analysis plan (a paragraph): describe additional analysis that you will do and any assumptions you would like to relax.\n\n\n\n23.5.3 Presentation\nDue Dec 5th, worth 10% of final grade\nThe presentations will be held on the last day of class during regular class hours (Dec 5th, 2-4 pm). Each presentation will be 10 minutes long, followed by 2 minutes of questions from the audience. If you cannot make it to class for your presentation, please get in touch with us to make alternative arrangements no later than Dec 1st.\n\n\n23.5.4 Report\nDue Dec 8th, worth 20% of final grade\nThis report will be styled as a journal article, with these sections:\n\nAbstract\nIntroduction\nMethods (including “Data Description” and “Data Analysis” subsections)\nResults\nDiscussion\nReferences\nSupplementary material consisting of data and code required to reproduce analysis\n\nFor your sake (and ours), we are enforcing a two page limit (single spaced, excluding figures, tables, code, references, and appendices). Please use a standard font, size 12, with regular margins. One goal of this assignment is to write clearly and concisely – it is often clarifying to put your analyses in as few words as possible.\nFor the report, you are expected to:\n\nPut your research questions in the context of existing research and literature.\nHave clear and explicit objectives, hypotheses, and/or predictions.\nAdequately describe and properly cite the data source(s) you will analyze. If your project involves modeling, describe other modeling work that is relevant.\nDescribe your analysis in sufficient detail for others to understand.\nDiscuss the interpretation of your results and their implications.\n\nThe data and code associated with your report is expected to be entirely reproducible. Your supplementary files must include the following:\n\nA description of what every column/row in your submitted data file.\nA well-annotated R script or R notebook file. We must be able to run your code once you submit the project. This lesson on best practices for writing R code is a good starting place. Also check out this coding style guide and these simple rules on how to write code that is easy to read.\n\nHermann et al. 2016 is a great example of what we expect your code to look like. Refer to their supplementary materials for examples of how to describe your data set and how to annotate your code."
  },
  {
    "objectID": "projects.html#project-grading-rubric",
    "href": "projects.html#project-grading-rubric",
    "title": "23  Project description",
    "section": "23.6 Project grading rubric",
    "text": "23.6 Project grading rubric\n\n23.6.1 Project proposal\n4 marks total\nOption 1: Two marks each for 1) your hypotheses and associated predictions and 2) a description of your data source(s). Students are expected to demonstrate effort in formulating hypotheses and predictions, and identifying a suitable dataset.\nOption 2: Two marks each for 1) a clear description of the question or problem in ecology or evolution you would like to address using a model, and 2) a description of the kind of model you envision using, including what variables to track.\nOption 3: One mark for simulating realistic data using appropriate tools, and one mark for your hypotheses and associated predictions, and two marks for describing the appropriate analyses.\nThese components will be graded mostly on completion. The purpose of this assignment is to ensure you start early and are heading towards the right track.\n\n\n23.6.2 Mid-project update\n6 marks total\nOptions 1 and 3: Two marks are given to clearly stating hypotheses and predictions. In the case that these are different from the original submission in the proposal, the rationale for refinement needs to be clearly explained.\nEach of the following criteria are scored out of 2: 2 == excellent, 1.5 == good, 1 == acceptable, but needs improvement.\n\nData description\n\nThe data source(s) are sufficiently described, specifically, where was the obtained and how it was originally collected.\nThe data is sufficient described, including any initial observations from your exploratory data analyses.\nThe suitability of the data is justified.\nAny manipulations done to the data are thoroughly explained and well-justified.\n\nData analysis plan\n\nClearly lay out the statistical test(s) you will use to test each prediction.\nState how you will validating assumptions associated with each statistical test.\n\n\nOption 2: Each of the following criteria are scored out of 3: 3 == excellent, 2 == good, 1 == acceptable, but needs improvement.\n\nDescription of question, previous work, the model, modeling assumptions, and any predictions you have ahead of the analysis\n\nThe question you want to address and previous work in that direction (modeling or otherwise) is described in detail.\nThe relationship between the question/problem and modeling approach is clear and well-justified.\nModeling assumptions and choices (including limitations) are clear and well-motivated.\nPredictions for how the model will behave, what it might have to say about the question/problem, etc. are inclued and well thought out.\n\nAnalysis and analysis plan\n\nThe details of all analysis (mathematical or computational) are explained clearly.\nThe biological interpretations of results so far are clearly presented and their validity/applicability is discussed.\nClearly lay out plans for remaining analysis (e.g., relaxing model assumptions) and justify why they are reasonable.\n\n\n\n\n23.6.3 The presentation\n10 marks total\nEach of the following criteria are scored out of 3: 3 == excellent, 2 == adequate, 1 == needs improvement.\n\nContent – background and methods\n\nThe context for the study, along with hypotheses and predictions, are clearly set up.\nData source(s), manipulations, and statistical tests used are succinctly and adequately described.\nIf modeling, the relationship between the question/problem addressed and modeling approach is well-explained, and previous work (modeling or otherwise) is discussed.\n\nContent – results and conclusions\n\nResults are accurately described and interpreted, with particular attention to how they related to the hypotheses and predictions the group set out to test.\nThe conclusion to the study is succinct and clear.\n\nDelivery\n\nAll students participated in presenting the information.\nAll students spoke clearly and without jargon.\nThe presentation is well organized and ideas flowed naturally from one to the next.\nThe presentation is well rehearsed and is an appropriate length.\nFigures are easy to read (e.g., axis labels are big enough to read and are informative) and are explained thoroughly (e.g., x and y axis and what each data point is).\n\n\nThe final 1 mark will be assigned to the question period, and students will be assessed on whether they are able to answer questions thoughtfully.\n\n\n23.6.4 The report\n20 marks total\nEach of the following criteria are scored out of 4: 4 == excellent, 3 == good, 2 == acceptable, 1 == needs improvement.\n\nContent and concepts\n\nAuthors demonstrate a full understanding of the existing literature on the topic, and these concepts are critically integrated into their own insights.\nOptions 1 and 3: Hypotheses and predictions are clearly defined, and rational for choosing/simulating this data is justified.\nOption 2: The question, modeling approach, and relevant work are thoughtfully explained; the rationale for using the model (and its assumptions) is justified.\n\nCommunication\n\nWriting is succinct, clear, logical, and free of grammatical and spelling errors.\n\nAnalysis: see below.\nResults\n\nResults are accurately and sufficiently described.\nConclusions are supported by evidence.\nFigures and tables are clearly presented and are informative.\n\nCoding style and reproducibility\n\nData and code are well-organized and well-documented.\nThe analysis is easily reproducible.\nAll team members have pushed to a common GitHub repo via the command line.\n\n\nNote: marks for the 3rd criterion (Analysis) depend on if groups did a modeling or data-driven project:\nOptions 1 and 3: Statistical analysis\n\nStatistical tests chosen or modeling choices made are appropriate.\nAssumptions for each statistical test is validated.\nLimitations in the data and analysis are discussed.\n\nOption 2: Analysis of model\n\nCharacterization of the model is appropriate and explained in detail.\nImportantly, biological conclusions explained in detail and in terms of the processes described (or not described) by the model.\nLimitations of modeling assumptions are discussed, and extensions are proposed.\n\nPlease note that we are only going to be marking the two pages of your report. Please do not go over the page limit (with the exception of tables, figures, references, and appendices)."
  },
  {
    "objectID": "projects.html#tips-on-writingpresenting-a-research-project",
    "href": "projects.html#tips-on-writingpresenting-a-research-project",
    "title": "23  Project description",
    "section": "23.7 Tips on writing/presenting a research project",
    "text": "23.7 Tips on writing/presenting a research project\nWe know that students have very unique research interests and ideas, and we hope that your project encapsulates that! As instructors, we do not know everything, but we are excited to learn from you and your projects. Below are some tips that we have gathered that you may find helpful when preparing for the project presentation and writing your report.\n\nUse a title that summarizes your project/results clearly.\nDefine everything! Do not assume that we know about your question, study system, etc. For your presentations, adding some pictures will help when you are defining something.\nAfter introducing your study system, tell us clearly your hypothesis and prediction: “I hypothesize that there are more mosquitoes in the boreal forest because it is warmer. I predict this because insects have a thermal tolerance”. Then, after your methods, results, etc., remind us of your hypothesis again! For your presentation, you can even show the same slide you used for your hypothesis with a big red X or a big green checkmark. Assume we forgot and that we know nothing about the system.\nNEVER EVER USE THE WORD “prove”. Science cannot prove or disprove anything — the evidence can only support (or fail to support) how we think the world works.\nUse an appropriate font and font size. Also, use colours wisely (e.g., avoid red and blue together because of folks that are colourblind).\nA 10-minute presentation is about 10 slides (more or less depending on if you use animations). A note about animations: use “Appear”, not any of the fancy stuff. And no slide transitions!\nWe will ask questions after your presentation, but we are not trying to trick you — we just want more information. Give us your best answer, and remember that it’s okay to say “I don’t know, but I think that…” or “I can test this further by doing this”. At this point, you should know more about your projects than we do. Also, when preparing for the presentation, it useful to think about what questions listeners may have and try to answer them preemptively.\nPractice your presentation at least once with your group! It’ll get rid of any nerves you have if you already know the words you are going to say. It’ll also help you ensure that you speak louder and slower. We know you all will do great projects, and we are excited to hear about them!\n\nReading widely and often is one of the best ways to learn how to write well. Here are some papers which we think are clear, concise, and free of grammatical and logical flaws.\n\nViral zoonotic risk is homogenous among taxonomic orders of mammalian and avian reservoir hosts\nNonsystemic fungal endophytes increase survival but reduce tolerance to simulated herbivory in subarctic Festuca rubra\nEstimation of the strength of mate preference from mated pairs observed in the wild\nHumans introduce viable seeds to the Arctic on footwear\nEffects of environmental warming during early life history on libellulid odonates\nThe role of evolution in the emergence of infectious diseases\nCoevolution of parasite virulence and host mating strategies\nA rigorous measure of genome-wide genetic shuffling that takes into account crossover positions and Mendel’s second law\nThe role of divergent ecological adaptation during allopatric speciation in vertebrates"
  },
  {
    "objectID": "databases.html",
    "href": "databases.html",
    "title": "24  Some open-access databases",
    "section": "",
    "text": "Below are some resources that would be a good to look at if you are in search of data for the term project, or in search of a question in ecology or evolution:\n\nGenBank: annotated collection of all publicly available DNA/protein sequences. It is possible to download sequences manually, but command line tools can help to automate the process.\nPanTHERIA: database of ecology, life history, and geography of all extant and recently extinct mammal species. Includes body size, lifespan, litter size, and other trait data at the species level.\nGene Expression Omnibus: repository of gene expression, methylation, and annotated genomic data which are (like GenBank) most readily accessible using command line tools.\nContinuous Plankton Recorder Survey: data (going back to 1958!) on northern hemisphere plankton species, including the location (latitude, longitude) and date of sampling.\nRed-backed salamander abundance: abundance of red-backed salamanders from 4 sites in the Bruce Peninsula from 2004 to 2017.\nNorth American Bird Breeding Survey: repository containing information regarding the number of birds at multiple sites in North America. Many datasets of varying size that need to be linked together.\nMalaria Atlas Project: publicly available and up-to-date malaria prevalence and distribution data. Vector distribution, bednet coverage, etc. data also available.\n\nLet Vicki or Mete know if data from any of these resources interests you, or if you would like to discuss where data to answer questions you find interesting live (…or if they exist at all!)."
  },
  {
    "objectID": "tests-assignment-3.html#a-quick-review",
    "href": "tests-assignment-3.html#a-quick-review",
    "title": "25  More on hypothesis testing",
    "section": "25.1 A quick review",
    "text": "25.1 A quick review\nTo conduct a hypothesis test at significance level \\(\\alpha\\), one does the following:\n\nState the null and alternative hypothesis (\\(H_0\\) and \\(H_1\\), respectively) and significance level \\(\\alpha\\). The significance level is the probability of a false positive (rejecting the null hypothesis) given the null hypothesis holds. \\(\\alpha = 0.05\\) is the most common choice of significance level.\nCollect data \\(X_1,\\dots,X_n\\), possibly with knowledge of the sample size \\(n\\) required to achieve a certain power. Power is the probability of a true positive, given the null hypothesis is false.\nCalculate the realized value \\(s\\) of a test statistic \\(S\\) from the data. The test statistic must have a known distribution under the null hypothesis.\nCompute the probability of observing the realized value of the test statistic or something more extreme, given the null hypothesis is true, i.e., \\(p = \\Pr(S &gt; s | H_0)\\). This probability is called a \\(p\\) value. If \\(p &lt; \\alpha\\), we reject the null hypothesis at significance level \\(\\alpha\\) and, if not, we fail to reject \\(H_0\\).\n\nIn the intro to inference lecture, we applied the likelihood ratio test to simulated data. We also discussed \\(t\\) tests (a special case of the LRT). In Assignment 3, you were asked to preform a \\(t\\)-test to examine whether beaver’s body temperature differ by activity level. These are not the only tests you will see in ecology and evolution papers. In these notes, we describe two tests you may see in the literature."
  },
  {
    "objectID": "tests-assignment-3.html#the-chi-square-test",
    "href": "tests-assignment-3.html#the-chi-square-test",
    "title": "25  More on hypothesis testing",
    "section": "25.2 The chi square test",
    "text": "25.2 The chi square test\nSuppose we count the number of individuals in a population that carry an allele at a specific locus. The number of individuals with this allele could be modeled as a realization of Binomial distribution with \\(n\\) trials and success probability \\(p\\), i.e., \\(Y \\sim \\text{Binomial}(n,p)\\). (Binomial random variables count the number of successes in some number of trials; in the context of the example above, if any individual carries the allele of interest, we count that as a success.) A \\(\\chi^2\\) test determines which of the following hypothesis is consistent with the data: \\(H_0 \\colon p = p_0\\) vs \\(H_1 \\colon p \\neq p_0\\). In words, the null hypothesis is that the probability of carrying the allele of interest is \\(p_0\\) and the alternative hypothesis is that is not \\(p_0\\).1 To conduct a test of these hypotheses, one must calculate the value of the test statistic\n\\[S = \\frac{(Y-E(Y))^2}{\\text{Var}(Y)} = \\frac{(Y-np_0)^2}{np_0(1-p_0)}\\] \nusing the observed number of individuals that carry the allele of interest. Under the null hypothesis, \\(S\\) has an approximate \\(\\chi^2\\) distribution one degree of freedom. Simplifying this expression, one can write the test statistic in another way that is commonly used:\n\\[S = \\frac{(Y-np_0)^2}{np_0} + \\frac{((n-Y)-n(1-p_0))^2}{n(1-p_0)},\\] \nwhere the numerator in the first term corresponds to the \\(\\#\\) of individuals carrying the allele (\\(=Y\\)) minus the number of individuals that are expected to carry the allele under the null hypothesis (\\(=np_0)\\). The denominator is the number expected to carry the allele. Similarly, in the second term, we have the number of individuals sampled that do not carry the allele (\\(=n-Y\\)) minus the number of individuals that are expected to not carry the allele (\\(n(1-p_0)\\)).\nIf the probability of observing a test statistic more extreme than\n\\[s = \\frac{(y-np_0)^2}{np_0} + \\frac{((n-y)-n(1-p_0))^2}{n(1-p_0)},\\]\nis \\(&lt; \\alpha\\), then we reject the null hypothesis. (The reason being that we are unlikely to live in a world where the null hypothesis is true if the test statistic does not line up with what is expected under the null hypothesis.) Tables of \\(\\chi^2\\) cutoff values are sometimes used to determine the \\(p\\)-value (i.e., the probability of observing something more extreme than the test statistic that was calculated), but is more convenient to do the calculation in R.\nIn the following code chunk, we test \\(H_0 \\colon p = 0.4\\) vs \\(H_1 \\colon p \\neq 0.4\\) by calculating the test statistic and \\(p\\) value. Data is simulated.\n\nn &lt;- 100 # this is the population (and sample) size\nY &lt;- rbinom(1, size = n, prob = 0.45) # prob is the true value of p in this case!\nY # this is the number individuals in the pop with the allele\n\n[1] 47\n\nexpected_carrying_allele &lt;- 100*0.4 \n# number individuals expected to carry the allele under H0: p = 0.4\n\nexpected_NOT_carrying_allele &lt;- 100*(1-0.4)\n# number individuals expected to NOT carry the allele under H0: p = 0.4\n\nS &lt;- (Y-expected_carrying_allele)^2/expected_carrying_allele +\n  ((n-Y)-expected_NOT_carrying_allele)^2/expected_NOT_carrying_allele\n\npchisq(S, df = 1, lower.tail = F) # since p &gt; 0.05, we fail to reject the null hypothesis\n\n[1] 0.1530419\n\n\nPlay around with this code to see when, depending on \\(p_0\\), the true value of \\(p\\) used to simulate the data, and the sample size, the \\(p\\)-value is \\(&lt; \\alpha = 0.05\\)."
  },
  {
    "objectID": "tests-assignment-3.html#permutation-tests",
    "href": "tests-assignment-3.html#permutation-tests",
    "title": "25  More on hypothesis testing",
    "section": "25.3 Permutation tests",
    "text": "25.3 Permutation tests\nMany ecological and evolutionary questions ask: is the observed pattern different than what we would expect by random chance? Permutation tests (sometimes called randomization tests) allow us to test whether the observed data are different from a random distribution generated by reordering our observed data. If the pattern is random, then it should be just as likely as any other pattern generated by reordering the data. If it is not random, then it should occur more or less frequently than we expect under this distribution. The key step of a permutation test is to generate a null distribution by shuffling the data. In the following example of a permutation test, we shuffle observations between groups to test if the group means are different.\nThe steps for a permutation test of the difference between group means is as follows:\n\nCalculate the value of a test statistic (e.g., the difference in means, \\(t\\) statistic) given data \\(X_1,\\dots,X_n\\) for the first group and \\(Y_1,\\dots,Y_n\\) for the second group.\nRandomly reshuffle observations among the treatment groups, each time calculating a test statistic.\nRepeat step (2) multiple times, generating a distribution of test-statistics.\nCalculate the proportion of times the actual test statistic is outside the distribution of test-statistics.\n\nWe will perform the above steps on simulated data. Imagine we venture to South America and collect 30 male and 30 female Hercules beetles. We brought the beetles back to the lab and measured the width of their bodies at the largest point. The question we are interested in is: Do male and female Hercules beetles differ in body width? We will simulate body width data that is normally distributed. Our test statistic will be the difference in group means.\n\ndf_males &lt;- data.frame(width = rnorm(30, mean=17, sd=3), sex = \"male\")\ndf_females &lt;- data.frame(width = rnorm(n=30, mean=16, sd=2), sex = \"female\")\n\ndf_body_widths &lt;- rbind(df_males, df_females) # combine data!\nhead(df_body_widths)\n\n     width  sex\n1 15.33681 male\n2 15.58366 male\n3 19.41999 male\n4 16.87948 male\n5 17.83409 male\n6 14.62976 male\n\ndf_body_widths %&gt;% ggplot(aes(x = as.numeric(width), \n                              fill = as.factor(sex))) + geom_histogram() +\n  labs(x = \"width\", y = \"count\", fill = \"sex\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nmean_males &lt;- mean(df_males$width)\nmean_females &lt;- mean(df_females$width)\ndiff_means_obs &lt;- mean_males - mean_females # test statistic w/out permutation\ndiff_means_obs\n\n[1] -0.4707169\n\nreshuffled &lt;- df_body_widths\nreshuffled$width &lt;- sample(reshuffled$width, size = nrow(reshuffled), replace = F)\nhead(reshuffled)\n\n     width  sex\n1 15.61102 male\n2 12.82959 male\n3 13.54969 male\n4 18.27156 male\n5 18.24766 male\n6 11.25838 male\n\npermute_and_calculate_mean_diff &lt;- function(){\n  reshuffled &lt;- df_body_widths\n  reshuffled$width &lt;- sample(reshuffled$width, size = nrow(reshuffled), replace = F)\n  \n  mean_males_permuted &lt;- mean(reshuffled %&gt;% filter(sex == \"male\") %&gt;% pull(width))\n  mean_females_permuted &lt;- mean(reshuffled %&gt;% filter(sex == \"female\") %&gt;% pull(width))\n  \n  mean_diff_permuted &lt;- mean_males_permuted - mean_females_permuted\n  # test statistic after permutation\n  \n  return(mean_diff_permuted)\n}\n\npermute_and_calculate_mean_diff()\n\n[1] -0.763379\n\nn_sims &lt;- 5000 # number of times to permute data to generated null distribution\ntest_stats &lt;- c()\n\nfor (i in 1:n_sims){\n  test_stats[i] &lt;- permute_and_calculate_mean_diff()\n}\n\nggplot() + geom_histogram(aes(x = test_stats), fill = \"gray\") +\n  geom_vline(xintercept = diff_means_obs, color = \"red\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n## red line corresponds to value of test statistic from the un-permuted data\n\nFinally, to get our \\(p\\)-value, we calculate the number of times the simulated mean difference exceeded the observed mean difference from our data. Because we are performing a two-tailed test, this amounts to determining the number of times the simulated mean difference is either greater or lesser than the observed difference. We can do this by asking how many times the absolute value of the simulated mean difference is greater or equal to the absolute value of the observed mean difference.\n\nabs_simulated_means &lt;- abs(test_stats)\nabs_diff_means_obs &lt;- abs(diff_means_obs)\nexceed_count &lt;- length(abs_simulated_means[abs_simulated_means &gt;= \n                                               abs_diff_means_obs])\np_val &lt;- exceed_count/n_sims\np_val\n\n[1] 0.513"
  },
  {
    "objectID": "tests-assignment-3.html#footnotes",
    "href": "tests-assignment-3.html#footnotes",
    "title": "25  More on hypothesis testing",
    "section": "",
    "text": "In population genetics, so-called Hardy-Weinberg equilibrium is the null hypothesis we wish to test. This is a model of evolution in which there is no mutation, selection, migration, recombination — really, there is nothing interesting happening. In this case, we have an expectation for the frequency of any allele at a given locus. Given count data, the \\(\\chi^2\\) test allows us to determine if the Hardy-Weinberg equilibrium is a reasonable description of the population and its evolution.↩︎"
  },
  {
    "objectID": "power-analysis-practice.html#exercise",
    "href": "power-analysis-practice.html#exercise",
    "title": "26  Power analysis practice",
    "section": "26.1 Exercise",
    "text": "26.1 Exercise\nWrite a function to estimate, for a given sample size, the power at level \\(\\alpha = 0.06\\). That is, from a large number of simulations (generated using the data_generator() function), determine the fraction of simulations for which fitting a linear model (of mutation rate on genome size) the coefficient associated to genome size has a \\(p\\) value which is \\(&lt; \\alpha\\).\nLooping over many sample sizes, use the function you have written to determine the minimum sample size needed to detect a significant effect &gt;99% of the time."
  }
]